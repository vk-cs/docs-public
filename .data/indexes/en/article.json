[{"uuid":"6b71ccab-d440-4758-aa1c-fc6023f198f0","title":"Terminology","data":"\n\nMore terms can be found in the corresponding section:\n\nCloud Servers.\nCloud Networks.\nCloud Containers.\n\nInstances\n\nA virtual machine (VM) is a collection of resources that emulate the behavior of a real computer. For its operation, the virtual machine uses the resources of a physical server (host). Multiple VMs can be hosted on the same host.\n\nFlavors\n\nOn the VK Cloud platform, the number of processors and the amount of RAM of the VM are set using flavors. Ready-made flavors are available to users, as well as individual (on request).\n\nVolumes\n\nA disk on the VK Cloud platform is a network block storage device that connects to a VM. The amount of stored data and the speed of access to it depends on the size and type of disk.\n\nThe VK Cloud disk subsystem uses network drives. Disk fault tolerance, continuous access to data and their safety are ensured by replication.\n\nNetwork drives can be partitioned and formatted in the same way as regular locally connected drives. The advantage of network drives: they can be “moved” between VMs located in the same data center.\n\nMultiple disks can be attached to a VM, but at least one of them must be bootable and contain the installed operating system.\n\nAvailability zone\n\nAn availability zone is one or more data centers in which components of the cloud infrastructure can be placed. In VK Cloud, the availability zone corresponds to a separate Tier III data center.\n\nEach zone is isolated from failures in other availability zones. Placing virtual resources in multiple zones provides fault tolerance and reduces the likelihood of data loss.\n\nAll availability zones in the Moscow region are connected into a fully redundant fiber-optic network, which provides high speed and low latency when transferring data between availability zones.\n\nImage\n\nThe configuration and data of the virtual machine are stored as one or more files. These files can be used as an image to run a VM in a local environment or to create a new VM.\n\nOn the VK Cloud platform, you can create an image based on a VM disk or upload an image file. The resulting image can be used to create a new virtual machine.\n\nLoad balancer\n\nThe load balancer allows you to distribute incoming traffic between multiple virtual backend servers and thereby ensure high availability for the services provided by those servers. If one or more servers fail, traffic will be redirected to the remaining servers. Also the separate type of load balancer is used to power other VK Cloud platform services.\n\nThe VK Cloud platform load balancer is based on OpenStack Octavia, which has HAProxy at its core.\n\nSecurity group\n\nSecurity group is a sets of configurable permissive rules that regulate network access rights (sign in via a specific protocol, through a specific port) for certain IP addresses or other security groups.\n","url":"/en/intro/start/concepts/help-glossary","description":"Terms specific to cloud infrastructure.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"159ca4db-a57f-4f84-ad84-f96f4504388e","title":"Platform architecture","data":"Virtualization, monitoring, container, and data storage services can be deployed in VK Cloud. The architecture is based on four groups of services:\n\nIaaS services are built on the basis of OpenStack and components of its own design. Manage dynamic resource allocation, scaling, and fault tolerance. Hosted in the same cloud data center with PaaS services.\n\n    List of IaaS services\n\n  Cloud Servers (IaaS Compute) — provides virtual machines with the ability to allocate public IP and unlimited traffic of 1 GBit/s (also available virtual machines with NVIDIA GPU). Responsible for virtualization and resource integration, uses the KVM hypervisor.\n  Cloud Storage (IaaS Storage) — provides S3-compatible object storage. The workload on the storage is distributed among all storage nodes. You can deploy a storage with a custom architecture.\n  Cloud Networks (IaaS Network) — provides network interaction within the selected project using SDN (Software Defined Network) technology. It operates on the basis of OpenStack Neutron and on the basis of proprietary software — Sprut. Includes components:\n\n    DNS — supports public and private DNS, which provides name resolution for VK Cloud platform services.\n    CDN — organizes the transfer of content from your servers to users with minimal delay.\n    Load Balancer — distributes the load on the infrastructure, providing fault tolerance and flexible scaling of applications.\n    AntiDDoS — filters traffic coming to VK Cloud resources deployed in the project to block DDoS attacks.\n    WAF (Web Application Firewall) — configures incoming and outgoing traffic filtering rules to detect and block network attacks.\n\n\nPaaS services include open source solutions (Kubernetes) and solutions from technology partners. Built-in monitoring of VK Cloud services and individual entities is supported.\n\n    List of PaaS services\n\n  Cloud Containers — allows you to create and manage Kubernetes clusters in which you can run services and applications.\n  Cloud Databases — provides scalable DBMS: MySQL, PostgreSQL, Postgres Pro, ClickHouse, MongoDB, Redis, Tarantool, OpenSearch, Arenadata DB based on Greenplum.\n  Cloud Big Data — used for big data analysis based on Arenadata Hadoop, supports scaling.\n  Cloud Streams — provides Arenadata Streaming-based clusters for processing streaming data.\n  Cloud ML Platform — supports services for a full ML development cycle.\n  Cloud Voice — provides a REST API for speech recognition and synthesis based on machine learning.\n  Vision — provides a REST API for face and object recognition based on machine learning.\n  Cloud Alerting —  configures notifications about changes in key metrics of VK Cloud services.\n  Cloud Logging — aggregates and analyzes service logs in VK Cloud.\n  Monitoring — provides monitoring of metrics specific to PaaS services, for example, analytics on K8s container feeds, PostgreSQL DBMS transaction statistics.\n  Additional services:\n\n    1C:Ready workplace — provides resources and software for the deployment of 1C services: Accounting, Salary and Personnel Management, Management of our company.\n    Marketplace — allows you to quickly deploy web development and administration environments based on virtual machines.\n\n\nCommon&Security-services ensure the safe operation of users and support a role model when using VK Cloud resources.\n\n    List of Common&Security Services\n\n  Billing — keeps records of resource usage and expense control, generates financial reports, provides interaction with payment systems when paying for services.\n  Audit — generates an audit log of user actions in VK Cloud.\n  IAM — manages authentication and authorization of users and services in conjunction with Keystone.\n  Cloud Monitoring — provides monitoring of cloud services and user applications.\n  Keystone — provides API client authentication, service discovery, and distributed multi-tenant authorization.\n\n\nVK Cloud provides technical support for its services. Partner solutions (for example, AntiDDoS, WAF) are supported jointly with a partner.\n\nThe general scheme of VK Cloud components and services is shown below.\n\n\n\nCloud services are managed using the API. Users interact with the API through one or more tools:\n\nVK Cloud personal account;\nOpenStack CLI;\nTerraform with a separate provider.\n\nFault tolerance\n\nPhysical fault tolerance is implemented by placing VK Cloud in two Tier III reliability level data centers in the Russian Federation. With this placement, it is guaranteed to maintain the SLA of data centers of more than 98% with a total SLA of 99.95%.\n\nNetwork fault tolerance is provided by communication of data centers with routes with a bandwidth of 200 GB/sec. Communication of each server with backbone providers goes through two independent channels, through two routers.\n\nLogical fault tolerance is implemented using availability zones, which are combined into regions.\n\n\n\nThe status of the availability zones, services and infrastructure of VK Cloud can be checked on the services page status page.\n\nAvailability zones\n\nEach availability zone has one or more data processing centers (data centers) where cloud infrastructure objects are physically located. In VK Cloud, the availability zone corresponds to a separate Tier III data center. VK Cloud provides availability zones:\n\nGZ1: includes a data center Goznak;\n\n  The address of the data center: Moscow, Mira avenue, 105, building 6.\n\nMS1: includes a data center DataLine NORD4;\n\n  The address of the data center: Moscow, Korovinskoe highway, 41.\n\nME1: includes a data center «Medvedkovo» Rostelecom-DPC;\n\n  The address of the data center: Moscow, Chermyanskaya st., 4.\n\nQAZ: includes a data center QazCloud.\n\n  The address of the data center: Republic of Kazakhstan, Akmola region, Kosshy, Republic str. 1.\n\nEach data center is equipped with independent power supply and cooling systems.\n\nAvailability zones within a region are connected using a redundant dedicated high-bandwidth and low-latency fiber network for high data transfer rates between zones.\n\nThe infrastructure of availability zones MS1 and GZ1 of the Moscow VK Cloud region is protected in accordance with the Federal Law of the Russian Federation “On Personal Data” No. 152-FZ.\n\nSecurity\n\nVK Cloud security is ensured by constant monitoring of VK Cloud services, conducting external audits. VK Cloud participates in the security audit program HackerOne. Infrastructure certified FSTEC of Russia for working with personal data.\n\nFor more information, see the section Platform security.\n\nMigration to VK Cloud\n\nVK Cloud supports migration using third-party software or platform tools. The transfer of virtual resources is cheaper due to the implementation of services based on OSS (Open Source Solutions).\n","url":"/en/intro/start/concepts/architecture","description":"Description of VK Cloud architecture, security and fault tolerance of the platform, migration to VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"06da76c0-a256-4e4d-a3a3-ef75a2b20dac","title":"Registration in VK Cloud","data":"This guide will help you get started with VK Cloud:\n\ncreate an account and sign in to your personal account;\nconfirm your VK Cloud account;\nconfigure the project that is created during registration;\nconnect to the project via the OpenStack CLI command line interface.\n\n\n\nTo join an existing VK Cloud project, follow the instructions in article Join a project by invitation.\n\n1. Create an account\n\nGo to VK Cloud main page and click Create a VK Cloud Account in the upper right corner.\nIn the window that appears, fill in the fields:\n\n    Work email: your email address.\n\n      For some corporate accounts, special registration conditions are available. Therefore, specifying a work email is recommended.\n\n      The maximum length of email address is 320 characters.\n\n\n      You cannot use the + character in the address.\n\n\n    Password: a new password.\n\n      The minimum password length is 8 characters. The password must contain:\n\n      uppercase and lowercase Latin letters;\n      at least one number or one special character ?, !, ~, @, #, $, %, ^, &, _, -, +, *, =, ;, :, ,, ., /, \\, |, `  `, [, ], {, }, (, )`.\n\n      The password must not contain repeating groups of characters, such as `aaa123123123`.\n\nClick Create an account.\n\nThe account confirmation window opens. A confirmation email is sent to the specified email address.\n\n2. Confirm your account\n\nIf you do not immediately confirm the account, when you try to perform an action (for example, create an object or activate services), the confirmation will continue from the step where it was stopped.\n\nConfirm your email: wait for an email from the address noreply@cloud.vk.com and follow the link in the email. If the email has not arrived or has expired, request it again by clicking Send confirmation again in the confirmation window.\nConfirm your phone:\n\n\n   For the region Moscow, only phone numbers registered in the Russian Federation (starting with +7) are accepted. One number can only be assigned to one account.\n\n\n   Enter your number in the Phone number field and click Request a confirmation code.\n   Enter the SMS code into the provided field and click Confirm number.\n\n      If the SMS doesn't arrive or has expired, request it again by clicking The code didn't arrive? button which appears in 60 seconds.\n\nIf additional account verification is requested, send a request to technical support:\n\n   Why does this check appear?\n\n   VK Cloud platform automatically validates the security status of the account. Depending on the results of the check, one of these options is offered:\n\n   Link a card right away — the Payment card tab opens.\n   Send a request to technical support — the Account activation tab opens. Linking a card becomes available after the request is processed by technical support.\n\n\n   Copy the identifier (PID) of your project from the VK Cloud site URL.\n\n      An example for the region Moscow: https://msk.cloud.vk.com/app/mcs123456789/main, where mcs123456789 is the project identifier (PID).\n\n   Click Contact support.\n   On the technical support portal page that opens click the Новая заявка button.\n   Sequentially select a category from the lists:\n\n      Тип услуги: VK Cloud.\n      Категория услуги: VK Cloud: Учетная запись.\n      Группа услуги: Учетная запись: Активация и доступ.\n      Услуга: VK Cloud Активация и доступ: Активировать учетную запись.\n\n   In the form that opens, fill in the fields:\n\n      Тема: specify a topic like Project Activation .\n      Регион: select the project region (Moscow or Kazakhstan).\n      Проект: specify the project ID (PID).\n      Описание: write the reason for the request — you need to check your account.\n\n   Attach additional materials according to the instructions on the page.\n   Click the Отправить button.\n   Wait for your request to be approved.\n\nLink a card of supported payment system. The specified amount will be debited from your card and credited to your project balance.\n\n\n   You cannot use a virtual card for account confirmation.\n\n   One card can be linked to any number of projects within one account.\n\n\n(Optional) On the Autopayment tab that opens, configure the autopayment settings.\nClick Get started with VK Cloud.\nWait for your personal account page to open. A message about successful services activation appears.\n\nA project is automatically created for your account. After the services activation, sign-up bonuses are credited to your balance. These bonuses are credited only once after the registration of each new account. If no bonus points are credited within 3 working days, contact technical support.\n\n3. Activate API access\n\n\n\nVK Cloud supports working with the platform using additional tools: REST API, OpenStack CLI, Terraform and others — in order to ensure security, access to them is activated by the user independently.\n\n\n\nEnable 2FA for account.\nActivate API access.\n\n4. (Optional) Invite other users\n\nYou are the owner of the project account created during registration. To add other users to the project:\n\nInvite users to your project and assign them with roles.\nTurn on mandatory 2FA in the project to reduce the risk of unauthorized access.\n\n5. (Optional) Connect to the project via OpenStack CLI\n\nInstall OpenStack сlient and authenticate yourself in the project.\nTo check the connection, open a console and run a command, for example:\n\n        openstack project list\n\n    Upon successful connection, a list of created projects will be displayed.\n\nWhat's next?\n\nCustomize the project to make payments through the legal entity.\nRead information about the quotas and limits in VK Cloud projects.\nLearn about the payment methods available and the key pricing model principles.\nFind out how to create and connect a virtual machine, a Kubernetes cluster, a database.\n","url":"/en/intro/start/account-registration","description":"Creating an account in the platform's personal account, activating API access.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"fcaa4917-b306-492e-a39f-d208689fdfa5","title":"Terms of support","data":"The VK Cloud team provides technical support to clients using VK Cloud services.\n\nMethods of treatment\n\nYou can apply to technical support through the following available communication channels:\n\n| Channel | Information                 | Working hours    |\n|---------|-----------------------------|------------------|\n| Portal  | https://support.mcs.mail.ru | around the clock |\n| Telegram bot | @vk_tech_support_bot | around the clock |\n| Mail    | support@mcs.mail.ru         | around the clock |\n| Phone   | +7 (499) 350-97-03          | around the clock |\n\n\n\nTo speed up the processing of the request, inform the technical support of the account details: login, project name and describe the issue in as much detail as possible.\n\nIf possible, attach any available diagnostic information: screenshots, logs and other materials.\n\n\n\nSLA indicators are given in the article SLA.\n\nStages of service readiness\n\n[cols=\"1,3\", options=\"header\"]\n|===\n| Stage\n| Description\n\n| Beta\n| Preliminary service versions at the testing stage. They are suitable for developers who are ready to use the services, the stability of which is not guaranteed: failures or incorrect operation of some functions are possible.\n\nThe use of service beta versions is not charged. If the beta version of the service during the operation uses any charged VK Cloud service, it is paid according to the price list.\n\nTechnical support for beta versions is limited: providing responses and correcting even critical errors are not guaranteed. Also, the Service Level Agreement (SLA) is not applied, and the response time is unlimited.Documentation for beta versions may be incomplete or absent\n\n| General Availability\n| Stable versions of services that are available to all users for purchase. They can be fully used to solve problems.\n\nThese services are subject to billing and the technical support is fully provided according to the Service Level Agreement (SLA)\n\n|===\n\nSupport competencies\n\nTechnical support:\n\nProvides architectural advice regarding the use of VK Cloud services.\nPerforms diagnostics of malfunctions in the operation of resources on the project.\nDeals with issues of restoring access to account.\nInforms about the work of billing.\nAdvises on troubleshooting issues in the platform.\nHelps in setting up VK Cloud services.\n\n\n\nThe VK Cloud Technical Support team does not administer and support operating systems and other services created using the Platform, unless this is agreed in a separate agreement.\n\nNotifications from technical support\n\nThe support team informs clients about failures that have occurred with the service or resource in the client's account, and also provides recommendations for eliminating potential problems in the client's project.\n\nNotifications also include informing project owners about security threats or other cases that may lead to data loss or interruptions in access to VK Cloud services.\n\nIndividual support\n\nThe list of VK Cloud platform services includes the possibility of providing priority support (Managed Services).\n\nThis service includes extended support for all issues of implementation, maintenance and troubleshooting in the client's projects:\n\n24/7 system administration:\n\n  outsourcing of IT infrastructure and cloud services;\n  technical support;\n  monitoring and availability support;\n  Incident response;\n  crash recovery;\n\nAudit and consulting:\n\n  architecture consulting;\n  infrastructure audit;\n  performance audit.\n\nConfiguration and implementation:\n\n  monitoring settings;\n  setting up backup, high availability (HA), crash recovery systems;\n  software update of PaaS services;\n  migration of data and information systems.\n\nTo learn more about the service and submit a request for cost calculation, follow the link.\n","url":"/en/intro/start/support/support-info","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"061d2442-574c-4d2e-827a-be03375571f8","title":"SLA","data":"The response time and resolution of the appeal depends on the urgency and impact of the request on the information resources of the clients. Below are tables of response times and solutions for all types of appeals:\n\nClassification of incidents\n\nIncident classification table\n\n[cols=\"1,3\", options=\"header\"]\n|===\n| The severity of the incident\n| Description of the problem\n\n| Critical\n|* The Customer's business processes are blocked: Services are not working / are not deployed / are operating in a significant degradation mode.\nThe detected problem potentially jeopardizes the confidentiality,availability and integrity of the Customer's information.\nThe detected problem leads to a violation of legal requirements regarding the protection of confidential information.\n\n| High\n| The customer's business processes are functioning with significant degradation: Significant degradation / slowdown of components and/or Services.\n\n| Medium\n| The customer's business processes are functioning normally: Minor degradation/slowdown of components or Services.\n The services continue to function without restrictions.\n\n| Low\n| Detection of errors and problems that do not lead to stopping or degradation of components and Services.\n\n|===\n\nThe types of requests Service Request and Request for Information are equated to incidents of low importance.\n\nQuery Description table\n\n[cols=\"1,3\", options=\"header\"]\n|===\n| Name\n| Description\n\n| Service Request\n| Requests to support the functioning of the services: Creating/changing/enabling/disabling services, settings, entities).\n\n| Request for Information\n| Consultation on the settings and operation of the serviceswithin the framework of technical support.\n\n|===\n\nReaction time\n\nThe response time to the appeal is no more than 60 minutes.\n\n\n\nThe countdown of the response time to the appeal begins from the moment the Customer is notified of his registration and assignment of an identification number until the moment of the first response from the technical support service.\n\nSolution statistics table\n\n| Degree ofimportance| Execution time|\n|----------------------|----------------------------------------------|\n| Critical| 95% of critical priority requests are solved in 2h.|\n| High| 95% of high priority requests are solved in 12h.|\n| Medium| 95% of medium priority requests are solved in 24h.|\n| Low| 95% of low priority requests are resolved in 48 hours.|\n\nThe table shows the astronomical clock.\n","url":"/en/intro/start/support/sla","description":"Agreement on the level of service provided between VK Cloud and the client organization.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"f191556a-8c40-459a-829a-2f44bc1d6785","title":"Partner's personal account","data":"How to become a partner\n\nGo to the page Russian Enterprise Cloud Services.\nClick the Contact sales button.\nFill the fields in the form:\n\n    if you have not previously been registered in your VK Cloud personal account:\n\n        Name: name or full name to contact about the request.\n        Email: the contact email address to which the invitation will be sent and the partner's personal account will be linked.\n        Phone number: phone number for contact.\n        Company: name of your organization.\n        Message: message if necessary.\n\nClick the Send button.\n\nThe manager of the partner department will contact you within 48 hours from the moment of sending the request.\n\n\n\nThe partner's personal account will be available at the URL https://cloud.vk.com/partner/app/\n\n\n\n\n\nIf you have not been contacted within 48 hours, please contact the support service at the contacts.\n\nManaging clients in the partner's personal account\n\nClient management in the partner's personal account is performed at URL https://cloud.vk.com/partner/app/en/clients/.\n\nThe following operations are available:\n\nadding a client;\nchecking the balance;\nclient block/unblock;\ndeleting the client.\n\nAdding new client\n\nOpen clients list.\nClick the Add button.\nFill the fields in the form:\n\n    Email: the contact email address to which the invitation will be sent. Required to fill in.\n    Name: name of the client.\n    Telephone: contact phone number of the client being added.\n    Identifier: the unique identifier of the client, if he was previously registered in VK Cloud.\n\nClick the Add button.\n\n\n\nAn invitation letter will be sent to the specified email address, in which the client will need to confirm the addition to the partner.\n\nChecking the balance\n\nOpen clients list.\nIn the context menu to the right of the client, select Balance.\n\nOn the tab Detail tab will display detailed information on the client's debits. On the tab Expenses on services — statistics of expenses for the selected period of time.\n\n\n\nWhen displaying the details, all deposits and withdrawals are calculated based on the Moscow time zone (UTC+3).\n\nclient block and unblock\n\n\n\nBlocking the client means that it is impossible to sign in to the VK Cloud personal account. Its virtual resources continue to work.\n\n\n\nTo block a client:\n\nOpen clients list.\n\n    To block one client, in the context menu to the right of the client, select Block.\n    To block one or more clients, select the clients in the list and click the Block button.\n\nConfirm the operation.\n\n\n\nUnlocking works in the same way — instead of the option Block, select the option Unblock.\n\nDeleting the client\n\n\n\nDeleting a client implies its detachment from the partner, the resources used are not deleted.\n\n\n\nOpen clients list.\n\n    To delete one client, in the context menu to the right of the client, select Delete.\n    To delete one or more clients, select the clients in the list and click the Delete button.\n\nConfirm the operation.\n","url":"/en/intro/start/partners/partners-profile","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"ab6b91ee-9043-4956-945e-f7f14a0ffddd","title":"Terms of Use","data":"Terms of use of the platform \"VK Cloud\"\n\nThis document is an agreement (hereinafter referred to as the \"Agreement\") governing the relationship between VK LLC (hereinafter referred to as the \"Company\") and you (hereinafter referred to as the \"Customer\") in connection with the provision of various types of services, access to which is provided through the VK Cloud (hereinafter referred to as the \"Platform\"), on the terms set forth below:\n\n1\\. Regulation of the Platform use issues. Procedure for concluding Agreement\n\n1.1. The rules for using the Platform and the Services available through the Platform are set out in the following documents:\n\nthis Agreement,\nagreements on the use of certain Services provided by the Company or the Partners of the Company,\nCompany Privacy Policy.\n\nHereinafter, the above documents are collectively referred to as the \"Applicable Rules\".\n\nThe provisions of this Agreement apply to the relations between the Parties on the use of individual Services, unless other rules are expressly provided for by the relevant agreement on the use of the Service. By accepting the terms of this Agreement, the Customer agrees with the provisions of agreements on the use of individual Services, which are considered concluded between the Customer and the Company from the moment the Customer begins to use the corresponding Service.\n\n1.2. The applicable rules (this Agreement, as well as agreements on the use of individual Services) also contain provisions that are binding on Users (hereinafter referred to as the End User Rules). The Customer undertakes to ensure that the Users are familiar with the specified rules and is responsible for their observance on the part of the Users.\n\n1.3. With each access and / or actual use of the Platform or any of the Services, the Customer agrees to the terms of this Agreement, as well as the corresponding agreement on the use of the Service, in the editions that were in effect at the time of the actual use of the Platform and / or Services. At the same time, the Customer guarantees that with each access and / or actual use of the Platform or any of the Services by the User of the Project, which was created by the Customer, such User is familiar with the Rules for end users and agrees with them in the version in effect at the time of the actual use of the Platform by the User and / or Services.\n\n1.4. The applicable rules can be changed by the Company at any time without any prior notice to the Customer. Any such changes will take effect on the day following the day such changes are posted on the Site. The Customer undertakes to independently check the Applicable Rules for changes. Failure by the Customer to take steps to familiarize himself with the Applicable Rules and / or the revised edition cannot serve as a basis for the Customer's failure to fulfill his obligations related to the Use of the Platform and individual Services. The Customer has the right to refuse to accept changes and additions to the Applicable Rules, which means the Customer's refusal to use the Platform and / or the relevant Services.\n\n2\\. Terms and definitions\n\nSite — an Internet site located at https://mcs.mail.ru, which provides the User with access to the Platform and various services. The Company places information on the Site that is mandatory for Users.\n\nPlatform — a free computer program available to the Customer through the Site and operated by the Company (hereinafter referred to as a computer program), which allows the Customer, in accordance with the terms of the Agreement, to create and delete Projects, create and delete Accounts for Project Users within the framework of such Projects, as well as provide access and restrict the access of such Users to the selected Services.\n\nService is a software module of the Platform that allows you to implement certain functionality within the Platform that is provided by the Company or Partners. Granting the Customer the right to use any Service does not depend on the provision of access to other Services.\n\nCompany — Limited Liability Company Mail.Ru (OGRN 1027739850962, Russia, 125167, Moscow, Leningradsky Prospect, 39, bldg. 79), which provides the Customers with services for providing access to the Platform and Services.\n\nPartner — a person with whom the Company has entered into an agreement in order to ensure the possibility for the Customers to use the Services and / or the Partner's services through the Company's Platform.\n\nCustomer — a person with the necessary legal capacity and legal capacity to conclude the Agreement, who created an Account on the Platform.\n\nCustomer account — information provided by the Customer (the presence of which is a prerequisite) for registration and subsequent authorization of the Customer on the Platform in the process of rendering services to him by the Company, which is the name (login), password for access to the Personal Account (password), and also other information. Access to the Account is carried out through the Personal Account.\n\nThe Customer's personal account is a closed area of the Platform, which allows the Customer to manage settings, edit the Customer's account, gain access to statistics data, as well as perform other actions necessary to implement the functionality of the Platform and individual Services.\n\nA project is a set of logically interconnected web pages that are tied to a specific domain name or other identifier and operate according to computer program algorithms. Within the framework of one Project, using the Platform's web interface, Accounts for Project Users working within the framework of such a Project are created, who can be granted access to one or more Platform Services.\n\nProject Owner — the Customer who created the Project by using the functionality of the Platform. The Project Owner has the full scope of rights to manage the Project, including the ability to grant the rights of the Project Administrator to another person, change the scope of the granted rights, revoke the rights of the Project Administrator from such person, create and delete Accounts for Users within the Project, as well as provide access and restrict the access of such Users to the selected Services.\n\nProject Administrator — a person with the necessary legal capacity and legal capacity, who is the Owner of the Project or the User of the Project, to whom the Project Owner has granted the right to manage the Project.\n\nProject User (User) — a person with the necessary legal capacity and legal capacity, for whom the Project Administrator has created an Account. The Project User is a user of the Service to which the Project Administrator has granted him access.\n\nAn even account of the Project User — information about the Project User required for registration and subsequent authorization of the Project User on the Platform, which is the User's name (login), password for access to the Personal Account (password), as well as other information. Access to the Project User Account is carried out through the Personal Account.\n\nThe Personal Account of the Project User is a closed area of the Platform that allows the Project User to manage the settings, edit the Account and perform other actions necessary to implement the functionality of the Platform and the Service within the framework of the powers granted to him by the Project Administrator.\n\nPersonal balance (account) of the Customer — a set of records reflecting the financial relationship between the Customer and the Company for the services provided. The Customer's balance increases by the amount of funds transferred by the Customer to the Company and credited by the Company to the Customer's balance as prepayment for services, and decreases by the cost of the services connected by the Customer.\n\nBonus balance (account) of the Customer — a set of records reflecting the crediting / writing off of points by the Company to the Customer when conducting promotions or when paying the Customer compensation in connection with the violation of the agreed level of services by the Company. The bonus balance is reduced by the amount of services connected by the Customer. If funds are available on the Bonus and Personal Balance, debits are made first of all from the Bonus Balance, then from the Personal Balance.\n\nZero balance — the absence of bonuses / funds on the Bonus and Personal balance (account) of the Customer for the Company to write off the cost of services for providing access to the Services.\n\nReporting period — the period of time during which the Customer is provided with services for the use of the Service. The duration of the Reporting Period is established by the corresponding agreements on the use of individual services.\n\n3\\. Subject of the Agreement\n\n3.1. Under the terms of the Agreement, the Company provides the Customer with access to the Platform, which allows the Customer around the clock with an Internet connection, unless otherwise provided by the Applicable Rules, to use the Personal Account to gain access to the Platform Services.\n\n3.2. In the process of using the Platform, the Customer can create and delete Projects, create and delete Accounts for Project Users within such Projects, as well as provide access and restrict such Users' access to the selected Services. At the same time, the Customer is responsible to the Company for the actions of the Users of the Project and undertakes to ensure that such persons comply with the Rules for end users.\n\n3.3. The right to use the Platform and the Services provided by the Company is free, unless otherwise provided by the relevant agreements on the use of individual services.\n\n3.4. The operation and maintenance of the Platform and the Services provided by the Company is carried out by the Company, and the Services of the Partners — by the respective Partners. The use of the Platform and Services by the Customer and the Users of the Projects is carried out in an interactive (online) mode by connecting the specified persons via the Internet to the Site. The Company is the owner of the necessary amount of rights to the Platform and Services provided by the Company, as well as to all their elements, taken separately or in aggregate. The right to use the Services of the Partners is granted to the Customer by virtue of the agreements concluded between the Company and the Partners.\n\n3.5. The Company makes commercially reasonable efforts to ensure the functioning of the Platform and the Services provided by the Company around the clock, but does not guarantee the absence of interruptions associated with technical malfunctions, maintenance, and does not guarantee full or partial operability of their individual elements. Service agreements may include service level guarantees. The Company does not guarantee that the Platform, Services or any of their elements will function at any particular time in the future or that they will not stop working. The Company is not responsible for the quality and functioning of the Services provided by the Partners.\n\n3.6. The functions of the Platform, Services, including all scripts, content, individual elements and design of the Platform and Services are provided \"as is\". The Company does not provide any guarantees that the Platform, Services or their elements may be suitable for specific purposes of use. The Company cannot guarantee and does not promise any specific results from the use of the Platform, Services and their individual elements.\n\n4\\. Conditions and procedure for use of the Platform by the Customer\n\n4.1. To use the Platform, the Customer must register on the Platform and create an Account. When creating an Account on the Platform, the Customer, in a special window with fields, enters his login (his email address) and twice a password, and also confirms his agreement with this Agreement, the Platform's Privacy Policy and Applicable Rules. To confirm registration, a letter is sent to the e-mail address specified by the Customer, which contains a link to confirm the creation of an Account on the Platform.\n\nAuthorization and entry to the Platform is carried out through the Site. A person authorized on the Platform using the Customer's login and password is considered the proper owner of the corresponding account, access to the use and management of which was obtained as a result of such authorization.\n\n4.2. After authorization on the Platform (entering the Personal Account), the Customer has the right to use the Platform and the Services, subject to the terms of the agreement, as well as the corresponding agreements on the use of individual services.\n\n4.3. The platform provides the Customer with the following capabilities:\n\nmake settings for your Account within the functionality of the Platform;\ncreate and delete Projects using the functionality of the Platform.\nconnect the Services available through the Platform, including the created Projects, as well as refuse to use the connected Services. Agreements on the use of certain services may provide for the need to pay for the services of the Company or Partners for providing access to the relevant Service.\ncreate for individuals with the necessary legal capacity and legal capacity, accounts within the Projects, as well as delete the created accounts.\nprovide persons who have created accounts with access to the Services of the Platform, restrict or close access to these Services.\ngrant the rights of the Project Administrator to another person, change the scope of the granted rights, up to the deprivation of the right to manage the Project.\ncarry out other actions not prohibited by the legislation of the Russian Federation and the Applicable Rules related to the use of the Platform and the Services.\n\n4.4. When using the Platform and Services, the Customer undertakes:\n\ncomply with the provisions of the Applicable Rules without any restrictions;\nindependently take appropriate measures to ensure the security of his Personal Account and prevent unauthorized access to it by third parties (in particular, to ensure that the password is not saved in the browser, including when using cookies technology, if third parties may use the Customer's computer device). The customer is responsible for illegal actions carried out using his Personal Account as for his own.\nfollow all instructions and instructions of the Company. If the Customer fails to comply with such instructions, the Company has the right to suspend, restrict, terminate the provision of the Customer with access to the Platform, Services or their individual functions;\nat the request of the Company in connection with the conclusion and execution of the Agreement, confirm the credentials specified by the Customer when creating / editing his Account, as well as the credentials specified in the User Accounts;\nperiodically review the content and monitor for changes to the Applicable Rules.\nensure that Project Users are familiar with the End User Guidelines and comply with their provisions. The Customer is responsible for illegal actions committed using the User's Personal Account as for his own.\ncomply with other requirements and fulfill other obligations provided for by the Applicable Rules.\n\n4.5. The Company shall not be liable for the information security of the Customer's infrastructure designed using the Company's Services.\n\n4.6. The Services shall be provided to the Customer in accordance with the internationally accepted AS IS principle. This means that the Company shall not be held liable for the issues, errors, failures arising during the use of Services, including any issues connected with compatibility of the Services with any other software products, issues arising out of an ambiguous interpretation of the Services documentation, inconsistency between the results of use of the Services and the expectations of the Customer, etc.\n\n4.7. The Company shall not administer and support operating systems and other services created using the Platform Services, unless otherwise agreed upon by the Parties in a separate agreement.\n\n5\\. Conditions and order of use of the Platform by the User\n\n5.1. The User of the Project gets access to the Platform after the Administrator of the Project opens for him an account linked to the Project. The creation of a User account is carried out by specifying the e-mail address of the Project User as a login and the User following the link received by his e-mail to confirm the creation of the account. The User is authorized on the Platform within the framework of the Project by entering a login (his email address) and a password received from the Project Administrator.\n\n5.2. The Project Administrator can provide the Project User with access to one or several Platform Services. In this case, the User of the Project is the User of the corresponding Service.\n\n5.3. The User of the Project can be authorized on the Platform through the Site, as well as by using the mobile application for the corresponding Service, if such an opportunity is provided by the functionality of the Platform and the settings of the Project. A person authorized on the Platform is considered to be the proper owner of the corresponding account, access to the use and management of which was obtained as a result of such authorization.\n\n5.4. Authorization of the User of the Project on the Platform means the consent of such a person with the Rules for end users.\n\n5.5. The rights and obligations of the User of the Project using a separate Service are determined in this Agreement, as well as in the agreement on the use of the corresponding service.\n\n5.6. The Project User, to whom the Project Owner has granted the rights of the Project Administrator, implements the opportunities provided to him to the extent of the rights provided for by the Project settings. The Project Administrator undertakes to ensure that the Project Users comply with the End User Rules. If the Project Administrator finds that the User of the Project has violated the Rules for End Users, the Administrator must block the Account of such User and report this fact to the Company's support service.\n\n5.7. When using the Platform and Services, the User undertakes:\n\ncomply with the provisions of the Rules for End Users without any restrictions;\nindependently take appropriate measures to ensure the security of his Personal Account and prevent unauthorized access to it by third parties (in particular, to ensure that the password is not saved in the browser, including when using cookies technology, if third parties may use the User's computer device).\nfollow all instructions and instructions of the Company. If the User fails to comply with such instructions, the Company has the right to suspend, restrict, stop providing the User with access to the Platform, Services or their individual functions;\nat the request of the Company, confirm the credentials specified in the User Account;\nperiodically review the content and monitor changes to the Rules for end users.\ncomply with other requirements and fulfill other obligations provided for by the Rules for End Users.\n\n6\\. Limits of use of the Platform and individual Services\n\n6.1. When using the Platform and certain Services, the Customer and Users are prohibited from:\n\n6.2. The customer is prohibited from:\n\nviolate the intellectual property rights of the Company and / or Partners in relation to the Platform, Services or any of their elements, in particular, it is prohibited to copy, broadcast, send, publish, and otherwise distribute and reproduce materials posted by the Company and Partners within the Platform and Services ( text, graphic, audio-video materials), fragments and individual elements (including computer programs, scripts and other elements) of the Platform and Services without the written consent of the Company and / or Partners;\ncollect credentials of other Customers and Users of Projects created by other Customers;\nuse any automatic or automated means to collect information posted on the Platform and Services;\ncarry out, through the Platform and Services, propaganda or agitation inciting social, racial, national or religious hatred and enmity, propaganda of war, social, racial, national, religious or linguistic superiority, as well as sending information that promotes suicide, contains a description of methods of suicide and any incitement to commit it; information about narcotic and psychotropic substances, including information about the distribution of drugs, recipes for their manufacture, advice on their use and other information, the dissemination of which is prohibited by the current legislation of the Russian Federation;\nsend spam — mass mailing of commercial, political, advertising and other information (including hyperlinks leading to Internet sites with such information and / or to Internet sites containing malicious software) if the recipients did not express their consent to receive such information kind of information;\nindicate when creating a Personal Account and filling out the registration form or subsequently enter deliberately false or fictitious information about yourself;\ntransfer the password from the Personal Account to persons who do not have the proper authority to act on behalf of the Customer (or User);\ntake actions aimed at destabilizing the functioning of the Platform and Services, make attempts to unauthorized access to the management of the Platform and Services or their closed sections (including sections to which only the Company is allowed access), as well as perform any other similar actions;\ntake actions to create new structures (including blocks in the blockchain) using the functionality of the Platform and Services to ensure the functioning of cryptocurrency platforms or for mining cryptocurrencies in any other way;\ncarry out any other actions that contradict the policy and goals of creating the Platform and Services or violate the current legislation of the Russian Federation.\n\n7\\. Rights and obligations of the Company\n\n7.1. The Company carries out the current management of the Platform, independently determines the structure, appearance, allows or restricts the access of Customers, Users to the Platform and the Services provided by the Company in case of violation of the Applicable Rules and Rules for end users by the specified persons.\n\n7.2. In terms of providing an opportunity for interaction between Customers and Users, including providing these persons with the opportunity to independently perform certain actions within the Platform and Services, the Company acts exclusively as a person who organized the technical possibility of such interaction. The transfer, storage and provision of access via the Internet to the Platform and Services provided by the Customers, Users of the Projects, information, graphics and other materials associated with such interaction, are carried out without changing such materials or influencing their content on the part of the Company.\n\n7.3. The Company independently solves questions about the procedure for placing advertisements within the Platform and Services, participation in partner programs, inclusion of other information in the content of the Platform and Services.\n\n7.4. The company has the right to:\n\nat any time change the design and user interface of the Platform, the Services provided by the Company, the content of the functions provided, change or supplement the scripts used, software, the Company's content and other objects used or stored within the Services, any server applications, without mandatory notification of this Customer;\nat its discretion, delete any information and materials posted by the Customer within the Platform and Services that violate the laws of the Russian Federation, Applicable rules, the rights of other Customers, Users of Projects created by other Customers or third parties, harm them or threaten their safety;\ndemand payment for services to provide access to the Services in the amount and in the manner provided for by agreements on the use of individual Services;\nwhen using the Platform and Services by the Customers and Users of the Projects, make comments to the specified persons, warn, notify, inform them about non-compliance with the terms of the Applicable Rules and the Rules for end users. The Company's instructions are binding on the above persons when using the Platform and Services;\nsuspend, restrict or terminate the access of the Customer, Project Users to all or any of the sections of the Platform, Services at any time without explanation, with or without prior notice in case of violation of the legislation of the Russian Federation, Applicable Rules or Rules for end users;\nsend messages to Customers, Users of Projects (including messages by e-mail, sms messages, etc.), which are notifications of the introduction of new or cancellation of old functions of the Platform and / or Services, as well as containing advertising information about the capabilities of the Platform and Services;\ntake measures not prohibited by law to protect their own intellectual rights in relation to the Platform and the Services provided by the Company.\n\n7.5. The Company undertakes, on the terms set forth in the Agreement, to provide the Customer with services for providing access to the Platform.\n\n8\\. Payment for Company Services\n\n8.1. In cases stipulated by agreements on the use of certain Services, the services of the Company or Partners to provide access to the Services are paid and are provided on a prepaid basis.\n\n8.2. To gain access to the Services, the Customer must replenish his Personal balance (account) in the amount in rubles necessary to pay for services in the amount required for the Customer in accordance with the tariffs in force at the time of the payment. The Customer's obligations to pay for services are considered fulfilled from the moment the funds are received to the Company's current account.\n\nThe cost of services for providing access to the Service is determined in the agreement on the use of the corresponding Service. The Company may unilaterally change the cost of services for providing access to the Services. New rates are applied from the reporting period following the period in which the rates were changed.\n\nThe confirmation of payment for services, at the discretion of the Company, may be a copy of a document with a bank mark, indicating the transfer of funds. In this case, the Company has the right to request the original document confirming the fact of payment. The Customer agrees that all risks of losses when paying for the Company's services are the Customer's risks.\n\nAfter the receipt of funds to the Company's current account, the status of the Customer's Personal Balance (account) is updated by indicating the amount transferred by the Customer to the Company's current account as available for payment for services to provide access to the Services.\n\n8.3. The Company has the right to provide the Customer with bonuses when conducting promotions or when paying the Customer compensation in connection with the violation of the agreed level of services by the Company by crediting the bonuses to the Customer's Bonus balance (account). The bonus balance is reduced by the amount of services connected by the Customer. If funds are available on the Bonus and Personal Balance, debits are made first of all from the Bonus Balance, then from the Personal Balance.\n\n8.4. The Customer agrees that if, at the time of termination of the obligations of the Parties or termination of the Agreement, the amount of the prepayment made by the Customer for services exceeds the cost of the actually rendered services, the difference between these amounts is recognized as an advance payment of the Customer for other services provided by the Company under other (including future) contracts, except for the cases of written agreement on the terms of termination of obligations or termination of the concluded Agreement on other terms.\n\n8.5. A customer who is a legal entity or an individual entrepreneur, subject to payment for services by transferring funds to a bank account, undertakes, at the end of the Reporting Period, during which the Company provided him with services to provide him with access to the Services, to sign the Acceptance Certificate of the Services Rendered (hereinafter — \"Act\") and an invoice (or a universal transfer document (hereinafter referred to as \"UPD\"). The Company sends the FRTD to the Customer through the electronic document management system (hereinafter referred to as \"EDM\"). signature (hereinafter referred to as \"ES\").\n\n8.6. The Customer undertakes to sign one copy of the Act / UPD and send it to the Company within 10 working days from the date of its receipt, or sign the UPD via EDM using electronic signature within the same time frame. Otherwise, the services shall be considered rendered, and the Certificate/UTD shall be considered executed.\n\n9\\. Territory and duration of the Agreement\n\n9.1. The Customer has the right to use the Platform and Services in the ways described in this Agreement throughout the Russian Federation, as well as other territories where they are available using standard computer tools and programs.\n\n9.2. This Agreement is concluded for a period of one calendar year and enters into force from the date of acceptance by the Customer of the Applicable Rules. If, 30 (thirty) calendar days before the expiration of the Agreement, none of the Parties notifies in writing about the termination of its validity, it will be considered an extended period of one year. The number of renewals is not limited.\n\n9.3. This agreement is terminated if:\n\nThe Company will decide to change the terms of this Agreement, the need to conclude a new agreement with the Customers, terminate this Agreement in relation to the Customer, terminate the administration and maintenance of the Platform and terminate access to it or terminate access to the use of the Platform in relation to the Customer.\nThe Company has the right at any time without notifying the Customer and without giving reasons to terminate the Agreement unilaterally out of court with immediate termination of access and the ability to use the Platform and without reimbursement of any costs, losses or return received under the Agreement, including, but not limited to, in case:\n\n    closure or termination of the Platform;\n    any, including one-time, violation by the Customer or the User of the Project created by the Customer, the terms of the Applicable Rules and / or the Rules for end users;\n    on other grounds that, in the opinion of the Company, are sufficient to make a decision to terminate the Agreement.\n\n9.4. This Agreement does not provide for the assignment of any exclusive rights or the issuance of an exclusive license from the Company to the Customer for any components of the Platform and Services.\n\n10\\. Final provisions\n\n10.1. The invalidity of one or several provisions of the Agreement, recognized in the prescribed manner by a court decision that has entered into force, does not entail the invalidity of the Agreement as a whole for the Parties. In the event that one or several provisions of the Agreement are recognized as invalid in the prescribed manner, the Parties undertake to fulfill their obligations under the Agreement as close as possible to those implied by the Parties when concluding and / or agreeing to amend the Agreement in a manner.\n\n10.2. This Agreement and the relationship between the Parties in connection with this Agreement and the use of the Service are governed by the legislation of the Russian Federation.\n\n10.3. With regard to the form and method of concluding this Agreement, the norms of the Civil Code of the Russian Federation (\"Civil Code of the Russian Federation\") governing the procedure and conditions for concluding an agreement by accepting a public offer are applied.\n\n10.4. All disputes between the parties under this Agreement shall be resolved through correspondence and negotiations using the mandatory pre-trial (claim) procedure. If it is impossible to reach agreement between the parties through negotiations within 60 (sixty) calendar days from the moment the other Party receives a written claim, consideration of the dispute should be referred by any interested party to the court at the location of the Company (excluding the jurisdiction of the case to any other courts), if otherwise not provided by applicable law.\n\n10.5. For questions related to the execution of the Agreement, please contact the Company's location: Russian Federation, 125167 Moscow, Leningradsky prospect, 39, bldg. 79, or by email at support@mcs.mail.ru.\n\nRevision: from 03/16/2023\n","url":"/en/intro/start/legal/legal-terms","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"1f549820-6bf0-497d-9066-5da21a7e755d","title":"User agreement for the service “CDN”","data":"Настоящий документ представляет собой оферту в соответствии со ст. 435 ГК РФ (далее по тексту – «Оферта») на заключение между ООО «ВК» (125167, г. Москва, Ленинградский проспект, д. 39, стр. 79, ОГРН 1027739850962) (далее – «Компания») и Вами (далее – «Заказчик») пользовательского соглашения на условиях настоящей Оферты (далее – «Соглашение»), предметом которого является оказание услуг по предоставлению доступа к Сервису «CDN» в рамках Платформы «VK Cloud» (далее – «Платформа»).\n\nНастоящее Соглашение доступно в сети Интернет по адресу: https://mcs.mail.ru/help/ru_RU/legal/policy-cdn и является неотъемлемой частью Условий использования платформы «VK Cloud», доступных в сети Интернет по адресу: https://mcs.mail.ru/help/legal/termofuse и Политики Конфиденциальности Компании, в сети Интернет по адресу: https://mcs.mail.ru/help/legal/privacy (далее совместно — «Применимые правила»). В случае противоречия между положениями Условий использования Платформы и настоящего Соглашения, приоритет имеют положения настоящего Соглашения. К отношениям Сторон, которые не урегулированы настоящим Соглашением, применяются Условия использования Платформы.\n\nИспользуя Сервис, Вы в соответствии со ст. 438 ГК РФ безоговорочно и в полном объеме принимаете условия Соглашения.\n\n1\\. Термины и определения\n\nСайт — интернет-сайт, расположенный по адресу https://mcs.mail.ru/, предоставляющий Заказчику доступ к Сервису. Компания размещает на Сайте информацию, обязательную для Заказчика.\n\nПлатформа «VK Cloud» (Платформа) — доступная Заказчику через Сайт и оперируемая Компанией программа для ЭВМ, которая предоставляет Заказчику доступ к выбранным Сервисам.\n\nСервис CDN (Сервис) — услуга Компании, известная под наименованием «CDN», состоящая в представлении Заказчику возможности использования географически распределённой сетевой инфраструктуры, позволяющей оптимизировать доставку и дистрибуцию содержимого конечным пользователям в сети Интернет.\n\nКонтент — информационное содержание сайта, генерируемое на сервере в момент получения запроса (изменяемое пользователем, либо загружаемое из базы данных), либо хранимое на сервере в неизменяемом виде (бинарные файлы, аудио и видео файлы, JC и CSS).\n\nНедоступность Сервиса (Даунтайм) — интервал времени с момента отправки Заказчиком в службу поддержки Компании сообщения по электронной почте на адрес support@mcs.mail.ru или через форму обратной связи в Личном кабинете, факт которой был впоследствии подтверждён Компанией, и моментом возобновления работы Сервиса после завершения Компанией восстановительных работ.\n\nКомпенсация — сумма, подлежащая зачислению на Лицевой счет Заказчика в случае Даунтайма в порядке, предусмотренном настоящим Соглашением. Компенсация не предполагает выплаты Заказчику денежных средств.\n\nЛичный кабинет - закрытая область Платформы, позволяющая Заказчику управлять настройками, редактировать Учетную запись Заказчика, получать доступ к данным статистики, а также совершать иные действия, необходимые для реализации функциональных возможностей Платформы и отдельных Сервисов.\n\nЛицевой счет – совокупность записей, отражающая финансовые взаимоотношения между Заказчиком и Компанией по предоставляемым услугам. Баланс Заказчика увеличивается на сумму средств, перечисленных Заказчиком Компании и зачисленных Компанией на Лицевой счет в счет предоплаты.\n\nТарифный план - форма коммерческого предложения, в которой указывается перечень услуг и порядок определения их стоимости.\n\nДоговор - соглашение, регулирующее отношения между ООО «ВК» и Заказчиком в связи с оказанием услуг доступа к Сервису.\n\nОтчетный период — календарный месяц.\n\nИные термины определены в Условиях использования Платформы «VK Cloud».\n\n2\\. Предмет Соглашения\n\n2.1. По условиям настоящего Соглашения Компания предоставляет Заказчику круглосуточно при наличии подключения к сети Интернет «Сервис CDN» (далее – Сервис), позволяющий Заказчику использовать географически распределенную сетевую инфраструктуру, позволяющую оптимизировать доставку и дистрибуцию контента конечным пользователям в сети Интернет, а Заказчик обязуется оплатить услуги Компании за использование предоставленного доступа к сервису в размере и порядке, которые определяются Применимыми правилами. Компания вправе предоставлять доступ к Сервису на основании соглашений, заключенных с правообладателями прав на Сервис или третьими лицами, владеющими такими правами на договорной основе.\n\n2.2. Условием использования Сервиса является полное и безоговорочное принятие Заказчиком положений настоящего Соглашения. Доступ к Сервису осуществляется в рамках Платформы, в связи с чем условием использования Сервиса является использование Платформы и принятие Условий использования Платформы, доступных по ссылке https://mcs.mail.ru/docs/intro/start/legal/legal-terms/, а также Политики Конфиденциальности Компании, доступной по ссылке https://mcs.mail.ru/docs/ru/intro/start/legal/policy-privacy (далее вместе именуемые «Применимые правила»). В случае противоречия между положениями Условий использования Платформы и настоящего Соглашения, приоритет имеют положения настоящего Соглашения. К отношениям Сторон, которые не урегулированы настоящим Соглашением, применяются Условия использования Платформы.\n\n2.3. При каждом доступе и/или фактическом использовании Сервиса Заказчик соглашается с положениями настоящего Соглашения и Применимыми правилами в редакции, которые действовали на момент фактического использования Сервиса.\n\n2.4. Применимые правила, в том числе настоящее Соглашение, могут быть изменены Компанией в любое время без какого-либо предварительного уведомления Заказчика. Любые такие изменения вступают в силу в день, следующий за днем опубликования таких изменений на Сайте. Заказчик обязуется самостоятельно проверять Применимые правила на предмет изменений. Неосуществление Заказчиком действий по ознакомлению с Применимыми правилами и/или измененной редакцией не может служить основанием для неисполнения Заказчиком своих обязанностей, связанных с использованием Платформы и Сервиса. Заказчик вправе отказаться от принятия изменений и дополнений в Применимые правила, что означает отказ Заказчика от использования Платформы и/или Сервиса.\n\n2.5. Функциональные возможности Сервиса, использование Сервиса:\n\nЗаказчик может использовать Сервис в объеме прав, предоставленных ему Администратором Платформы и Сервиса, любыми способами, допустимыми функциональными возможностями Платформы и Сервиса, за исключением способов, прямо запрещенных Условиями использования Платформы и Соглашением, а также применимым законодательством. В частности, Заказчик может создавать новые CDN-ресурсы, осуществлять их настройку, создание, загрузку файлов в ресурс, кэширование, настраивать HTTP-заголовки, контент, группы источников, DNS. В любом случае функционал Сервиса предоставляется «как есть».\n\n2.6. Компания вправе осуществлять мониторинг Контента в целях проверки соблюдения Заказчиком требований настоящего Соглашения. Заказчик обязуется не создавать Компании препятствия при осуществлении такого мониторинга, при этом вправе использовать инструменты для сохранения конфиденциальности Контента (как например шифрование, брандмауэры).\n\n2.7. Доступ к Сервису может предоставляться одним из следующих способов:\n\n2.7.1. При предоплатной системе расчетов: после внесения денежных средств в объеме, предусмотренном Тарифами. Заказчику предоставляется доступ для работы с Сервисом.\n\n2.7.2. При постоплатной системе расчетов: после подписания договора на постоплату. Заказчику предоставляется доступ для работы с Сервисом.\n\n3\\. Права и обязанности Сторон\n\n3.1. Компания обязуется:\n\n3.1.1. Предоставить Заказчику возможность использования Сервиса Компании.\n\n3.1.2. Обеспечить постоянную, круглосуточную доступность Сервиса Заказчику, при наличии подключения к сети Интернет.\n\n3.1.3. Осуществлять техническую поддержку Заказчика по выбору Компании: лично либо с привлечением третьих лиц, в том числе ПАО «МегаФон» (ОГРН 1027809169585) через каналы связи, предусмотренные функционалом Сервиса.\n\n3.1.4. Не разглашать данные Заказчика, за исключением случаев, предусмотренных законодательством РФ, настоящим Соглашением и Применимыми правилами.\n\n3.2. Компания вправе:\n\n3.2.1. Блокировать доступ к Сервису и к Контенту Заказчика в случае, если Компании станет известно о нарушении условий настоящего Соглашения, Применимых правил или законодательства РФ. Любые убытки, причиненные Заказчику в связи с блокировкой Компанией доступа к Сервису или к Контенту Заказчика, не возмещаются Компанией.\n\n3.2.2. В уведомительном, одностороннем порядке вносить изменения в Применимые правила.\n\n3.2.3. Требовать от Заказчика предоставления информации и документов, подтверждающих соответствие Контента требованиям настоящего Соглашения.\n\n3.3. Заказчик обязуется:\n\n3.3.1. Оплачивать услуги Компании согласно условиям настоящего Соглашения.\n\n3.3.2. На регулярной основе самостоятельно знакомиться с действующей редакцией Применимых правил и соблюдать их в полном объеме.\n\n3.2.3. Нести ответственность за свои действия и/или бездействия по использованию Сервиса Компании.\n\n4\\. Стоимость услуг и порядок расчетов\n\n4.1. Использование Сервисов является платным. Компания в целях определения стоимости использования Сервиса устанавливает цену и соответствующий перечень услуг согласно Тарифам.\n\n4.2. Тарифы устанавливаются Компанией и доводятся до сведения Заказчика путем публикации на официальном сайте компании. Действующие Тарифы доступны по ссылке: https://mcs.mail.ru/docs/networks/cdn/about-cdn. Тарифы BOX 1, 3, 5, 10, 20 могут быть изменены Компанией в одностороннем порядке, что доводится до сведения Заказчика путем обновления информации на https://mcs.mail.ru/docs/networks/cdn/about-cdn. Компания может в одностороннем порядке изменять Тарифы, которые применяются с Отчетного периода, следующего за тем периодом, в котором были изменены Тарифы. Стоимость услуг по тарифу Индивидуальный и порядок расчетов устанавливаются Договором.\n\n4.3 В случае отсутствия денежных средств на Лицевом счете для списания оплаты стоимости услуг за предоставление доступа к Сервису или за оказанные в предыдущем Отчётном периоде услуги (далее – Нулевой баланс), доступ Заказчика к Сервису и Контенту прекращается. В случае использования тарифа Индивидуальный доступ к Сервису и Контенту, а также приостановление или блокировка доступа определяется условиями Договора.\n\n4.4. При Нулевом балансе Заказчика Компания вправе удалить файлы, занимающие дисковое пространство с предварительным уведомлением Заказчика по электронной почте, на которую зарегистрирован Личный кабинет. В случае использования тарифа Индивидуальный право на удаление файлов, занимающих дисковое пространство, определяется условиями договора.\n\n4.5. Для восстановления доступа к Сервису Заказчик должен пополнить средства на его Лицевом счете на сумму, необходимую для оплаты услуг в требуемом для Заказчика объеме в соответствии с Тарифами, действующими на момент совершения платежа. В случае использования тарифа Индивидуальный условия возобновления доступа к Сервису, определяется условиями договора.\n\n4.6. Заказчик вправе воспользоваться демо-доступом (без осуществления платы в целях ознакомления с функционалом) к Сервису 1 (один) раз на срок, не превышающий 7 (семь) календарных дней и не превышающий 1 Тб трафика.\n\n4.7 Демо-доступ предоставляется автоматически при подключении Сервиса в личном кабинете.\n\n4.8 При завершении срока демо-доступа, либо при исчерпании трафика доступного в рамках демо-доступа, Сервис предоставляется в соответствии с тарифами, действующими на момент платежа.\n\n5\\. Ответственность Сторон\n\n5.1. Размер ответственности Компании ограничен исключительно соответствующими Применимыми правилами, размещенным на Сайте в разделе https://help.mail.ru/mcs/legal/.\n\n5.2. Упущенная выгода Заказчика возмещению Компанией не подлежит.\n\n5.3. Компания не несёт ответственности перед Заказчиком за блокировку доступа к Контенту Заказчика, или уничтожение Контента в случае, если Компанией были получены соответствующие претензии третьих лиц, либо предупреждения, предписания или иные акты органов государственной власти в отношении действий Заказчика с использованием Сервиса.\n\n5.4. Компания не несет ответственности за любые нарушения действующего законодательства, которые имеют или могут иметь место на проектах, реализуемых Заказчиком. В случае предъявления к Компании претензий от третьих лиц в связи с такими имевшими место нарушениями Заказчик обязан освободить Компании от ответственности и урегулировать такие претензии самостоятельно, а также возместить все убытки, понесённые Компанией по вине Заказчика, в полном объеме, в том числе, но не ограничиваясь: по урегулированию претензий, споров, исполнению санкций органов государственной власти, должны быть возмещены Компании в случае наличия таковых.\n\n6\\. Прочие условия\n\n6.1. Заказчик гарантирует, что его Контент, размещаемый с использованием Сервиса Компании, не содержит информацию и изображения порнографического характера, не пропагандирует насилие, расовое, половое, религиозное или другие формы неравенства, либо иную деятельность, прямо запрещенную законодательством Российской Федерации, не нарушает законодательство об авторском праве и иное действующее законодательство, поскольку это может повлиять на репутацию Компании.\n\n6.2. Заказчик гарантирует, что размещение Контента Заказчика происходит с соблюдением условий действующего законодательства РФ.\n\n6.3. Формы использования Заказчиком изображений, текстов, наименований, эмблем и символов Компании должны быть согласованы с представителями Компании.\n\n6.4. Компания гарантирует, что услуги по предоставлению доступа к Сервису оказываются 24 (двадцать четыре) часа в сутки, 7 (семь) дней в неделю.\n\n6.5. Компания оставляет за собой право прерывать работу Сервиса для проведения необходимых профилактических работ, в том числе в рабочие дни. Такие случаи не будут считаться перерывами в оказании услуг, если Компания предварительно уведомит Заказчика за 24 (двадцать четыре) часа до момента начала профилактических работ, влекущих перерывы в работе Сервиса, путем публикации на Сайте соответствующей информации (в том числе продолжительность профилактических работ). Время недоступности Сервиса, связанное с проведением профилактических работ, не подлежит компенсации по правилам п. 6.6. Соглашения.\n\n6.6. Недоступность Сервиса («Даунтайм») определяется как интервал времени с момента отправки Заказчиком в службу поддержки Компании сообщения через форму обратной связи, доступную в Личном кабинете, содержащего уведомление о недоступности Сервиса, либо сообщения по электронной почте на адрес: support@mcs.mail.ru, факт которой был впоследствии подтверждён Компанией, и моментом возобновления работы Сервиса после завершения Компанией восстановительных работ. Если Компания располагает собственными данными о начале Даунтайма, указывающими на более раннее время начала недоступности Сервиса, чем время отправки сообщения по электронной почте на адрес support@mcs.mail.ru или форму обратной связи в Личном кабинете, Компания может использовать такие данные. Разногласия о времени начала и окончания Даунтайма решаются путём переговоров Сторон. Если Даунтайм начался в одном Отчетном периоде, а окончился в следующем Отчетном периоде, то недоступность Сервиса полностью относится к тому Отчетному периоду, в котором имела место большая часть Даунтайма.\n\n6.7. В случае недоступности Сервиса, Компания обязуется перечислить Заказчику (зачислить на Лицевой счет) Компенсацию, размер которой зависит от фактического времени доступности рассматриваемого Сервиса и его стоимости, списанной с Лицевого баланса в Отчетном периоде, в котором был зафиксирован Даунтайм.\n\nДля целей расчета Компенсации не учитывается стоимость других сервисов, доступных на Платформе, по которым не был зафиксирован Даунтайм.\n\nРазмер Компенсации определяется на основании нижеприведенной таблицы (далее – Таблица №1):\n\n| Время доступности (в %)\\* | Время недоступности (мин или ч) в одном Отчетном периоде суммарно | Размер Компенсации |\n| --- | --- | --- |\n| 99.95% и выше | менее 22 минут | не выплачивается |\n| 99,9%–99,95% | от 22 до 45 минут | 3% суммы, списанной с Лицевого баланса за Сервис за Отчётный период, в котором был Даунтайм |\n| 99,0%–99,9% | от 45 минут до 7,5 часов | 10% суммы, списанной с Лицевого баланса за Сервис за Отчетный период, в котором был Даунтайм |\n| 95,0%–99,0% | от 7,5 часов до 36 часов | 20% суммы, списанной с Лицевого баланса за Сервис за Отчетный период, в котором был Даунтайм |\n| 90,0%–95,0% | от 36 часов до 73 часов | 25% суммы, списанной с Лицевого баланса за Сервис за Отчетный период, в котором был Даунтайм |\n| менее 90% | более 73 часов | 50% суммы, списанной с Лицевого баланса за Сервис за Отчетный период, в котором был Даунтайм |\n\n\\*Расчет произведен исходя из 43 800 минут в Отчетном периоде\n\n6.8. В любых случаях нарушения гарантий уровня Сервиса Заказчик обязуется сотрудничать с Компанией для определения источника проблем, его устранения и восстановления работоспособности Сервиса или удаленного/поврежденного Контента.\n\n6.9. Расчет Компенсации осуществляется автоматически на основании данных статистики Компании. Сумма Компенсации перечисляется на счет Заказчика в размере, определяемом на основании правил п. 6.7. настоящего Соглашения в течение 7 (семи) рабочих дней с момента начала Отчетного периода, следующего за тем, в котором имели место события, послужившие основанием для Компенсации.\n\n6.10. Размер Компенсации в течение Отчетного периода по обоим основаниям, указанным в п.п. 6.6. и 6.7. Соглашения, не может превышать размер средств, оплаченных Заказчиком за Отчетный период, в котором имели место события, послужившие основанием для Компенсации.\n\n6.11. Не подлежит Компенсации недоступность Сервиса, вызванная действиями Заказчика или третьих лиц, либо обстоятельствами непреодолимой силы, которые повлекли за собой недоступность услуг; приостановка оказания услуг, связанная с проведением запланированных профилактических работ, как определено в п. 6.5. настоящего Соглашения; приостановка оказания услуг по требованию государственных или муниципальных органов в случаях и порядке, определенным применимым законодательством. Компания ни при каких обстоятельствах не возмещает Заказчику Сервиса косвенные убытки, в том числе упущенную выгоду и причиненный вред деловой репутации.\n\nРедакция: от 18.10.2022 г.\n","url":"/en/intro/start/legal/policy-cdn","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"75ccf8ec-01b5-4a05-ad33-35d97241d2e2","title":"User agreement for the service “Cloud for hot data” (Hotbox)","data":"This document is an official offer of Mail.ru LLC (hereinafter — the \"Company\") to conclude a user agreement (hereinafter — the \"Agreement\") with you (hereinafter — the \"Customer\") on the provision of services for providing access to the Service \"Cloud for Hot Data\" as part of the VK Cloud Platform (hereinafter referred to as the Platform), on the terms set forth in this Agreement, which is available on the Internet at: https://mcs.mail.ru/docs//en/intro/start/legal/policy-hotbox.\n\n1\\. Terms and definitions\n\nSite — an Internet site located at https://mcs.mail.ru/, which provides the Customer with access to the Platform and the Service. The Company places information on the Site that is mandatory for Customers.\n\nCloud storage for hot data (Service) is a service of the Company, known as \"Cloud for hot data\" (HotBox), consisting in providing the Customer and Users with the ability to post Content on the Company's servers, use, reproduce, modify and delete such Content by using programs for computers, allowing to organize the work of the Service and the activities of Users to work with the Content.\n\nThe Company's services may include the provision by the Company of the rights to use certain computer programs that are necessary for the full use of the Service. The specified computer programs can be used by the Customer and Users only as part of the service for providing access to the Service, cannot be transferred to third parties and used outside the Service.\n\nContent — electronic documents, graphics, photographs, scripts, texts, videos, music, sounds and other objects (files) uploaded by the Customer or Users posted on the Service.\n\nProhibited Content — any Content described in clause 4.1. of this Agreement, as well as other materials, the use of which is prohibited by the Applicable Rules.\n\nA bucket is a disk space with files.\n\nService unavailability (Downtime) — the time interval from the moment the User sent a message to the Company's support service by e-mail to support@mcs.mail.ru or through the feedback form in the personal account, the fact of which was subsequently confirmed by the Company, and the moment the Service was resumed after the completion of the restoration work by the Company.\n\nCompensation is the amount of funds transferred by the Company to the User in case of violation of the guarantees of the level of the Service or in case of loss / damage of the Content as a result of an accident due to the fault of the Company.\n\nThe reporting period is a calendar month.\n\nOther terms are defined in the Terms of Use of the Platform.\n\n2\\. Subject of the agreement, conditions for obtaining access to the Service\n\n2.1. Under the terms of this Agreement, the Company provides the Customer around the clock, with an Internet connection, the Cloud for Hot Data Service (hereinafter referred to as the Service), which allows the Customer to place Content on the disk space of the Company's servers by using computer programs, including viewing, changing, deleting Content, manage access to it by other persons (Project Users, third parties) and perform other actions with the Content provided for by the functionality of the Service, and the Customer undertakes to pay for the Company's services for using the provision of access to the Service in the amount and in the manner determined by the Applicable Rules.\n\n2.2. The condition for using the Service is the complete and unconditional acceptance of the provisions of this Agreement by the User. Access to the Service is carried out within the Platform, in connection with which the condition for using the Service is to use the Platform and accept the Terms of Use of the Platform available at https://mcs.mail.ru/help/legal/termofuse, as well as the Company's Privacy Policy available at https://mcs.mail.ru/help/legal/privacy (hereinafter collectively referred to as the \"Applicable Rules\"). In the event of a conflict between the provisions of the Terms of Use of the Platform and this Agreement, the provisions of this Agreement shall prevail. The Terms of Use of the Platform apply to the relations of the Parties that are not regulated by this Agreement.\n\n2.3. With each access and / or actual use of the Service, the Customer agrees with the provisions of this Agreement, as well as with other Applicable rules, as amended, which were in effect at the time of the actual use of the Service. At the same time, the Customer guarantees that with each access and / or actual use of the Service by the User of the Project, which was created by the Customer, such User is familiar with the Rules for end users and agrees with them in the version in effect at the time of the actual use of the Service by the User.\n\n2.4. The applicable rules, including this Agreement, may be changed by the Company at any time without any prior notice to the Customer. Any such changes will take effect on the day following the day such changes are posted on the Site. The Customer undertakes to independently check the Applicable Rules for changes. Failure by the Customer to take steps to familiarize himself with the Applicable Rules and / or the revised edition cannot serve as a basis for the Customer's failure to fulfill his obligations related to the Use of the Platform and individual Services. The Customer has the right to refuse to accept changes and additions to the Applicable Rules, which means the Customer's refusal to use the Platform and / or the relevant Services.\n\n3\\. Functionality of the Service, use of the Service\n\n3.1. The Customer can use the Service in any way that is acceptable by the functionality of the Platform and the Service, with the exception of methods expressly prohibited by the Terms of Use of the Platform and the Agreement, as well as by applicable law. In particular, the Customer can place the Content on the disk space of the Company's servers, view, modify, delete the Content, create Buckets and manage access to it for other persons (Project Users, third parties) and perform other actions with the Content provided for by the functionality of the Service.\n\n3.2. The Service user can use the Service in ways determined by the functionality of the Platform and the Service, to the extent of the rights granted to him by the Project Administrator. The User is not entitled to use the Service in ways expressly prohibited by the Terms of Use of the Platform and the Agreement, as well as by applicable law.\n\n3.3. The User of the Service can provide access to other Users of the Service to files and / or Buckets with files indicating the scope of the granted rights.\n\n3.4. The Service user who has gained access to the Bucket can use the files included in such Bucket to the extent of the rights granted to him.\n\n4\\. Rights and obligations of the Parties in connection with the use of the Content\n\n4.1. The placement of Content on the servers of the Company and its use must not violate any applicable law, in particular, the Customer and / or Users are not entitled to:\n\nupload, store, transfer, provide access to other persons or otherwise bring to the public knowledge (post on the Service) the Content and other results of intellectual activity of the copyright holders in the absence of the explicitly expressed consent of these persons and / or the required amount of rights to such actions;\nupload, store, transfer, provide access to other persons to the Content prohibited by applicable law, or the Content, storage or use of which requires obtaining special permissions from authorized bodies or persons, if the Customer does not have the appropriate permissions.\n\nThe content described in this clause is hereinafter referred to as “Prohibited Content”.\n\n4.2. The Company has the right to require the Customer to provide information and documents confirming the compliance of the Content with the requirements of this Agreement.\n\nThe Company has the right to monitor the Content in order to verify that the Customer and Users comply with the requirements of this Agreement for the Content and to prevent the Prohibited Content. The Customer undertakes not to create obstacles for the Company in the implementation of such monitoring, while at the same time he has the right to use tools to preserve the confidentiality of the Content (such as encryption, firewalls).\n\n4.3. The Company has the right to block or remove Prohibited Content on its own initiative, as well as at the request of authorized government agencies. Any losses caused to the Customer in connection with the removal of the Prohibited Content by the Company will not be reimbursed by the Company.\n\n5\\. Cost of services\n\n5.1. The use of the Service within the Platform is Paid. In order to determine the cost of using the Service, the Company sets the price and the corresponding list of services in accordance with the Tariffs.\n\n5.2. The tariff is set by the Company and communicated to the Customer. The current Tariffs are posted on the page: mcs.mail.ru/hotbox. The tariff can be changed by the Company unilaterally, which is brought to the attention of the Customer by updating the information on the page: mcs.mail.ru/hotbox. The Company may unilaterally change the Tariffs that are applied from the Reporting Period following the period in which the Tariffs were changed.\n\n6\\. End of service and consequences of violation of payment terms\n\n6.1. In the absence of bonuses / funds on the Bonus and / or Personal balance (account) of the Customer to write off the cost of services for providing access to the Service (hereinafter — Zero balance), the access of the Users of the Service / Platform to Buckets and files is terminated.\n\n6.2. With a zero balance of the Customer, the Company has the right to delete files that take up disk space with prior notification of the Customer by e-mail to which the Project is registered.\n\n6.3. To restore access to the Service, the Customer must replenish funds on his Personal balance (account) for the amount necessary to pay for services in the amount required for the Customer in accordance with the Tariffs in force at the time of the payment.\n\n7\\. Responsibility for interruptions in the operation of the Service (guarantees of the level of the Service)\n\n7.1. The Company guarantees to the User that the services for providing access to the Service are provided 24 (twenty four) hours a day, 7 (seven) days a week.\n\nThe Company guarantees the continuous operation of the Service, provided that the Customer observes its permissible functionality, that is, when no more than 100 (one hundred) calls to the Service are made per second. An increase in the functionality of the Service (volume of performance) is possible by signing a separate agreement between the Customer and the Company. The Customer has the right to inform the Company about his desire to conclude such an agreement by sending a request by e-mail to support@mcs.mail.ru.\n\n7.2. The Company reserves the right to interrupt the operation of the Service to carry out the necessary maintenance work, including on weekdays. Such cases will not be considered interruptions in the provision of services if the Company notifies the User in advance 24 (twenty four) hours before the start of preventive work, entailing interruptions in the operation of the Service, by publishing relevant information on the Site (including the duration of preventive work). The time of unavailability of the Service associated with maintenance work is not subject to compensation according to the rules of this section of the Agreement.\n\n7.3. The unavailability of the Service (Downtime) is defined as the time interval from the moment the User sends a message to the Company's support service through the feedback form available in the Personal Account containing a notification about the unavailability of the Service, or an e-mail message to the address: support@mcs.mail.ru, the fact of which was subsequently confirmed by the Company, and the moment the Service was resumed after the completion of the restoration work by the Company. If the Company has its own data about the beginning of Downtime, indicating an earlier start time of the Service's unavailability than the time of sending an e-mail to support@mcs.mail.ru or the feedback form in the personal account, the Company may use such data. Disagreements about the start and end times of Downtime are resolved through negotiations between the Parties. If Downtime started in one Reporting Period and ended in the next Reporting Period, then the unavailability of the Service fully refers to the Reporting Period in which most of Downtime took place.\n\nIf the Service is unavailable, the Company undertakes to pay the User Compensation, the amount of which is determined on the basis of the table below (hereinafter referred to as the Table):\n\n[cols=\"1,2,3\", options=\"header\"]\n|===\n| Availability time (in%)\\*\n| Unavailability time (min. Or hour) in one Reporting period in total\n| Compensation Amount\n\n| 99.95% and above\n| less than 45 minutes\n| not paid\n\n| 99,9%–99,95%\n| 45 minutes to 7.3 hours (438 minutes)\n| 3% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was\n\n| 99,0%–99,9%\n| from 7.3 to 36 hours\n| 10% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was\n\n| 95,0%–99,0%\n| from 36 to 73 hours\n| 20% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was\n\n| 90,0%–95,0%\n| more than 73 hours\n| 50% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was\n\n|===\n\n\\* Calculated based on 43 800 minutes in the Reporting Period\n\n7.4. In case of loss / damage to the Content as a result of an accident through the fault of the Company, the Customer is provided with Compensation in the amount of 100% of the amount debited from the Bonus and / or Personal balance (account) of the User in the Reporting period during which the loss / damage to the Content occurred.\n\n7.5. In any cases of violation of the guarantees of the level of the Service, the Customer undertakes to cooperate with the Company to determine the source of the problems, eliminate it and restore the serviceability of the Service or deleted / damaged Content.\n\n7.6. The Compensation is calculated automatically based on the Company's statistics. The Compensation amount is transferred to the Bonus balance (account) of the User in the amount determined on the basis of the data in Table 1 or according to the rules of clause 7.4. of this Agreement within 7 (seven) business days from the beginning of the Reporting period following the one in which the events that served as the basis for the Compensation took place.\n\n7.7. The amount of the Compensation during the Reporting Period on both grounds specified in cl. 7.3. and 7.4. Of the Agreement may not exceed the amount of funds debited by the Company from the Bonus and / or Personal Balance (account) of the Customer for the Reporting Period in which the events took place that served as the basis for the Compensation.\n\n7.8. The unavailability of the Service caused by the actions of the Customer, Users or third parties, or by force majeure circumstances (such as natural disasters, civil unrest, wars, etc.) that led to the inaccessibility of services is not subject to Compensation; suspension of the provision of services associated with the planned maintenance work, as defined in clause 7.2. present agreement; suspension of the provision of services at the request of state or municipal authorities in cases and in the manner determined by applicable law. The Company under no circumstances will reimburse the Users for indirect losses, including lost profits and damage to business reputation.\n\nRevision: from 10/22/2021\n","url":"/en/intro/start/legal/policy-hotbox","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"dad7c7fe-9825-406e-bd91-1b1c796d5b7d","title":"User Agreement for the service “Cloud for cold data” (Icebox)","data":"This document is an official offer of Mail.ru LLC (hereinafter referred to as the \"Company\") to conclude a user agreement (hereinafter referred to as the \"Agreement\") with you (hereinafter referred to as the \"Customer\") on the provision of services for providing access to the Cloud for Cold Data Service within the framework of the VK Cloud Platform (hereinafter referred to as the Platform), on the terms set forth in this Agreement, which is available on the Internet at: https://mcs.mail.ru/help/ru_RU/legal/policy-icebox.\n\n1\\. Terms and definitions\n\nSite — an Internet site located at https://mcs.mail.ru/, which provides the Customer with access to the Platform and the Service. The Company places information on the Site that is mandatory for Customers.\n\nCloud storage for hot data (Service) is a service of the Company known as \"Cloud for cold data\" (IceBox), consisting in providing the Customer and Users with the ability to post Content on the Company's servers, use, reproduce, modify and delete such Content by using programs for computers, allowing to organize the work of the Service and the activities of Users to work with the Content.\n\nThe Company's services may include the provision by the Company of the rights to use certain computer programs that are necessary for the full use of the Service. The specified computer programs can be used by the Customer and Users only as part of the service for providing access to the Service, cannot be transferred to third parties and used outside the Service.\n\nContent — electronic documents, graphics, photographs, scripts, texts, videos, music, sounds and other objects (files) uploaded by the Customer or Users posted on the Service.\n\nProhibited Content — any Content described in clause 4.1. of this Agreement, as well as other materials, the use of which is prohibited by the Applicable Rules.\n\nA bucket is a disk space with files.\n\nService unavailability (Downtime) — the time interval from the moment the User sent a message to the Company's support service by e-mail to support@mcs.mail.ru or through the feedback form in the personal account, the fact of which was subsequently confirmed by the Company, and the moment the Service was resumed after the completion of the restoration work by the Company.\n\nCompensation is the amount of funds transferred by the Company to the User in case of violation of the guarantees of the level of the Service or in case of loss / damage of the Content as a result of an accident due to the fault of the Company.\n\nThe reporting period is a calendar month.\n\nOther terms are defined in the Terms of Use of the Platform.\n\n2\\. Subject of the agreement, conditions for obtaining access to the Service\n\n2.1. Under the terms of this Agreement, the Company provides the Customer around the clock, with an Internet connection, the Cloud for Hot Data Service (hereinafter referred to as the Service), which allows the Customer to place Content on the disk space of the Company's servers by using computer programs, including viewing, changing, deleting Content, manage access to it by other persons (Project Users, third parties) and perform other actions with the Content provided for by the functionality of the Service, and the Customer undertakes to pay for the Company's services for using the provision of access to the Service in the amount and in the manner determined by the Applicable Rules.\n\n2.2. The condition for using the Service is the complete and unconditional acceptance of the provisions of this Agreement by the User. Access to the Service is carried out within the Platform, in connection with which the condition for using the Service is to use the Platform and accept the Terms of Use of the Platform available at https://mcs.mail.ru/help/legal/termofuse, as well as the Company's Privacy Policy available at https://mcs.mail.ru/help/legal/privacy (hereinafter collectively referred to as the \"Applicable Rules\"). In the event of a conflict between the provisions of the Terms of Use of the Platform and this Agreement, the provisions of this Agreement shall prevail. The Terms of Use of the Platform apply to the relations of the Parties that are not regulated by this Agreement.\n\n2.3. With each access and / or actual use of the Service, the Customer agrees with the provisions of this Agreement, as well as with other Applicable rules, as amended, which were in effect at the time of the actual use of the Service. At the same time, the Customer guarantees that with each access and / or actual use of the Service by the User of the Project, which was created by the Customer, such User is familiar with the Rules for end users and agrees with them in the version in effect at the time of the actual use of the Service by the User.\n\n2.4. The applicable rules, including this Agreement, may be changed by the Company at any time without any prior notice to the Customer. Any such changes will take effect on the day following the day such changes are posted on the Site. The Customer undertakes to independently check the Applicable Rules for changes. Failure by the Customer to take steps to familiarize himself with the Applicable Rules and / or the revised edition cannot serve as a basis for the Customer's failure to fulfill his obligations related to the Use of the Platform and individual Services. The Customer has the right to refuse to accept changes and additions to the Applicable Rules, which means the Customer's refusal to use the Platform and / or the relevant Services.\n\n2.5. To use the Service, the Customer undertakes to ensure that the technical and software capacities of the Customer comply with the Company's requirements, which are available on the page at the link mcs.mail.ru/icebox and may be updated from time to time.\n\n3\\. Functionality of the Service, use of the Service\n\n3.1. The Customer can use the Service in any way that is acceptable by the functionality of the Platform and the Service, with the exception of methods expressly prohibited by the Terms of Use of the Platform and the Agreement, as well as by applicable law. In particular, the Customer can place the Content on the disk space of the Company's servers, view, modify, delete the Content, create Buckets and manage access to it for other persons (Project Users, third parties) and perform other actions with the Content provided for by the functionality of the Service.\n\n3.2. The Service user can use the Service in ways determined by the functionality of the Platform and the Service, to the extent of the rights granted to him by the Project Administrator. The User is not entitled to use the Service in ways expressly prohibited by the Terms of Use of the Platform and the Agreement, as well as by applicable law.\n\n3.3. The User of the Service can provide access to other Users of the Service to files and / or Buckets with files indicating the scope of the granted rights.\n\n3.4. The Service user who has gained access to the Bucket can use the files included in such Bucket to the extent of the rights granted to him.\n\n4\\. Rights and obligations of the Parties in connection with the use of the Content\n\n4.1. The placement of Content on the servers of the Company and its use must not violate any applicable law, in particular, the Customer and / or Users are not entitled to:\n\nupload, store, transfer, provide access to other persons or otherwise bring to the public knowledge (post on the Service) the Content and other results of intellectual activity of the copyright holders in the absence of the explicitly expressed consent of these persons and / or the required amount of rights to such actions;\nupload, store, transfer, provide access to other persons to the Content prohibited by applicable law, or the Content, storage or use of which requires obtaining special permissions from authorized bodies or persons, if the Customer does not have the appropriate permissions.\n\nThe content described in this clause is hereinafter referred to as “Prohibited Content”.\n\n4.2. The Company has the right to require the Customer to provide information and documents confirming the compliance of the Content with the requirements of this Agreement.\n\nThe Company has the right to monitor the Content in order to verify that the Customer and Users comply with the requirements of this Agreement for the Content and to prevent the Prohibited Content. The Customer undertakes not to create obstacles for the Company in the implementation of such monitoring, while at the same time he has the right to use tools to preserve the confidentiality of the Content (such as encryption, firewalls).\n\n4.3. The Company has the right to block or remove Prohibited Content on its own initiative, as well as at the request of authorized government agencies. Any losses caused to the Customer in connection with the removal of the Prohibited Content by the Company will not be reimbursed by the Company.\n\n5\\. Cost of services\n\n5.1. The use of the Service within the Platform is Paid. In order to determine the cost of using the Service, the Company sets the price and the corresponding list of services in accordance with the Tariffs.\n\n5.2. The tariff is set by the Company and communicated to the Customer. The current Tariffs are posted on the page: mcs.mail.ru/icebox. The tariff can be changed by the Company unilaterally, which is brought to the attention of the Customer by updating the information on the page: mcs.mail.ru/icebox. The Company may unilaterally change the Tariffs that are applied from the Reporting Period following the period in which the Tariffs were changed.\n\n6\\. End of service and consequences of violation of payment terms\n\n6.1. In the absence of bonuses / funds on the Bonus and / or Personal balance (account) of the Customer to write off the cost of services for providing access to the Service (hereinafter — Zero balance), the access of the Users of the Service / Platform to Buckets and files is terminated.\n\n6.2. With a zero balance of the Customer, the Company has the right to delete files that occupy disk space with prior notification of the Customer by e-mail to which the Project is registered.\n\n6.3. To restore access to the Service, the Customer must replenish funds on his Personal balance (account) for the amount necessary to pay for services in the amount required for the Customer in accordance with the Tariffs in force at the time of the payment.\n\n7\\. Responsibility for interruptions in the operation of the Service (guarantees of the level of the Service)\n\n7.1. The Company guarantees to the User that the services for providing access to the Service are provided 24 (twenty four) hours a day, 7 (seven) days a week.\n\nThe Company guarantees the continuous operation of the Service, provided that the Customer observes its permissible functionality, that is, when no more than 100 (one hundred) calls to the Service are made per second. An increase in the functionality of the Service (volume of performance) is possible by signing a separate agreement between the Customer and the Company. The Customer has the right to inform the Company about his desire to conclude such an agreement by sending a request by e-mail to support@mcs.mail.ru.\n\n7.2. The Company reserves the right to interrupt the operation of the Service to carry out the necessary maintenance work, including on weekdays. Such cases will not be considered interruptions in the provision of services if the Company notifies the User in advance 24 (twenty four) hours before the start of preventive work, entailing interruptions in the operation of the Service, by publishing relevant information on the Site (including the duration of preventive work). The time of unavailability of the Service associated with maintenance work is not subject to compensation according to the rules of this section of the Agreement.\n\n7.3. The unavailability of the Service (Downtime) is defined as the time interval from the moment the User sends a message to the Company's support service through the feedback form available in the Personal Account containing a notification about the unavailability of the Service, or an e-mail message to the address: support@mcs.mail.ru, the fact of which was subsequently confirmed by the Company, and the moment the Service was resumed after the completion of the restoration work by the Company. If the Company has its own data about the beginning of Downtime, indicating an earlier start time of the Service's unavailability than the time of sending an e-mail to support@mcs.mail.ru or the feedback form in the personal account, the Company may use such data. Disagreements about the start and end times of Downtime are resolved through negotiations between the Parties. If Downtime started in one Reporting Period and ended in the next Reporting Period, then the unavailability of the Service fully refers to the Reporting Period in which most of Downtime took place.\n\nIf the Service is unavailable, the Company undertakes to pay the User Compensation, the amount of which is determined on the basis of the table below (hereinafter referred to as the Table):\n\nAvailability time (in%) *Unavailability time (min. Or hour) in one Reporting period in totalCompensation Amount99.95% and aboveless than 45 minutesnot paid99.9%–99.95%45 minutes to 7.3 hours (438 minutes.)3% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was99.0%–99.9%from 7.3 to 36 hours10% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was95.0%–99.0%from 36 to 73 hours20% of the amount debited from the Personal Balance for the Reporting Period in which Downtime wasLess than 95%more than 73 hours50% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was\n\n\\* Calculated based on 43 800 minutes in the Reporting Period\n\n7.4. In case of loss / damage to the Content as a result of an accident through the fault of the Company, the Customer is provided with Compensation in the amount of 100% of the amount debited from the Bonus and / or Personal balance (account) of the User in the Reporting period during which the loss / damage to the Content occurred.\n\n7.5. In any cases of violation of the guarantees of the level of the Service, the Customer undertakes to cooperate with the Company to determine the source of the problems, eliminate it and restore the serviceability of the Service or deleted / damaged Content.\n\n7.6. The Compensation is calculated automatically based on the Company's statistics. The Compensation amount is transferred to the Bonus balance (account) of the User in the amount determined on the basis of the data in Table 1 or according to the rules of clause 7.4. of this Agreement within 7 (seven) business days from the beginning of the Reporting period following the one in which the events that served as the basis for the Compensation took place.\n\n7.7. The amount of the Compensation during the Reporting Period on both grounds specified in cl. 7.3. and 7.4. Of the Agreement may not exceed the amount of funds debited by the Company from the Bonus and / or Personal Balance (account) of the Customer for the Reporting Period in which the events took place that served as the basis for the Compensation.\n\n7.8. The unavailability of the Service caused by the actions of the Customer, Users or third parties, or by force majeure circumstances (such as natural disasters, civil unrest, wars, etc.) that led to the inaccessibility of services is not subject to Compensation; suspension of the provision of services associated with the planned maintenance work, as defined in clause 7.2. present agreement; suspension of the provision of services at the request of state or municipal authorities in cases and in the manner determined by applicable law. The Company under no circumstances will reimburse the Users for indirect losses, including lost profits and damage to business reputation.\n\nEdition: 22.10.2021\n","url":"/en/intro/start/legal/policy-icebox","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"b44b36d6-c385-42e5-b0c1-dd2f212b32fc","title":"User agreement for the “Infra” service","data":"This document is an official offer of Mail.ru LLC (hereinafter referred to as the \"Contractor\") to conclude a user agreement (hereinafter referred to as the \"Agreement\") with you (hereinafter referred to as the \"Customer\") on the provision of services for providing access to the Infra virtual infrastructure Service within the Platform \"VK Cloud\" (hereinafter referred to as the \"Platform\"), on the terms set out in this Agreement, which is available on the Internet at: https://mcs.mail.ru/docs/en/intro/start/legal/policy-infra.\n\n1\\. Terms and definitions\n\nSite — an Internet site located at https://mcs.mail.ru/, providing the Customer with access to the Service. The Contractor posts on the Site information that is binding on the Customer.\n\nThe VK Cloud Platform (Platform) is a computer program available to the Customer through the Website and operated by the Contractor, which provides the Customer with access to the selected Services.\n\nVirtual Infrastructure Service (Service) — a service of the Contractor, known under the name \"Virtual Infrastructure Service\" (Infra), consisting in presenting the Customer with the possibility of creating virtual servers and virtual disks on the Contractor's servers.\n\nContent — electronic documents, graphics, photographs, scripts, texts, videos, music, sounds, computer programs and other objects (files) uploaded by the Customer posted on the Service.\n\nProhibited Content — any Content described in clause 2.8. of this Agreement, as well as other materials, the use of which is prohibited by the Applicable Rules.\n\nService unavailability (Downtime) — the time interval from the moment the Customer sent a message to the Contractor's support service by e-mail to support@mcs.mail.ru or through the feedback form in the personal account, the fact of which was subsequently confirmed by the Contractor, and the moment the Service was resumed after the completion of the restoration work by the Contractor.\n\nCompensation — the amount to be credited to the Customer's Personal account in the case of Downtime in the manner prescribed by this Agreement. Compensation does not imply payment of funds to the Customer.\n\nThe reporting period is a calendar month.\n\nApplicable rules — the rules for using the Contractor's Service, available on the Internet at: https://mcs.mail.ru/docs/en/intro/start/legal/policy-infra, containing the conditions for access and use of the Service and set out in the following documents:\n\nTerms of use of the VK Cloud Platform (https://help.mail.ru/mcs/legal/termofuse/);\nContractor's privacy policy (https://help.mail.ru/mcs/legal/privacy/).\n\nIn the event that this Agreement uses terms that are not defined in Section 2 of this Agreement, the definitions of these terms of the Applicable Rules are used.\n\n2\\. Subject of the Agreement\n\n2.1. Under the terms of this Agreement, the Contractor provides the Customer around the clock, with an Internet connection, a \"Virtual Infrastructure Service\" (Infra) (hereinafter referred to as the Service), which allows the Customer to create virtual servers and virtual disks on the Contractor's servers, and the Customer undertakes to pay for the Contractor's services for using the provision of access to the Service in the amount and in the manner determined by the Applicable Rules.\n\n2.2. The condition for using the Service is the complete and unconditional acceptance by the Customer of the provisions of this Agreement. Access to the Service is carried out within the Platform, in connection with which the condition for using the Service is the use of the Platform and acceptance of the Terms of Use of the Platform available at https://help.mail.ru/mcs/legal/termofuse/, as well as the Contractor's Privacy Policy, available at https://help.mail.ru/mcs/legal/privacy/ (hereinafter collectively referred to as the \"Applicable Rules\"). In the event of a conflict between the provisions of the Terms of Use of the Platform and this Agreement, the provisions of this Agreement shall prevail. The Terms of Use of the Platform apply to the relations of the Parties that are not regulated by this Agreement.\n\n2.3. With each access and / or actual use of the Service, the Customer agrees to the provisions of this Agreement and the Applicable Rules as amended, which were in effect at the time of the actual use of the Service.\n\n2.4. The applicable rules, including this Agreement, may be changed by the Contractor at any time without any prior notice to the Customer. Any such changes take effect on the day following the day such changes are posted on the Site. The Customer undertakes to independently check the Applicable Rules for changes. Failure by the Customer to take steps to familiarize himself with the Applicable Rules and / or the revised edition cannot serve as a reason for the Customer's failure to fulfill his obligations related to the use of the Platform and the Service. The Customer has the right to refuse to accept changes and additions to the Applicable Rules, which means the Customer's refusal to use the Platform and / or the Service.\n\n2.5. Functionality of the Service, use of the Service:\n\nThe Customer can use the Service to the extent of the rights granted to him by the Platform and Service Administrator, in any way that is acceptable by the functionality of the Platform and Service, with the exception of methods expressly prohibited by the Terms of Use of the Platform and the Agreement, as well as by applicable law. In particular, the Customer can place the Content on virtual disks located on the disk space of the Contractor's servers, download computer programs to virtual disks, execute them, view, modify, delete the Content and perform other actions with the Content provided for by the functionality of the Service.\n\n2.6. The placement of the Content on the Contractor's servers and its use must not violate any applicable law, in particular, the Customer is not entitled to:\n\nupload, store, transfer, provide access to other persons or otherwise bring to the public knowledge (post on the Service) Content and other results of intellectual activity of rightholders in the absence of the explicit consent of these persons and / or the required amount of rights to such actions;\nupload, store, transfer, provide access to other persons to the Content prohibited by applicable law, or the Content, the storage or use of which requires obtaining special permissions from authorized bodies or persons, if the Customer does not have the appropriate permissions.\n\nThe content described in this clause is hereinafter referred to as “Prohibited Content”.\n\nThe Contractor has the right to monitor the Content in order to verify that the Customer complies with the requirements of this Agreement for the Content and to prevent the Prohibited Content. The Customer undertakes not to create obstacles for the Contractor in the implementation of such monitoring, while he has the right to use tools to preserve the confidentiality of the Content (such as encryption, firewalls).\n\n3\\. Rights and obligations of the Parties\n\n3.1. The Contractor undertakes:\n\n3.1.1. Provide the Customer with the opportunity to use the Contractor's Service.\n\n3.1.2. Ensure constant, round-the-clock availability of the Service to the Customer, if there is an Internet connection.\n\n3.1.3. Provide technical support to the Customer.\n\n3.1.4. Not to disclose the Customer's data, except as otherwise provided by the legislation of the Russian Federation, this Agreement and the Applicable Rules.\n\n3.2. The Contractor has the right:\n\n3.2.1. Block access to the Service and to the Customer's Content if the Contractor becomes aware of a violation of the rules for posting Customer's Content, the terms of this Agreement, Applicable rules or legislation of the Russian Federation.\n\n3.2.2. In a notifying, unilateral manner, amend the Applicable Rules.\n\n3.2.3. Require the Customer to provide information and documents confirming the compliance of the Content with the requirements of this Agreement.\n\n3.2.4. Block or remove Prohibited Content on your own initiative, as well as at the request of authorized government agencies. Any losses caused to the Customer in connection with the removal of the Prohibited Content by the Contractor are not reimbursed by the Contractor.\n\n3.3. The customer undertakes:\n\n3.3.1. Pay for the services of the Contractor in accordance with the terms of this Agreement.\n\n3.3.2. On a regular basis, independently familiarize yourself with the current version of the Applicable Rules and comply with them in full.\n\n3.3.3. Be responsible for your actions and / or inactions on the use of the Contractor's Service.\n\n4\\. Cost of services and payment procedure\n\n4.1. The use of the Services is paid. The Contractor, in order to determine the cost of using the Service, sets the price and the corresponding list of services in accordance with the Tariffs.\n\n4.2. The tariff is set by the Contractor and communicated to the Customer. The current Tariffs are posted on the following pages: https://mcs.mail.ru/infra/, https://mcs.mail.ru/containers/ https://mcs.mail.ru/bigdata/. The tariff can be changed by the Contractor unilaterally, which is brought to the attention of the Customer by updating the information on the pages: https://mcs.mail.ru/infra/, https://mcs.mail.ru/containers/, https://mcs.mail.ru/bigdata/. The Contractor can unilaterally change the Tariffs that are applied from the Reporting period following the period in which the Tariffs were changed.\n\n4.3. In the absence of funds on the Personal balance (account) of the Customer to write off the cost of services for providing access to the Service (hereinafter — Zero balance), the Customer's access to the Service and files is terminated.\n\n4.4. If the Customer has a Zero balance, the Contractor has the right to delete files that occupy disk space with prior notification of the Customer by e-mail to which the Project is registered.\n\n4.5. To restore access to the Service, the Customer must replenish funds on his Personal Balance (account) in the amount necessary to pay for services in the amount required for the Customer in accordance with the Tariffs in force at the time of the payment.\n\n5\\. Responsibility of the Parties\n\n5.1. The extent of the Contractor's liability is limited solely by the relevant Applicable rules posted on the Site in the section https://help.mail.ru/mcs/legal/.\n\n5.2. The lost profit of the Customer is not subject to compensation by the Contractor.\n\n5.3. The Contractor is not liable to the Customer for blocking access to the Customer's Content, or destroying the Content if the Contractor has received relevant claims from third parties, or warnings, instructions or other acts of state authorities in relation to the Customer's Content.\n\n5.4. The Contractor is not responsible for any violations of the current legislation that have or may take place on projects implemented by the Customer. In the event of claims against the Contractor from third parties in connection with such violations that have occurred, the Customer is obliged to release the Contractor from liability and settle such claims independently, all losses incurred by the Contractor through the fault of the Customer, in full, including, but not limited to: settlement of claims, disputes, enforcement of sanctions by state authorities, must be reimbursed to the Contractor, if any.\n\n6\\. Other conditions\n\n6.1. The Customer guarantees that his Content posted using the Contractor's Service does not contain information and images of a pornographic nature, does not promote violence, racial, sexual, religious or other forms of inequality, or other activities expressly prohibited by the legislation of the Russian Federation, does not violate copyright laws. law and other applicable legislation, as this may affect the reputation of the Contractor.\n\n6.2. The Customer guarantees that the placement of the Customer's Content takes place in compliance with the conditions of the current legislation of the Russian Federation.\n\n6.3. The forms of the Customer's use of images, texts, names, emblems, and characters of the Contractor must be agreed with the representatives of the Contractor.\n\n6.4. The Contractor guarantees that the services for providing access to the Service are provided 24 (twenty four) hours a day, 7 (seven) days a week.\n\n6.5. The Contractor reserves the right to interrupt the work of the Service to carry out the necessary maintenance work, including on weekdays. Such cases will not be considered interruptions in the provision of services if the Contractor notifies the Customer in advance 24 (twenty four) hours before the start of preventive work, entailing interruptions in the operation of the Service, by publishing relevant information on the Site (including the duration of preventive work). The time of unavailability of the Service associated with the maintenance work is not subject to compensation according to the rules of clause 6.6. Agreements.\n\n6.6. The unavailability of the Service (\"Downtime\") is defined as the time interval from the moment the Customer sends a message to the Contractor's support service through the feedback form available in the Personal Account containing a notification about the unavailability of the Service, or by e-mail to the address: support@mcs.mail.ru, the fact of which was subsequently confirmed by the Contractor, and the moment the Service was resumed after the completion of the restoration work by the Contractor. If the Contractor has his own data on the beginning of Downtime, indicating an earlier start time of the Service's unavailability than the time of sending an e-mail message to support@mcs.mail.ru or the feedback form in his personal account, the Contractor may use such data. Disagreements about the start and end times of Downtime are resolved through negotiations between the Parties. If Downtime began in one Reporting Period and ended in the next Reporting Period, then the unavailability of the Service fully refers to the Reporting Period in which most of Downtime took place.\n\n6.7. If the Service is unavailable, the Contractor undertakes to transfer Compensation to the Customer (credited to the Personal Balance (account)), the amount of which is determined on the basis of the table below (hereinafter — Table No. 1):\n\nAvailability time (in%) *Unavailability time (min. Or hour) in one Reporting period in totalCompensation Amount99.95% and aboveless than 22 minutesnot paid99.9%–99.95%from 22 to 45 minutes3% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was99.0%–99.9%from 45 minutes to 7.5 hours10% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was95.0%–99.0%from 7.5 hours to 36 hours20% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was90.0%–95.0%from 36 hours to 73 hours25% of the amount debited from the Personal Balance for the Reporting Period in which Downtime wasless than 90%more than 73 hours50% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was\n\n\\* Calculated based on 43 800 minutes in the Reporting Period\n\n6.8. In any cases of violation of the guarantees of the level of the Service, the Customer undertakes to cooperate with the Contractor to determine the source of problems, eliminate it and restore the serviceability of the Service or deleted / damaged Content.\n\n6.9. Compensation is calculated automatically based on the Contractor's statistics. The Compensation amount is transferred to the Customer's account in the amount determined on the basis of the rules of clause 6.7. of this Agreement within 7 (seven) business days from the beginning of the Reporting Period following the one in which the events that served as the basis for the Compensation took place.\n\n6.10. The amount of the Compensation during the Reporting Period on both grounds specified in cl. 6.6. and 6.7. Of the Agreement, cannot exceed the amount of funds paid by the Customer for the Reporting Period in which the events took place that served as the basis for Compensation.\n\n6.11. The unavailability of the Service caused by the actions of the Customer or third parties, or by force majeure circumstances that led to the inaccessibility of services is not subject to Compensation; suspension of the provision of services associated with the planned maintenance work, as defined in clause 6.5. present agreement; suspension of the provision of services at the request of state or municipal authorities in cases and in the manner specified by applicable law. The Contractor under no circumstances shall reimburse the Customer of the Service for indirect losses, including lost profits and damage to business reputation.\n","url":"/en/intro/start/legal/policy-infra","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"6d272412-04c5-4c83-8ab7-528ae36f75aa","title":"License agreement for the Vision program","data":"This License Agreement (hereinafter the \"Agreement\") governs the relationship between Mail.Ru LLC (OGRN 1027739850962, Russia, 125167, Moscow, Leningradsky Prospekt 39, p. 79), hereinafter referred to as the \"Licensor\", and you, hereinafter referred to as the \"Licensee\", by using the computer program \"Vision\" (hereinafter referred to as the \"Program\"), on the following conditions:\n\n1\\. Terms and definitions\n\nLicensor — Mail.Ru limited liability company, which is a party to this Agreement, which grants the Licensees the right to use the Program under the terms of a non-exclusive license.\n\nLicensee is a person who has the necessary legal capacity to enter into this Agreement, who has a sufficient scope of rights, who, in accordance with this Agreement, is granted the right to use the Program within the limits provided for in this Agreement. The Licensee is a Party to this Agreement.\n\nAPI is an application programming interface that allows you to receive the Licensor's Program and continue to use it by the Licensee on the terms set forth in this Agreement.\n\nProgram — a computer program developed by the Licensor using machine learning methods.\n\nOther applicable terms and definitions are contained in the Terms of Use.\n\n2\\. Terms of accession to this Agreement\n\n2.1. The condition for using the Program is full and unconditional acceptance by the Licensee of the provisions of this Agreement. Access to the Program is carried out within the Platform, and therefore the condition for obtaining a license is the Licensee's consent to the Terms of Use of the VK Cloud platform (hereinafter referred to as the Terms of Use and the Platform), which are available at https://help.mail.ru/mcs/legal/vision_termofuse, as well as the Licensor's Privacy Policy available at https://help.mail.ru/mcs/legal/privacy (hereinafter collectively referred to as the \"Applicable Rules\"). In the event of a conflict between the provisions of the Terms of Use and this Agreement, the provisions of this Agreement shall prevail. The Terms of Use apply to the relations of the Parties that are not regulated by this Agreement.\n\n2.2. With each access and / or actual use of the Program, the Licensee agrees with the provisions of this Agreement, as well as with other\n\nApplicable rules, as amended, which were in effect at the time of the actual use of the Program.\n\n2.3. The applicable rules, including this Agreement, may be changed by the Licensor at any time without any prior notice to the Licensee. Any such changes come into force on the day following the day of publication of such changes in the \"Vision\" section of the Licensee's Personal Account. Licensee agrees to independently check the Applicable Rules for changes. Failure by the Licensee to familiarize himself with the Applicable Rules and / or the revised edition cannot serve as a basis for the Licensee's failure to fulfill his obligations related to the use of the Platform and / or the Program. The Licensee has the right to refuse to accept the changes and additions made by the Licensor to the Applicable Rules, which means the Licensee's refusal to use the Platform and / or the Program.\n\n3\\. Subject of the agreement\n\n3.1. Under the terms of this Agreement, the Licensor grants the Licensee, on the terms of a paid simple non-exclusive license, the right to use the Program, as well as the API, within the limits specified in this Agreement, and the Licensee undertakes to pay the Licensor a license fee in the amount and in the manner established by the Licensor.\n\n3.2. The Licensee's access to the Program is carried out around the clock through the API, if there is an Internet connection.\n\n3.3. The Licensor grants the Licensee the right to use the Program in the following ways: reproduction on the Licensor's hardware and gaining access through the API, using the provided functionality of the Program. Methods of using the Program not expressly specified in this agreement are not considered provided to the Licensee.\n\n3.4. The right to use the basic features of the Program is provided by the Licensor to the Licensee free of charge.\n\n3.5. The right to use the advanced features of the Program is provided by the Licensor to the Licensee for a fee. The Licensor has the right to change the amount of the license fee payable by the Licensee, about which he notifies the Licensee by sending notifications to the Licensee's e-mail address or by updating the information in the \"Vision\" section in the Licensee's Personal Account. The new rates are applied from the month following the month in which the Licensee was notified of the change in the amount of the license fee. Up-to-date information on tariffs is available to the Licensee in the \"Vision\" section in the Licensee's Personal Account.\n\n3.5. The license is valid throughout the world during the term of this Agreement.\n\n3.6. The Licensee is granted a license under the conditions described in this section, provided that the Licensee complies with all licensing requirements and restrictions, including the requirements for the software and hardware of the Licensee, specified in this Agreement and on the Site.\n\n4\\. Software capabilities of the Program and the order of their use\n\n4.1. The Program is a computer program developed by the Licensor using machine learning methods, which provides Licensees with the ability to use computer vision technology.\n\n4.2. To gain access to the Program, the Licensor provides the Licensee with an API by issuing authorization keys by e-mail or in the Licensee's Personal Account.\n\n5\\. Rights and obligations of the Licensee\n\n5.1. The licensee has the right:\n\n5.1.1. use the Program in the ways, at the time and on the territory directly specified in this Agreement.\n\n5.1.2. perform other actions not prohibited by the legislation of the Russian Federation or this Agreement related to the use of the Program, which should not contradict the usual use of the Program and should not unreasonably infringe on the legitimate interests of the Licensor, as well as third parties.\n\n5.2. The licensee is obliged to:\n\n5.2.1. comply with the terms of this Agreement without any restrictions;\n\n5.2.2. ensure that only individuals who are employees of the Licensee or persons who have entered into civil contracts with the Licensee who need access to the Program solely for the purpose of performing labor functions or obligations under a civil law contract (hereinafter — end users) have access to the Program. At the same time, the Licensee guarantees that end users have been familiar with the terms of this Agreement and undertake to comply with their provisions. The licensee is responsible for illegal actions committed end users using the Licensee's Personal Account as their own.\n\n5.2.3. follow all instructions and instructions of the Licensor. If the Licensee fails to comply with such instructions, the Licensor has the right to suspend, limit, terminate the granting of the Licensee the rights to use the Program or its individual functions;\n\n5.2.4. periodically get acquainted with the content of this Agreement in the \"Vision\" section in the Licensee's Personal Account and follow the changes made to it;\n\n5.2.5. pay the Licensor the license fee in the amount established by the Licensor and in the manner provided for in section 8 of the Terms of Use;\n\n5.2.6. not to provide third parties with the right to use the Program (not to conclude sub-license agreements), except for cases when such third parties are employees of the Licensee;\n\n5.2.7. not to violate the intellectual property rights of the Licensor in relation to the Program, API or any of their elements, in particular, the Licensee has no right to copy, disassemble, reverse-engineer, modify the source code of the Program, unless otherwise provided by applicable law;\n\n5.2.8. not to carry out any other actions that contradict the goals of creating the Program or violate the current legislation of the Russian Federation.\n\n5.3. The Licensee warrants that it has all the necessary powers to enter into this Agreement.\n\n5.4. Any data obtained using the Program, the Licensee uses at his own risk and is solely responsible for the possible consequences of using the specified data, including for damage that this may cause to third parties.\n\n5.5. The Licensee has the right to use the Program only for purposes not prohibited by applicable law. If the Licensor suspects that the Program is being used by the Licensee for illegal purposes, the Licensor has the right to immediately suspend the Licensee's use of the Program and demand the provision of documents and explanations confirming the legality of using the Program.\n\n6\\. Rights and obligations of the Licensor, limitation of liability of the Licensor\n\n6.1. The licensor has the right:\n\n6.1.1. at any time, without any notice to the Licensee, change and / or supplement the algorithms of the Program, introduce new and / or cancel old functions of the Program, change the procedure for accessing the Program (including making changes to the API), as well as the design and user the interface through which the Program is available;\n\n6.1.2. suspend, restrict or terminate the Licensee's access to the Program by suspending access to the API if the Licensee violates the terms of this Agreement (including, but not limited to, in the absence of funds on the Licensee's balance, when the Licensee commits actions that may be regarded as violating the terms of this agreement or requirements of applicable law);\n\n6.1.3. send notifications to the Licensee through the Personal Account, or by sending an e-mail, sms about the introduction of new and / or cancellation of old functions of the Program, API or user interface, as well as containing advertising information about the functions of the Program / Platform, about products and partner programs / promotions of the Licensor. The Licensee agrees to receive, through the Licensor's services and / or third-party services, e-mail, sms and other types of mailings of advertising and information content, including from the Licensor's partners, to whom the Licensee has agreed to send him messages;\n\n6.1.4. make comments to the Licensees, warn, notify, inform them about the Licensees' non-compliance with this Agreement. The Licensor's instructions given to the Licensee during the process of using the Program / Site are mandatory for the Licensee;\n\n6.1.5. collect and process data related to the Licensee's use of the Program;\n\n6.1.6. take measures not prohibited by law to protect their own intellectual rights in relation to the Program.\n\n6.2. The Licensor undertakes to grant the Licensee, under the terms of a simple non-exclusive license, the right to use the Program in the amount and in the manner determined by this Agreement, upon payment by the Licensee of the license fee in the amount and procedure established by the Licensor.\n\n6.3. The Licensor is not responsible for any losses (both direct damage and lost profits) that may be incurred by the Licensee when using the Program, including as a result of unauthorized access by any third parties to the functions of the Program, in the event of loss or interception of data downloaded / transferred by the Licensee to the Program via the API.\n\n6.4. The Program is provided to Licensees on an \"as is\" basis, i.e. The Licensor does not provide any guarantees that the Program can be suitable for specific purposes of use, and also cannot guarantee and does not promise any specific results from the use of the Program.\n\n6.5. The Licensor is not responsible for the loss of the Licensee's data, including due to possible errors or malfunctions in the hardware of the Licensee or third parties involved by him. The Company is not responsible for any types of losses incurred as a result of the use or inability of the Licensee to use the Program or its individual parts / functions, including due to possible errors or malfunctions, except as expressly provided for by law.\n\n7\\. Final provisions\n\n7.1. This Agreement may be changed by the Licensor at any time without any prior notice to the Licensee. Any changes to the Agreement made by the Licensor unilaterally come into force on the day following the day of publication of such changes on the Site. The Licensee undertakes to independently check the Agreement for changes. Failure by the Licensee to take steps to familiarize himself with the Agreement and / or the amended version of the Agreement cannot serve as a basis for the Licensee's failure to fulfill its obligations and the Licensee's failure to comply with the restrictions established by the Agreement.\n\n7.2. The Agreement comes into force from the moment the Licensee accepts the terms of this Agreement and is valid for a year. If none of the Parties sends a prior written notice 15 (fifteen) calendar days in advance of the termination of the Agreement, it is extended for the next year.\n\n7.3. Either Party has the right to refuse to execute the Agreement by sending a prior written notice to the other Party 15 calendar days prior to the expected date of termination of the Agreement.\n\n7.4. The Licensor has the right to unilaterally and extrajudicially refuse to execute the Agreement in the event of repeated material violation by the Licensee of the terms of this Agreement.\n\n7.5. The Licensee has the right to unilaterally and extrajudicially refuse to execute the Agreement in the event of repeated material violation by the Licensor of the terms of this Agreement.\n\n7.6. In case of termination of this Agreement by agreement of the Parties or by virtue of the Licensor's refusal from the Agreement, on the basis of clause 7.3. of this Agreement, or by virtue of the Licensee's refusal from the Agreement on the basis of clause 7.5. of this Agreement, the license fee paid by the Licensee is refunded in proportion to the unused period.\n\n7.7. In case of termination of this Agreement due to the refusal of the Licensee from the Agreement, on the basis of clause 7.3. of this Agreement, or by virtue of the Licensor's refusal from the Agreement, on the basis of clause 7.4. of this Agreement, the license fee paid by the Licensee is non-refundable.\n\n7.8. The invalidity of one or several provisions of the Agreement, recognized in the prescribed manner by a court decision that has entered into force, does not entail the invalidity of the agreement as a whole for the Parties. In the event that one or several provisions of the Agreement are recognized as invalid in the prescribed manner, the Parties undertake to fulfill their obligations under the Agreement as close as possible to those implied by the Parties when concluding and / or agreeing to amend the Agreement in a manner.\n\n7.9. This Agreement and the relationship between the Parties in connection with this Agreement and the use of the Service are governed by the legislation of the Russian Federation.\n\n7.10. Regarding the form and method of concluding this Agreement, the norms of the Civil Code of the Russian Federation (\"Civil Code of the Russian Federation\") governing the procedure and conditions for concluding an agreement by accepting an offer are applied.\n\n7.11. All disputes between the parties under this agreement are subject to resolution through correspondence and negotiations using the mandatory pre-trial (claim) procedure. If it is impossible to reach agreement between the parties through negotiations within 60 (sixty) calendar days from the moment the other Party receives a written claim, the consideration of the dispute must be referred by any interested party to the court at the location of the Licensor (excluding the jurisdiction of the case to any other courts), if otherwise not provided by applicable law.\n\nEdition: from 22.10.2021\n","url":"/en/intro/start/legal/policy-vision","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"5c46548e-f25a-45e6-96d9-a0d37d6513b0","title":"VK Cloud platform privacy policy","data":"1\\. The subject of regulation of this Privacy Policy (hereinafter referred to as the \"Policy\") is the relationship between the Limited Liability Company Mail.Ru (OGRN 1027739850962, Russia, 125167, Moscow, Leningradsky Prospekt 39, str. 79) \"Mail.Ru\" or \"Company\" providing Services within the VK Cloud Platform (hereinafter \"Platform\", \"Services\", and separately — \"Service\") located on the Internet at https://mcs.mail.ru/, and you, hereinafter referred to as the \"User\", regarding the processing of information about the User, including personal data and other data, in the process of using the Services of the VK Cloud Platform by such a person (hereinafter referred to as the \"Information\").\n\n2\\. This Policy is an integral part of the Mail.Ru Services Privacy Policy posted on the Internet at: https://help.mail.ru/legal/terms/common/privacy. This Policy contains special rules that describe the terms of use of information about the Users of the Services, have a direct effect and priority in relation to the rules contained in the Privacy Policy of Mail.Ru Services and other rules governing the operation of the Service.\n\n3\\. In the absence of provisions in this Policy that fully determine (disclose) the conditions for processing information about the Users of the Services, the provisions of the Privacy Policy of Mail.Ru Services shall apply. A number of terms used in this Policy have the meaning defined in the Privacy Policy of Mail.Ru Services posted on the Internet at: https://help.mail.ru/legal/terms/common/privacy, in particular the terms \"Account data” and “Other data”.\n\n4\\. Relationships related to the processing of information, including but not limiting the collection, use, storage, distribution and protection of information about Users of the Platform Services, are governed by this Policy, the Terms of Use of the VK Cloud platform, agreements on the use of individual Services, as well as the current legislation of the Russian Federation. Before starting to use the Services, the User is obliged to familiarize himself with the terms of this Policy, as well as the terms of agreements, terms and conditions of the Services used by him.\n\n5\\. Upon registration, each access and / or actual use of any of the Services of the Platform, the User agrees to the terms of this Policy, as well as to the terms of agreements, terms and conditions of the Services used by him, which are posted on the pages (in sections) of the respective Services, in editions that were in effect at the time of the actual use of such Services.\n\nAll terms and definitions not specified in the Policy are contained, respectively, in the Terms of Use and / or agreements on the use of individual Services.\n\n6\\. By accepting the terms of this Policy, as well as using the Platform and / or any of the Platform's Service, Users accept and agree to the processing of Information that becomes available to the Company in the process of using the Platform or Services by the specified persons and includes:\n\nCollected informationPurpose of processingCredentials, namely:1) User data provided by the User to create an account during the registration process on the Platform: email address.2) Additional information filled in by the User when editing his account in the process of using the Platform: e-mail address, first and last name, company, position.Execution of the agreement with the User to provide access to the Platform, in particular, to confirm that the account belongs to the applicant who has applied for the restoration of access to the account, to manage and administer services, and more.3) Other data related to the User, which become available to the Company in the process of interaction of the User with the Platform, including, but not limited to, the following data: (ip-address, information about the User's browser (including the interface language, screen sizes, information about cookies (enabled / disabled), information contained in cookies, User Agent browser, etc.), sizes and supported colors of the User's personal device screen, time zone, system language, information about the operating system, support by the User's device of certain technologies, and protocols (Retina, WebSocket, HTTP2), the presence / absence of a number of programs on the User's device.Information about the User's activity while using the Service, namely, clicks and transitions within the Service interface.Execution of the agreement with the User on the provision of the Service, in particular, in cases of internal control, improvement of the operation and content of the Service, prevention and suppression of any errors that may arise when using the Service, the need to notify the User of changes in the Service and personalize the use of the Service.\n\n7\\. The processing of User data when using the Service is carried out on the basis of the legislation of the Russian Federation.\n\n8\\. The company reserves the right to make changes and additions to this Policy by notifying the User by posting a new version of the amended Policy on the website. The new version of the Policy comes into force on the day following the day of its posting on the website at: https://mcs.mail.ru/help/ru_RU/legal/policy-privacy. The User undertakes to independently regularly familiarize himself with the new editions of the Policy.\n\nEdition: from 22.10.2021\n","url":"/en/intro/start/legal/policy-privacy","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"551fc669-ef6a-4256-8752-4f248c480155","title":"Company details","data":"Requisites for the conclusion of contracts and invoicing\n\n| Full name                  | Limited Liability Company «VK»                                                                         |\n|----------------------------|------------------------------------------------------------------------------------------------------- |\n| Abbreviated name           | LLC «VK»                                                                                               |\n| OGRN                       | 1027739850962 от 20.12.2002                                                                            |\n| Legal address              | 125167, Moscow, Leningradsky prospect, 39, building 79                                                 |\n| Address for correspondence | Russian Federation, 125167, Moscow, Leningradsky prospect, 39, building 79, BC «SkyLight»              |\n| INN                        | 7743001840                                                                                             |\n| KPP                        | 997750001                                                                                              |\n| Checking account           | 40702810100000003759                                                                                   |\n| Name of the bank           | Gazprombank (Joint Stock Company)                                                                      |\n| Correspondent account      | 30101810200000000823                                                                                   |\n| BIK                        | 044525823                                                                                              |\n| OKPO code                  | 52685881                                                                                               |\n| OKATO                      | 45277598000                                                                                            |\n| OKVED                      | 72.40                                                                                                  |\n| Telephones                 | (495) 725-63-57                                                                                        |\n| Fax number                 | (495) 725-63-59                                                                                        |\n| Head of the organization   | General Director of the managing organization «VK Management Company» LLCBagudina Elena Gennadevna |\n| Chief Accountant           | Glukhikh Elena Alexandrovna                                                                            |\n","url":"/en/intro/start/legal/org-details","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"7a8e5de5-371b-4aad-9ab5-3ad1dbe99635","title":"User agreement for the service “1C: Ready Workplace”","data":"This document is an official offer of Mail.Ru LLC (hereinafter referred to as the \"Contractor\") to conclude a user agreement (hereinafter referred to as the \"Agreement\") with you (hereinafter referred to as the \"Customer\") on the provision of services for providing access to the 1C Service: within the VK Cloud Platform (hereinafter referred to as the “Platform”), on the terms set forth in this Agreement, which is available on the Internet at: https://mcs.mail.ru/help/legal/1cgrm-termofuse.\n\n1\\. Terms and definitions\n\nSite — an Internet site located at https://mcs.mail.ru/, which provides the Customer with access to the Service. The Contractor posts on the Site information that is binding on the Customer.\n\nVK Cloud Platform (Platform) is a computer program available to the Customer through the Website and operated by the Contractor, which provides the Customer with access to the selected Services.\n\nService 1C: Ready-made workplace (Service) — the Contractor's service, known under the name \"1C: Ready-made workplace\", consisting in presenting to the Customer the possibility of remote access to the selected 1C: Enterprise 8 programs deployed in the cloud infrastructure through a browser or thin client from 1C with work through a browser.\n\nContent — electronic documents, graphics, photographs, scripts, texts, videos, music, sounds, computer programs and other objects (files) uploaded by the Customer posted on the Service.\n\nService unavailability (Downtime) — the time interval from the moment the Customer sent a message to the Contractor's support service by e-mail to support@analit.ru or through the feedback form in the Personal Account, the fact of which was subsequently confirmed by the Contractor, and the moment the Service was resumed after completion The executor of restoration work.\n\nCompensation — the amount to be credited to the Customer's Personal account in the case of Downtime in the manner prescribed by this Agreement. Compensation does not imply payment of funds to the Customer.\n\nThe reporting period is a calendar month.\n\nApplicable rules — this Agreement, available on the Internet at: https://mcs.mail.ru/help/legal/1cgrm-termofuse, containing the conditions for access and use of the Service, as well as the following documents, to the extent necessary for use Service:\n\nTerms of use of the VK Cloud Platform (https://help.mail.ru/mcs/legal/termofuse);\nContractor's privacy policy (https://help.mail.ru/mcs/legal/privacy/).\n\nIn the event that this Agreement uses terms that are not defined in Section 2 of this Agreement, the definitions of these terms of the Applicable Rules are used.\n\nPersonal Account is a closed area of the Platform that allows the Customer to manage settings, edit the Customer's Account, access statistics data, and also perform other actions necessary to implement the functionality of the Platform and individual Services.\n\nPersonal account — a set of records reflecting the financial relationship between the Customer and the Contractor for the services provided. The Customer's balance is increased by the amount of funds transferred by the Customer to the Contractor and credited by the Contractor to the Personal Account as prepayment for services, and is reduced by the cost of the services connected by the Customer.\n\nA module is a type of 1C Service application: Ready workstation.\n\n2\\. Subject of the Agreement\n\n2.1. Under the terms of this Agreement, the Contractor provides the Customer around the clock with an Internet connection \"Service 1C: Ready Workplace\" (hereinafter referred to as the Service), which allows the Customer to use the capabilities of remote access to selected programs \"1C: Enterprise 8\" deployed in the cloud infrastructure through a browser or a thin client from 1C with work through a browser, and the Customer undertakes to pay for the Contractor's services for using the provision of access to the Service in the amount and in the manner determined by the Applicable Rules. The Contractor has the right to provide access to the Service on the basis of agreements concluded with the copyright holders of the rights to the Service or third parties who own such rights on a contractual basis.\n\n2.2. The condition for using the Service is the complete and unconditional acceptance by the Customer of the provisions of this Agreement. Access to the Service is carried out within the Platform, in connection with which the condition for using the Service is the use of the Platform and acceptance of the Terms of Use of the Platform, available at https://help.mail.ru/mcs/legal/termofuse/, as well as the Contractor's Privacy Policy, available at https://help.mail.ru/mcs/legal/privacy/ (hereinafter collectively referred to as the \"Applicable Rules\"). In the event of a conflict between the provisions of the Terms of Use of the Platform and this Agreement, the provisions of this Agreement shall prevail. The Terms of Use of the Platform apply to the relations of the Parties that are not regulated by this Agreement.\n\n2.3. With each access and / or actual use of the Service, the Customer agrees to the provisions of this Agreement and the Applicable Rules as amended, which were in effect at the time of the actual use of the Service.\n\n2.4. The applicable rules, including this Agreement, may be changed by the Contractor at any time without any prior notice to the Customer. Any such changes will take effect on the day following the day such changes are posted on the Site. The Customer undertakes to independently check the Applicable Rules for changes. Failure by the Customer to take steps to familiarize himself with the Applicable Rules and / or the revised edition cannot serve as a basis for the Customer's failure to fulfill his obligations related to the use of the Platform and the Service. The Customer has the right to refuse to accept changes and additions to the Applicable Rules, which means the Customer's refusal to use the Platform and / or the Service.\n\n2.5. To use the Service, the Customer undertakes to ensure that the technical and software capacities of the Customer comply with the requirements of the Contractor, which are available on the page at the link https://help.mail.ru/mcs and may be updated from time to time.\n\n2.6. Functionality of the Service, use of the Service:\n\nThe Customer can use the Service to the extent of the rights granted to him by the Administrator of the Platform and Service, in any way that is acceptable by the functionality of the Platform and the Service, with the exception of methods expressly prohibited by the Terms of Use of the Platform and the Agreement, as well as by applicable law. In particular, the Customer can create new applications for modules of software products of the \"1C: Enterprise 8. Cloud Infrastructure\" family, launch, install, and uninstall them. In any case, the functionality of the Service is provided \"as is\".\n\n2.7. The Contractor has the right to monitor the Content in order to verify the Customer's compliance with the requirements of this Agreement. The Customer undertakes not to create obstacles to the Contractor in the implementation of such monitoring, while he has the right to use tools to preserve the confidentiality of the Content (such as encryption, firewalls).\n\n2.8. Access to the Service can be provided in one of the following ways:\n\n2.8.1. With a prepaid settlement system: after depositing funds in the amount specified in the Tariffs. The customer is given access to work with the Modules.\n\n2.8.2. With a postpaid settlement system: The Customer is provided with access to work with the Modules.\n\n3\\. Rights and obligations of the Parties\n\n3.1. The Contractor undertakes:\n\n3.1.1. Provide the Customer with the opportunity to use the Contractor's Service.\n\n3.1.2. Ensure constant, round-the-clock availability of the Service to the Customer, if there is an Internet connection.\n\n3.1.3. Provide technical support to the Customer at the option of the Contractor: in person or with the involvement of third parties, including 1C-ANALITIKS LLC (PSRN 1107746736690) through the communication channels provided by the functionality of the Service.\n\n3.1.4. Not to disclose the Customer's data, except as provided by the legislation of the Russian Federation, this Agreement and the Applicable Rules.\n\n3.2. The Contractor has the right:\n\n3.2.1. Block access to the Service and to the Customer's Content in the event that the Contractor becomes aware of a violation of the terms of this Agreement, Applicable rules or legislation of the Russian Federation. Any losses caused to the Customer in connection with the blocking by the Contractor of access to the Service or to the Customer's Content are not reimbursed by the Contractor.\n\n3.2.2. In a notifying, unilateral manner, amend the Applicable Rules.\n\n3.2.3. Require the Customer to provide information and documents confirming the compliance of the Content with the requirements of this Agreement.\n\n3.3. The customer undertakes:\n\n3.3.1. Pay for the services of the Contractor in accordance with the terms of this Agreement.\n\n3.3.2. On a regular basis, independently familiarize themselves with the current version of the Applicable Rules and comply with them in full.\n\n3.2.3. Be responsible for your actions and / or inactions on the use of the Contractor's Service.\n\n4\\. Cost of services and payment procedure\n\n4.1. The use of the Services is chargeable. The Contractor, in order to determine the cost of using the Service, sets the price and the corresponding list of services in accordance with the Tariffs.\n\n4.2. The tariff, including, but not limited to, the terms of payment (prepayment / postpay) is set by the Contractor and communicated to the Customer. The current Tariffs are available at https://cloud.vk.com/pricelist. The tariff can be changed by the Contractor unilaterally, which is brought to the attention of the Customer by updating the information on the https://cloud.vk.com/pricelist page. The Contractor may unilaterally change the Tariffs that are applied from the Reporting Period following the period in which the Tariffs were changed.\n\nIn the absence of funds on the Personal Account to write off the cost of services for providing access to the Service or for services rendered in the previous Reporting Period (hereinafter referred to as the Zero Balance), the Customer's access to the Service and files is terminated.\n\n4.4. With a zero balance of the Customer, the Contractor has the right to delete files that occupy disk space with prior notification of the Customer by e-mail, to which the Personal Account is registered\n\n4.5. To restore access to the Service, the Customer must replenish funds on his Personal account in the amount necessary to pay for services in the amount required for the Customer in accordance with the Tariffs in force at the time of the payment.\n\n4.6. The Customer has the right to use demo access for one application of each Module (without making a fee in order to get acquainted with the functionality) to the Service 1 (one) time for a period not exceeding thirty (30) calendar days.\n\n5\\. Responsibility of the Parties\n\n5.1. The amount of the Contractor's liability is limited solely by the relevant Applicable Rules posted on the Site in the section https://help.mail.ru/mcs/legal/.\n\n5.2. Lost profits of the Customer are not subject to compensation by the Contractor.\n\n5.3. The Contractor is not liable to the Customer for blocking access to the Customer's Content, or destroying the Content if the Contractor received relevant claims from third parties, or warnings, instructions or other acts of state authorities in relation to the Customer's actions using the Service.\n\n5.4. The Contractor is not responsible for any violations of the current legislation that have or may take place on projects implemented by the Customer. In the event of claims against the Contractor from third parties in connection with such violations that have occurred, the Customer is obliged to release the Contractor from liability and settle such claims on his own, all losses incurred by the Contractor through the fault of the Customer, in full, including but not limited to: settlement of claims, disputes, enforcement of sanctions by state authorities, must be reimbursed to the Contractor, if any.\n\n6\\. Other conditions\n\n6.1. The Customer guarantees that his Content posted using the Contractor's Service does not contain information and images of a pornographic nature, does not promote violence, racial, sexual, religious or other forms of inequality, or other activities directly prohibited by the legislation of the Russian Federation, does not violate copyright laws. law and other applicable legislation, as this may affect the reputation of the Contractor.\n\n6.2. The Customer guarantees that the placement of the Customer's Content takes place in compliance with the conditions of the current legislation of the Russian Federation.\n\n6.3. The forms of use by the Customer of images, texts, names, emblems, and characters of the Contractor must be agreed with the representatives of the Contractor.\n\n6.4. The Contractor guarantees that the services for providing access to the Service are provided 24 (twenty four) hours a day, 7 (seven) days a week.\n\n6.5. The Contractor reserves the right to interrupt the operation of the Service to carry out the necessary maintenance work, including on weekdays. Such cases will not be considered interruptions in the provision of services if the Contractor notifies the Customer in advance 24 (twenty four) hours before the start of preventive work, entailing interruptions in the operation of the Service, by publishing relevant information on the Site (including the duration of preventive work). The time of unavailability of the Service associated with maintenance work is not subject to compensation according to the rules of clause 6.6. Agreements.\n\n6.6. The unavailability of the Service (\"Downtime\") is defined as the time interval from the moment the Customer sends a message to the Contractor's support service through the feedback form available in the Personal Account containing a notification about the unavailability of the Service, or an e-mail message to the address: support@analit.ru, the fact of which was subsequently confirmed by the Contractor, and the moment the Service was resumed after the completion of the restoration work by the Contractor. If the Contractor has his own data about the beginning of Downtime, indicating an earlier start time of the Service's unavailability than the time of sending an e-mail to support@analit.ru or the feedback form in the Personal Account, the Contractor may use such data. Disagreements about the start and end times of Downtime are resolved through negotiations between the Parties. If Downtime began in one Reporting Period and ended in the next Reporting Period, then the unavailability of the Service fully refers to the Reporting Period in which most of Downtime took place.\n\n6.7 If the Service is unavailable, the Contractor undertakes to transfer the Compensation to the Customer (credited to the Personal Account), the amount of which is determined on the basis of the table below (hereinafter — Table No. 1):\n\nAvailability time (in%) *Unavailability time (min. Or hour) in one Reporting period in totalCompensation Amount99.95% and aboveless than 22 minutesnot paid99.9%–99.95%from 22 to 45 minutes3% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was99.0%–99.9%from 45 minutes to 7.5 hours10% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was95.0% –99.0%from 7.5 hours to 36 hours20% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was90.0%–95.0%from 36 hours to 73 hours25% of the amount debited from the Personal Balance for the Reporting Period in which Downtime wasless than 90%more than 73 hours50% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was\n\n\\* Calculated based on 43 800 minutes in the Reporting Period\n\n6.8 In any cases of violation of the guarantees of the level of the Service, the Customer undertakes to cooperate with the Contractor to determine the source of the problems, eliminate it and restore the functionality of the Service or deleted / damaged Content.\n\n6.9. The Compensation is calculated automatically based on the Contractor's statistics. The amount of the Compensation is transferred to the Customer's account in the amount determined on the basis of the rules of clause 6.7. of this Agreement within 7 (seven) business days from the beginning of the Reporting period following the one in which the events that served as the basis for the Compensation took place.\n\n6.10. The amount of the Compensation during the Reporting Period on both grounds specified in cl. 6.6. and 6.7. Of the Agreement, cannot exceed the amount of funds paid by the Customer for the Reporting period in which the events took place that served as the basis for the Compensation.\n\n6.11. The unavailability of the Service caused by the actions of the Customer or third parties, or force majeure circumstances that led to the unavailability of services is not subject to Compensation; suspension of the provision of services associated with the planned maintenance work, as defined in clause 6.5. present agreement; suspension of the provision of services at the request of state or municipal authorities in cases and in the manner determined by applicable law. The Contractor under no circumstances will reimburse the Customer of the Service for indirect losses, including lost profits and damage to business reputation.\n\n6.12. By accepting the terms of this Agreement, the Customer expresses and provides his informed consent in favor of Mail.Ru LLC (OGRN 1027739850962), 1C-ANALITIKS LLC (OGRN 1107746736690) in accordance with Federal Law No. 152-FZ dated July 27, 2006 \"About personal data\" for the implementation of the following actions with all the personal data provided by the Customer (namely: surname, name, patronymic, phone number, email, taxpayer identification number): collection, systematization, storage, clarification (update and change), use, provision, depersonalization, blocking and destruction for the purposes of execution by the Customer and the Contractor of the terms of this Agreement. Consent is provided without limiting the period for processing personal data, but in any case until the goals of providing consent are achieved or until the Customer withdraws consent to the processing of personal data by sending a written message to Mail.Ru LLC to the address: 125167, Moscow, Leningradsky prospect 39, building 79 and LLC 1C-ANALITIKS \"written application to the address: 125167, Moscow, Planetnaya street, house 11, room 6/25 rm-2. The provisions on personal data not regulated by the provisions of this clause of the Agreement are governed by the provisions of the Contractor's Privacy Policy (https://help.mail.ru/mcs/legal/privacy/).\n\nRevision: from 10/22/2021\n","url":"/en/intro/start/legal/policy-1cgrm","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"5944546a-4b20-4c8b-9181-bc946778b1d4","title":"User agreement for the service “Cloud FZ-152”","data":"This document constitutes an offer in accordance with Art. 435 of the Civil Code of the Russian Federation (hereinafter referred to as the \"Offer\") for a conclusion between Mail.Ru LLC (125167, Moscow, Leningradsky prospect, 39, p. 79, OGRN 1027739850962) (hereinafter referred to as the \"Company\") and You, a legal entity, (hereinafter referred to as the \"Customer\") of the user agreement on the terms of this Offer (hereinafter referred to as the \"Agreement\"), the subject of which is the provision of services for providing access to the Cloud FZ-152 Service within the framework of the VK Cloud Platform (hereinafter referred to as the \"Platform\").\n\nThis Agreement is available on the Internet at: https://mcs.mail.ru/help/legal/152-fz-termofuse and is an integral part of the Terms of Use of the VK Cloud platform, available on the Internet at: https://mcs.mail.ru/help/legal/termofuse and the Company's Privacy Policy, on the Internet at: https://mcs.mail.ru/help/legal/privacy (hereinafter collectively — \"Applicable Rules\"). In the event of a conflict between the provisions of the Terms of Use of the Platform and this Agreement, the provisions of this Agreement shall prevail. The Terms of Use of the Platform apply to the relations of the Parties that are not regulated by this Agreement.\n\nBy using the Service you are in accordance with Art. 438 of the Civil Code of the Russian Federation, you unconditionally and in full accept the terms of the Agreement.\n\nThe use of the Service is also governed by an agreement concluded between the Company and the Customer (hereinafter referred to as the \"Agreement\"). In the event of a conflict between the terms of the Agreement and the Agreement, the terms of the Agreement shall apply.\n\n1\\. Terms and definitions\n\nSite — an Internet site located at https://mcs.mail.ru/, which provides the Customer with access to the Platform and the Service. The Company places information on the Site that is mandatory for Customers.\n\nCloud FZ-152 / Service is a service of the Company known under the name \"Cloud FZ-152\", which consists in providing the Customer with the possibility of creating virtual servers and virtual disks on the Contractor's servers in the certified perimeter of ISPDN. The security of the certified perimeter of the ISPDN is confirmed by a certificate of conformity, which gives the Customer the opportunity to process Personal Data in the ISPDN.\n\nContent — electronic documents, graphic images, photographs, scripts, texts, videos, music, sounds, computer programs and other objects (files) uploaded by the Customer placed in the Service, which may contain personal data.\n\nProhibited Content — any Content, including its constituent materials and personal data, the use of which is prohibited by the Applicable Rules.\n\nProcessing of Personal Data — actions of the Customer performed using automation tools or without using such tools with Personal Data in ISPDN when using the Service, namely (the list is closed): recording (uploading to the Company's server), systematization, accumulation, storage, clarification (update, change), extraction, use, transfer (access), depersonalization, blocking, deletion, destruction (hereinafter referred to as \"Processing Methods\").\n\nPurpose — the purpose of the Processing of Personal Data, determined by the Customer, specified in the Agreement.\n\nPersonalnye data (data) — any information relating directly or indirectly to an identified or identifiable natural person (personal data subject), processed by the Customer through the Service. The list of Data that the Customer intends to use in the Service (the \"List\") is agreed by the Parties in the Agreement.\n\nSubjects — Subjects of Personal Data, whose Data is processed by the Customer when using the Service.\n\nISPDN is an information system of personal data Cloud Servers from VK Cloud, formerly known as \"VK Cloud Infra\", which complies with the information security requirements of the legislation of the Russian Federation and provides the 3rd level of protection of personal data, which is confirmed by the Certificate of Compliance (available on the Internet at: https://hb.bizmrg.com/mcs-static/\\\\\\_files/certificates/security_certificate.pdf).\n\nService unavailability (Downtime) — the time interval from the moment the Customer sent a message to the Company's support service by e-mail to support@mcs.mail.ru or through the feedback form in the personal account, the fact of which was subsequently confirmed by the Company, and the moment the Service was resumed after the completion of the restoration work by the Company.\n\nCompensation is the amount of funds transferred by the Company to the User in case of violation of the guarantees of the level of the Service or in case of loss / damage of the Content as a result of an accident due to the fault of the Company.\n\nThe reporting period is a calendar month.\n\nOther terms are defined in the Terms of Use of the VK Cloud Platform.\n\n2\\. Subject of the Agreement, conditions for obtaining access to the Service\n\n2.1. Under the terms of this Agreement, the Company provides the Customer around the clock with an Internet connection, the Cloud FZ-152 Service, which allows the Customer to create virtual servers and virtual disks in a secure ISPDN on the Contractor's servers, and the Customer undertakes to pay for the Contractor's services for using access to the Service in the amount of and the order as determined by the Applicable Rules. Protected ISPDN enables the Customer to process Personal Data in ISPDN under the terms of the Agreement (hereinafter also referred to as \"services\"). Access to the Service is provided to the Customer only if the Customer specifies the List of Personal Data and the Purpose.\n\n2.2. When using the Service, the Customer is prohibited from processing (uploading to the Company's server) information, the processing of which in the Service violates a person's right to privacy, personal and family secrets.\n\n2.3. Processing of Personal Data is allowed only if the conditions for processing such Data require the provision of 3 levels of security or lower in accordance with the legislation of the Russian Federation, including by-laws (in particular, the Decree of the Government of the Russian Federation of 01.11.2012 N 1119).\n\n2.4. With each access and / or actual use of the Service, the Customer agrees with the provisions of this Agreement, as well as with other Applicable rules, as amended, which were in effect at the time of the actual use of the Service.\n\n2.5. The applicable rules and this Agreement may be changed by the Company at any time without any prior notice to the Customer. Any such changes will take effect on the day following the day such changes are posted on the Site. The Customer undertakes to independently check the Applicable Rules for changes. Failure by the Customer to take steps to familiarize himself with the Applicable Rules and the Agreement and / or their amended version cannot serve as a basis for the Customer's failure to fulfill his obligations related to the use of the Service. The Customer has the right to refuse to accept changes and additions to the Applicable Rules and / or the Agreement, which means the Customer's refusal to use the Service.\n\n2.6. Functionality of the Service, use of the Service:\n\nThe Customer can use the Service to the extent of the rights granted to him by the Company in any way that is acceptable by the functionality of the Service, with the exception of methods expressly prohibited by the Terms of Use of the Platform and the Agreement, as well as by applicable law. In particular, the Customer can place the Content on virtual disks located on the disk space of the Contractor's servers, download computer programs to virtual disks, execute them, view, modify, delete the Content and perform other actions with the Content provided for by the functionality of the Service.\n\n2.7. The placement of Content on the Contractor's servers and its use must not violate any applicable law, in particular, the Customer is not entitled to:\n\nupload, store, transfer, provide access to other persons or otherwise bring to the public knowledge (post on the Service) Content, including incl. intellectual property objects of rightholders in the absence of sufficient legal grounds for such actions;\nupload, store, transfer, provide access to other persons to the Content prohibited by applicable law, or the Content, storage or use of which requires obtaining special permissions from authorized bodies or persons, if the Customer does not have the appropriate permissions.\n\n2.8. The Contractor has the right to monitor the Content solely in order to verify that the Customer complies with the requirements of this Agreement for the Content and to prevent the use of Prohibited Content. The Customer undertakes not to create obstacles for the Contractor in the implementation of such monitoring, while the Customer has the right to use tools to preserve the confidentiality of the Content (such as encryption, firewalls). 2.11. The Customer instructs the Company to process the List of Personal Data of Subjects with a Purpose using automation tools in the following ways: access, storage, destruction to the extent that allows the Customer to provide the Service under the terms of the Agreement.\n\n3\\. Customer guarantees\n\n3.1. The Customer represents and warrants that the Processing of Personal Data is governed exclusively by the legislation of the Russian Federation.\n\n3.2. The Customer certifies and guarantees the existence of legal grounds provided for by the Federal Law \"On Personal Data\" dated July 27, 2006 N 152-ФЗ (hereinafter — \"ФЗ О Personal Data\"), for the Customer to process the List of Personal Data in ISPDN as part of using the Service for the Purpose and Methods of Processing.\n\n3.3. The Customer certifies and guarantees that he has received the consent of the Subject, drawn up in full accordance with the Federal Law On Personal Data on the instructions of the Customer to the Company for processing the List of Personal Data of Subjects with a Purpose using automation tools in the following ways: access, storage, destruction in an amount that allows the Customer to provide the Service on the terms of the Agreement. Such consent must include the name and registered office of the Company.\n\n3.4. The Customer certifies and guarantees that the data processing conditions require the provision of 3 levels of protection or lower in accordance with the legislation of the Russian Federation.\n\n3.5. The Customer represents and warrants that his use of the Service on the terms of the Agreement does not violate applicable law and the rights of third parties.\n\n3.6. The Customer certifies and guarantees that he is a legal entity registered in accordance with the legislation of the Russian Federation, fully legally capable, having all the necessary rights, permissions and powers to conclude the Agreement.\n\n4\\. Rights and obligations of the Parties\n\n4.1. The Contractor undertakes:\n\n4.1.1. Provide the Customer with the opportunity to use the Contractor's Service.\n\n4.1.2. Ensure constant, round-the-clock availability of the Service to the Customer, if there is an Internet connection.\n\n4.1.3. Provide technical support to the Customer.\n\n4.1.4. Not to disclose the Customer's data, except as provided by the legislation of the Russian Federation, this Agreement and the Applicable Rules.\n\n4.2. The Contractor has the right:\n\n4.2.1. Block access to the Service and to the Customer's Content if the Contractor becomes aware of a violation of the rules for posting Customer's Content, the terms of this Agreement, Applicable rules or legislation of the Russian Federation.\n\n4.2.2. To inform, unilaterally amend the Applicable Rules by publishing a new edition.\n\n4.2.3. Require the Customer to provide information and documents confirming the compliance of the Content with the requirements of this Agreement.\n\n4.2.4. Block or remove Prohibited Content on your own initiative, as well as at the request of authorized government agencies. Any losses caused to the Customer in connection with the removal of the Prohibited Content by the Contractor are not reimbursed by the Contractor.\n\n4.3. The customer has the right:\n\n4.3.1. Process the List of Personal Data for the Purposes and Methods of Processing using the available functionality of the Service.\n\n4.3.2. Change the List of Personal Data, subject to the restrictions established in relation to it by the Agreement with the obligatory agreement with the Company of the amended list in the annexes / supplementary agreements to the Agreement.\n\n4.4. The customer is obliged:\n\n4.4.1. Pay for the services of the Contractor in accordance with the terms of the Agreement.\n\n4.4.2. On a regular basis, independently familiarize themselves with the current version of the Applicable Rules and comply with them in full.\n\n4.4.3. Be responsible for your actions and / or inactions when using the Contractor's Service.\n\n4.4.4. Comply with all prohibitions and requirements stipulated by the Agreement.\n\n4.4.5. When using the Service (in particular, in the event of unlawful processing of Personal Data (part of it) in the Service), the Customer must comply with the requirements of the Federal Law On Personal Data and related bylaws.\n\n4.4.6. Without involving the Company, on their own and at their own expense, resolve all disputes arising in connection with the violation of the Agreement by the Customer.\n\n4.4.7. Within 3 (three) business days from the date of receipt of the relevant request from the Company, provide it with evidence of compliance with its assurances and guarantees.\n\n4.5. Company rights:\n\n4.5.1. Improve the functionality of the Service, introduce new functionality.\n\n4.5.2. Request from the Customer documents confirming compliance with the assurances and guarantees under the Agreement, and in the event of their absence / discrepancy / failure to provide within the period specified in clause 4.2.4, unilaterally refuse to execute the Agreement, closing the Customer's access to the Service and destroying the List Personal data without compensation for losses to the Customer. In this case, the Company notifies the Customer of the refusal by e-mail.\n\n4.6. Obligations of the Company\n\n4.6.1. ensure the confidentiality of Personal data processed on behalf of the Customer, and ensure the security of such data during their processing.\n\n4.6.2. Comply with the following requirements for the protection of Personal Data:\n\nOrganize a regime for ensuring the security of the premises in which the data processing systems are located, preventing the possibility of uncontrolled entry or stay in these premises of persons who do not have the right to access these premises;\nEnsure the safety of the media containing the Data;\nDetermine the list of persons whose access to the Data processed by the Company is necessary for the performance of their official (labor) duties;\nDetermine the person responsible for ensuring the security of the Data.\n\n4.6.3. Process the Data only in the amount, for the purposes and in the ways specified in the order of the Customer.\n\n4.6.4. Comply with other legal requirements in connection with the provision of services provided for by the Agreement.\n\n5\\. Cost of services\n\n5.1. The use of the Service within the Platform is Paid. The cost of services and the procedure for settlements are established by the Agreement.\n\n6\\. Responsibility\n\n6.1. For violation of the terms of the Agreement, the parties are responsible in accordance with the legislation of the Russian Federation and the Agreement. The sanctions stipulated by the legislation and the Agreement are subject to payment by the violating Party within 5 working days from the date of receipt of a written request from the injured Party.\n\n6.2. In the event that, as a result of a violation by the Customer of its guarantees, assurances and / or obligations under the Agreement, claims, claims of any third parties, as well as instructions, warnings and other acts of the regulatory authorities are brought against the Company, the Customer undertakes to resolve them on his own and at his own account, without involving the Company.\n\n7\\. Other conditions\n\n7.1. The applicable legislation under this Agreement is the legislation of the Russian Federation.\n\n7.2. The Customer guarantees that his Content posted using the Contractor's Service does not contain information and images of a pornographic nature, does not promote violence, racial, sexual, religious or other forms of inequality, or other activities directly prohibited by the legislation of the Russian Federation, does not violate copyright laws. law and other applicable legislation, as this may affect the reputation of the Contractor.\n\n7.3. The forms of use by the Customer of images, texts, names, emblems, and characters of the Contractor must be agreed with the representatives of the Contractor.\n\nRevision: from 10/22/2021\n","url":"/en/intro/start/legal/policy-152fz","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"0f8c21f1-e506-4fa7-8aca-538e10ccba91","title":"User agreement for the “Cloud Servers” service","data":"This document is the official offer of Mail.ru LLC (hereinafter referred to as the \"Contractor\") to conclude a user agreement (hereinafter referred to as the \"Agreement\") with you (hereinafter referred to as the \"Customer\") on the provision of services for providing access to the Cloud Servers virtual infrastructure Service within Platforms \"VK Cloud\" (hereinafter referred to as the \"Platform\"), on the terms set forth in this Agreement, which is available on the Internet at: https://mcs.mail.ru/docs/en/intro/start/legal/cloud-servers.\n\n1\\. Terms and definitions\n\nSite — an Internet site located at https://mcs.mail.ru/, which provides the Customer with access to the Service. The Contractor posts on the Site information that is binding on the Customer.\n\nVK Cloud Platform (Platform) is a computer program available to the Customer through the Website and operated by the Contractor, which provides the Customer with access to the selected Services.\n\nVirtual Infrastructure Service (Service) — a service of the Contractor, known under the name \"Virtual Infrastructure Service\" (Cloud Servers), consisting in providing the Customer with the possibility of creating virtual servers and virtual disks on the Contractor's servers.\n\nContent — electronic documents, graphics, photographs, scripts, texts, videos, music, sounds, computer programs and other objects (files) uploaded by the Customer posted on the Service.\n\nProhibited Content — any Content described in clause 2.8. of this Agreement, as well as other materials, the use of which is prohibited by the Applicable Rules.\n\nUnavailability of the Service (Downtime) — the time interval from the moment the Customer sent the message to the Contractor's support service by e-mail to support@mcs.mail.ru or through the feedback form in the personal account, the fact of which was subsequently confirmed by the Contractor, and the moment the Service was resumed after the completion of the restoration work by the Contractor.\n\nCompensation — the amount to be credited to the Customer's Personal account in the case of Downtime in the manner prescribed by this Agreement. Compensation does not imply payment of funds to the Customer.\n\nThe reporting period is a calendar month.\n\nApplicable rules — the rules for using the Contractor's Service, available on the Internet at: https://mcs.mail.ru/docs/en/intro/start/legal/cloud-servers, containing the conditions for access and use of the Service and set out in the following documents:\n\nTerms of use of the VK Cloud Platform (https://help.mail.ru/mcs/legal/termofuse);\nContractor's privacy policy (https://help.mail.ru/mcs/legal/privacy/).\n\nIn the event that this Agreement uses terms that are not defined in Section 2 of this Agreement, the definitions of these terms of the Applicable Rules are used.\n\n2\\. Subject of the Agreement\n\n2.1. Under the terms of this Agreement, the Contractor provides the Customer around the clock, with an Internet connection, a \"Cloud Servers\" service (hereinafter referred to as the Service), which allows the Customer to create virtual servers and virtual disks on the Contractor's servers, and the Customer undertakes to pay for the Contractor's services for using the provision access to the Service in the amount and in the manner determined by the Applicable Rules.\n\n2.2. The condition for using the Service is the complete and unconditional acceptance by the Customer of the provisions of this Agreement. Access to the Service is carried out within the Platform, in connection with which the condition for using the Service is the use of the Platform and acceptance of the Terms of Use of the Platform, available at https://help.mail.ru/mcs/legal/termofuse/, as well as the Contractor's Privacy Policy, available at https://help.mail.ru/mcs/legal/privacy/ (hereinafter collectively referred to as the \"Applicable Rules\"). In the event of a conflict between the provisions of the Terms of Use of the Platform and this Agreement, the provisions of this Agreement shall prevail. The Terms of Use of the Platform apply to the relations of the Parties that are not regulated by this Agreement.\n\n2.3. With each access and / or actual use of the Service, the Customer agrees to the provisions of this Agreement and the Applicable Rules as amended, which were in effect at the time of the actual use of the Service.\n\n2.4. The applicable rules, including this Agreement, may be changed by the Contractor at any time without any prior notice to the Customer. Any such changes will take effect on the day following the day such changes are posted on the Site. The Customer undertakes to independently check the Applicable Rules for changes. Failure by the Customer to take steps to familiarize himself with the Applicable Rules and / or the revised edition cannot serve as a basis for the Customer's failure to fulfill his obligations related to the use of the Platform and the Service. The Customer has the right to refuse to accept changes and additions to the Applicable Rules, which means the Customer's refusal to use the Platform and / or the Service.\n\n2.5. Functionality of the Service, use of the Service:\n\nThe Customer can use the Service to the extent of the rights granted to him by the Administrator of the Platform and Service, in any way that is acceptable by the functionality of the Platform and the Service, with the exception of methods expressly prohibited by the Terms of Use of the Platform and the Agreement, as well as by applicable law. In particular, the Customer can place the Content on virtual disks located on the disk space of the Contractor's servers, download computer programs to virtual disks, execute them, view, modify, delete the Content and perform other actions with the Content provided for by the functionality of the Service.\n\n2.6. The placement of Content on the Contractor's servers and its use must not violate any applicable law, in particular, the Customer is not entitled to:\n\nupload, store, transfer, provide access to other persons or otherwise bring to the public knowledge (post on the Service) the Content and other results of intellectual activity of the copyright holders in the absence of the explicitly expressed consent of these persons and / or the required amount of rights to such actions;\nupload, store, transfer, provide access to other persons to the Content prohibited by applicable law, or the Content, storage or use of which requires obtaining special permissions from authorized bodies or persons, if the Customer does not have the appropriate permissions.\n\nThe content described in this clause is hereinafter referred to as “Prohibited Content”.\n\nThe Contractor has the right to monitor the Content in order to verify that the Customer complies with the requirements of this Agreement for the Content and to prevent Prohibited Content. The Customer undertakes not to create obstacles to the Contractor in the implementation of such monitoring, while he has the right to use tools to preserve the confidentiality of the Content (such as encryption, firewalls).\n\n3\\. Rights and obligations of the Parties\n\n3.1. The Contractor undertakes:\n\n3.1.1. Provide the Customer with the opportunity to use the Contractor's Service.\n\n3.1.2. Ensure constant, round-the-clock availability of the Service to the Customer, if there is an Internet connection.\n\n3.1.3. Provide technical support to the Customer.\n\n3.1.4. Not to disclose the Customer's data, except as provided by the legislation of the Russian Federation, this Agreement and the Applicable Rules.\n\n3.2. The Contractor has the right:\n\n3.2.1. Block access to the Service and to the Customer's Content if the Contractor becomes aware of a violation of the rules for posting Customer's Content, the terms of this Agreement, Applicable rules or legislation of the Russian Federation.\n\n3.2.2. In a notifying, unilateral manner, amend the Applicable Rules.\n\n3.2.3. Require the Customer to provide information and documents confirming the compliance of the Content with the requirements of this Agreement.\n\n3.2.4. Block or remove Prohibited Content on your own initiative, as well as at the request of authorized government agencies. Any losses caused to the Customer in connection with the removal of the Prohibited Content by the Contractor are not reimbursed by the Contractor.\n\n3.3. The customer undertakes:\n\n3.3.1. Pay for the services of the Contractor in accordance with the terms of this Agreement.\n\n3.3.2. On a regular basis, independently familiarize themselves with the current version of the Applicable Rules and comply with them in full.\n\n3.2.3. Be responsible for your actions and / or inactions on the use of the Contractor's Service.\n\n4\\. Cost of services and payment procedure\n\n4.1. The use of the Services is chargeable. The Contractor, in order to determine the cost of using the Service, sets the price and the corresponding list of services in accordance with the Tariffs.\n\n4.2. The tariff is set by the Contractor and brought to the attention of the Customer. The current Tariffs are posted on the following pages: https://mcs.mail.ru/infra/, https://mcs.mail.ru/containers/, https://mcs.mail.ru/bigdata/. The tariff can be changed by the Contractor unilaterally, which is brought to the attention of the Customer by updating the information on the pages: https://mcs.mail.ru/infra/, https://mcs.mail.ru/containers/, https://mcs.mail.ru/bigdata/. The Contractor may unilaterally change the Tariffs that are applied from the Reporting Period following the period in which the Tariffs were changed.\n\nIn the absence of funds on the Personal balance (account) of the Customer to write off the payment for the cost of services for providing access to the Service (hereinafter — Zero balance), the Customer's access to the Service and files is terminated.\n\n4.4. With a zero balance of the Customer, the Contractor has the right to delete files that occupy disk space with prior notification of the Customer by e-mail to which the Project is registered.\n\n4.5. To restore access to the Service, the Customer must replenish funds on his Personal balance (account) for the amount necessary to pay for services in the amount required for the Customer in accordance with the Tariffs in force at the time of the payment.\n\n5\\. Responsibility of the Parties\n\n5.1. The amount of the Contractor's liability is limited solely by the relevant Applicable Rules posted on the Site in the section https://help.mail.ru/mcs/legal/.\n\n5.2. Lost profits of the Customer are not subject to compensation by the Contractor.\n\n5.3. The Contractor is not liable to the Customer for blocking access to the Customer's Content, or destroying the Content if the Contractor received relevant claims from third parties, or warnings, instructions or other acts of state authorities in relation to the Customer's Content.\n\n5.4. The Contractor is not responsible for any violations of the current legislation that have or may take place on projects implemented by the Customer. In the event of claims against the Contractor from third parties in connection with such violations that have occurred, the Customer is obliged to release the Contractor from liability and settle such claims on his own, all losses incurred by the Contractor through the fault of the Customer, in full, including but not limited to: settlement of claims, disputes, enforcement of sanctions by state authorities, must be reimbursed to the Contractor, if any.\n\n6\\. Other conditions\n\n6.1. The Customer guarantees that his Content posted using the Contractor's Service does not contain information and images of a pornographic nature, does not promote violence, racial, sexual, religious or other forms of inequality, or other activities directly prohibited by the legislation of the Russian Federation, does not violate copyright laws. law and other applicable legislation, as this may affect the reputation of the Contractor.\n\n6.2. The Customer guarantees that the placement of the Customer's Content takes place in compliance with the conditions of the current legislation of the Russian Federation.\n\n6.3. The forms of use by the Customer of images, texts, names, emblems, and characters of the Contractor must be agreed with the representatives of the Contractor.\n\n6.4. The Contractor guarantees that the services for providing access to the Service are provided 24 (twenty four) hours a day, 7 (seven) days a week.\n\n6.5. The Contractor reserves the right to interrupt the operation of the Service to carry out the necessary maintenance work, including on weekdays. Such cases will not be considered interruptions in the provision of services if the Contractor notifies the Customer in advance 24 (twenty four) hours before the start of preventive work, entailing interruptions in the operation of the Service, by publishing relevant information on the Site (including the duration of preventive work). The time of unavailability of the Service associated with maintenance work is not subject to compensation according to the rules of clause 6.6. Agreements.\n\n6.6. The unavailability of the Service (Downtime) is defined as the time interval from the moment the Customer sends a message to the Contractor's support service through the feedback form available in the Personal Account containing a notification about the unavailability of the Service, or by e-mail to the address: support@mcs.mail.ru, the fact of which was subsequently confirmed by the Contractor, and the moment the Service was resumed after the completion of the restoration work by the Contractor. If the Contractor has his own data about the beginning of Downtime, indicating an earlier start time of the Service's unavailability than the time of sending an e-mail message to support@mcs.mail.ru or the feedback form in his personal account, the Contractor can use such data. Disagreements about the start and end times of Downtime are resolved through negotiations between the Parties. If Downtime began in one Reporting Period and ended in the next Reporting Period, then the unavailability of the Service fully refers to the Reporting Period in which most of Downtime took place.\n\n6.7 If the Service is unavailable, the Contractor undertakes to transfer Compensation to the Customer (credited to the Personal Balance (account)), the amount of which is determined on the basis of the table below (hereinafter — Table No. 1):\n\nAvailability time (in%) *Unavailability time (min. Or hour) in one Reporting period in totalCompensation Amount99.95% and aboveless than 22 minutesnot paid99.9% -99.95%from 22 to 45 minutes3% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was99.0%–99.9%from 45 minutes to 7.5 hours10% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was95.0%–99.0%from 7.5 hours to 36 hours20% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was90.0%–95.0%from 36 hours to 73 hours25% of the amount debited from the Personal Balance for the Reporting Period in which Downtime wasless than 90%more than 73 hours50% of the amount debited from the Personal Balance for the Reporting Period in which Downtime was\n\n\\* Calculated based on 43 800 minutes in the Reporting Period\n\n6.8 In any cases of violation of the guarantees of the level of the Service, the Customer undertakes to cooperate with the Contractor to determine the source of the problems, eliminate it and restore the functionality of the Service or deleted / damaged Content.\n\n6.9. The Compensation is calculated automatically based on the Contractor's statistics. The amount of the Compensation is transferred to the Customer's account in the amount determined on the basis of the rules of clause 6.7. of this Agreement within 7 (seven) business days from the beginning of the Reporting period following the one in which the events that served as the basis for the Compensation took place.\n\n6.10. The amount of the Compensation during the Reporting Period on both grounds specified in cl. 6.6. and 6.7. Of the Agreement, cannot exceed the amount of funds paid by the Customer for the Reporting period in which the events took place that served as the basis for the Compensation.\n\n6.11. The unavailability of the Service caused by the actions of the Customer or third parties, or force majeure circumstances that led to the unavailability of services is not subject to Compensation; suspension of the provision of services associated with the planned maintenance work, as defined in clause 6.5. present agreement; suspension of the provision of services at the request of state or municipal authorities in cases and in the manner determined by applicable law. The Contractor under no circumstances will reimburse the Customer of the Service for indirect losses, including lost profits and damage to business reputation.\n\nRevision: from 10/22/2021\n","url":"/en/intro/start/legal/cloud-servers","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"73e42643-683d-4e6c-9575-4648f01f532e","title":"Contributor License Agreement","data":"\nMoscow, Russian Federation\n\n1. Terms and Definitions\n\n“VK” means VK LLC, a legal entity registered under the laws of the Russian Federation, OGRN 1027739850962, located at: 39 Leningradsky Avenue, bldg. 79, Moscow, 125167.\n\n“GitHub” is an Internet service, available at https://github.com/, used for hosting and joint development of IT projects.\n\n“Contributions” mean the independent results of intellectual activity (including, but not limited to, source code, libraries, design objects, computer programs, databases or components thereof, texts, documentation, user instructions) provided by the Contributor to VK hereunder.\n\n“Contributor” means, where the Contributor is an individual acting on his/her own behalf, an individual who has the necessary legal capacity to enter into and perform this Agreement; and where the Contribution is provided by an individual on behalf of a legal entity or an individual entrepreneur (including on behalf of the employer) the respective legal entity or individual entrepreneur.\n\n“Derivative Work” means any modification, improvement of, work that is based on (or derived from) the Contribution (including translation) made by VK or a third party for VK.\n\n“Affiliates” mean legal entities who can directly or indirectly control the operations of VK or other members of the VK group, or legal entities whose operations VK can control directly or indirectly.\n\n“Agreement” means this license agreement entered into as set out in Section 2 hereof.\n\n“Party” means either party to this Agreement.\n\n2. Entry into Force\n\n2.1. By providing Contributions to VK in any form, the Contributor fully and unconditionally consents to all provisions of this Agreement.\n\n2.2. The Agreement is deemed to become effective upon the VK’s receipt of the Contribution from the Contributor in any form, including, but not limited to, in electronic form through the tools and communication channels available on GitHub.\n\n2.3. If by the date hereof the Contributor has already provided the Contributions to VK, the provisions of this Agreement shall apply to the relations that existed between the Parties before the date of this Agreement became effective, since the moment of delivery the first Contribution to VK by Contributor.\n\n3. Subject of the Agreement. Terms of License\n\n3.1. The Contributor grants to VK the right to use the Contributions under a royalty-free, non-exclusive, irrevocable, worldwide license by all means specified in this Agreement.\n\n3.2. The license is provided for the entire duration of the Contributor’s exclusive rights to the Contributions.\n\n3.3. The Contributions may be provided (transferred) by the Contributor to VK in any possible manner, including in a tangible or in electronic form. The Contributions shall be considered provided since the moment of their receipt by VK.\n\n3.4. VK may use the Contributions at its own discretion and without any prior approval by the Contributor or any third party, in whole or in part, in any way not contrary to law, in particular, but not limited to:\n\n3.4.1. reproduce the Contributions in any way, including but not limited to by recording to the computer; install, copy, or launch the Contributions;\n\n3.4.2. remake, update or modify the Contributions without any limitations, create other intellectual property objects based on or by using the Contributions, including derivative works or composite works; translate the Contributions into other languages, including other programming languages;\n\n3.4.3. use the Contributions with the business name, any trademark (service mark) or logo of VK or its Affiliates or other designation at VK’s discretion;\n\n3.4.4. provide the Contributions and the right to use the Contributions (sublicense) to any third party, either directly or as part of any other product/service;\n\n3.4.5. distribute the Contributions through sale, distribute the Contributions in any other way not contrary to law, including to import the Contributions;\n\n3.4.6. display in public and make the Contributions available to public;\n\n3.4.7. disassemble, decompile (convert object code into source code) or reverse engineer the Contributions or their components (if the Contribution is a computer program, database, software package or similar object);\n\n3.4.8. make alterations, deletions or additions to the Contributions, add commentary, illustrations or any explanations when using the Contributions, including those affecting the integrity of perception;\n\n3.4.9. use the Contributions without mentioning the author’s name and/or pseudonym (anonymously);\n\n3.4.10. make the Contributions public, i.e. take or consent to the actions that make the Contributions available to public for the first time.\n\n3.5. The Parties agree that VK shall have a full exclusive right to the Derivative Works and may use them in any way, for any purpose and in any form without any limitations or the consent of the Contributor or any third party.\n\n3.6. VK may use the Contributions for business and non-business purposes, including for service provision to third parties; VK may use the Contributions for its advertising and marketing materials or other products/services, both with open or closed source code.\n\n3.7. The Contributor shall reserve the right to use the Contributions by any legal means, provided that such uses do not impact negatively VK business reputation. For the avoidance of doubt, the Contributor shall not acquire any rights to the intellectual property which right holder is VK or its Affiliates.\n\n3.8. VK shall not submit any reports on the use of the Contributions to the Contributor or any third party.\n\n3.9. Without affecting the terms and conditions hereof in respect of the free of charge right to use the Contributions hereunder, the Contributor acknowledges that the market value of the non-exclusive license for the right to use each individual Contribution provided by the Contributor to VK shall be RUB 100 (one hundred Russian Rubles).\n\n4. Representations and Warranties\n\n4.1. The Contributor hereby represents and warrant to VK that as at the date of this Agreement and each date of the Contribution provision:\n\n4.1.1. The Contributor has full legal capacity and has all the rights required to execute and perform this Agreement. Where the Contributor is a legal entity or an individual entrepreneur, the Contributor warrants to VK that the individual who acts on its behalf to execute and performs this Agreement has all the required powers and authorities to execute and perform this Agreement.\n\n4.1.2. The Contributions provided by the Contributor to VK are developed (created) by the Contributor, except for the Contributions or parts of Contributions, the exclusive rights to which belong to other parties (the Rightholders), but which, by the direct expression of will of such Rightholders, may be distributed freely without the Rightholders approval or payment of fees (free libraries, etc.), and the Contributor uses (and provides to VK) such Contributions legally;\n\n4.1.3. The Contributions are not encumbered with any rights of third parties, including the rights of the Contributor's employer;\n\n4.1.4. The Contributor has the consent of the authors and Rightholders of the results of intellectual activity included in the Contributions, to use them in all the ways specified in this Agreement; and VK is not prevented from the use of the Contributions which include such intellectual property;\n\n4.1.5. the use of the Contributions under this Agreement shall not infringe any intellectual property rights or other rights or legitimate interests of third parties, including the Contributor employers.\n\nThe Contributor agrees and acknowledges that VK unconditionally relies on the representations and warranties given by the Contributor in Section 4.1 hereof, and such representations and warranties are material for VK.\n\n5. Liability\n\n5.1. Should any claim or action be brought against VK by a third party (including the Contributor employer) in respect of the use of the Contributions by VK hereunder, the Contributor shall indemnify VK for any losses resulting from such claim or action. Losses herein shall mean any expenses incurred by VK in connection with any such claim or action, including, but not limited to: payments to third parties or federal and/or local authorities made by VK in the settlement of such claim or action, or an obligation of VK to make such payments in pursuance of any effective court decision, writ of execution, judgement, instruction or other resolution of federal or local authorities, and other expenses incurred by VK in court or pre-trial proceedings.\n\n6. Limitation of Liability\n\n6.1. The Contributor provides the Contributions \"AS IS\", without any warranties, express or implied, including the warranties of quality, merchantability or fitness for a specific purpose, except for the representations and warranties expressly set out in Section 4.1 hereof.\n\n7. VK Designations\n\n7.1. The Contributor shall not use the logo, trademark or other VK’s means of identification without VK’s prior written consent.\n\n8. Governing Law and Dispute Resolution\n\n8.1. This Agreement shall be governed by and interpreted in accordance with the laws of the Russian Federation, exclusive of conflict of laws provisions. If the Parties fail to resolve a dispute, such dispute shall be submitted for resolution to a court at the location of VK.\n\n9. Miscellaneous\n\n9.1. This Agreement constitutes the entire and exhaustive agreement between the Parties in respect of the subject matter hereof and supersedes any previous arrangements and correspondence between the Parties existing before the date hereof (if any).\n\n9.2. If any provision of this Agreement shall be found invalid or unenforceable, such invalidity or unenforceability shall not affect the validity and unenforceability of other provision hereof or generally this Agreement.\n\n9.3. This Agreement is made in Russian and English languages. The Russian version is available at: https://mcs.mail.ru/docs/intro/start/legal/contributor. The English version is available at: https://mcs.mail.ru/docs/en/intro/start/legal/contributor. In the event of any discrepancy between the provisions of the Russian and English versions hereof, the Russian version shall prevail.\n\nRevision: from 09.26.2022\n","url":"/en/intro/start/legal/contributor","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"4809fc3b-1228-43ea-a4d0-3c2014cc060e","title":"FAQ","data":"\n\n\nHow do I contact support?\n\nTechnical support can be contacted through the communication channels listed at Terms of support.\n\n\n\nTo speed up the processing of your request, inform the technical support about your account information: login, project name and describe the issue in as much detail as possible.\n\n\n\nIf possible, attach any available diagnostic information: screenshots, logs and other materials.\n\n\n\n\n\nHow quickly does support respond?\n\nInformation about the timing of the response and the provision of a solution is in the SLA section.\n\n\n\n\n\nWhat is your work schedule?\n\nYou can contact technical support at any time, 24/7.\n\n\n\n\n\nI cannot change my phone number\n\nAfter registering a phone number, changing it is possible only through a request to technical support. The support team will contact the current owner to confirm the phone change.\n\n\n\n\n\nCan't register with my phone number\n\nRegistration via VK Cloud personal account is only possible for Russian numbers that start with +7.\n\n\n\n\n\nI forgot my account password\n\nRestore the password from the account. To do this, click Recover password on the authorization page.\n\n\n\n\n\nHow do I enable two-factor authentication?\n\nUse the instruction Enabling 2FA.\n\n\n\n\n\nLost my two-factor authentication app\n\nContact technical support. Specialists will make sure that you are the owner of the account, and then deactivate this option for your account. After that, re-enable two-factor authentication in personal account.\n\n\n\n\n\nI did not receive bonus points upon registration\n\nBonus points are awarded when the first project is created in VK Cloud; this option is not available for subsequent projects.\n\n\n\n\n\nI do not have enough quotas\n\nThe expansion of resource quotas is performed upon request to technical support.\n\n\n\n\n\nHow do I delete my account?\n\nThe account is deleted upon request to technical support on behalf of the account of the project owner. Before deleting the account, delete all existing resources, as well as untie the bank card (if available).\n\n\n\n\n\nI do not receive letters from support\n\nIf you have not received a reply by email:\n\nCheck the Spam and Trash folders in the mail.\nIf you have your own mail domain, check whether the MX record is configured correctly on it.\nMake sure that other emails reach the selected email address. To do this, send a test email to him from any other mailbox.\nContact technical support via another communication channel.\n\n\n\n\n\nWhere can I view data on the health and failures of services?\n\nThis data is published on the system status page.\n\n\n\n\n\nWhere can I change the data of a legal entity?\n\nTo fill in the data about the legal entity, go to personal account, click on the name of your account and select Project Settings from the drop-down menu. On the page that opens, on the General Information tab, you can change the data of the legal entity.\n\n\n\n\n\nIs there individual support for services in VK Cloud?\n\nIn VK Cloud you can connect the priority support service (Managed Services) from VK Cloud experts. To use this service, leave a request on the website.\n\nAlso check out the full list of Managed Services.\n\n\n\n\n\nDo you have an affiliate program?\n\nYes. Read more about the affiliate program in the documentation.\n\n\n\n\n\nWhat if I didn't find the answer to my question?\n\nIf you did not find the answer to your question on the portal, send your question to our support.\n\n\n","url":"/en/intro/start/faq","description":"Frequently asked questions.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"61332826-8acc-409e-9f27-1395010b0487","title":"About documentation portal","data":"Bookmarks\n\nVK Cloud documentation contains the special Bookmarks section for the quick access to favorite articles.\n\nBookmarks are saved within the current browser only.\n\nTo open the Bookmarks section, click the   icon in the page header.\n\nTo bookmark a page, click the   icon next to the title of the required article or section. The icon for adding a page is visible on section pages only on hover on article tile.\n\nTo remove a page from bookmarks, click the    icon next to the header of the required article or section.\n\nFeedback\n\nArticle evaluation\n\nTo evaluate the usefulness of the article, use the icons at the bottom of the page:\n\n  if the article was useful for you.\n  if you didn't find the answer to your question in the documentation. To get help or consultation, please contact the technical support.\n\nWhen you click an icon:\n\nyour evaluation of the article is sent to the VK company;\nthe icon changes its color saving your rate:  ;\npopup window appears with the corresponding text.\n\nSuggestions for changing the documentation\n\nIf you notice a typo or error in the documentation or you want to adjust a section, suggest changes directly in the\nsource texts. For this, click the Edit on GitHub button in the bottom of the page.\n\nFor more details, see the GitHub repository.\n\nContent formatting\n\nVK Cloud Documentation uses several ways to highlight different information types.\n\nInfo boxes\n\nThere are three types of info boxes:\n\nA hint with additional information that may be helpful.\n\nInformation that is important to know before proceeding.\n\nCritical information related to possible security issues or data loss.\n\nCode blocks\n\nExecutable commands, configuration files, and program code are displayed in special boxes with syntax highlighting.\n\nTo copy the contents of such a block, click the   icon in its header.\n\n// sample JavaScript code\nconsole.log('Hello World');\n\nHidden text\n\nFor large blocks of code and long examples, blocks of hidden text are used:\n\n\nA closed block looks like this\nAn example of hidden text.\n\n\nClick the block to see its contents. Click again if you want to hide the text back.\n\nTabs\n\nTabs are used for alternate instructions like commands for different operating systems terminals.\n\n\n\nFirst tab\nSecond tab\n\n\n\nThis is what the text looks like on the first tab.\n\nClick the second tab to switch to it.\n\n\n\n\nThis is what the text looks like on the second tab.\n\nClick the first tab to switch to it.\n\n\n\n\nIf the tabs contain instruction steps that are executed sequentially, the tabs are numbered.\n","url":"/en/intro/start/about-help","description":"Description of structural elements and design features specific to this portal.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Getting started","link":"/en/intro/start","isActive":false,"evenSimpler":true}]},{"uuid":"92614faf-c9f1-4b52-b48f-594719eaeaba","title":"Migration using Hystax Migration","data":"{include(/en/applications-and-services/marketplace/initial-configuration/migrate-hystax/migrate-hystax.md)}\n","url":"/en/intro/migration/migrate-hystax-mr","description":"Steps to migrate the client's infrastructure to the target VK Cloud project using Hystax Acura Migration.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Migration to VK Cloud","link":"/en/intro/migration","isActive":false,"evenSimpler":true}]},{"uuid":"5ca8afdc-504a-402b-8a32-52557ff8c120","title":"Hyper-V VM Migration to VK Cloud","data":"1. Check the possibility of migration\n\nThe Hyper-V virtual machine must meet the following requirements:\n\nThe VM operating system has a 64-bit architecture.\nVM uses BIOS emulation.\nThe current user has administrator rights.\nAt least one disk is connected to the VM.\n\n\n\nTo migrate a VM with UEFI emulation, use Hystax or transfer data to a new Hyper-V VM with BIOS emulation.\n\n2. Prepare the VM for migration\n\n\n\nLinux\nWindows\n\n\n\nCheck availability of VirtIO drivers in the system.\nCheck availability QEMU Guest Agent:\n\n      systemctl status qemu-guest-agent\n\n   If necessary, install QEMU Guest Agent.\n\n\n\n\nInstall VirtIO drivers.\nInstall QEMU Guest Agent.\nAdd driver information to the Windows registry:\n\n   Download Virtio Registry File.\n   Run the file and allow changes to be made to the registry.\n\n3. Export the virtual machine\n\n\n\nHyper-V Manager\nPowerShell\n\n\n\nStop the VM.\nStart the Hyper-V Manager.\nRight-click on the required VM and select Export.\nSelect where to place the VM files and click Export.\n\n\n\n\nStop the VM.\nRun PowerShell as an administrator.\nRun the command:\n\n      Export-VM -Name  -Path\n\n\n4. Import the VM image to VK Cloud\n\nTo load a VM image, use the OpenStack CLI to avoid possible errors when processing large files by the web interface.\n\nMake sure that OpenStack client is installed and authenticate to the project.\nConvert the resulting export file .vhdx to RAW format.\nUpload the file to an existing VK Cloud project:\n\n      openstack image create --private --container-format bare --disk-format raw --property store=s3 --file\n\n   If the VM needs to support backup, upload the file .raw indicating the metadata of the presence of a guest agent:\n\n      openstack image create --private --container-format bare --disk-format raw --file  --property hw_qemu_guest_agent=yes --property store=s3 --property os_require_quiesce=yes\n\nCheck the image download in personal account VK Cloud in section Cloud Computing → Images or via CLI:\n\n      openstack image list\n\n   The image should appear in the list and have the status ACTIVE.\n\n\n\nWhen creating a VM from a downloaded image, choose a disk size that is 25% larger than the size of the image, since it is stored in compressed form.\n\n\n","url":"/en/intro/migration/migrate-hyperv","description":"Migration of a Hyper-V VM to VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Migration to VK Cloud","link":"/en/intro/migration","isActive":false,"evenSimpler":true}]},{"uuid":"c569df82-b902-4560-8723-f9a14f11aafa","title":"VMware VM Migration to VK Cloud","data":"1. Check the possibility of migration\n\nThe VMware virtual machine must meet the following requirements:\n\nThe VM operating system has a 64-bit architecture.\nVM uses BIOS emulation.\nThe current user has administrator rights.\nAt least one disk is connected to the VM.\n\n\n\nTo migrate a VM with UEFI emulation, use Hystax or transfer data to a new VMware VM with BIOS emulation.\n\n2. Prepare the VM for migration\n\n\n\nLinux\nWindows\n\n\n\nCheck availability of VirtIO drivers in the system.\nCheck availability QEMU Guest Agent:\n\n      systemctl status qemu-guest-agent\n\n   If there is no QEMU guest agent, install it.\n\nCheck if the Cloud-Init utility is installed:\n\n      cloud-init --version\n\n   If the utility is missing, install it.\n\nCreate a file /etc/netplan/50-cloud-init.yaml with the following contents:\n\n      network:\n       ethernets:\n           ens3:\n               dhcp4: true\n       version: 2\n\nUninstall VMware Tools if this software is installed.\n\n\n\n\nCheck that the operating system updates are installed and restart the VM.\nInstall VirtIO drivers.\nInstall QEMU Guest Agent.\nAdd driver information to the Windows registry:\n\n   Download Virtio Registry File.\n   Run the file and allow changes to be made to the registry.\n\nUninstall VMware Tools if this software is installed.\n\n3. Export the virtual machine\n\nStop the VM.\n\n\n   If, after removing VMware Tools, the connection to the VM via SSH or RDP does not work, use the VMware console.\n\n\nSelect the required VM and export to the format .ovf.\n\nSeveral .ovf and .vmdk files will be created — a second file will be needed for further work.\n\n4. Import the VM image to VK Cloud\n\nTo load a VM image, use the OpenStack CLI to avoid possible errors when processing large files by the web interface.\n\nMake sure that the OpenStack client is installed and you can sign in to it.\nConvert a disk file from VMDK format to RAW:\n\n      qemu-img convert -f vmdk -O raw\n\nUpload the resulting export file .raw to an existing VK Cloud project.\n\n   Linux\n   Windows\n\n\n      openstack image create --private --container-format bare --disk-format raw --property store=s3 --file\n\n\n   When importing a Windows image, specify the disk bus type — IDE (hw_disk_bus parameter):\n\n      openstack image create --progress --private --container-format bare --disk-format raw  --property store=s3 --property os_type=windows --property hw_disk_bus=ide --min-disk 40\n\n\n   If the VM needs to support backup, add parameters to the command:\n\n      --property hw_qemu_guest_agent=yes --property os_require_quiesce=yes\n\nCheck the image download in personal account VK Cloud in section Cloud Computing → Images or via CLI:\n\n      openstack image list\n\n   The image should appear in the list and have the status ACTIVE.\n\n5. Create a virtual machine\n\n\n\nLinux\nWindows\n\n\n\n\nUse the imported image to create a Linux VM:\n\nwhen creating a VM in your personal account, select an image from the list;\nwhen creating via the OpenStack CLI, specify the image ID in the appropriate command.\n\n\n\n\n\nUse the imported image to create an intermediate Windows VM.\nAdd the VirtIO HBA driver to the Windows boot.\n\n   Create a disk minimum size and connect it to VM.\n   Run the virtual machine.\n   Run the VirtIO installer in repair mode.\n   Stop the virtual machine.\n\nCreate a image from the VM's boot disk.\nChange the disk bus type of the new image:\n\n      openstack image set --property hw_disk_bus=virtio\n\nCreate a target Windows VM from a new image.\nDelete the intermediate VM created in step 1, as well as the imported image.\n\n\n\n","url":"/en/intro/migration/migrate-vmware","description":"Migration of a VMware virtual machine to VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Migration to VK Cloud","link":"/en/intro/migration","isActive":false,"evenSimpler":true}]},{"uuid":"e4fa8b6f-a9cb-4025-9a49-4105c440542d","title":"Quick start","data":"A quick start will help you set up a new project to pay for services.\n\nAfter completing all the quick start steps, you will link a payment card to the project and top up the project balance for the first time. Besides that:\n\nindividuals will set up auto-replenishment of the balance;\nlegal entities will set up electronic document management and prepare a project for the correct unloading of invoices for payment.\n\nPreparatory steps\n\nStudy the cost of VK Cloud services:\n\n   Check out the price list.\n   Study the articles about the payment features for the services you need. Such articles are called Billing and are in the documentation of the required service. For example, article on Cloud Servers pricing.\n   Use the calculator to calculate the cost of the configuration you need.\n\nCheck out the list of accounting documents that the platform provides to individuals and legal entities.\nRegister in VK Cloud. When registering, a project will be created for you in region Moscow.\n\n1. Check the project balance\n\nGo to personal account VK Cloud.\nClick on the user name in the header of the page, select Balance and payments from the drop-down list.\n\nThe project balance will be displayed on the page, on it you will see the welcome bonuses accrued after registration.\n\nFor more information about the balance, see the article Balance management.\n\n2. Make sure that a map is linked to the project\n\nGo to personal account VK Cloud.\nClick on the user name in the header of the page, select Balance and payments from the drop-down list.\nCheck if the card is linked:\n\n   If the card is linked, its last 4 digits are displayed on the page.\n   If the card is not linked, the button is displayed on the page Link a card. In this case, bind it.\n\nOne card can be linked to only one project. When linking the card, a small amount will be debited from it, which will be credited to the project balance.\n\n3. Customize the project\n\n\n\nIndividuals\nLegal entities\n\n\n\nConfigure auto-completion from the card, if it was not configured earlier.\n\n\n\n\nDisable auto-completion from the card if it was enabled earlier.\nSet up electronic document management (EDM) with VK Cloud, following the instructions of your EDM operator.\n\n   If you need help with connecting EDM, send a request to the VK Cloud Document Management Department (docs_vktech@vk.company).\n\nGo through pre-registration to generate invoices for payment with full details.\n\n4. Add the initial amount to the balance\n\n\n\nIndividuals\nLegal entities\n\n\n\nGo to personal account VK Cloud.\nClick on the user name in the header of the page, select Balance and payments from the drop-down list.\nTop up the project balance in a convenient way for you.\n\n\n\n\nGo to personal account VK Cloud.\nClick on the user name in the header of the page, select Balance and payments from the drop-down list.\nCreate an invoice for the required amount in your personal account.\nPay the bill by bank transfer.\n\nCrediting of funds can take up to three banking days.\n\n\n\n\nFunds will be debited from the account as the paid resources are used. If the project goes into negative territory, it will be frozen.\n\nWhat's next?\n\nSee the available options expense details.\nSet a monthly spending limit for the project and set up notifications when the limit is reached.\n","url":"/en/intro/billing/quick-start","description":"Setting up the VK Cloud project for the convenience of paying for services and the first crediting of funds.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"59055898-aabb-4021-b64e-a7a215720010","title":"Project balance","data":"The balance of a project is the overall sum of funds on the project accounts, the main and the bonus ones.\n\nYou can view the project balance in the personal account. Information about the balance is available if the services are activated in the project.\n\nThe main and the bonus account\n\nFor each new project, two accounts are created automatically:\n\nThe main account:\n\n  You can replenish it using one of the available payment methods.\n  Funds on it can be refunded.\n  Funds on it never expire.\n\nThe bonus account — a dedicated account for storing bonuses:\n\n  You cannot replenish this account, funds can be credited to it by the platform only.\n  Funds on it cannot be refunded.\n  Funds on it can sometimes expire.\n\n    See details in subsection Bonuses.\n\nYou can pay for services from both accounts. Funds are first debited from the bonus account, and after all funds on the bonus account are spent from the main account.\n\nExpense and negative balance\n\nThe balance starts to be spent after the first chargeable objects are created, such as virtual machines and floating IP addresses.\n\nIf the funds on both accounts run out, the project balance becomes negative, and the project is automatically frozen. The project objects are placed in a queue for deletion, and a corresponding notification is sent to the project owner's email.\n\n\n\nFor legal entities that work on a post-payment, a negative balance does not lead to the freezing of the project.\n\n\n\nTo resume the services and avoid deleting objects, top up the balance. If the balance becomes positive, the project will be automatically unfrozen within 4 hours. After defrosting, all objects that have been stopped must be started manually.\n\nBonuses\n\nBonuses, or bonus points, are virtual funds credited to the project balance by the VK Cloud platform itself. When paying for services, bonuses are the same as real funds.\n\nBonuses are credited:\n\nWhen a new account is registered on the platform. These sign-up bonuses are credited only once, after services are activated in the project that is created for the account. The amount of bonuses depends on the region of the project:\n\n  Moscow — 3000 bonuses;\n  Kazakhstan — 24000 bonuses.\n\n  One bonus point for region Moscow equals one ruble, for region Kazakhstan — one tenge.\n\n\n  Unspent sign-up bonuses expire in 60 calendar days after the account registration.\n\n\nDuring some advertising campaigns.\nWhen paying refunds for the technical failures that occurred through the fault of VK Cloud.\n","url":"/en/intro/billing/concepts/balance","description":"What is a project balance in VK Cloud, what are bonuses and when they are credited.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"2574e73f-bfa9-4800-b532-a6aeb9009d8c","title":"Billing of individuals and legal entities","data":"This article describes the work schemes and opportunities that are available in VK Cloud to individuals and legal entities.\n\nBy default, the scheme of work for individuals is used in new projects. This scheme does not imply receiving accounting accounting documents from VK Cloud.\n\nFor the convenience of legal entities, opportunities have been implemented that allow you to receive accounting documents, optimize document management and help pay for services more conveniently. These options need to be additionally connected and configured in the project.\n\nOrganizations and individual entrepreneur can work with VK Cloud as individuals or legal entities of their choice.\n\n\n\nIf you plan to work with VK Cloud as a legal entity, go through pre-registration in your personal account.\n\nPayment scheme\n\nPrepayment\n\nThe user deposits the project balance in advance for a certain amount. The deposited funds are transferred to the main account of the project, from where they are then spent on paying for services. If a project goes into negative territory, it is automatically frozen until the balance becomes positive.\n\n\n\nIndividuals\nLegal entities\n\n\n\nIndividuals work with VK Cloud only on prepayment.\n\n\n\n\nBy default, legal entities work on prepayment, but they can also work on post-payment.\n\nWhen working on prepayment, only a universal transfer document is provided to a legal entity from accounting documents. Other documents are optional.\n\nPost-payment\n\nThe user is billed once a month for using the services, which he pays for. The project balance under this scheme is always negative, but the project is not frozen.\n\n\n\nIndividuals\nLegal entities\n\n\n\nIndividuals cannot work on a post-payment.\n\n\n\n\nPost-payment work is available if you conclude an individual contract with VK Cloud, in which this scheme is fixed. By default, legal entities work on prepayment.\n\nWhen working on post-payment, only a universal transfer document is provided to a legal entity from accounting documents. Other documents are optional.\n\nType of contract\n\n\n\nIndividuals\nLegal entities\n\n\n\nIndividuals work with the platform under public offer agreements relevant services. On the part of VK Cloud, public offer agreements are concluded:\n\nfor projects in region Moscow — on behalf of “VK” LLC, registered in the Russian Federation;\nfor projects in the region of Kazakhstan — on behalf of “QazCloud” LLP, registered in Kazakhstan.\n\n\n\n\nLegal entities work with VK Cloud either on the basis of contracts public offer of the relevant services, or under an individual contract.\n\nIf you have any questions related to contracts, please write to the document management department of VK Cloud at docs_vktech@vk.company.\n\nPublic offer agreements\n\nBy default, all users work under public offer contracts. On the part of VK Cloud, public offer agreements are concluded:\n\nFor projects in region Moscow — on behalf of “VK” LLC, registered in the Russian Federation.\nFor projects in the region of Kazakhstan — on behalf of “QazCloud” LLP, registered in Kazakhstan.\n\nIndividual agreement\n\nIf your legal entity is not a resident of the country corresponding to the project region, or you need more flexible terms of the contract — for example, a postpaid settlement scheme — enter into an individual agreement with VK Cloud.\n\nRecommended payment methods\n\nBoth individuals and legal entities have access to all payment methods in the relevant region, but some of them are more convenient for individuals, others for legal entities.\n\n\n\nIndividuals\nLegal entities\n\n\n\nA bank card linked to the project is the most convenient payment method. Advantages of this method:\n\nyou can set up auto-top-up balance;\nwhen depositing manually, you do not need to enter the card details every time.\n\n\n\n\nIt is recommended to pay for services by bank transfer, since only such payments can be reflected in accounting documents.\n\n\n\nLegal entities can also top up the project balance with a bank card — for example, if you need to urgently transfer funds to avoid freezing the project. Write-offs for payments from bank cards are not included in accounting documents for legal entities: billing report, reconciliation report.\n\nAdditional registration of legal entities\n\nAdditional registration means saving the details of a legal entity in the project settings in the personal account. You do not need to specify banking details manually, they are filled in automatically after entering the individual taxpayer number.\n\nAfter additional registration, those organizations that work on prepayment can upload invoices for payment with full details from the personal cabinet.\n\nEDM\n\n\n\nIndividuals\nLegal entities\n\n\n\nFor individuals, working with VK Cloud through electronic document management systems (EDM) is not available.\n\n\n\n\nOrganizations that work with VK Cloud through the electronic document management systems (EDM), receive from VK Cloud reporting documents in electronic form.\n\nOrganizations that do not work according to EDO receive original accounting documents to their mailing address.\n\nBy default, EDM is not connected.\n\n\n\nEDM work is available only to organizations-residents of the Russian Federation who work in region Moscow.\n\nDifferences for individuals and legal entities: results\n\n| | Individuals | Legal entities |\n| -- | -- | -- |\n| Payment scheme | Prepayment | Prepayment or post-payment |\n| Type of contract | Public offer agreements | Public offer contracts or individual contract |\n| Recommendedpayment method | The card linked to the project | Bank transfer |\n| Additional registration | — | Optional |\n| Connecting to EDM | — | Optional |\n\nA dash (—) means that the option is unavailable or irrelevant.\n","url":"/en/intro/billing/concepts/physical-corporate","description":"Differences in payment and document management procedures in VK Cloud for individuals and legal entities.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"380afae3-e63e-4599-98ca-5bafa2b05a98","title":"Payment methods","data":"There are several ways to top up the balance of the project. The deposited funds are transferred to the main account of the project, from where they are then spent on paying for services.\n\nThe list of available payment methods is displayed on the Balance page of the personal account when you click on the button Make a payment. The available methods depend on region.\n\n\n\nMoscow\nKazakhstan\n\n\n\nBank card.\n\n\n  It is recommended to use physical bank cards. Payments from virtual cards can be rejected by the security system.\n\n\n  Bank cards issued in Russia with ruble currency are supported.\n\n  For all cards except UnionPay, you can set up auto-top-up.\n\nElectronic money.\n\n  SberPay and YooMoney are supported.\n\nMobile payments from the phones of the main mobile operators.\nBank transfer.\n\n\n   In accounting documents for legal entities, only payments made by bank transfer are taken into account.\n\n\nBank card.\n\n    Only physical Visa and MasterCard cards issued by banks of Kazakhstan, Armenia, Azerbaijan, Belarus, Kyrgyzstan, Uzbekistan, Tajikistan, Lithuania, Latvia, Estonia and Georgia are supported.\n\n    Auto-completion is not supported in the Kazakhstan region.\n\nBank transfer.\n\n    This option is not available in your personal account. To pay using this method, contact technical support and request an invoice, providing the details of the organization, project ID (PID) and the amount in tenge.\n\n\n\n","url":"/en/intro/billing/concepts/payment-methods","description":"Payment methods for the VK Cloud project in the Moscow region, payment methods for the project in the Kazakhstan region.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"5368e348-60d2-47bd-9a02-894fdfb48fd8","title":"Accounting documents","data":"Users receive financial documents from VK Cloud for all payments made. The composition of such documents differs for individuals and legal entities.\n\nIndividuals\n\nWhen replenishing an account, individuals receive a payment receipt in accordance with Federal Law 54 «О применении контрольно-кассовой техники». The receipt is sent to the VK Cloud project owner's email address. The original payment receipt is not provided.\n\nLegal entities\n\nAt the end of each reporting period, VK Cloud provides the counterparty with a package of accounting accounting documents. The reporting period is equal to one calendar month, the documents are signed on the last date of this month.\n\nThe documents are sent to the recipients in the way that was chosen at the beginning of the cooperation, in the following terms:\n\nIf an agreement of amounts is required — around the 15th of the month following the reporting period.\nIf approval is not required — on the 7th-10th of the month following the reporting period.\n\n\n\nIf an organization has several projects, one set of documents is generated for all projects. Upon request, separate packages of documents for each project can be provided.\n\n\n\nHow to start receiving reporting documents from VK Cloud, read the article Additional project configuration for legal entities. If you have any questions related to accounting documents, please write to the document management department of VK Cloud at docs_vktech@vk.company.\n\n\n\nThe accounting documents reflect only expenses carried out as bank transfers from a legal entity.\n\nComposition of accounting documents\n\n\n\nWork under the public offer agreement\nWork under an individual contract\n\n\n\nThis refers to work on service offer agreements. This scheme is used by default in all projects, for individuals and legal entities.\n\nLegal entities working under such an agreement are provided with only one document — a universal transfer document. The form of the universal transfer document contains all the details of the invoice, set out in p.p. 5, 5.1, 5.2 art. 169 tax code of the Russian federation.\n\nThe invoice and the Certificate of work performed are not provided, as the universal transfer document replaces both of these documents.\n\n\n\n\nThe composition of the accounting documents depends on the specific terms of the contract. In particular, there are differences for organizations working on prepayment and on post-payment.\n\nWhen working on prepayment:\n\n  Main documents: universal transfer document.\n\n  Additional documents, if required by contract:\n\n    Invoice\n    Certificate of completed works\n\n\n    If an invoice is provided, the universal transfer document is not provided. The certificate of completed works can be provided together with the universal transfer document.\n\n\n    An invoice for payment is not provided. Organizations that work on prepayment form it independently in their personal account.\n\nWhen working on a post-payment:\n\n  Basic documents:\n\n    Universal transfer document\n    Payment invoice\n\n  Additional documents, if required by contract:\n\n    Invoice\n    Certificate of completed works\n\n\n    If an invoice is provided, the universal transfer document is not provided. The certificate of completed works can be provided together with the universal transfer document.\n\n\nIn addition to the documents provided for in the contract, upon request, the following documents can be provided:\n\nBilling report for the selected period.\n\n    It contains details on resource consumption and is used to confirm amounts in accounting documents. You can upload such a report from your personal account.\n\nReconciliation report with the details of mutual settlements.\n\nMethods of obtaining accounting documents\n\nDocuments are sent through the electronic document management system (EDM).\n\nIf electronic document flow with the organization is not possible, VK Cloud sends paper originals of documents to the address of the organization.\n\n\n\nVK Cloud never sends documents to a legal address without an explicit agreement with the organization.\n\nElectronic document management system (EDM)\n\nVK Cloud works in the EDM system Kontur.Diadoc.\n\nIf your organization works with another EDI operator from the register of the Federal Tax Service of the Russian Federation, set up roaming to exchange documents with VK LLC using its EDM participant ID — 2BM-7743001840-2012052807514600749280000000000.\n\nYou can configure EDM with VK Cloud yourself or send a request for configuration to the document management department of VK Cloud (docs_vktech@vk.company).\n\n\n\nEDM work is only available for projects in region Moscow and only organizations registered in the Russian Federation.\n\nDelivery of original documents\n\nTo set up paper workflow, contact the document management department of VK Cloud:\n\nWrite to docs_vktech@vk.company.\nSpecify:\n\n   The required method of document delivery: by mail or courier service.\n   (If courier delivery is selected) The recipient's contact phone number.\n   Full name of the recipient.\n   Recipient's address. If the address matches the legal address of the organization, specify it explicitly in the request.\n\nScanned copies of documents\n\nVK Cloud can provide scanned copies of accounting documents in PDF format by e-mail — every month or on a one-time basis, upon request. To get scanned copies, write to the document management department of VK Cloud at docs_vktech@vk.company.\n\nTax accounting\n\nFor tax residents of the Russian Federation, the amount of VAT is indicated in payment and accounting documents.\n","url":"/en/intro/billing/concepts/report","description":"What financial documents are provided to VK Cloud, in what time frame and how to get them.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"5be509ba-75ca-4b5e-9170-407592b584c9","title":"Operations with a bank card","data":"Bind the card\n\nThe card can be linked to the project:\n\nwhen registering account (if required);\nwhen activating services in the project (if required);\nindependently at any time.\n\nIt is recommended to use physical bank cards to pay for services, since payments from virtual cards can be rejected by the security system.\n\n\n\nLegal entities are recommended to pay for services using bank transfers. Card payments are not taken into account in accounting documents.\n\n\n\nTo bind the card:\n\nGo to your personal account VK Cloud.\nOpen the project balance page in one of the following ways:\n\n    Click on the user name in the header of the page, select Balance and payments from the drop-down list.\n    Go to Balance.\n\nClick the Add card button.\nOn the Add card tab enter the card details in the fields Номер карты, месяц / год and CV-код.\n\n    The amount at the top of the window will be debited from the card and credited to the main account of the project.\n\nClick the Добавить карту button.\nIn the payment confirmation window, enter the SMS code received from the bank.\n\n    After successful debiting, the card will be linked.\n\n(Optional) On the tab that opens Auto-top-up configure auto-top-up balance. To skip this step, turn off the option Enable auto-top-up balance.\nClick the Save button.\n\n\n\nThe full list of payment methods is in the article Payment methods.\n\nConfigure auto-completion\n\nAuto-top-up allows you to automatically top up the [main project account] (../../concepts/balance) from the linked card when the balance on the account reaches the specified value. You can set up auto-completion at any time.\n\nThe enabled auto-completion does not guarantee that there will always be funds on the project account. For example, if there are not enough funds on the card, the payment will not be made.\n\n\n\nIn region Kazakhstan auto-completion is not available.\n\n\n\nTo activate auto-completion:\n\nGo to your personal account VK Cloud.\nOpen the project balance page in one of the following ways:\n\n    Click on the user name in the header of the page, select Balance and payments from the drop-down list.\n    Go to Balance.\n\nOn the balance page, click on the link Activate auto-top-up.\nEnable the option Enable auto-top-up balance.\nFill in the fields:\n\n    Auto-top-up amount: specify the amount in the range from 100 to 10000 rubles for which the balance will be automatically replenished.\n    Replenishment with a balance: specify the balance of funds at which auto-replenishment will occur — an amount in the range from 0 to 9999999 rubles.\n\nClick the Save button.\n\nUnbind the card\n\nTo untie the card, contact technical support and provide information:\n\nproject ID (PID);\nthe last 4 digits of the linked card number.\n\n\n\nIf you are a legal entity and work on prepayment, leave the card as an additional means of payment for urgent payments. In emergency cases, a one-time card payment can help you to avoid project freezing.\n\nWrite-offs for payments from bank cards are not included in accounting documents for legal entities: billing report, reconciliation report.\n\n\n","url":"/en/intro/billing/service-management/add-card","description":"How to link or unlink a card, how to set up auto-completion in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"133535c8-339f-4d8d-b209-136c3f119a6f","title":"Balance management","data":"\n\nViewing project expenses details and making payments is available to users whose role in the project is Project owner, Superadministrator, or Billing administrator.\n\nViewing the balance\n\nAt the top of the personal account page, the balance total is displayed: the sum of funds on both project accounts.\n\nTo view the balance:\n\nGo to VK Cloud personal account.\nMake sure that the project has services activated.\nOpen the balance page using one of these options:\n\n   Click on your user name in the top part of the personal account page and select Balance and payments from the drop-down list.\n   Select the Balance section in the side menu.\n\n   The page displays the balance of main and bonus funds.\n\n\n\nThe page also displays detailed expenses on all projects in which your role is Owner, Superadministrator, or Billing Administrator. The data on the Details и Expenses on services tabs are by default pre-filtered by the current project.\n\nMaking a payment\n\nIt is possible to credit funds to the main account of the project only and in the project currency only.\n\nTo make a payment:\n\nGo to VK Cloud personal account.\nClick on the Make a payment icon at the top of the personal account page or click on the Make a payment button on the balance page.\nSpecify the amount of the deposit.\n\n   The minimum amount that can be credited is 10 rubles for projects in the Moscow region and 100 tenge for projects in the Kazakhstan region.\n\nSelect one of the payment methods.\n\n   Two options are available for paying via a bank card:\n\n   Card: for paying with a card that is linked to the project.\n\n      When this option is selected, it is not necessary to enter the card details.\n\n   Bank card: for paying with a card that is not linked to the project.\n\n\n   For legal entities, the recommended payment method is Bank transfer. Payments made using other methods are not included in the financial documents for legal entities.\n\n\nExpenses notifications\n\nYou can set up a monthly expenses limit for each of your projects. When an expenses total for the current calendar month exceeds the limitation, a notification is sent to the project Owner email.\n\nTo set up an expenses limitation and notifications:\n\nGo to VK Cloud personal account.\nOpen the balance page using one of these options:\n\n   Click on your user name in the top part of the personal account page and select Balance and payments from the drop-down list.\n   Select the Balance section in the side menu.\n\nOn the balance page, go to the Limitation tab.\n\n   The tab displays a list of all projects in which your role is Owner or Superadminitrator.\n\nClick on the Settings icon opposite to the project.\n\n   The limit configuration window opens.\n\nEnable the Notify of excess costs option.\nSpecify the limit value (from 1 to 999999999 in the currency of the project) and click Save changes.\n","url":"/en/intro/billing/service-management/payment","description":"How to view, top up the VK Cloud project balance and configure notifications.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"507a6d31-137c-4cd1-88ec-0716d91bc7fb","title":"Detailing expenses","data":"All deductions and additions for projects are recorded in the VK Cloud personal account on the project balance page.\n\nExpenses details are available to users with the roles of Owner, Superadministrator, Project administrator and Billing administrator.\n\n\n\nDeductions and additions for projects in the Moscow region are tied to the Moscow Time Zone (GMT+3), in the Kazakhstan region —  to the Astana Time Zone (GMT+6).\n\nThe same principle applies to the formation of accounting documents.\n\nViewing deductions and additions\n\nGo to personal account VK Cloud.\nOpen the balance page using one of these options:\n\n   Click on your user name in the top part of the personal account page and select Balance and payments from the drop-down list.\n   Select the Balance section in the side menu.\n\nGo to Detail tab.\n\n\n   Amounts on the tab are rounded to two decimal places. A similar principle applies for services cost calculation and accounting documents.\n\n\nIf necessary, set filters on the displayed records:\n\n   Set the date, if necessary, use the details for the period.\n   Specify the type of funds movement: deductions or additions.\n   Configure the displayed projects by clicking on the icon Filter and by selecting projects from the list.\n\nViewing resource consumption statistics\n\nGo to personal account VK Cloud.\nOpen the balance page using one of these options:\n\n   Click on your user name in the top part of the personal account page and select Balance and payments from the drop-down list.\n   Select the Balance section in the side menu.\n\nGo to Expenses on services tab.\nIf necessary, set filters on the displayed records:\n\n   Set the date, if necessary, use the details for the period.\n   Specify the type of funds movement: deductions or additions.\n   Configure the displayed projects by clicking on the icon Filter and by selecting projects from the list.\n\n     The list shows projects in which your role is owner or superadministrator. By default, the data is filtered by the current project.\n\nDownloading the report\n\nGo to personal account VK Cloud.\nOpen the balance page using one of these options:\n\n   Click on your user name in the top part of the personal account page and select Balance and payments from the drop-down list.\n   Select the Balance section in the side menu.\n\nIf necessary, set filters on the displayed records.\nGo to Detail tab.\nClick on the icon Download.\nIn the window that opens:\n\n   Report type: specify the type of report to download: docx, xlsx, Для бухгалтерии.\n   Select the projects for which the report will be uploaded.\n\nClick the Download the Report.\n","url":"/en/intro/billing/service-management/detail","description":"View statistics on resource consumption and balance changes on VK Cloud projects.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"8b6a00be-9b3a-4f74-93c1-83ca0390cafc","title":"Formation of invoices for payment","data":"It is necessary to form invoices:\n\nTo pay for services by bank transfer. When paying by other [methods] (../../concepts/payment-methods), the account is not needed.\nIf the user works with VK Cloud on prepayment. When working on a post-payment, the user receives invoices for payment from VK Cloud.\n\nTo create an invoice:\n\nGo to personal account VK Cloud.\nOpen the balance page using one of these options:\n\n   Click on your user name in the top part of the personal account page and select Balance and payments from the drop-down list.\n   Select the Balance section in the side menu.\n\nClick the Make a payment button.\nIn the window that opens, specify the amount of the deposit.\nSelect the option Bank transfer and click Continue.\n\nAn invoice for the specified amount will be automatically generated and uploaded to your computer.\n\nFor individuals and legal entities who have not passed additional registration in the personal account, the account contains only the details of the service provider, the purpose of the payment and the amount to be paid.\n\nIf a legal entity has passed additional registration, the generated account contains the full details necessary for processing a bank payment order.\n\n\n\nIn the payment order, in the purpose of the payment, be sure to specify project identifier (PID).\n\n\n","url":"/en/intro/billing/service-management/bill-generation","description":"How to create an invoice for VK Cloud services for an individual and a legal entity.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"70f4757d-306b-40d8-bd91-415b601ed4f9","title":"Refund","data":"Terms of return\n\nThe request for a refund must be issued by the project owner.\nThe money can only be refunded from the main account of the project. The refund amount cannot exceed the amount of unspent funds on the main account.\nThe money will be refunded:\n\n  to individuals — to a bank card or an electronic wallet from which the balance was replenished, based on checks;\n  to legal entities — to the settlement account of the legal entity specified in the application, on the basis of an application for a refund.\n\nThe refund is possible only in the project currency.\n\n\n\nThe refund is available regardless of how long ago the project account was replenished.\n\nMaking a refund for individuals\n\nGo to your VK Cloud personal account under the account of the project owner.\nPreserve the project to stop debiting funds.\nCopy the project ID (PID) from the VK Cloud website URL.\n\n   Example URL for the Moscow region: https://msk.cloud.vk.com/app/en/mcs123456789/main, where mcs123456789 is the project ID (PID).\n\nIn the header of the personal account page, open the menu Help and select Contact support.\n\n   You can also go to the technical support portal by direct link.\n\nCreate a request with the following parameters: VK Cloud: Биллинг: Оплата и возврат: Возврат денежных средств.\nSpecify in the request:\n\n   Region.\n   Project ID (PID).\n   Refund amount and payment date.\n\n     If there were several payments with the same details, specify the date of the first payment and the total amount of the refund.\n\n   The reason for the return in the field Описание.\n\nSpecify the details for the refund. Usually the funds are returned according to the details from which the payment was made.\n\n    If the amount to refund was transferred from the card, enter the first 6 digits and the last 4 digits of the card in the Укажите номер карты field.\n\n\n      The refund can be made to the card account even if the card is inactive.\n\n\n    If the account was replenished using other payment methods (for example, YooMoney), in the Укажите номер карты field, enter any numbers and specify the details of the wallet or means of payment in the request text.\n\n    If you need to refund to bank details different from those from which the account was replenished:\n\n      In the field Укажите номер карты enter any numbers.\n      Attach the file with the refund request. The request is written in free form, can be handwritten or printed, it must specify:\n\n          the reason for changing the details — for example, due to account closure;\n          current details for the refund;\n          the signature of the project owner and the date.\n\nAttach screenshots of electronic checks for replenishment of the balance for the specified amounts or links to such checks to the request.\n\nThe response to the request will be sent to the email linked to your account.\n\nThe refund request is processed within 48 hours. The time of receipt of funds depends on the payment method. When credited to the card, it can take up to 30 days, depending on the bank.\n\nMaking a refund for legal entities\n\nGo to your VK Cloud personal account under the account of the project owner.\nPreserve the project to stop debiting funds.\nCopy the project ID (PID) from the VK Cloud website URL.\n\n   Example URL for the Moscow region: https://msk.cloud.vk.com/app/en/mcs123456789/main, where mcs123456789 is the project ID (PID).\n\nIn the email account associated with your VK Cloud personal account, create an email with the subject “VK Cloud: Возврат денежных средств”.\nSpecify in the email:\n\n   Region.\n   Project ID (PID).\n   Refund amount and payment date.\n\n     If there were several payments with the same details, specify the date of the first payment and the total amount of the refund.\n\n   The reason for the return.\n\nAttach a file with an application for a refund. The application:\n\n    must be certified by the signature and seal of the general director or an authorized power of attorney;\n    must contain the refund amount;\n    must contain the full payment details for the refund.\n\nSend the email to the VK Cloud document management department at docs_vktech@vk.company.\n\nThe response to your request will be sent to the email linked to your account.\n\nThe refund request is processed within 48 hours. The time of receipt of funds depends on the payment method. When credited to the card, it can take up to 30 days, depending on the bank.\n","url":"/en/intro/billing/service-management/refund","description":"Methods and conditions for returning funds from the main account of your VK Cloud project.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"e6f3b098-9490-4a45-a8a4-6e2715b6a0d1","title":"Additional project configuration for legal entities","data":"VK Cloud provides special features for legal entities. Contractors-legal entities can receive accounting accounting documents from the platform. They have the opportunity to conclude an individual contract with VK Cloud.\n\nConclusion of an individual contract\n\nLegal entities by default work with VK Cloud on the basis of contracts public offer. It is possible to conclude an individual contract with VK Cloud, for more information, see the article Billing of individuals and legal entities.\n\nTo conclude an individual contract:\n\nWrite to VK Cloud customer service at sales-team@mcs.mail.ru.\nSpecify:\n\n   Full name of the organization and its details.\n   Full name and phone number of the contact person.\n\nThe manager will contact you to clarify the required terms of the contract.\n\nSetting up the workflow\n\nA contractor who works with VK Cloud as a legal entity receives a package of accounting documents on their projects from the platform on a monthly basis. The document flow is carried out in electronic form (according to EDM) or, if work on EDM is impossible, in paper form.\n\nTo set up workflow with VK Cloud:\n\nWrite to the document management department of VK Cloud at docs_vktech@vk.company.\nSpecify:\n\n   Full name of the organization and its details.\n   What accounting documents you require.\n   Document management format (EDM or paper).\n   Full name and phone number of the contact person.\n\nAdditional registration of legal entities\n\nAfter registration in VK Cloud, you can go to the project settings and complete the additional registration of the legal entity, specifying your details.\n\n\n\nFilling in the details in the personal account is only available in region Moscow and only Russian legal entities.\n\n\n\nIf the organization works with VK Cloud on prepayment, after additional registration, you will be able to upload from your personal account accounts that specify the details of both the supplier and the buyer of services. If the additional registration is not completed, only the details of the supplier are indicated in the invoice.\n\nTo complete additional registration:\n\nIn your VK Cloud personal account, go to project settings to the General information tab.\nClick the Fill button.\nIn the Pre-registration window, enter the TIN of the organization in the field of the same name.\n\n   If the TIN is found in the contractor database, most of the organization's details will be filled in automatically.\n\nFill in the fields that were left empty and click Next step.\nIn the window that opens, specify the legal address of the organization.\n\n   If the TIN is found in the contractor database, the address is filled in automatically. If necessary, the pre-filled field values can be edited.\n\n\n   If there is a paper document flow with the organization, VK Cloud always specifies which address to send them to before sending documents.\n\n\nClick the Save button.\n","url":"/en/intro/billing/service-management/corporate","description":"Additional project configuration for the correct registration of legal and financial documents in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"f1ced811-f6c4-4183-ab37-7540f6fbe1f3","title":"Tariffication","data":"Tariffication is based on the “pay as you go” principle: you are charged only for the resources you consume to the nearest minute, the fee for using paid OS is charged once a day.\n\nThe cost of network services is given in price list. You can use calculator to calculate the total cost of platform services.\n\nAvailable operations:\n\nTo calculate the cost of services, add the necessary services and configure the amount of resources consumed.\nTo configure the number of such configurations for the service, use the field Number of services in this configuration.\nTo view detailed information about the received configuration cost, click on the icon Detailed calculation or Detailed calculation button.\nTo add another configuration of the selected service, click Add new configuration.\nTo delete the added configuration, click Remove from calculation.\n\nThe total cost will be displayed in the line Final calculation.\n\n\n\nThe total cost is rounded to two decimal places. A similar principle applies for detailing and  accounting documents.\n\n\n","url":"/en/intro/billing/tariffication","description":"Calculation of the cost of using VK Cloud services: manually using a price list or using an online calculator.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"732e19ed-0900-4af0-b265-662acf16caf9","title":"FAQ","data":"Payment, refund, bonuses\n\n\n\nHow do I get bonuses for registering in my personal account?\n\nBonuses are credited when registering a new account. Bonuses are credited after the activation of services in the project, when all the stages of account confirmation have been completed.\n\n\n\nIf an existing user creates a new project or a new user enters the project by invitation, bonuses are not awarded.\n\n\n\n\n\n\n\nProcedure for charging bonuses. Why could they have expired?\n\nBonuses in VK Cloud are spent on using cloud services and resources. One bonus is equal to one ruble.\n\nBonuses are given for 60 days, after that they burn off. You can see the details of your expenses on the Balance page in your personal account.\n\n\n\nYou cannot withdraw bonuses from the project.\n\n\n\n\n\n\n\nAre funds being charged after the end of the test period?\n\nAfter the test period has expired or bonuses have been burned off, the main balance is used to pay for resources. If the balance is zero, the project is frozen. If the balance is positive, funds will be charged for resource usage.\n\n\n\n\n\nHow are funds charged?\n\nVK Cloud records the consumption of resources in the cloud on a minute-by-minute basis, and funds for the use of resources are charged from the balance also on a minute-by-minute basis. Licenses are charged once a day.\n\n\n\n\n\nThe project does not display the balance at all. How do I turn it on?\n\nBalance is displayed:\n\nIf your role in the project is the owner, superadministrator or billing administrator.\nAfter the services are activated in the project. To understand if the services are activated:\n\n  Go to VK Cloud personal account.\n  Go to the section of any service. If the Enable services button is displayed, activate them by pressing this button.\n\n    During activation, you may need to confirm the phone number and link the payment card.\n\n\n\n\n\nWhat is the risk of a negative balance?\n\nNegative balance is a negative value of personal account balance for clients on prepaid payment system.\n\nNo penalties are charged on negative balance. To use the services and resources of the project it is necessary to have a positive balance.\n\n\n\nUnlocking of services takes up to 4 hours from the moment of crediting.\n\n\n\n\n\n\n\nHow much do I need to replenish my account to resume work?\n\nProject is automatically resumed when the balance is positive.\n\nIt is recommended to deposit an amount equal to the daily charge, or enable auto-top-up.\n\n\n\n\n\nI have paid the bill, the services in the project are active. Why is the balance negative?\n\nYou are probably a legal entity and work with VK Cloud for a post-payment. In this case, the balance of your project will always be negative.\n\nIf you do not know your payment scheme, contact the document management department of VK Cloud:\n\nWrite to docs_vktech@vk.company.\nSpecify:\n\n   Full name of the organization.\n   Details of the organization.\n\n\n\n\n\nHow to pay for the work of a service from a legal entity?\n\nIf you work with VK Cloud on prepayment, form an invoice and pay it in your personal account.\n\nWhen working on the post-payment scheme, the invoice and accounting documents for the reporting period are sent via the electronic document management system (EDM) within five working days of the month following the reporting one.\n\nIf you have not received the accounting documents, contact the document management department of VK Cloud:\n\nWrite to docs_vktech@vk.company.\nSpecify:\n\n   Full name of the organization.\n   Details of the organization.\n\n\n\n\n\nHow to connect the post-payment scheme?\n\nPost-payment is available:\n\nOnly for legal entities.\nOnly within the framework of an individual contract with VK Cloud.\n\nTo activate the postpayment scheme, conclude an individual contract and perform additional project configuration for legal entities in the VK Cloud personal account.\n\n\n\n\n\nHow long does it take for funds to be credited when paying by bank transfer?\n\nUp to 3 banking days.\n\n\n\n\n\nWhy should I link a card if I am a legal entity and pay according to the details?\n\nYou may need the card if you work on a prepaid basis and need to make an emergency top-up, for example, to avoid freezing a project.\n\n\n\nCharges for bank card payments are not included in accounting documents for legal entities: billing report, reconciliation report.\n\n\n\n\n\n\n\nWhen and how will I receive the accounting documents after payment?\n\n\n\nIndividuals\nLegal entities\n\n\n\nImmediately after the funds are credited, the receipt is sent to the email address of the project owner.\n\n\n\n\nAs part of the package of accounting documents for the corresponding reporting period (month).\n\nWhen working through the electronic document management system (EDM), documents are available immediately. If the organization maintains a paper document flow with VK Cloud, the delivery time of the original documents is added.\n\nIf you have not received your original documents or need scanned copies, write to the document management department of VK Cloud at docs_vktech@vk.company.\n\n\n\n\n\n\n\n\nHow do I get a payment receipt from a specific date?\n\nContact technical support.\nSpecify:\n\n   Project ID (PID)\n   Project region\n   Amount and date of payment\n   Payment confirmation\n\n\n\n\n\nI have topped up the balance, but the payment is not credited. What to do?\n\nThe terms of crediting funds depend on the payment method:\n\nPayment by bank transfer — payment can take up to three banking days from the date of transfer of funds by the bank.\nPayment by other methods — the payment is credited within 5-10 minutes after payment confirmation.\n\nIf the payment is not received within the specified time, contact technical support to search for it and specify:\n\nProject ID (PID)\nProject region\nPayment confirmation\n\n\n\n\n\nHow do I make a refund?\n\nContact technical support. For more information about the refund conditions and the requirements for the application, see Refund of funds.\n\nYou can refund only funds that you have previously credited to your account. Bonuses cannot be refunded.\n\nFor individuals, the funds will be returned to the account from which the payment has been made.\n\nFreezing the project\n\n\n\nThe balance has become negative, how can the services be restored?\n\nWhen working on prepayment with a negative balance, the project is automatically frozen. Its objects are placed in the queue for deletion for a period:\n\n3 days if you have never replenished the project account.\n30 days if you have topped up the project account at least once.\n\n\n   As soon as the storage period in the queue expires, all project resources will be permanently deleted.\n\n\nTo restore the services:\n\nGo to VK Cloud personal account.\nReplenish the balance of the main account of the project in a convenient way. Use physical bank cards to get the funds to the balance faster. Funds are credited from them immediately, but if you pay by bank transfer, the process can take up to 3 working days.\nWait for the activation of services in the project. It can take up to 2 hours.\nManually activate project resources:\n\n   Virtual machines\n   Kubernetes containers\n   VM backup\n   Other resources\n\n\n\nTo find out how long resources are in the queue for deletion, contact technical support and specify:\n\nProject ID (PID)\nRegion\n\n\n\n\n\n\n\nI have topped up my balance to unblock the services. How long to wait?\n\nUnlocking can take up to 4 hours.\n\nAfter unlocking, manually start objects that have been stopped, such as virtual machines.\n\n\n\n\n\nCan I take the promised payment to avoid freezing?\n\nThe promised payment is not supported.\n\n\n\n\n\nI don't have time to pay for services by transfer. How to avoid freezing?\n\nPay for services with a bank card, payments from cards are faster.\n\n\n\nCharges for bank card payments are not included in accounting documents for legal entities: billing report, reconciliation report.\n\n\n\n\n\n\n\nI have topped up the balance, but the payment failed. How to avoid freezing?\n\nContact technical support.\nSpecify:\n\n   Project ID (PID)\n   Project region\n   Payment confirmation\n\nWorking with legal entities\n\n\n\nWe are a company not from the Russian Federation, how to start working with VK Cloud?\n\nWith VK Cloud, you can work with a service infrastructure that is physically deployed:\n\nOn the territory of Russia — to do this, register an account in region Moscow at https://cloud.vk.com.\nOn the territory of Kazakhstan — to do this, register an account in region Kazakhstan at https://vkcloud.kz или https://kz.cloud.vk.com/.\n\nEach region has its own peculiarities of work for non-residents of the Russian Federation:\n\n[cols=\"1,2,2\", options=\"header\"]\n|===\n\n| Features\n| Moscow\n| Kazakhstan\n\n| Contractual relations\n| Non-resident organizations of the Russian Federation can work with VK Cloud as legal entities and receive accounting documents only under an individual contract\n| Legal entities-residents of Kazakhstan can work both under a public offer agreement and under an individual agreement\n\n| Phone number\n| To link a foreign phone number to your account, please contact technical support\n| —\n\n| Payment for services\n| Currency of payments and other payment terms are discussed at the time of contract execution\n| In the region of Kazakhstan it is possible to pay for services:\n  Only in tenge.\n  If paying by card, only cards issued by banks of Armenia, Azerbaijan, Belarus, Estonia, Georgia, Kazakhstan, Kyrgyzstan, Latvia, Lithuania, Tajikistan, or Uzbekistan are accepted\n\n|===\n\n\n\n\n\nWe are a company from the Russian Federation, how to pay for services?\n\n\n\nPrepayment work\nPost-payment work\n\n\n\nComplete the additional registration of the legal entity, if it has not already been done.\nCreate an invoice for the required amount in your personal account.\nPay the bill by bank transfer.\n\n\n\n\nWait for the invoice from VK Cloud. The invoice comes as part of a package of accounting documents for the corresponding reporting period (month).\nPay the bill by bank transfer.\n\n\n\n\n\n\n\n\nWe are a company not from the Russian Federation, how to pay for services?\n\n\n\nPrepayment work\nPost-payment work\n\n\n\nRequest an invoice from the document management department of VK Cloud:\n\nWrite to docs_vktech@vk.company.\nSpecify:\n\n   Full name and details of the organization.\n   Invoice amount.\n\nPay the invoice received from VK Cloud using a bank transfer.\n\n\n\n\nWait for the invoice from VK Cloud. The invoice comes as part of a package of accounting documents for the corresponding reporting period (month).\nPay the invoice received from VK Cloud using a bank transfer.\n\n\n\n\n\n\n\n\nHow to conclude an individual contract with VK Cloud?\n\nWrite to the customer service department of VK Cloud at sales-team@mcs.mail.ru.\nSpecify:\n\n   Full name and details of the organization.\n   Full name and phone number of the contact person.\n\nThe manager will contact you to clarify the required terms of the contract.\n\n\n\n\n\nHow do I find out VK Cloud banking details?\n\n\n\nMoscow\nKazakhstan\n\n\n\nIf your project is in region Moscow, and you are working with “VK” LLC, its main details are available at https://cloud.vk.com/contacts.\n\nYou can also generate an invoice in your personal account and view the contents of the Supplier field.\n\n\n\n\nIf your project is in region Kazakhstan, and you are working with “QazCloud” LLC, its details are available at https://vkcloud.kz/contacts/.\n\n\n\n\n\n\n\n\nHow do I find out the terms of my contract with VK Cloud?\n\nAsk this question in the document management department of VK Cloud:\n\nWrite to docs_vktech@vk.company.\nSpecify:\n\n   Full name of the organization.\n   Details of the organization.\n\n\n\n\n\nHow to get accounting documents for accounting?\n\nWrite to the document management department of VK Cloud at docs_vktech@vk.company.\nSpecify:\n\n   Full name and details of the organization.\n   The composition of accounting documents that you require.\n   Method of obtaining documents: through electronic document management system (EDM) or in paper form to the postal address.\n\n\n   EDM work is available only to organizations registered in the Russian Federation.\n\n\nAccounting documents will be sent once in the reporting period (month).\n\nVK Cloud can duplicate reporting documents in the form of scanned copies in PDF format to e-mail. If you want to receive scanned copies, write about it in the request and specify email addresses of recipients.\n\n\n\n\n\nHow do I get a reconciliation report for the required period?\n\nWrite to the document management department of VK Cloud at docs_vktech@vk.company.\nSpecify:\n\n   Full name and details of the organization.\n   The period for which the act is required.\n\n\n\n\n\nI have unloaded the invoice, it does not contain my details. How do I add them?\n\nTo make the data of your organization appear in the Payer field in the invoices, that you generate in your personal account, perform the the additional registration of your legal entity.\n\nAdditional registration is only available in region Moscow and only for Russian resident companies.\n\n\n\n\n\nHow to register a project to another legal entity?\n\n\n\nYou can only change the mail of the project owner via changing the owner.\n\n\n\nTo register a project to another organization:\n\nWrite to the document management department of VK Cloud at docs_vktech@vk.company.\nSpecify:\n\n   Full name and details of the organization to which the project was registered.\n   Full name and details of the organization to which the project needs to be reissued.\n\nFor legal entities-residents of the Russian Federation in the Moscow region: specify new details in your personal account.\n\n\n\n\n\nHow do I get scans of your constituent documents?\n\nWrite to the document management department of VK Cloud at docs_vktech@vk.company.\nSpecify the list of documents that you require copies of.\n\n\n\n\n\nHow do we get scans of our accounting documents?\n\nWrite to the document management department of VK Cloud at docs_vktech@vk.company.\nSpecify:\n\n   Full name and details of the organization.\n   List of documents copies of which you need.\n   Email address to send copies to.\n\nCopies are provided in PDF format.\n\nIf you want to receive copies every reporting period, specify this in the request.\n\n\n","url":"/en/intro/billing/faq","description":"How to pay for the project, what to do when the project is frozen, how to return funds from the project, why bonuses are not accrued in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Billing","link":"/en/intro/billing","isActive":false,"evenSimpler":true}]},{"uuid":"d86333a6-2c54-4d1e-857c-ad967ee76bcf","title":"Platform security","data":"To ensure a high level of security for the VK Cloud platform, the security measures and practices described below are used.\n\nAdditional information is available on the Cloud Security page.\n\nMonitoring and countering attacks\n\nSecurity Operations Center (SOC VK) provides monitoring of VK Cloud, analyzes security events of VK Cloud servers and identifies anomalies using a SIEM (Security Information and Event Management) class system.\n\nThe following mechanisms also work:\n\n\n\nAntifraud\nTracking for suspicious activity\n\n\n\nVK Cloud antifraud is a set of security measures and rules aimed at filtering automatic registrations of bots and users, as well as preventing potential attacks on the resources of the VK Cloud platform.\n\nWhen activating VK Cloud services, you may need to confirm user data. In this case, use one of the proposed methods of identity verification:\n\nLinking a bank card**. Link your card and, if necessary, pay for VK Cloud services.\nCompany card (for legal entities)**. In your message, please attach a file with the details of the organization on whose behalf you are registering. The postal address must indicate the name or other details of the organization.\nContact technical support**. Create an account activation request on the technical support portal. Application category — account, group — activation and access.\n\n\n\n\nInternet users and automated services have the right to complain both manually and automatically about suspicious activity originating from IP addresses belonging to VK Cloud. For example, suspicious activity may include accessing the same web page at short intervals, multiple password guessing attempts, etc.\n\nTo provide users with uninterrupted services, VK Cloud technical support promptly responds to such complaints.\n\n\n\nThe VK Cloud user will be sent a warning about the presence of a complaint about suspicious activity from an IP address included in his project. If there is no response within one day, the IP address can be disconnected from the virtual machine and removed from the project to eliminate suspicious activity.\n\n\n\nSuspicious activity may be associated with an attacker gaining unauthorized access to a user's virtual machine. To reduce the risk of such a situation occurring, follow safety rules:\n\ndo not set simple passwords for accounts\ndo not provide uncontrolled access to project resources\nbe careful about the software you download and install on the virtual machine\nscan virtual machines for malicious software or code\n\nConducting security checks\n\nExternal inspections are carried out at least once a year with the participation of an external contractor. The check is carried out, among other things, using the model of an internal violator.\n\nVK Cloud also conducts its own information security audits and participates in Bug Bounty programs to find vulnerabilities:\n\nStandoff 365.\nBug Bounty Ru.\nBI.ZONE Bug Bounty.\n\nThis allows you to quickly identify and eliminate vulnerabilities in VK Cloud.\n\nApplying secure development principles when building the platform\n\nInformation security training for platform developers.\nIntegration and automation of security tools and practices at all stages of the development and operation life cycle (DevSecOps).\nArchitectural review and security audit of each service.\n\nApplying industry best practices\n\nIsolation of VK Cloud segments and services from each other using a firewall.\nDifferentiation of access to VK Cloud resources using a role model at the Identity and Access Management (IAM) level.\nAccess to the platform is available only to a limited number of VK Cloud administrators with mandatory authentication. Trusted and secure hosts (jump hosts) are used for access.\nSeparation of responsibility for security between VK Cloud and the user (more details on the Cloud Security page).\n","url":"/en/intro/it-security/platform-security","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Information security","link":"/en/intro/it-security","isActive":false,"evenSimpler":true}]},{"uuid":"203e71ab-3f67-4de7-b069-3e53b42c482e","title":"Technical solutions","data":"Antifraud\n\nAntifraud in VK Cloud is a set of security measures and rules aimed at filtering bot and auto user registrations, as well as preventing potential attacks on VK Cloud platform resources.\n\nIf after confirming the mailing address and phone number in the VK Cloud panel, when trying to activate services, a notification appears about the need to confirm your data, you should use one of the proposed methods of verifying your identity.\n\nWhen you select a method, the corresponding section will open:\n\nBinding a bank card**. Link a card and, if necessary, pay for VK Cloud services.\nSelfie with passport* *(only for citizens of the Russian Federation)**. In your message, attach a high-quality photo that clearly shows your face and your passport details.\nCompany card (for legal entities)**. Attach a file with the details of the organization on behalf of which the registration is being carried out. The postal address must, at the same time, indicate the name or other details of the organization.\n\nSuspicious activity\n\nUsers of Internet resources, as well as automated services, have the right to complain about suspicious activity performed by clients of the VK Cloud platform. Mechanisms include manual and automatic registration of complaints.\n\nThey detect IP addresses with suspicious behavior, such as if a visitor accesses the same web page at short intervals, fills in the wrong password multiple times, etc.\n\nSuch behavior is marked as potentially dangerous, then the IP address from which suspicious activity occurs is added to the gray lists, after which a complaint is sent to the owner of the IP address. If the activity is removed, the IP address is removed from the greylist.\n\nThis security feature is globally active on all of our servers and cannot be disabled. Without this feature, the security of the VK Cloud platform would be low.\n\nThe IP addresses allocated to virtual machines belong to the VK Cloud platform and can be reassigned by both the user and the system when the user releases the IP address assigned to his project. In order to provide customers with uninterrupted services, technical support promptly responds to such complaints by eliminating activity.\n\n\n\nTechnical support reports the existence of a complaint to the project owner at the registered postal address.\n\nIf you are an active user of the services, then technical support will definitely warn you about the existence of such a complaint. If there is no response for more than 1 day, the IP address can be disconnected from the instance and released from the project.\n\nIn order not to encounter a situation of suspicious activity conducted from a virtual machine, you should follow the security rules:\n\ndo not set simple passwords for accounts;\ndo not provide uncontrolled access to your created resources;\npay attention to downloaded and installed software;\nscan instances for malware or code.\n\nDDoS Protection\n\nDoS (Denial of Service) is a hacker attack on a computer system with the aim of bringing it to failure, that is, the creation of such conditions under which conscientious users of the system will not be able to access the provided system resources (servers), or this access will be difficult. The failure of the attacked system can also be a step towards mastering the system (if, in an emergency situation, the software gives out any critical information - for example, version, part of the program code, etc.).\n\nVK Cloud provides DDoS protection service. For more detailed information, as well as the calculation of the cost, you should send a request through your personal account in the Special Services section.\n\nThe cost of services for services \"Protection from DDoS\" is calculated individually.\n","url":"/en/intro/it-security/tech","description":"Description of technical solutions to improve the security of working with the VK Cloud platform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How to start","link":"/en/intro","isActive":false,"evenSimpler":true},{"title":"Information security","link":"/en/intro/it-security","isActive":false,"evenSimpler":true}]},{"uuid":"68f79276-fd71-4b20-90ea-0324a0407ee4","title":"Quick start","data":"Using Cloud Servers, a Linux virtual machine (VM) will be created in the VK Cloud personal account and connected to it.\n\nA quick start will help you get started with the service and get acquainted with its capabilities.\n\nAfter going through all the steps of a quick start, you will:\n\nCreate a new virtual machine (VM).\nConnect to the created VM via SSH.\n\n\n\nA running VM consumes computing resources.\n\nAfter passing the quick start, stop or delete the VM if you no longer need it.\n\n1. Preparatory steps\n\nRegister on the VK Cloud platform.\nActivate the services.\n\nThe account balance should be positive, and quotas should be enough to create the target configuration of the virtual machine.\n\n2. Create a VM\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nClick the  Create instance or Add button.\nIn the field Operating system choose Ubuntu 22.04.\nClick the Next step button.\nIn the field Network choose External network (ext-net).\n\n   An external IP address will be automatically assigned to the virtual machine on the external network.\n\nIn the field Virtual machine key choose Create a new key.\nIn the field Firewall settings choose Only ssh is allowed.\nClick the Next step and Create instance buttons.\n\n\n\nDo not close the page until the VM creation process is complete.\n\n\n\nSave the suggested key file .pem.\nWait until the virtual machine creation process is completed and the page of the new VM based on Ubuntu 22.04 opens.\n\n3. Wait for the VM configuration to complete\n\nAfter the VM is successfully created, wait at least 10-15 minutes. During this period, the operating system is deployed to the VM disk, and system tools are also working to configure the virtual machine in accordance with the specified parameters.\n\n4. Connect to the VM\n\nYou can connect to a Linux-based virtual machine created from an Ubuntu image via SSH.\n\n\n\nLinux/macOS\nWindows 10/11\nWindows 7/8\n\n\n\nRun the command in the terminal:\n\nssh -i  ubuntu@\n\nFor example:\n\nssh -i ubuntu-key.pem ubuntu@213.219.212.130\n\nThe external IP address of the VM can be viewed in VK Cloud personal account to Cloud Computing → Virtual machines.\n\nThe first time you connect to the VM, you will be asked to confirm the connection to a previously unknown host:\n\nThe authenticity of host '213.219.212.130 (213.219.212.130)' can't be established.\nECDSA key fingerprint is SHA256:aYZIWs9N6KRtfFOuic6eoWcluhSp6+jha/DSBgd9McI.\nAre you sure you want to continue connecting (yes/no)?\n\nType yes in the terminal and press Enter.\n\n\n\n\nRun the command in the terminal:\n\nssh -i  ubuntu@\n\nFor example:\n\nssh -i ubuntu-key.pem ubuntu@213.219.212.130\n\nThe external IP address of the VM can be viewed in VK Cloud personal account to Cloud Computing → Virtual machines.\n\nThe first time you connect to the VM, you will be asked to confirm the connection to a previously unknown host:\n\nThe authenticity of host '213.219.212.130 (213.219.212.130)' can't be established.\nECDSA key fingerprint is SHA256:aYZIWs9N6KRtfFOuic6eoWcluhSp6+jha/DSBgd9McI.\nAre you sure you want to continue connecting (yes/no)?\n\nType yes in the terminal and press Enter.\n\n\n\n\nIn Windows 7/8, use the PuTTY application to connect via SSH.\n\nInstall PuTTY.\nLaunch the PuTTYgen app that comes with the package:\n\n   In the Conversions menu choose Import Key and specify the available key .pem.\n   Click the Save private key button, select the directory and specify the required key name .ppk.\n\nLaunch the Pageant app that comes with the package:\n\n   Right-click on the Pageant icon on the taskbar.\n   In the context menu, select Add key.\n   Select the generated private key in the format .ppk.\n\nRun PuTTY:\n\n   In the field Host Name (or IP address) enter the external IP address of the VM. Specify the port 22 and the connection type SSH.\n\n\n   The external IP address of the VM can be viewed in VK Cloud personal account to Cloud Computing → Virtual machines.\n\n\n   In the tree on the left, go to the category Connection → SSH → Auth.\n   Set the flag Allow agent forwarding.\n   Open the item in the tree on the left Auth → Credentials.\n   In the field Private key file for authentication select the .ppk file with the private key.\n   Click the Open button.\n\n      If this is the first connection to the VM, confirm it in the window that appears.\n\n   In the terminal window that appears, enter your login ubuntu.\n\n      If everything is configured correctly, a connection to the server will be established.\n\nDelete unused resources\n\nA running VM consumes computing resources. If you don't need it anymore:\n\nstop it to use it later;\ndelete it.\n","url":"/en/computing/iaas/quick-start","description":"Creating a virtual machine with a pre-installed OS and connecting to it.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"2586c86d-9565-4346-ba85-17a52c1356b3","title":"About service","data":"Cloud Servers is a cloud computing service for managing virtual machines on the VK Cloud.\n\nVirtual machines\n\nA virtual machine (VM) is a collection of resources that emulate the behavior of a real computer. For its operation, the virtual machine uses the resources of a physical server (host). Multiple VMs can be hosted on the same host.\n\nWhen creating a virtual machine, its configuration is set:\n\nnumber of processors (CPU);\nthe amount of RAM;\ndisk type and size;\navailability zone;\noperating system.\n\n\n\nVirtual machines created on the VK Cloud platform use BIOS emulation. UEFI emulation is not supported.\n\nServer Group\n\nA server group is a collection of servers within a cluster that are combined in accordance with a specific policy. The server refers to resources:\n\nvirtual machines;\nDB instances;\nK8s clusters;\napplication instances from the App Store.\n\nThe policy determines the allocation of resources on the platform compute nodes. Supported policies:\n\naffinity — place a VM on a single compute node;\nanti-affinity — place VMs on different compute nodes;\nsoft-affinity — if possible, place the VM on one compute node (if there is not enough space on one node, instead of a creation error, as in the case of affinity, a server will be created on another compute node);\nsoft-anti-affinity — place the VM on different compute node, if possible.\n\nExample of using a server group: deploy multiple application VMs on only one node to speed up communication between these VMs.\n\nFlavors\n\nOn the VK Cloud platform, the number of processors and the amount of RAM of the VM are set using flavors. Ready-made flavors are available to users, as well as individual (on request).\n\nVirtual machine categories\n\nVMs are grouped into categories in a personal account:\n\n| Category | Description | Display condition |\n| --- | --- | --- |\n| Intel Cascade Lake (Intel Xeon Gen 2) | VMs located on servers with Intel Cascade Lake CPU | Available by default |\n| Intel Ice Lake (Intel Xeon Gen 3) | VMs located on servers with Intel Ice Lake CPU | Available by default |\n| Archived VM types (legacy)| Old VM configuration templates | Not displayed by default filtration |\n| High-performance CPU |  VMs located on servers with increased CPU clock speed | Displayed when ordering the types of VM through technical support |\n| Virtual machines with GPU | VMs with connected graphics card | Displayed when ordering the types of VM through cloud.vk.com |\n| VMs with local disks | VMs with the ability to use local hypervisor disks | Displayed when ordering the types of VM through technical support |\n| VMs located on dedicated servers | VMs located on hypervisors dedicated to the needs of one client | Displayed when ordering the types of VM through your manager |\n\nBy default, only actual configuration templates are displayed: on servers with Intel Cascade Lake and Intel Ice Lake CPUs, as well as VMs with a connected graphics card, on high-performance or dedicated servers, if they were added to the project.\n\nSelect the Archive VM types category to find the old VM configuration templates. The server for VMs of the old configuration is selected randomly: Intel Cascade Lake or Intel Ice Lake, but charged at the price of Intel Ice Lake. If the server is unavailable (for example, during maintenance work), these VMs can move to a server of a different generation.\n\n\n    Archived VM types\n\n| Flavor's name | Configuration Parameters | Description |\n| --- | --- | --- |\n| Basic | Up to 2 vCPU  Up to 4 Gb RAM | Basic flavors for creating a VM with low performance |\n| Standard | From 2 to 4 vCPUs  From 4 Gb to 16 Gb RAM | Configurations with increased CPU and RAM capacity |\n| Advanced | From 8 to 16 vCPU  From 16 Gb to 64 Gb RAM | Configurations for creating high-performance VMs |\n| Heavy | Up to 16 vCPU  Up to 64 Gb RAM | Customized configurations for creating high-performance VMs |\n| Custom | Unlimited | Individual configurations |\n\n\n\nTo create additional VM templates (for example, with a large amount of resources) contact technical support. The cost of a VM in this case is calculated individually for each request.\n\n\n\nThe number and type of CPU, as well as the amount of RAM of an already created virtual machine, can be changed by changing the VM template. This process will require a VM reboot.\n\nName of VM configuration templates\n\nThe name of the VM template on servers with Intel Cascade Lake, Intel Ice Lake CPUs and a VM with a connected graphics card is formed as follows:\n\n-----\n\nExamples:\n\n|VM template name | Description |\n| --- | --- |\n| STD2-2-4 | VM with Intel Cascade Lake processor, 2 vCPUs and 4 GB of RAM |\n| STD3-4-8 | VM with Intel Ice Lake processor, 4 vCPU and 8 GB RAM |\n| GPU1A-32-96-A100-1 | VM with AMD EPYC 7662 processor and one connected Nvidia Tesla A10040GB graphics card |\n\nCPU generations\n\nThe performance of a VM depends on the resources of the host on which it is hosted. The VK Cloud platform provides server hardware that allows you to configure VMs of different performance levels.\n\nVK Cloud platform has different CPU for VM hosting:\n\n| CPU | Type and generation of CPU | Availability |\n| --- | --- | --- |\n| Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz  Intel(R) Xeon(R) Gold 6238R CPU @ 2.20GHz | Intel Cascade Lake (Intel Xeon Gen 2) | Available by default |\n| Intel Xeon Gold 6338 @ 2.0GHz  Intel Xeon Platinum 8380 CPU @ 2.3GHz | Intel Ice Lake (Intel Xeon Gen 3) | Available by default |\n| Intel(R) Xeon(R) Gold 6230 CPU @ 3.40GHz  Intel(R) Xeon(R) Gold 6238R CPU @ 3.70GHz  Intel(R) Xeon(R) Platinum 8380 CPU @ 3.4GHz | High-performance processors | Available on request |\n| AMD EPYC 7662 | Select GPU VMs only | Available on request |\n\nThe configurations available by default allow you to create VMs on servers with standard CPU.\n\nDisks\n\nA disk on the VK Cloud platform is a network block storage device that connects to a VM. The amount of stored data and the speed of access to it depends on the size and type of disk.\n\nThe VK Cloud disk subsystem uses network drives. Disk fault tolerance, continuous access to data and their safety are ensured by replication.\n\nNetwork drives can be partitioned and formatted in the same way as regular locally connected drives. The advantage of network drives: they can be “moved” between VMs located in the same data center.\n\nLow Latency NVME local disks are available in high-performance configurations. To create such a configuration contact technical support.\n\nAfter creating a disk, you can change its size, type, and availability zone. The disk can be renamed, connected or disconnected from the VM, cloned or deleted. The boot disk can be made non-bootable and vice versa.\n\nMultiple disks can be attached to a VM, but at least one of them must be bootable and contain the installed operating system.\n\n\n\nIf the VM and disk are located in different availability zones, the disk access speed may be reduced.\n\nDisks types\n\nThe VK Cloud platform supports various types of disks:\n\n| Disk Type | Name in the API | Availability zones | Description |\n|-----------|----------------|------------------|----------|\n| Network HDD | ceph-hdd | MS1, GZ1, ME1 | An ordinary magnetic hard drive. Suitable for storing large amounts of information. Triple replication to different storage servers |\n| Network SSD | ceph-ssd | MS1, GZ1, ME1 | Solid-state drive. High speed of reading and writing information. The performance is higher than that of HDD. Triple replication to different storage servers |\n| NetworkHigh-IOPS SSD | high-iops | MS1, GZ1, ME1 | SSD with increased speed of operation. Double replication: both copies are on the same storage server |\n| NetworkHigh-IOPS HA SSD | high-iops-ha | ME1 | SSD with increased speed of operation. Double replication to different storage servers |\n| LocalLow Latency NVME | ef-nvme | nova | SSD, which is located on the same hypervisor with the VM. High-speed operation and fast response. Double replication: both copies are on the same hypervisor |\n\n\n   Information about outdated resource names\n\nSome names of disk types and availability zones are outdated, but are displayed in the output of commands.\n\nCompliance of outdated and current names:\n\n| Disk Type | Outdatedname in the API | Availabilityzone | Actualname in the API | Actualavailability zone |\n|---------|-----------------------------|------------------|---------------------------|-----------------------------|\n| Network HDD | dp1                     | DP1              | ceph-hdd                  | GZ1                         |\n|             | ms1                     | MS1              | ceph-hdd                  | MS1                         |\n| Network SSD | dp1-ssd                 | DP1              | ceph-ssd                  | GZ1                         |\n|             | ko1-ssd                 | MS1              | ceph-ssd                  | MS1                         |\n| Network High-IOPS SSD | dp1-high-iops | DP1              | high-iops                 | GZ1                         |\n|                       | ko1-high-iops | MS1              | high-iops                 | MS1                         |\n\nUnavailable disk types:\n\n| Disk Type | Name in the API | Availability zone |\n|---------|--------------------|------------------|\n| Network HDD with geo-replication | ceph   | nova    |\n| Network SSD with geo-replication | ssd    | nova    |\n\nAvailability zone\n\nAn availability zone is one or more data centers in which components of the cloud infrastructure can be placed. In VK Cloud, the availability zone corresponds to a separate Tier III data center.\n\nEach zone is isolated from failures in other availability zones. Placing virtual resources in multiple zones provides fault tolerance and reduces the likelihood of data loss.\n\nVK Cloud platform resources are hosted in data centers:\n\nGZ1 – Goznak data center. Address: Moscow, Prospekt Mira, 105, building 6.\nMS1 (ko1) – DataLine NORD4 data center. Address: Moscow, Korovinskoe highway, 41.\nQAZ — QazCloud data center. Address: Republic of Kazakhstan, Akmola region, Kosshy, Republic str. 1.\n\nIn the MS1 availability zone, the VK Cloud infrastructure is protected in accordance with the Federal Law of the Russian Federation “On Personal Data” No. 152-FL.\n\nVK Cloud distributes the following virtual resources across availability zones:\n\nvirtual machines;\ndisks: are placed in network block storage and are automatically replicated within their availability zone;\nload balancers.\n\nNetworks and routers are not tied to availability zones.\n\n\n\nA network created in the QAZ availability zone cannot be united with networks in other availability zones, since QAZ zone is located in the Kazakhstan region. In this case, networks can be connected over VPN.\n\nOperating system\n\nOn the VK Cloud platform, when creating a virtual machine, you can choose an operating system of the Microsoft Windows or Linux family. To create a VM with an operating system that is not in the platform list, use the VM image file with the required OS.\n\nVK Cloud supports migration of Windows server versions:\n\nWindows Server 2008 / 2008 R2;\nWindows Server 2012 / 2012 R2;\nWindows Server 2016;\nWindows Server 2019;\nWindows Server 2022.\n\n\n\nIt is not possible to use Windows 7 / 8 / 8.1 / 10 / 11 operating systems in VK Cloud. This restriction is set for all projects and cannot be lifted.\n\nImage\n\nThe configuration and data of the virtual machine are stored as one or more files. These files can be used as an image to run a VM in a local environment or to create a new VM.\n\nOn the VK Cloud platform, you can create an image based on a VM disk or upload an image file. The resulting image can be used to create a new virtual machine.\n\nAn image, the source of which is a disk, allows you to create a copy of a virtual machine with an installed and configured operating system and applications.\n\nThe image downloaded from the file can be used for VM migration or to create a virtual machine with your operating system.\n\n\n\nCloud Servers supports images in the raw format.\n\nFile storage\n\nFile storage is a virtual file system that can be connected to multiple virtual machines and used to share data. When creating a file storage, the network access protocol to it (NFS or CIFS) is specified.\n\n\n\nFile storage is accessed only from virtual machines inside the VK Cloud project.\n\n\n\nThe size of the file storage is set at the creation stage. If necessary, the size of the file storage can be increased, but the file storage cannot be reduced.\n\nSnapshot creation and snapshot recovery operations are available for file repositories.\n\nBackup\n\nA VM backup is needed to restore the state of the VM saved at a certain point in time. The VK Cloud platform provides a service for creating VM backups in manual or automatic mode.\n\nIn manual mode, the creation of backups is started by the user. At the same time, full VM backups are created.\n\nAutomatic mode supports GFS strategy and allows you to configure a backup schedule to create full and incremental backups.\n\nLicenses\n\nVK Cloud has the right to license software based on agreements with companies:\n\nMicrosoft (Microsoft Services Provider License Agreement — SPLA);\nRED SOFT;\nBaseALT;\nAstra Linux.\n\nFor more information, see the VM Licenses section.\n","url":"/en/computing/iaas/concepts/new-vm-concept","description":"Cloud Servers Concept — basic concepts related to virtual machines.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"857af423-bdae-4a4b-a6ab-6228144ba462","title":"VK Cloud Disk Performance","data":"Various types of disks are used in the data storage system on the VK Cloud platform:\n\nHDD\nSSD\nHigh-IOPS SSD\nHigh-IOPS HA SSD\nLow Latency NVME\n\nRead more in the article Cloud Servers overview.\n\nCertain performance characteristics are guaranteed for each type of disk.\n\n\n\nThe latency value is guaranteed only for Low Latency NVME disks.\nFor other types of disks, the delay value is approximate and is indicated for reference.\n\n\n\n| Disk type(name in the API) | Read, IOPSmin.—max. | Read,IOPS/GB | Write, IOPS min.max. | Write,IOPS/GB | Latency, msecmax.|\n|----|----|----|----|----|----|\n| Network HDD(ceph-hdd)          | 300–2400    | 1  | 150–800    | 1  | 20   |\n| Network SSD(ceph-ssd)          | 1000–16000  | 30 | 500–8000   | 15 | 3    |\n| High-IOPS SSD(high-iops)       | 10000–45000 | 30 | 5000–30000 | 25 | 1    |\n| High-IOPS HA SSD(high-iops-ha) | 7500–35000  | 25 | 2000–12000 | 15 | 1    |\n| Low Latency NVME(ef-nvme)      | 10000–75000 | 75 | 5000–50000 | 35 | 0,5  |\n\n\n\nDisk performance depends on its volume. In some cases, to increase the speed of data processing, it is enough to increase the size of the disk.\n\nNetwork HDD\n\nBelow are detailed performance characteristics for network HDD drives of different sizes.\n\n| Size, GB | Read, IOPSbs=4k,iodepth=32 | Read, MB/secbs=1M,iodepth=16 | Write, IOPSbs=4k,iodepth=32 | Write, MB/secbs=1M,iodepth=16 |\n|--------------|--------------|--------------|--------------|--------------|\n| 10           | 300          | 38           | 150          | 19           |\n| 50           | 300          | 38           | 150          | 19           |\n| 100          | 300          | 38           | 150          | 19           |\n| 250          | 300          | 38           | 250          | 31           |\n| 500          | 500          | 63           | 500          | 63           |\n| 1000         | 1000         | 125          | 800          | 100          |\n| 1500         | 1500         | 188          | 800          | 100          |\n| 2000         | 2000         | 250          | 800          | 100          |\n\nHere, bs and iodepth are performance testing parameters.\n\nNetwork SSD\n\nBelow are detailed performance characteristics for network SSD drives of different sizes.\n\n| Size, GB | Read, IOPSbs=4k,iodepth=32 | Read, MB/secbs=1M,iodepth=16 | Write, IOPSbs=4k,iodepth=32 | Write, MB/secbs=1M,iodepth=16 |\n|--------------|--------------|--------------|--------------|--------------|\n| 10           | 1000         | 125          | 500          | 63           |\n| 50           | 1500         | 188          | 750          | 94           |\n| 100          | 3000         | 375          | 1500         | 188          |\n| 250          | 7500         | 400          | 3750         | 400          |\n| 500          | 15000        | 400          | 7500         | 400          |\n| 1000         | 16000        | 400          | 8000         | 400          |\n| 1500         | 16000        | 400          | 8000         | 400          |\n| 2000         | 16000        | 400          | 8000         | 400          |\n\nHere, bs and iodepth are performance testing parameters.\n\nHigh-IOPS SSD\n\nBelow are detailed performance characteristics for network High-IOPS SSD drives of different sizes.\n\n| Size, GB | Read, IOPSbs=4k,iodepth=32 | Read, MB/secbs=1M,iodepth=16 | Write, IOPSbs=4k,iodepth=32 | Write, MB/secbs=1M,iodepth=16 |\n|--------------|--------------|--------------|--------------|--------------|\n| 10           | 10000        | 500          | 5000         | 500          |\n| 50           | 10000        | 500          | 5000         | 500          |\n| 100          | 10000        | 500          | 5000         | 500          |\n| 250          | 10000        | 500          | 6250         | 500          |\n| 500          | 15000        | 500          | 12500        | 500          |\n| 1000         | 30000        | 500          | 25000        | 500          |\n| 1500         | 45000        | 500          | 30000        | 500          |\n| 2000         | 45000        | 500          | 30000        | 500          |\n\nHere, bs and iodepth are performance testing parameters.\n\nHigh-IOPS HA SSD\n\nBelow are detailed performance characteristics for network High-IOPS SSD HA drives of different sizes.\n\n| Size, GB | Read, IOPSbs=4k,iodepth=32 | Read, MB/secbs=1M,iodepth=16 | Write, IOPSbs=4k,iodepth=32 | Write, MB/secbs=1M,iodepth=16 |\n|--------------|--------------|--------------|--------------|--------------|\n| 10           | 7500         | 375          | 2000         | 250          |\n| 50           | 7500         | 375          | 2000         | 250          |\n| 100          | 7500         | 375          | 2000         | 250          |\n| 250          | 7500         | 375          | 3750         | 300          |\n| 500          | 12500        | 375          | 7500         | 300          |\n| 1000         | 25000        | 375          | 12000        | 300          |\n| 1500         | 35000        | 375          | 12000        | 300          |\n| 2000         | 35000        | 375          | 12000        | 300          |\n\nHere, bs and iodepth are performance testing parameters.\n\nLow Latency NVME\n\nBelow are detailed performance characteristics for local Low Latency NVME drives of different sizes.\n\n| Size, GB | Read, IOPSbs=4k,iodepth=32 | Read, MB/secbs=1M,iodepth=16 | Write, IOPSbs=4k,iodepth=32 | Write, MB/secbs=1M,iodepth=16 |\n|--------------|--------------|--------------|--------------|--------------|\n| 10           | 10000        | 500          | 5000         | 500          |\n| 50           | 10000        | 500          | 5000         | 500          |\n| 100          | 10000        | 500          | 5000         | 500          |\n| 250          | 18750        | 586          | 8750         | 500          |\n| 500          | 37500        | 1172         | 17500        | 547          |\n| 1000         | 75000        | 1200         | 35000        | 900          |\n| 1500         | 75000        | 1200         | 50000        | 900          |\n| 2000         | 75000        | 1200         | 50000        | 900          |\n\nHere, bs and iodepth are performance testing parameters.\n\nDisk performance testing\n\nBefore testing, make sure that the following conditions are met:\n\n   the disk is non-bootable;\n   there is no load on the disk from the operating system.\n\n   When these conditions are met, the IOPS measurement results must correspond to the following values:\n\n   | Type of testing | Result, IOPS |\n   | ---- | ---- |\n   | Read/write in blocks of 4 KB in 32 threads | Complies with SLA   |\n   | Read/write in blocks of 8 KB in 32 threads | At least 75% of SLA |\n   | Read/write in blocks of 16 KB in 32 threads | At least 50% of the SLA |\n\nTest the disk.\n\n   Windows\n   Linux\n\n\n   To measure IOPS when reading and writing, use the DiskSpd or FIO utilities.\n\n\n   The measurement results obtained using DiskSpd and FIO may differ. DiskSpd is a utility created and recommended by Microsoft for testing disks in Windows OS.\n\n\n   DiskSpd\n\n   Run the command prompt as an administrator.\n   Create a temp directory and an empty file of at least 10 GB in size:\n\n            md C:\\temp\n      fsutil file createnew C:\\temp\\test.bin 10485760000\n\n   Download the utility and unpack to the required directory.\n   Go to the amd64 directory of the unpacked utility.\n   Run the command diskspd with parameters corresponding to the type of test:\n\n      -w — percentage of write operations, -w0 for the reading test, -w100 for the recording test;\n      -b — block size in bytes.\n\n      Detailed description of all command parameters diskspd — in official documentation.\n\n      Random write test in blocks of 4 KB:\n\n                  diskspd -Suw -b4K -o1 -t32 -r -w100 C:\\temp\\test.bin > C:\\temp\\random_write_results.txt\n\n      Random reading test in blocks of 4 KB:\n\n                  diskspd -Suw -b4K -o1 -t32 -r -w0 C:\\temp\\test.bin > C:\\temp\\random_read_results.txt\n\n   FIO\n\n   Download and install FIO.\n   Run the fio command with the parameters corresponding to the type of test:\n\n      --rw — randread or randwrite.\n      --bs — block size.\n      --filename — name of the test file.\n      --rate_iops — IOPS target value (optional). Use this parameter to get a more accurate latency value when testing the IOPS target value.\n\n      Detailed description of all command parameters — in FIO documentation.\n\n      Random write test in blocks of 4 KB:\n\n\n         Do not specify the name of the file with the necessary data in the filename parameter! During write tests, the contents of this file will be overwritten.\n\n         PowerShell\n         Command line\n\n\n                  fio `\n            --name=randwrite `\n            --iodepth=32 `\n            --rw=randwrite `\n            --bs=4k `\n            --direct=1 `\n            --size=10G `\n            --numjobs=1 `\n            --runtime=240 `\n            --group_reporting `\n            --filename=C:\\Users\\ADMIN\\test\n\n\n                     fio ^\n            --name=randwrite ^\n            --iodepth=32 ^\n            --rw=randwrite ^\n            --bs=4k ^\n            --direct=1 ^\n            --size=10G ^\n            --numjobs=1 ^\n            --runtime=240 ^\n            --group_reporting ^\n            --filename=C:\\Users\\ADMIN\\test\n\n\n      Random reading test in blocks of 4 KB:\n\n         PowerShell\n         Command line\n\n\n                  fio `\n            --name=randread `\n            --iodepth=32 `\n            --rw=randread `\n            --bs=4k `\n            --direct=1 `\n            --size=10G `\n            --numjobs=1 `\n            --runtime=240 `\n            --group_reporting `\n            --filename=C:\\Users\\ADMIN\\test\n\n\n                  fio ^\n            --name=randread ^\n            --iodepth=32 ^\n            --rw=randread ^\n            --bs=4k ^\n            --direct=1 ^\n            --size=10G ^\n            --numjobs=1 ^\n            --runtime=240 ^\n            --group_reporting ^\n            --filename=C:\\Users\\ADMIN\\test\n\n\n   To measure IOPS when reading and writing, use the FIO utility.\n\n\n   This technique is only applicable for testing partitions with the file system ext2, ext3, ext4 or xfs.\n\n\n   Update the package list:\n\n            sudo apt update\n\n   Install FIO:\n\n            sudo apt install fio\n\n   Run the fio command with the parameters corresponding to the type of test:\n\n      --rw — randread or randwrite.\n      --bs — block size.\n      --filename — the name of the test file. The user must have read and write permissions to the filename to run the tests.\n      --rate_iops — IOPS target value (optional). Use this parameter to get a more accurate latency value when testing the target IOPS value.\n\n      Detailed description of all command parameters — in FIO documentation.\n\n      Random write test in blocks of 4 KB:\n\n\n         Do not specify the name of the file with the necessary data in the filename parameter! During write tests, the contents of this file will be overwritten.\n\n\n                  fio \\\n            --name=randwrite \\\n            --ioengine=libaio \\\n            --iodepth=32 \\\n            --rw=randwrite \\\n            --bs=4k \\\n            --direct=1 \\\n            --size=512M \\\n            --numjobs=1 \\\n            --runtime=240 \\\n            --group_reporting \\\n            --filename=/home/user/test\n\n      Random reading test in blocks of 4 KB:\n\n                  fio \\\n            --name=randread \\\n            --ioengine=libaio \\\n            --iodepth=32 \\\n            --rw=randread \\\n            --bs=4k \\\n            --direct=1 \\\n            --size=512M \\\n            --numjobs=1 \\\n            --runtime=240 \\\n            --group_reporting \\\n            --filename=/home/user/test\n\n\nIf, when all conditions are met, the test results do not match the specified values, contact technical support.\n\n\n\nAfter testing is completed and the results are processed, delete the large test files to free up disk space.\n\n\n","url":"/en/computing/iaas/concepts/new-volume-sla","description":"Performance characteristics of different types of disks on the platform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"8a2a793c-233e-42f1-b47c-05748444a1e9","title":"About service","data":"Cloud Servers is a cloud computing service for managing virtual machines on the VK Cloud.\n\nVirtual machines\n\nA virtual machine (VM) is a collection of resources that emulate the behavior of a real computer. For its operation, the virtual machine uses the resources of a physical server (host). Multiple VMs can be hosted on the same host.\n\nWhen creating a virtual machine, its configuration is set:\n\nnumber of processors (CPU);\nthe amount of RAM;\ndisk type and size;\navailability zone;\noperating system.\n\n\n\nVirtual machines created on the VK Cloud platform use BIOS emulation. UEFI emulation is not supported.\n\nServer Group\n\nA server group is a collection of servers within a cluster that are combined in accordance with a specific policy. The server refers to resources:\n\nvirtual machines;\nDB instances;\nK8s clusters;\napplication instances from the App Store.\n\nThe policy determines the allocation of resources on the platform compute nodes. Supported policies:\n\naffinity — place a VM on a single compute node;\nanti-affinity — place VMs on different compute nodes;\nsoft-affinity — if possible, place the VM on one compute node (if there is not enough space on one node, instead of a creation error, as in the case of affinity, a server will be created on another compute node);\nsoft-anti-affinity — place the VM on different compute node, if possible.\n\nExample of using a server group: deploy multiple application VMs on only one node to speed up communication between these VMs.\n\nFlavors\n\nOn the VK Cloud platform, the number of processors and the amount of RAM of the VM are set using flavors. Ready-made flavors are available to users, as well as individual (on request).\n\nVirtual machine categories\n\nVMs are grouped into categories in a personal account:\n\n[cols=\"30%,30%,40%\", options=\"header\"]\n|===\n| Category\n| Description\n| Display condition\n\n| Intel Cascade Lake (Intel Xeon Gen 2)\n| VMs located on servers with Intel Cascade Lake CPU\n| Available by default\n\n| Intel Ice Lake (Intel Xeon Gen 3)\n| VMs located on servers with Intel Ice Lake CPU\n| Available by default\n\n| Archived VM types (legacy)\n| Old VM configuration templates\n| Not displayed by default filtration\n\n| High-performance CPU\n| VMs located on servers with increased CPU clock speed\n| Displayed when ordering the types of VM through technical support\n\n| Virtual machines with GPU\n| VMs with connected graphics card\n| Displayed when ordering the types of VM through cloud.vk.com\n\n| VMs with local disks\n| VMs with the ability to use local hypervisor disks\n| Displayed when ordering the types of VM through technical support\n\n| VMs located on dedicated servers\n| VMs located on hypervisors dedicated to the needs of one client\n| Displayed when ordering the types of VM through your manager\n\n|===\n\nBy default, only actual configuration templates are displayed: on servers with Intel Cascade Lake and Intel Ice Lake CPUs, as well as VMs with a connected graphics card, on high-performance or dedicated servers, if they were added to the project.\n\nSelect the Archive VM types category to find the old VM configuration templates. The server for VMs of the old configuration is selected randomly: Intel Cascade Lake or Intel Ice Lake, but charged at the price of Intel Ice Lake. If the server is unavailable (for example, during maintenance work), these VMs can move to a server of a different generation.\n\n\n    Archived VM types\n\n| Flavor's name | Configuration Parameters | Description |\n| --- | --- | --- |\n| Basic | Up to 2 vCPU  Up to 4 Gb RAM | Basic flavors for creating a VM with low performance |\n| Standard | From 2 to 4 vCPUs  From 4 Gb to 16 Gb RAM | Configurations with increased CPU and RAM capacity |\n| Advanced | From 8 to 16 vCPU  From 16 Gb to 64 Gb RAM | Configurations for creating high-performance VMs |\n| Heavy | Up to 16 vCPU  Up to 64 Gb RAM | Customized configurations for creating high-performance VMs |\n| Custom | Unlimited | Individual configurations |\n\n\n\nTo create additional VM templates (for example, with a large amount of resources) contact technical support. The cost of a VM in this case is calculated individually for each request.\n\n\n\nThe number and type of CPU, as well as the amount of RAM of an already created virtual machine, can be changed by changing the VM template. This process will require a VM reboot.\n\nName of VM configuration templates\n\nThe name of the VM template on servers with Intel Cascade Lake, Intel Ice Lake CPUs and a VM with a connected graphics card is formed as follows:\n\n-----\n\nExamples:\n\n|VM template name | Description |\n| --- | --- |\n| STD2-2-4 | VM with Intel Cascade Lake processor, 2 vCPUs and 4 GB of RAM |\n| STD3-4-8 | VM with Intel Ice Lake processor, 4 vCPU and 8 GB RAM |\n| GPU1A-32-96-A100-1 | VM with AMD EPYC 7662 processor and one connected Nvidia Tesla A10040GB graphics card |\n\n{heading(CPU generations)[id=cpu_generations]}\n\nThe performance of a VM depends on the resources of the host on which it is hosted. The VK Cloud platform provides server hardware that allows you to configure VMs of different performance levels.\n\nVK Cloud platform has different CPU for VM hosting:\n\n| CPU | Type and generation of CPU | Availability |\n| --- | --- | --- |\n| Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz  Intel(R) Xeon(R) Gold 6238R CPU @ 2.20GHz | Intel Cascade Lake (Intel Xeon Gen 2) | Available by default |\n| Intel Xeon Gold 6338 @ 2.0GHz  Intel Xeon Platinum 8380 CPU @ 2.3GHz | Intel Ice Lake (Intel Xeon Gen 3) | Available by default |\n| Intel(R) Xeon(R) Gold 6230 CPU @ 3.40GHz  Intel(R) Xeon(R) Gold 6238R CPU @ 3.70GHz  Intel(R) Xeon(R) Platinum 8380 CPU @ 3.4GHz | High-performance processors | Available on request |\n| AMD EPYC 7662 | Select GPU VMs only | Available on request |\n\nThe configurations available by default allow you to create VMs on servers with standard CPU.\n\nDisks\n\nA disk on the VK Cloud platform is a network block storage device that connects to a VM. The amount of stored data and the speed of access to it depends on the size and type of disk.\n\nThe VK Cloud disk subsystem uses network drives. Disk fault tolerance, continuous access to data and their safety are ensured by replication.\n\nNetwork drives can be partitioned and formatted in the same way as regular locally connected drives. The advantage of network drives: they can be “moved” between VMs located in the same data center.\n\nLow Latency NVME local disks are available in high-performance configurations. To create such a configuration contact technical support.\n\nAfter creating a disk, you can change its size, type, and availability zone. The disk can be renamed, connected or disconnected from the VM, cloned or deleted. The boot disk can be made non-bootable and vice versa.\n\nMultiple disks can be attached to a VM, but at least one of them must be bootable and contain the installed operating system.\n\n\n\nIf the VM and disk are located in different availability zones, the disk access speed may be reduced.\n\nDisks types\n\nThe VK Cloud platform supports various types of disks:\n\n[cols=\"2,1,2,4\", options=\"header\"]\n|===\n| Disk Type\n| Name in the API\n| Availability zones\n| Description\n\n| Network HDD\n| ceph-hdd\n| MS1, GZ1, ME1\n| An ordinary magnetic hard drive. Suitable for storing large amounts of information. Triple replication to different storage servers\n\n| Network SSD\n| ceph-ssd\n| MS1, GZ1, ME1\n| Solid-state drive. High speed of reading and writing information. The performance is higher than that of HDD. Triple replication to different storage servers\n\n| Network High-IOPS SSD\n| high-iops\n| MS1, GZ1, ME1\n| SSD with increased speed of operation. Double replication: both copies are on the same storage server\n\n| Local Low Latency NVME\n| ef-nvme\n| nova\n| SSD, which is located on the same hypervisor with the VM. High-speed operation and fast response. Double replication: both copies are on the same hypervisor\n\n|===\n\n\n   Information about outdated resource names\n\nSome names of disk types and availability zones are outdated, but are displayed in the output of commands.\n\nCompliance of outdated and current names:\n\n| Disk Type | Outdatedname in the API | Availabilityzone | Actualname in the API | Actualavailability zone |\n|---------|-----------------------------|------------------|---------------------------|-----------------------------|\n| Network HDD | dp1                     | DP1              | ceph-hdd                  | GZ1                         |\n|             | ms1                     | MS1              | ceph-hdd                  | MS1                         |\n| Network SSD | dp1-ssd                 | DP1              | ceph-ssd                  | GZ1                         |\n|             | ko1-ssd                 | MS1              | ceph-ssd                  | MS1                         |\n| Network High-IOPS SSD | dp1-high-iops | DP1              | high-iops                 | GZ1                         |\n|                       | ko1-high-iops | MS1              | high-iops                 | MS1                         |\n\nUnavailable disk types:\n\n| Disk Type | Name in the API | Availability zone |\n|---------|--------------------|------------------|\n| Network HDD with geo-replication | ceph   | nova    |\n| Network SSD with geo-replication | ssd    | nova    |\n\nAvailability zone\n\nAn availability zone is one or more data centers in which components of the cloud infrastructure can be placed. In VK Cloud, the availability zone corresponds to a separate Tier III data center.\n\nEach zone is isolated from failures in other availability zones. Placing virtual resources in multiple zones provides fault tolerance and reduces the likelihood of data loss.\n\nVK Cloud platform resources are hosted in data centers:\n\nGZ1 – Goznak data center. Address: Moscow, Prospekt Mira, 105, building 6.\nMS1 (ko1) – DataLine NORD4 data center. Address: Moscow, Korovinskoe highway, 41.\nQAZ — QazCloud data center. Address: Republic of Kazakhstan, Akmola region, Kosshy, Republic str. 1.\n\nIn the MS1 availability zone, the VK Cloud infrastructure is protected in accordance with the Federal Law of the Russian Federation “On Personal Data” No. 152-FL.\n\nVK Cloud distributes the following virtual resources across availability zones:\n\nvirtual machines;\ndisks: are placed in network block storage and are automatically replicated within their availability zone;\nload balancers.\n\nNetworks and routers are not tied to availability zones.\n\n\n\nA network created in the QAZ availability zone cannot be united with networks in other availability zones, since QAZ zone is located in the Kazakhstan region. In this case, networks can be connected over VPN.\n\nOperating system\n\nOn the VK Cloud platform, when creating a virtual machine, you can choose an operating system of the Microsoft Windows or Linux family. To create a VM with an operating system that is not in the platform list, use the VM image file with the required OS.\n\nVK Cloud supports migration of Windows server versions:\n\nWindows Server 2008 / 2008 R2;\nWindows Server 2012 / 2012 R2;\nWindows Server 2016;\nWindows Server 2019;\nWindows Server 2022.\n\n\n\nIt is not possible to use Windows 7 / 8 / 8.1 / 10 / 11 operating systems in VK Cloud. This restriction is set for all projects and cannot be lifted.\n\nImage\n\nThe configuration and data of the virtual machine are stored as one or more files. These files can be used as an image to run a VM in a local environment or to create a new VM.\n\nOn the VK Cloud platform, you can create an image based on a VM disk or upload an image file. The resulting image can be used to create a new virtual machine.\n\nAn image, the source of which is a disk, allows you to create a copy of a virtual machine with an installed and configured operating system and applications.\n\nThe image downloaded from the file can be used for VM migration or to create a virtual machine with your operating system.\n\n\n\nCloud Servers supports images in the raw format.\n\nFile storage\n\nFile storage is a virtual file system that can be connected to multiple virtual machines and used to share data. When creating a file storage, the network access protocol to it (NFS or CIFS) is specified.\n\n\n\nFile storage is accessed only from virtual machines inside the VK Cloud project.\n\n\n\nThe size of the file storage is set at the creation stage. If necessary, the size of the file storage can be increased, but the file storage cannot be reduced.\n\nSnapshot creation and snapshot recovery operations are available for file repositories.\n\nBackup\n\nA VM backup is needed to restore the state of the VM saved at a certain point in time. The VK Cloud platform provides a service for creating VM backups in manual or automatic mode.\n\nIn manual mode, the creation of backups is started by the user. At the same time, full VM backups are created.\n\nAutomatic mode supports GFS strategy and allows you to configure a backup schedule to create full and incremental backups.\n\nLicenses\n\nVK Cloud has the right to license software based on agreements with companies:\n\nMicrosoft (Microsoft Services Provider License Agreement — SPLA);\nRED SOFT;\nBaseALT;\nAstra Linux.\n\nFor more information, see the Licenses section.\n","url":"/en/computing/iaas/concepts/about","description":"Cloud Servers Concept — basic concepts related to virtual machines.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"813812a2-369d-429e-bf56-e865f9f57a1d","title":"VK Cloud Disk Performance","data":"Various types of disks are used in the data storage system on the VK Cloud platform:\n\nHDD\nSSD\nHigh-IOPS SSD\nLow Latency NVME\n\nRead more in the article Cloud Servers overview.\n\nCertain performance characteristics are guaranteed for each type of disk.\n\n\n\nThe latency value is guaranteed only for Low Latency NVME disks.\nFor other types of disks, the delay value is approximate and is indicated for reference.\n\n\n\n| Disk type(name in the API) | Read, IOPSmin.—max. | Read,IOPS/GB | Write, IOPS min.max. | Write,IOPS/GB | Latency, msecmax.|\n|----|----|----|----|----|----|\n| Network HDD(ceph-hdd)     | 300–2400    | 1  | 150–800    | 1  | 20   |\n| Network SSD(ceph-ssd)     | 1000–16000  | 30 | 500–8000   | 15 | 3    |\n| High-IOPS SSD(high-iops)  | 10000–45000 | 30 | 5000–30000 | 25 | 1    |\n| Low Latency NVME(ef-nvme) | 10000–75000 | 75 | 5000–50000 | 35 | 0,5  |\n\n\n\nDisk performance depends on its volume. In some cases, to increase the speed of data processing, it is enough to increase the size of the disk.\n\nNetwork HDD\n\nBelow are detailed performance characteristics for network HDD drives of different sizes.\n\n| Size, GB | Read, IOPSbs=4k,iodepth=32 | Read, MB/secbs=1M,iodepth=16 | Write, IOPSbs=4k,iodepth=32 | Write, MB/secbs=1M,iodepth=16 |\n|--------------|--------------|--------------|--------------|--------------|\n| 10           | 300          | 38           | 150          | 19           |\n| 50           | 300          | 38           | 150          | 19           |\n| 100          | 300          | 38           | 150          | 19           |\n| 250          | 300          | 38           | 250          | 31           |\n| 500          | 500          | 63           | 500          | 63           |\n| 1000         | 1000         | 125          | 800          | 100          |\n| 1500         | 1500         | 188          | 800          | 100          |\n| 2000         | 2000         | 250          | 800          | 100          |\n\nHere, bs and iodepth are performance testing parameters.\n\nNetwork SSD\n\nBelow are detailed performance characteristics for network SSD drives of different sizes.\n\n| Size, GB | Read, IOPSbs=4k,iodepth=32 | Read, MB/secbs=1M,iodepth=16 | Write, IOPSbs=4k,iodepth=32 | Write, MB/secbs=1M,iodepth=16 |\n|--------------|--------------|--------------|--------------|--------------|\n| 10           | 1000         | 125          | 500          | 63           |\n| 50           | 1500         | 188          | 750          | 94           |\n| 100          | 3000         | 375          | 1500         | 188          |\n| 250          | 7500         | 400          | 3750         | 400          |\n| 500          | 15000        | 400          | 7500         | 400          |\n| 1000         | 16000        | 400          | 8000         | 400          |\n| 1500         | 16000        | 400          | 8000         | 400          |\n| 2000         | 16000        | 400          | 8000         | 400          |\n\nHere, bs and iodepth are performance testing parameters.\n\nHigh-IOPS SSD\n\nBelow are detailed performance characteristics for network High-IOPS SSD drives of different sizes.\n\n| Size, GB | Read, IOPSbs=4k,iodepth=32 | Read, MB/secbs=1M,iodepth=16 | Write, IOPSbs=4k,iodepth=32 | Write, MB/secbs=1M,iodepth=16 |\n|--------------|--------------|--------------|--------------|--------------|\n| 10           | 10000        | 500          | 5000         | 500          |\n| 50           | 10000        | 500          | 5000         | 500          |\n| 100          | 10000        | 500          | 5000         | 500          |\n| 250          | 10000        | 500          | 6250         | 500          |\n| 500          | 15000        | 500          | 12500        | 500          |\n| 1000         | 30000        | 500          | 25000        | 500          |\n| 1500         | 45000        | 500          | 30000        | 500          |\n| 2000         | 45000        | 500          | 30000        | 500          |\n\nHere, bs and iodepth are performance testing parameters.\n\nLow Latency NVME\n\nBelow are detailed performance characteristics for local Low Latency NVME drives of different sizes.\n\n| Size, GB | Read, IOPSbs=4k,iodepth=32 | Read, MB/secbs=1M,iodepth=16 | Write, IOPSbs=4k,iodepth=32 | Write, MB/secbs=1M,iodepth=16 |\n|--------------|--------------|--------------|--------------|--------------|\n| 10           | 10000        | 500          | 5000         | 500          |\n| 50           | 10000        | 500          | 5000         | 500          |\n| 100          | 10000        | 500          | 5000         | 500          |\n| 250          | 18750        | 586          | 8750         | 500          |\n| 500          | 37500        | 1172         | 17500        | 547          |\n| 1000         | 75000        | 1200         | 35000        | 900          |\n| 1500         | 75000        | 1200         | 50000        | 900          |\n| 2000         | 75000        | 1200         | 50000        | 900          |\n\nHere, bs and iodepth are performance testing parameters.\n\nDisk performance testing\n\nBefore testing, make sure that the following conditions are met:\n\n   the disk is non-bootable;\n   there is no load on the disk from the operating system.\n\n   When these conditions are met, the IOPS measurement results must correspond to the following values:\n\n   | Type of testing | Result, IOPS |\n   | ---- | ---- |\n   | Read/write in blocks of 4 KB in 32 threads | Complies with SLA   |\n   | Read/write in blocks of 8 KB in 32 threads | At least 75% of SLA |\n   | Read/write in blocks of 16 KB in 32 threads | At least 50% of the SLA |\n\nTest the disk.\n\n   Windows\n   Linux\n\n\n   To measure IOPS when reading and writing, use the DiskSpd or FIO utilities.\n\n\n   The measurement results obtained using DiskSpd and FIO may differ. DiskSpd is a utility created and recommended by Microsoft for testing disks in Windows OS.\n\n\n   DiskSpd\n\n   Run the command prompt as an administrator.\n   Create a temp directory and an empty file of at least 10 GB in size:\n\n            md C:\\temp\n      fsutil file createnew C:\\temp\\test.bin 10485760000\n\n   Download the utility and unpack to the required directory.\n   Go to the amd64 directory of the unpacked utility.\n   Run the command diskspd with parameters corresponding to the type of test:\n\n      -w — percentage of write operations, -w0 for the reading test, -w100 for the recording test;\n      -b — block size in bytes.\n\n      Detailed description of all command parameters diskspd — in official documentation.\n\n      Random write test in blocks of 4 KB:\n\n                  diskspd -Suw -b4K -o1 -t32 -r -w100 C:\\temp\\test.bin > C:\\temp\\random_write_results.txt\n\n      Random reading test in blocks of 4 KB:\n\n                  diskspd -Suw -b4K -o1 -t32 -r -w0 C:\\temp\\test.bin > C:\\temp\\random_read_results.txt\n\n   FIO\n\n   Download and install FIO.\n   Run the fio command with the parameters corresponding to the type of test:\n\n      --rw — randread or randwrite.\n      --bs — block size.\n      --filename — name of the test file.\n      --rate_iops — IOPS target value (optional). Use this parameter to get a more accurate latency value when testing the IOPS target value.\n\n      Detailed description of all command parameters — in FIO documentation.\n\n      Random write test in blocks of 4 KB:\n\n\n         Do not specify the name of the file with the necessary data in the filename parameter! During write tests, the contents of this file will be overwritten.\n\n         PowerShell\n         Command line\n\n\n                  fio `\n            --name=randwrite `\n            --iodepth=32 `\n            --rw=randwrite `\n            --bs=4k `\n            --direct=1 `\n            --size=10G `\n            --numjobs=1 `\n            --runtime=240 `\n            --group_reporting `\n            --filename=C:\\Users\\ADMIN\\test\n\n\n                     fio ^\n            --name=randwrite ^\n            --iodepth=32 ^\n            --rw=randwrite ^\n            --bs=4k ^\n            --direct=1 ^\n            --size=10G ^\n            --numjobs=1 ^\n            --runtime=240 ^\n            --group_reporting ^\n            --filename=C:\\Users\\ADMIN\\test\n\n\n      Random reading test in blocks of 4 KB:\n\n         PowerShell\n         Command line\n\n\n                  fio `\n            --name=randread `\n            --iodepth=32 `\n            --rw=randread `\n            --bs=4k `\n            --direct=1 `\n            --size=10G `\n            --numjobs=1 `\n            --runtime=240 `\n            --group_reporting `\n            --filename=C:\\Users\\ADMIN\\test\n\n\n                  fio ^\n            --name=randread ^\n            --iodepth=32 ^\n            --rw=randread ^\n            --bs=4k ^\n            --direct=1 ^\n            --size=10G ^\n            --numjobs=1 ^\n            --runtime=240 ^\n            --group_reporting ^\n            --filename=C:\\Users\\ADMIN\\test\n\n\n   To measure IOPS when reading and writing, use the FIO utility.\n\n\n   This technique is only applicable for testing partitions with the file system ext2, ext3, ext4 or xfs.\n\n\n   Update the package list:\n\n            sudo apt update\n\n   Install FIO:\n\n            sudo apt install fio\n\n   Run the fio command with the parameters corresponding to the type of test:\n\n      --rw — randread or randwrite.\n      --bs — block size.\n      --filename — the name of the test file. The user must have read and write permissions to the filename to run the tests.\n      --rate_iops — IOPS target value (optional). Use this parameter to get a more accurate latency value when testing the target IOPS value.\n\n      Detailed description of all command parameters — in FIO documentation.\n\n      Random write test in blocks of 4 KB:\n\n\n         Do not specify the name of the file with the necessary data in the filename parameter! During write tests, the contents of this file will be overwritten.\n\n\n                  fio \\\n            --name=randwrite \\\n            --ioengine=libaio \\\n            --iodepth=32 \\\n            --rw=randwrite \\\n            --bs=4k \\\n            --direct=1 \\\n            --size=512M \\\n            --numjobs=1 \\\n            --runtime=240 \\\n            --group_reporting \\\n            --filename=/home/user/test\n\n      Random reading test in blocks of 4 KB:\n\n                  fio \\\n            --name=randread \\\n            --ioengine=libaio \\\n            --iodepth=32 \\\n            --rw=randread \\\n            --bs=4k \\\n            --direct=1 \\\n            --size=512M \\\n            --numjobs=1 \\\n            --runtime=240 \\\n            --group_reporting \\\n            --filename=/home/user/test\n\n\nIf, when all conditions are met, the test results do not match the specified values, contact technical support.\n\n\n\nAfter testing is completed and the results are processed, delete the large test files to free up disk space.\n\n\n","url":"/en/computing/iaas/concepts/volume-sla","description":"Performance characteristics of different types of disks on the platform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"946b9f5a-abf1-4214-8865-e01d6ea8755f","title":"Creating a VM","data":"In Cloud Servers, you can create virtual machines through your personal account, OpenStack CLI or Terraform. To create a VM using Terraform, use the VM creation instructions in the Terraform section.\n\nBefore starting work\n\nRegister to VK Cloud.\nActivate the services.\n\nThe account balance should be positive, and quotas should be sufficient to create the required virtual machine configuration.\n\nAvailable OS\n\nThe VK Cloud team has prepared images of some operating systems. The list of ready-made images is available in your personal account in the window for creating a new instance.\n\nIf necessary, you can import an OS image by yourself.\n\nCreate a VM\n\n\n\nThe available fields may differ for different operating systems.\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nClick the Create instance or Add button.\nSet VM parameters:\n     Name of virtual machine: use only Latin letters, numbers, or special characters -, _, and ..\n     Category of virtual machine: select the flavor of the VM. For more information, see Cloud Servers service overview.\n     Type of virtual machine: select the preset VM configuration. For more information, see Cloud Servers service overview.\n     Availability zone: select the data center where the VM will be launched.\n     Number of machines in configuration: specify the required number of VM.\n     Disk size: specify the required VM disk size in gigabytes.\n     Disk Type: select one of the values — HDD, SSD or High-IOPS SSD. For more information, see Cloud Servers service overview.\n     Operating system: select the operating system version or previously created image.\n     Tags: if necessary, specify the tag for the VM or create a new one.\nClick Next step.\nConfigure the VM connection to the network:\n   Network: select an existing network or create a new one. Depending on the choice of network type, the list of available fields will change.\n   SDN: select a virtual network management system. Resources created in networks of different SDNs will be accessible to each other only when using floating IPs. The option is available if a new network is created for the VM.\n   Subnet address: the option is available if a new network is created for the VM.\n   Use configuration disk: enable the option if you need to automatically configure a network on a virtual machine when there is no DHCP server on the network. The option is enabled automatically if a network with DHCP disabled is selected, including the external network (ext-net).\n   DNS-name: specify the DNS name for the VM. For more information, see Private DNS. The option is not available, if the external network is selected (ext-net).\n   Virtual machine key: select a key for SSH connection or create a new one.\n\n      If you select Create a new key, save the suggested key file .pem after the virtual machine creation is completed.\n\n   Firewall settings: select the appropriate security groups.\n\n      To connect via SSH, add the group ssh or ssh+www. For more information about configuring network access rules, see Managing firewall rules.\n\n   (Optional) Assign external IP: enable the option and specify the IP to be able to connect to the VM from the Internet. The option is available if the VM is on a network with an Internet connection. The option is enabled and automatically configured the IP address if the VM is located on an external network (ext-net).\n\n   (Optional) Link a domain to an external IP: enable the option and specify a domain to link the FQDN to the external IP. An A-record for the selected IP will be added to the DNS. The option is available if the Assign external IP option is enabled.\n\n\n      You cannot link an external IP to a domain that matches the CNAME record of the DNS zone. In this case, the VM will be created, but the A-record will not be added.\n\n\n   Enable monitoring: enable to use monitoring tools.\n\nClick Next step.\n\nIf necessary, enable the Use Backup option and specify settings.\nClick Create instance.\nWait for the VM to be created. This process may take some time. When the creation is completed, a page with VM characteristics and connection instructions will open.\n\n\n\n\nGet ready to work with the OpenStack CLI:\n\n   Enable two-factor authentication.\n   Activate API access.\n   Make sure that OpenStack client is installed and authenticate to the project.\n\nCollect the data:\n\n   Get a list of available VM types and save the required flavor_ID:\n\n            openstack flavor list\n\n   Get a list of available VM images and save the required image_ID:\n\n            openstack image list\n\n   Get a list of security groups:\n\n            openstack security group list\n\n       To create a Linux VM and connect to it via SSH, save the security_group_ID of the group ssh or ssh+www.\n       To create a Windows VM and connect to it via RDP, save the security_group_ID of the group rdp or rdp+www.\n\n      For more information about configuring network access rules, see Managing firewall rules.\n\n   Get a list of available networks and save the required network_ID:\n\n            openstack network list\n\n      If the ext-net network is selected, an external IP address will be automatically assigned to the virtual machine.\n      If a private network is selected, then a floating IP address can be assigned to the virtual machine after creation.\n\n   Get a list of available key pairs and save keypair_name:\n\n            openstack keypair list\n\n      To create a new key pair:\n         Generate a key:\n\n                        ssh-keygen -q -N \"\"\n\n         Upload the key:\n\n                        openstack keypair create --public-key ~/.ssh/id_rsa.pub --type ssh\n\nCreate a boot disk:\n\n      openstack volume create root-volume --size 10 --image  --availability-zone MS1 --bootable\n\nCreate a VM:\n\n      openstack server create\n                           --volume\n                           --network  \\\n                           --flavor  \\\n                           --key-name  \\\n                           --availability-zone MS1\n\n   After creating a VM, information about it will be displayed. Find the adminPass field and copy its value. You will need it to sign in to the server via the VNC console.\n\nCheck the status of the created VM:\n\n      openstack server list\n\n   The created machine should appear in the list of available VMs and have the status ACTIVE.\n\n\n\nSome features of creating a VM via the CLI:\n\nIf you do not specify an availability zone, a random one will be selected.\nIf you do not specify a disk, an “ephemeral disk” will be created (with restrictions).\n\nWhat's next?\n\nSee the ways to connect to Linux VM or Windows.\nLearn how to work with VM.\n","url":"/en/computing/iaas/service-management/vm/vm-create","description":"Creating a virtual machine based on Linux or Windows OS.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"2df95ef2-6b27-4ad6-974a-6d43350c0f00","title":"Connecting to a Linux VM","data":"The recommended way to connect to a Linux VM is via SSH using a key pair.\n\n1. Check the ability to connect via SSH\n\nTo connect via SSH, an SSH server must be running on the virtual machine. On Linux virtual machines created in VK Cloud, the OpenSSH server is installed and started by default.\n\nBefore connecting, make sure that the following conditions are met:\n\nthe virtual machine is running and the OS boot process has finished;\nremote access via SSH protocol (TCP port 22) is allowed in the VM network settings — add ssh security group;\nthe VM has an external (“floating”) IP address available for connection.\n\n\n   The external IP address of the VM can be viewed in personal account VK Cloud in the section Cloud computing → Virtual machines.\n\n\nInstead of an external IP address, you can use the fully qualified domain name of the virtual machine (FQDN). Properly configured DNS records are required to connect using a fully qualified domain name.\n\n\n\nVK Cloud provides DNS servers to which you can delegate your domain.\n\n\n\nThe virtual machines that were created from the FreeBSD and Bitrix images offered by the VK Cloud platform do not support working with an external ext-net network. To connect via SSH to such a VM, it is necessary that it is on a private network with DHCP enabled.\n\nIf SSH connection to the VM is not possible or unavailable, use the VNC console.\n\n2. Select the user name\n\nDetermine the user name (login) of the operating system that is deployed on the target VM.\n\nIn VK Cloud images (except Bitrix), the root account is blocked for security reasons and a default account has been added:\n\n| Operating system | Username  |\n| ---              | ---       |\n| AlmaLinux        | almalinux |\n| ALT Linux        | altlinux  |\n| Astra Linux      | astra     |\n| Bitrix           | root      |\n| CentOS           | centos    |\n| Debian           | debian    |\n| Fedora           | fedora    |\n| FreeBSD          | freebsd   |\n| openSUSE         | opensuse  |\n| Ubuntu           | ubuntu    |\n| RED OS           | redos     |\n\n3. Check for a key pair\n\nTo connect to a VM over SSH using a key pair, it is necessary that the public key is stored on the virtual machine in the file ~/.ssh/authorized_keys, and the file with the private key is located on the computer from which the connection is being made.\n\nIf the VM was created on the VK Cloud platform:\n\nthe public key was saved on the VM automatically;\nwhen selecting the option Create a new key, the private key in the file with the extension .pem was downloaded to the computer from which the VM was created.\n\nIf one or both of the keys from the pair are lost, follow the recovery instructions.\n\n4. Connect to the VM\n\n\n\nLinux/macOS\nWindows 10/11\nWindows 7/8\n\n\n\n(Optional) Configure access rights to the key file:\n\n      chmod 400\n\nRun the command in the terminal.\n\n   To connect by IP address:\n\n            ssh -i  @\n\n   To connect by a fully qualified domain name:\n\n           ssh -i  @\n\nIf this is the first connection to the VM, a confirmation request will appear:\n\n            The authenticity of host '213.219.212.130 (213.219.212.130)' can't be established.\n      ECDSA key fingerprint is SHA256:aYZIWs9N6KRtfFOuic6eoWcluhSp6+jha/DSBgd9McI.\n      Are you sure you want to continue connecting (yes/no)?\n\n      Type yes in the terminal and press Enter.\n\n\n      The first time you connect to a VM created from a Bitrix VK Cloud image, a request to change the password for the root user will appear in the console. Enter the current bitrix password and enter the new password twice.\n\n\n(Optional) Configure access rights to the private key file:\n\n   Open the properties of the private key file .pem.\n   Go to the Security tab and click Advanced. The Advanced Security Settings window opens.\n   Click the button Disable inheritance and in the window that opens, select Remove all inherited permissions from this object.\n   In the Advanced Security Settings window, click Add.\n   In the window that opens, click Select the subject and specify the Windows OS user on whose behalf the command prompt for connecting to the VM will be launched.\n   Confirm the changes by sequentially clicking OK in the open settings windows.\n\nRun the command in the command prompt.\n\n   To connect by IP address:\n\n            ssh -i  @\n\n   To connect by a fully qualified domain name:\n\n           ssh -i  @\n\nIf this is the first connection to the VM, a confirmation request will appear:\n\n            The authenticity of host '213.219.212.130 (213.219.212.130)' can't be established.\n      ECDSA key fingerprint is SHA256:aYZIWs9N6KRtfFOuic6eoWcluhSp6+jha/DSBgd9McI.\n      Are you sure you want to continue connecting (yes/no)?\n\n     Type yes in the command prompt and press Enter.\n\n\n      The first time you connect to a VM created from a Bitrix VK Cloud image, a request to change the password for the root user will appear in the console. Enter the current bitrix password and enter the new password twice.\n\n\nIn Windows 7/8, use the PuTTY application to connect via SSH.\n\nInstall PuTTY.\nCreate a private key file in PuTTY Private Key Files format:\n\n   Launch the PuTTYgen application that comes with PuTTY.\n   In the Conversions menu, click Import Key and in the window that opens, select the private key file .pem.\n   Click Save private key.\n   In the window that opens, select a directory, enter a file name, check that the .ppk extension is specified in the File type field, and click Save.\n\nAdd the private key file .ppk to Pageant:\n\n   Launch the Pageant application that comes with PuTTY.\n   Right-click on the Pageant icon on the taskbar.\n   In the context menu, select Add key.\n   Select the private key file .ppk created in the previous step.\n\nRun PuTTY:\n\n   In the Host Name (or IP address) field enter the external IP address of the virtual machine or its full domain name. Specify the port 22 and the connection type SSH.\n   Open the item in the tree on the left Connection → SSH → Auth.\n   Set the checkbox Allow agent forwarding.\n   In the Private key file for authentication field, select the private key file .ppk.\n   Click Open.\n   If this is the first connection to the VM, confirm it in the window that appears.\n   In the terminal window that opens, you will be prompted to enter the OS user name.\n   Enter the VM operating system user name. As a result, a connection to the server should be established.\n\n\n      The first time you connect to a VM created from a Bitrix VK Cloud image, a request to change the password for the root user will appear in the console. Enter the current bitrix password and enter the new password twice.\n\n\nIf you could not connect to the VM via SSH, use the instructions from the article Network in a Linux VM.\n\n\n","url":"/en/computing/iaas/service-management/vm/vm-connect/vm-connect-nix","description":"Connecting to a Linux VM.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"a6fcdab8-4551-43c3-9e2c-1e08674d6df9","title":"Connecting to a Windows VM","data":"The recommended way to connect to a Windows VM is via the RDP (Remote Desktop Protocol).\n\n1. Check the possibility of connecting via RDP\n\nMake sure that the following conditions are met:\n\nthe virtual machine is running and the OS boot process has finished;\nremote access via RDP protocol (TCP port 3389) is allowed in the VM network settings;\nthe VM has an external (“floating”) IP address available for connection.\n\n\n   The external IP address of the VM can be viewed in personal account VK Cloud in the section Cloud computing → Virtual machines.\n\n\nInstead of an external IP address, you can use the fully qualified domain name of the virtual machine (FQDN). Properly configured DNS records are required to connect using a fully qualified domain name.\n\n\n\nVK Cloud provides DNS servers to which you can delegate your domain.\n\n\n\nIf RDP connection to the VM is not possible or unavailable, use the VNC console.\n\n2. Prepare the credentials\n\nTo connect to a VM via RDP, you need to know the operating system user name and password.\n\nOn the VK Cloud platform, when creating a Windows virtual machine in the operating system, a user is created with the name Admin and a generated password. Use this username and password if the VM was created on a private network. If the VM was created on the ext-net network, before the first connection set a new password.\n\nIf the password has been lost, reset or recovery it.\n\n3. Connect to the VM\n\n\n\nLinux\nmacOS\nWindows\n\n\n\nIn Linux, use the Remmina utility to connect via RDP.\n\nDownload and install the Remmina client:\n\n      sudo apt-add-repository ppa:remmina-ppa-team/remmina-next\n   sudo apt-get update\n   sudo apt-get install remmina remmina-plugin-rdp libfreerdp-plugins-standard\n\nLaunch the Remmina client.\nCreate a new connection:\n\n   In the application window, select the protocol: RDP.\n   Click New connection profile.\n   In the window that opens, specify the parameters:\n\n      Name: connection name (for example, VM name).\n      Server: the external IP address or the FQDN of the virtual machine.\n      Username.\n      Password.\n\nClick Save and Connect to connect to the VM. The new connection will be saved in the list.\n\n\n\n\nIn macOS, use the Microsoft Remote Desktop program to connect via RDP. Specify the connection parameters manually or using the RDP configuration file.\n\nTo get the RDP configuration file:\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nSelect the VM you need and go to the Windows tab at the bottom of the page.\nClick Download RDP config.\n\nConnect to the VM:\n\nInstall and run Microsoft Remote Desktop.\nCreate a new connection.\n\n   Manual:\n\n      In the application window, click Add PC.\n      Specify the parameters of the new connection:\n         PC name: the external IP address or the FQDN of the virtual machine.\n         User account: select Add User Account and specify the credentials:\n            Username.\n            Password.\n      Click Add — the new connection will appear in the list.\n\n   Using the configuration file:\n\n      In the application window, in the upper drop-down menu, select Import from RDP file.\n      Select the configuration file and click Import.\n\nFind a new connection in the list and double-click on it to connect to the VM.\n\n\n\n\nTo connect via RDP, specify the connection parameters manually or using the RDP configuration file.\n\n\nHow to get the RDP configuration file for VK Cloud VM\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nClick on the name of the VM you need to go to the VM page.\nGo to the General information tab and select the Windows tab at the bottom of the page.\nClick Download RDP config.\n\n\n\nConnect to the VM:\n\nOpen Microsoft Remote Desktop Connection.\nSet up the connection.\n\n   Manual:\n\n      In the Computer field, enter the IP address or FQDN of the virtual machine.\n      Click Connect.\n      In the window that opens, enter the credentials:\n\n         Username.\n         Password.\n\n      Click OK.\n\n   Using the configuration file:\n\n      Click Show Options → General.\n      In the block Connection settings click Open.\n      Select the configuration file and click Open.\n      Press the button Connect and confirm the connection again in the window that opens.\n      In the operating system authorization window, enter ``.\n\n\n\n\n\n\nAt the first connection, confirm the use of the certificate.\n\n\n","url":"/en/computing/iaas/service-management/vm/vm-connect/vm-connect-win","description":"Connecting to a Windows VM.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"3e93d14e-0b97-44cb-ab2b-bae12eeb188d","title":"VM management","data":"You can change the parameters and status of a virtual machine on the VK Cloud platform in personal account or using the OpenStack command line interface.\n\nGetting a virtual machine ID\n\nTo manage a VM using the OpenStack CLI, you need a vm ID.\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nIn the list of VMs, click on the name of the VM whose ID you want to find out.\nOn the VM page, go to the General information tab.\nFind the row ID of the virtual machine in the table.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nDo one of the following.\n\n   If you know the VM name, get information about the VM and find the string with the id:\n\n            openstack server show\n\n   If the exact name of the VM is unknown, output a list of virtual machines and find the necessary one in it:\n\n            openstack server list\n\n\n{heading(Starting, stopping, reboot the VM)[id=start_stop_restart_vm]}\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nChange the VM state in one of the ways.\n\n    Using group operations — for multiple VMs:\n\n        In the list of virtual machines, select the VMs whose state you want to change.\n        Above the list of VMs, click the button with the required action.\n        Confirm the action.\n\n    Through the context menu — for one VM:\n\n        Click   for the VM whose state you want to change.\n        Select and confirm the action.\n\n    On the virtual machine page:\n\n        In the list of virtual machines, click on the name of the VM whose state you want to change.\n        On the VM page, go to the General information tab.\n        To the right above the table with VM parameters, click on the icon of the required action.\n        Confirm the action.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nRun the appropriate command.\n\n   Start VM:\n\n            openstack server start\n\n   Stop VM:\n\n            openstack server stop\n\n   Restart VM:\n\n            openstack server reboot\n\n\nForced VM reboot\n\nIf the VM is not responding, use a forced reboot.\n\n\n\nThe forced restart of the VM corresponds to the power off and on (power cycling).The reboot assumes the correct shutdown of the VM operating system (graceful shutdown).\n\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nForce a VM restart in one of the ways.\n\n    Through the context menu:\n\n        Click   for the required VM.\n        Select Force reboot and confirm the action.\n\n    On the virtual machine page:\n\n        In the list of virtual machines, click on the name of the VM whose state you want to change.\n        On the VM page, go to the General information tab.\n        Above the table with VM parameters, click More.\n        Click Force reboot and confirm the action.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nForce a VM reboot:\n\n      openstack server reboot --hard\n\n\nVM block and unblock\n\nBlock the VM if you need to prohibit changing its state and parameters.\n\nA blocked virtual machine cannot be started or stopped, nor can it be restarted. It is not possible to replace disks and change the network interface settings of a blocked VM.\n\n\n\n\nPersonal account\n\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nBlock or unblock the VM in one of the ways.\n\n    Through the context menu:\n\n        Click   for the required VM.\n        Select and confirm the action.\n\n    On the virtual machine page:\n\n        In the list of virtual machines, click on the name of the VM whose state you want to change.\n        On the VM page, go to the General information tab.\n        Above the table with VM parameters, click More.\n        Select and confirm the action.\n\n{heading(Deleting a VM)[id=delete_vm]}\n\nOn the VK Cloud platform, the virtual machine is deleted by default along with the main disk (root disk). Deleting additional disks depends on the settings of the corresponding policy.\n\n\n\nThe deletion operation is irreversible! Save the necessary data before deleting the VM.\n\n\n\nPrepare the VM for deletion.\n\n   If you need to save the main disk, clone it or replace it with an unnecessary one.\n\n   Disconnect from the VM additional disks that need to be saved.\n\n      Additional disks for which the delete_on_termination: False property is set in the deletion policy can not be disconnected from the VM.\n\n      Check the disk deletion policy — OpenStack CLI\n\n      Make sure that OpenStack client is installed and authenticate to the project.\n\n      Run the command:\n\n                  openstack server show  --os-compute-api-version 2.42\n\n         Find the string volumes_attached. It lists the disk IDs and specifies their deletion policy — property delete_on_termination (True or False).\n\n\nAfter saving the necessary data, delete the VM.\n\n   Personal account\n   OpenStack CLI\n\n\n   Go to personal account VK Cloud.\n   Go to Cloud Computing → Virtual machines.\n   Delete the VM in one of the ways.\n\n      Using group operations — for multiple VMs:\n\n         In the list of virtual machines, select the VMs that you want to delete.\n         In the menu above the VM list, click Delete.\n         Confirm the action.\n\n      Through the context menu — for one VM:\n\n         Click   for the VM that you want to delete.\n         Select Delete and confirm the action.\n\n      On the virtual machine page:\n\n         In the list of virtual machines, click on the name of the VM that you want to delete.\n         On the VM page, go to the General information tab.\n         To the right above the table with VM parameters, click on the trash icon.\n         Confirm the action.\n\n\n   Make sure that OpenStack client is installed and authenticate to the project.\n   Delete the VM:\n\n            openstack server delete\n\n\nRenaming and changing the VM type\n\nOn the VK Cloud platform, you can rename a virtual machine, as well as change the type of VM — the number of processors (vCPU) and the amount of RAM.\n\n\n\nIf the VM type changes, the VM will be rebooted.\n\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nChange the VM name or type in one of the ways.\n\n   Through the context menu:\n\n      Click   for the required VM.\n      Select an action.\n      Fill in the input field and click Save.\n\n   On the virtual machine page:\n\n      In the list of virtual machines, click on the name of the VM whose state you want to change.\n      On the VM page, go to the General information tab.\n      Above the table with VM parameters, click More.\n      Select an action.\n      Fill in the input field and click Save.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nPerform the required action.\n\n   Rename VM:\n\n            openstack server set --name\n\n   Change VM type:\n\n      Get a list of available configuration templates and copy the template ID:\n\n                  openstack flavor list --all\n\n      Start changing the VM type and wait for the process to finish:\n\n                  openstack server resize --flavor\n\n\nAssigning tags\n\nTags allow you to filter the list of virtual machines and find the right VM faster. Use existing tags or create your own tag and choose a color for it.\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nClick   for the required VM.\nCheck the tags or click Create a new tag.\nTo add a new tag, enter its name, select a color, and click Add tag.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nRun the appropriate command.\n\n   Output a list of VM tags:\n\n            nova server-tag-list\n\n   Assign a VM tag:\n\n            nova server-tag-add\n\n   Delete a tag from a VM:\n\n            nova server-tag-delete\n\n   Delete all tags from a VM:\n\n            nova server-tag-delete-all\n\n\nSetting and changing a password\n\nIn the operating system of the VK Cloud virtual machine, you can set or change the default password for the user.\n\nConditions for setting a password:\n\nthe virtual machine is running and the OS boot process has finished;\nthe QEMU guest agent is installed and running (state — running).\n\n\n\nIn virtual machines created from the VK Cloud image, the QEMU guest agent is available by default.\n\n\n\nCommand to check the status of the QEMU guest agent:\n\n\n\n\nWindows\nLinux\n\n\n\n\nsc query qemu-ga\n\n\n\n\n\nsystemctl status qemu-guest-agent\n\n\n\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nSet the password in one of the ways.\n\n    Through the context menu:\n\n        Click   for the required VM.\n        Select Set password.\n        Enter a new password or click Generate.\n        Click Set password.\n\n    On the virtual machine page:\n\n        In the list of virtual machines, click on the name of the VM whose state you want to change.\n        On the VM page, go to the General information tab or to the Console.\n        Above the table with VM parameters or above the console window, click Set password.\n        Enter a new password or click Generate.\n        Click Set password.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nSet a password:\n\n      openstack server set --root-password\n\n\nThe password must be at least 8 characters long and contain letters of the Latin alphabet, numbers, and characters !, \", #, $, %, &, (, ), *, +, ,, -, ., :, ;, `, ?, @, [, ], ^, _,   `, {, }, ~`.\n\nPassword recovery\n\nPassword recovery generated for the default user is available only for Windows virtual machines created on a private network. To get a password, you need the private key of the key pair selected when creating the VM.\n\n\n\nIf the password was changed after the VM was created or the VM was created on the ext-net network, set a new password.\n\n\n\n\n\n\nPersonal account\n\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nOpen the password receipt window using one of the following methods.\n\n    Through the context menu:\n\n        Click   for the required VM.\n        Select Get password.\n\n    On the virtual machine page:\n\n        In the list of virtual machines, click on the name of the VM whose state you want to change.\n        On the VM page, go to the General information tab.\n        Above the table with VM parameters, click More.\n        Choose Get password.\n\nIn the window that opens, click Private key file or paste the contents of the key into the Private key field.\nClick Decrypt password.\nA field will appear in the window Password. Copy the password by clicking the icon in the field.\n\nRestoring VM access by key\n\nTo restore access to a Linux virtual machine via SSH using a key pair, you need to know the password of the OS user.\n\nGet a public key.\n\n   If the public key file is lost, go to the directory with the private key file and run the command:\n\n            ssh-keygen -y -f  >\n\n   If the private key file is lost, create a new key pair.\n\nCreate a link to the file with the public key.\n\n   Go to personal account VK Cloud.\n   Go to Object storage → Buckets.\n   Use an existing package or create a new one.\n   Add file with a public key in the bucket.\n   Enable access by the link to this file.\n   Copy the link to the file with the public key.\n\nUpload the public key to the VM.\n\n   Go to Cloud Computing → Virtual machines.\n   In the list of virtual machines, click on the name of the VM you need.\n   On the VM page, go to the tab Console.\n   Use the default username and password to sign in to the VM OS.\n   Download the file with the public key:\n\n            wget\n\n   Copy the public key to the file authorized_keys:\n\n            cat  >> ~/.ssh/authorized_keys\n\nCheck the possibility of key access.\n\n   Go to the terminal of the computer where the private key is stored.\n   Connect to the VM via SSH:\n\n            ssh -i  @\n\nViewing the event log\n\nThe event log contains information about changes in the VM state and user actions.\n\n\n\n\nOpenStack CLI\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nRun the appropriate command.\n\n   View the event log (extended format):\n\n            openstack server event list --long\n\n   View the event log (short format):\n\n            openstack server event list\n\n   Use the event ID from the Request ID column to display detailed information about the event:\n\n            openstack server event show\n\nExamples of events:\n\nstart / stop — starting / stopping a VM;\nresize / confirmResize — starting VM type change / confirmation;\nextend_volume — increasing the disk size;\ncreate — creating a VM;\nlive-migration — VM migration.\n\n\n\n\n","url":"/en/computing/iaas/service-management/vm/vm-manage","description":"Description of the possibilities of managing a virtual machine, its state and resources.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"be312143-10d6-4d7a-88c9-f5414a131e07","title":"Connecting the network to the VM","data":"A virtual machine can be connected to the network using a port. Port — VM virtual network card for which entities are configured:\n\nsecurity groups;\nconnected network;\nDNS name.\n\nLearn more about networks and ports in the sections Managing networks and subnets and Managing ports.\n\nConnecting the network to the VM\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nSelect the required VM and go to the Networks tab.\nClick Add connection.\nIn the window that appears:\n\n   Name: set the network name.\n   Network for connect: select a value from the list:\n\n      Create new network: more about networks and ports in the sections Managing networks and subnets.\n      External network (ext-net): the virtual machine will be automatically assigned an IP address.\n      Existing network.\n\n      When selecting an existing network, set the following parameters:\n\n      Assign external IP: enable it if you need access to the VM via the Internet.\n      DNS-name: the name by which you can access the VM via private DNS.\n      Set IP-address: enable to set a specific IP address from the subnet address pool.\n\n   Firewall settings: specify the necessary security groups. For more information, see Managing firewall rules.\n\nClick Save.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nConnecting to an existing port\n\nGet the ID of the virtual machine that you plan to connect to the network:\n\n      openstack server list\n\nGet a list of networks and their subnets:\n\n      openstack network list\n\nGet a list of available ports on the required network:\n\n      openstack port list --network\n\nAttach the selected port to the VM:\n\n      openstack server add port\n\nMake sure that the port is successfully connected to the VM:\n\n      openstack port list --server\n\nCreating a new port\n\nGet a list of networks and their subnets:\n\n      openstack network list\n\nGet a list of IP addresses for the subnet you plan to connect to:\n\n      openstack subnet list --network\n\nGet a list of security groups:\n\n      openstack security group list\n\nCreate a port using one of the following methods:\n\n   In the right network and with the default security group:\n\n            openstack port create  --network\n\n   With parameters specified:\n\n            openstack port create  \\\n                            --network  \\\n                            --fixed-ip subnet=,ip-address= \\\n                            --security-group\n\n      To view the full list of supported parameters, run the command:\n\n            openstack port create --help\n\n      It is also possible to create a port with DHCP options.\n\n      If necessary, set a floating IP address to the port.\n\n   As a result, the ID of the created port and other information about it will be displayed.\n\nGet the ID of the virtual machine that you plan to connect to the network:\n\n      openstack server list\n\nAttach the created port to the VM:\n\n      openstack server add port\n\nMake sure that the port is successfully connected to the VM:\n\n      openstack port list --server\n\n\nDeleting a VM network\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual machines.\nSelect the required VM and go to the Networks tab.\nDo one of the following for the required network:\n\n   Select the network using the checkbox, then click the Delete button.\n   Click   for the connected network and select Delete connection.\n\nIn the window that appears, click Confirm.\n\nThe network connection to the VM will be deleted, but the previously created port will remain in the status Not connected. For more information about removing ports, see Managing ports.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\nGet the VM ID:\n\n      openstack server list\n\nGet a list of ports from the VM:\n\n      openstack port list --server\n\nDelete the unnecessary port:\n\n      openstack port delete\n\n","url":"/en/computing/iaas/service-management/vm/vm-add-net","description":"Connecting the network to the VM.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"0bb60b95-8bb4-4b70-9088-2e86d87cb3fe","title":"VM diagnostics","data":"In emergency situations and when there is no connection to the VM over the network, use VNC console or view VM message logs.\n\n\n\nInformation about the user's actions with the VM and about changes in its state is contained in the event log OpenStack.\n\nThe VNC console\n\nVNC Console is a virtual machine diagnostics and management tool that runs in the browser. Use the VNC console if:\n\nthe virtual machine does not have an external IP address or is not connected to the Internet;\nrecommended VM connection methods (via SSH or RDP) don't work.\n\nAccess to the VNC console appears immediately after the virtual machine is started. This allows you to detect failures during the boot of the operating system.\n\nFeatures of working with the VNC console:\n\nyou can only use the Latin alphabet to enter text;\nthe clipboard in the VNC console is not available, you cannot copy and paste the text.\n\nOpen the console in the VK Cloud personal account or use the address obtained through the OpenStack CLI.\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to VK Cloud personal account.\nGo to Cloud Computing → Virtual machines.\nIn the list of virtual machines, click on the name of the VM you need.\nOn the VM page, go to the tab Console.\nIf the VM is not running, click the start button on the tab.\n\n\n\nUse the VNC console on the tab or click Open in new window.\n\n\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nGet a link to the VNC console page.\n\n      openstack console url show\n\n   Example output:\n\n      +-------+-------------------------------------------------------------------------------------+\n   | Field | Value                                                                               |\n   +-------+-------------------------------------------------------------------------------------+\n   | type  | novnc                                                                               |\n   | url   | https://infra.mail.ru:6080/vnc_auto.html?token=20224980-43eb-4535-85c7-310a18e27941 |\n   +-------+-------------------------------------------------------------------------------------+\n\nUse the link to open the VNC console page in the browser.\n\n\n   If the link has expired, run the command again to get a new link.\n\n\nVNC console control buttons are available in the personal account interface and in a separate window:\n\nSend CtrAltDel** — the action of the button depends on the VM's operating system (reboot, opening the lock window, etc.).\nUpdate Session** — use it to reconnect to the VNC console.\n\nVM message logs\n\nOpenStack provides support for a console running through a VM serial port. This allows you to view the boot logs and other diagnostic information, regardless of the state of the operating system and access to the VM over the network.\n\nIf the VM was created from a custom image, configure the console. To do this, add the following parameters to GRUB:\n\nconsole=tty0 console=ttyS0,115200n8\n\n\n\nOpenStack CLI\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nOutput all log entries:\n\n      openstack console log show\n\nOutput a limited number of records:\n\n      openstack console log show --lines\n\n","url":"/en/computing/iaas/service-management/vm/vm-console","description":"Remote access to a Windows or Linux VM using a VNC console. VM diagnostics.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"a7580d16-9b75-4d26-bebd-bcf391328655","title":"Disks","data":"The data storage system on the VK Cloud platform is organized using network drives. The configuration of the main disk is set at the stage of creating a virtual machine, additional disks can be created, later connect to the necessary VMs. For created disks, the resizing and changing disk type operations are available. Disks can be shared between projects and virtual machines, disconnect from VM, make bootable and not bootable. Disks that are no longer in use can be deleted.\n\nCreating a disk\n\nCreating HDD and SSD drives is available by default in all configurations. To create LL NVME discs contact technical support and request access to high-performance configurations and disks. Learn more about disk types — to the article Cloud Servers overview.\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nOpen the page with the required list of disks.\n\n   All disks: go to Cloud Computing → Disks.\n\n   Disks of a specific virtual machine:\n\n      Go to Cloud Computing → Virtual machines.\n      In the list of virtual machines, click on the name of the VM for which you want to create a disk.\n      On the VM page, go to the Disks tab.\n\nAbove the list of disks, click Create disk.\nSpecify the disk parameters:\n\n   Disk Name: required field. If necessary, click Add description and enter the text in the field that appears.\n   Source: select the appropriate option for the parameter:\n\n      Empty disk: fill in the parameters Disk Type and Availability zone.\n      Disk snapshot: in the List of snapshots field, select the snapshot from which you want to create a disk.\n      Disk image: fill in the parameters Disk Type, Availability zone and Disk image.\n      Disk: in the Cloning disk field, select the required disk.\n\n   Size: specify the disk size in gigabytes.\n   Boot disk: enable the option if you need to make the disk bootable.\n\n\n      The boot disk can be used as the VM's main (root) disk or connected to the VM as an additional disk from which the operating system can be booted.\n\n\n   Connect disk to instance: if you need to connect a disk to a VM immediately after creation, enable this option and select the VM you need in the field Choose instance.\n\nClick the Create disk button.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nSelect in the table the type of disk, determine its name in the API and the availability zone suitable for placement.\n\nView the available disk types and copy the type ID corresponding to the name in the API.\n\n      openstack volume type list\n\nView the availability zones and copy the name of the selected zone:\n\n      openstack availability zone list --volume\n\nCreate a disk of a certain type and size in the selected availability zone:\n\n      openstack volume create --type  --size  --availability-zone\n\n   Additional command parameters:\n\n   --image  — ID of the image from which the disk will be created;\n   --snapshot  — ID of the snapshot from which the disk will be created;\n   --description  — custom disk description;\n   --property  — custom disk properties;\n   --bootable — create a boot disk.\n\nIncreasing the disk size with rebooting\n\nRestrictions related to changing the VM disk size on the VK Cloud platform:\n\nThe disk size cannot be reduced.\nIf disks with the High IOPS SSD and Low Latency NVME type have snapshots, you cannot increase the size of these disks.\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nOpen the page with the required list of disks.\n\n   All disks: go to Cloud Computing → Disks.\n\n   Disks of a specific virtual machine:\n\n      Go to Cloud Computing → Virtual machines.\n      In the list of virtual machines, click on the name of the VM whose disk size you want to increase.\n      On the VM page, go to the Disks tab.\n\nUse one of the methods to open the disk resizing window.\n\n   Via the disk context menu:\n\n      Click   for the disk.\n      Select Change disk size.\n\n   On the disk page:\n\n      Click on the name of the disk whose size you want to change.\n      On the disk page, go to the tab General information.\n      Above the table with the disk parameters, click the Change disk size button.\n\nIn the window that opens, specify Disk size.\nClick the button Save.\nReboot the VM.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nOutput a list of disks:\n\n      openstack volume list\n\nCheck the disk status in the Status column. Successful expansion is guaranteed only for disks with the status available or in-use.\n\nCopy the disk ID.\n\nIncrease the disk by specifying the new size in gigabytes.\n\n   If the disk is disconnected from the VM (Status: available):\n\n               openstack volume set --size\n\n   If the disk is connected to the VM (Status: in-use):\n\n               cinder extend\n\nReboot the VM.\n\nIncreasing the disk size without rebooting\n\nIncrease the disk size in your VK Cloud account or using Openstack CLI, but do not reboot it.\nIncrease the disk size in the VM's OS:\n\n   Windows\n   Linux\n\n\n   Open Disk Management with administrator permissions (diskmgmt.msc).\n   Choose Extend Volume in right-click menu of the volume that you want to extend.\n   Increase the disk size.\n\n\n   Run the command:\n\n      growpart /dev/vda 1\n   sudo resize2fs /dev/vda1\n\n\nCloning disk\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to personal account VK Cloud.\nOpen the page with the required list of disks.\n\n   All disks: go to Cloud Computing → Disks.\n   Disks of a specific virtual machine:\n\n      Go to Cloud Computing → Virtual machines.\n      In the list of virtual machines, click on the name of the VM whose disk size you want to increase.\n      On the VM page, go to the Disks tab.\n\nUse one of the methods to open the disk cloning window.\n\n   Via the disk context menu:\n\n      Click   for the disk.\n      Select Clone disk.\n\n   On the disk page:\n\n      Click on the name of the disk whose size you want to change.\n      On the disk page, go to the tab General information.\n      Above the table with the disk parameters, click the More and choose Clone disk option.\n\nOn the page that opens, specify the parameters of the new disk.\nClick the Create disk button.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\nDefine:\n\n   the required type of disk;\n   its name in the API;\n   an accessibility zone suitable for accommodation.\n\nView the available disk types and copy the type ID corresponding to the name in the API.\n\n      openstack volume type list\n\nView the availability zones and copy the name of the required zone:\n\n      openstack availability zone list --volume\n\nClone a disk based on an existing one:\n\n      openstack volume create --type  --size  --availability-zone  --source\n\n\nChanging the disk type\n\nCreating HDD and SSD drives is available by default in all configurations. To use LL NVME discs contact technical support and request access to high-performance configurations and disks. Learn more about disk types — to the article Cloud Servers overview.\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nOpen the page with the required list of disks.\n\n   All disks: go to Cloud Computing → Disks.\n\n   Disks of a specific virtual machine:\n\n      Go to Cloud Computing → Virtual machines.\n      In the list of virtual machines, click on the name of the VM whose disk type you want to change.\n      On the VM page, go to the Disks tab.\n\nUse one of the methods to open the disk type change window.\n\n   Via the disk context menu:\n\n      Click   for the disk.\n      Select Change disk type.\n\n   On the disk page:\n\n      Click on the name of the disk whose type you want to change.\n      On the disk page, go to the General Information tab.\n      Above the table with the disk parameters, click Change disk type.\n\n\n   If the Change disk type option is not active, disconnect the disk from the VM.\n\n\nIn the window that opens, select Disk Type and click Confirm.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nSelect in the table a new disk type and define its name in the API.\n\nView the list of available disk types and copy the type ID corresponding to the name in the API.\n\n      openstack volume type list\n\nView the list of disks and copy the ID of the disk whose type you want to change:\n\n      openstack volume list --long\n\nChange the disk type:\n\n      openstack volume set --type  --retype-policy on-demand\n\n\nChanging the bootable attribute\n\nMake the disk bootable to use it as the VM's primary (root) disk. The boot disk can also be used as an additional VM disk to boot the operating system.\n\nTo exclude the possibility of accidental booting from the disk, make it non-bootable.\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nOpen the page with the required list of disks.\n\n   All disks: go to Cloud Computing → Disks.\n\n   Disks of a specific virtual machine:\n\n      Go to Cloud Computing → Virtual machines.\n      In the list of virtual machines, click on the VM name.\n      On the VM page, go to the Disks tab.\n\nChange the “bootable” attribute in one of the ways.\n\n   Via the disk context menu:\n\n      Click   for the disk.\n      Select Make bootable (Make it non-bootable).\n\n   On the disk page:\n\n      Click on the disk name.\n      On the disk page, go to the General Information tab.\n      Above the table with the disk parameters, click More and select Make bootable (Make it non-bootable).\n\nIn the window that opens, click Confirm.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nRun the appropriate command.\n\n   Make a disk bootable:\n\n            openstack volume set --bootable\n\n   Make the disk non-bootable:\n\n            openstack volume set --non-bootable\n\nCheck the result:\n\n      openstack volume show\n\n\nConnecting a disk to a VM\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Disks.\nFind a disk in the list that is not connected to the VM: the icon to the left of the disk name is blue, when you hover over it, the inscription appears Не подключен к инстансу.\nUse one of the methods to open the virtual machine selection window to attach the disk.\n\n   Via the disk context menu:\n\n      Click   for the disk.\n      Select Connect to instance.\n\n   On the disk page:\n\n      Click on the name of the disk that you want to connect to the VM.\n      On the disk page, go to the General Information tab.\n      Above the list of disks, click More and select Connect to instance.\n\nIn the window that opens, specify the virtual machine in the field Choose instance.\nClick the Connect disk.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nOutput a list of disks and copy the disk ID:\n\n      openstack volume list\n\nPrint the list of virtual machines and copy the ID of the virtual machine to which you want to connect the disk:\n\n      openstack server list\n\nConnect the disk:\n\n      openstack server add volume\n\nView the disk information to check the result (the attachments field):\n\n      openstack volume show\n\n\nDisconnecting a disk from a VM\n\n\n\nTo disable the VM's main (root) disk, use the option Replacing the root disk.\n\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nOpen the page with the required list of disks.\n\n   All disks: go to Cloud Computing → Disks.\n\n   Disks of a specific virtual machine:\n\n      Go to Cloud Computing → Virtual machines.\n      In the list of virtual machines, click on the name of the VM from which you want to disconnect the disk.\n      On the VM page, go to the Disks tab.\n\nDisconnect the disk from the VM in one of the ways.\n\n   Via the disk context menu:\n\n      Click   for the disk.\n      Select Disconnect from instance.\n\n   On the disk page:\n\n      Click on the name of the disk that you want to disconnect from the VM.\n      On the disk page, go to the General Information tab.\n      Above the list of disks, click More and select Disconnect from instance.\n\nIn the window that opens, check the name of the disk and click Confirm.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nOutput a list of disks and copy the disk ID:\n\n      openstack volume list\n\nOutput a list of virtual machines and copy the ID of the virtual machine from which you want to disconnect the disk:\n\n      openstack server list\n\nDisconnect the disk:\n\n      openstack server remove volume\n\nView the disk information to check the result (the attachments field):\n\n      openstack volume show\n\n\nReplacing the root disk\n\nBefore replacing the main disk stop the VM.\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nDisconnect from VM the disk that will be used to replace the main one.\nClone the target disk if necessary.\nOpen the page with the required list of disks.\n\n   All disks: go to Cloud Computing → Disks.\n\n   Disks of a specific virtual machine:\n\n      Go to Cloud Computing → Virtual machines.\n      In the list of virtual machines, click on the name of the VM whose main disk you want to replace.\n      On the VM page, go to the Disks tab.\n\nUse one of the methods to open the disk replacement window.\n\n   Via the disk context menu:\n\n      Click   for the disk.\n      Select Replace root disk.\n\n   On the disk page:\n\n      Click on the name of the disk you want to replace.\n      On the disk page, go to the General Information tab.\n      Above the list of disks, click More and select Replace root disk.\n\nIn the window that opens, select New root disk and click Replace.\n\n\n   If the required disk is not in the list, check that it is disconnected from the VM.\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nGet access token.\nView the list of virtual machines and copy the ID of the virtual machine whose main disk needs to be replaced:\n\n      openstack server list\n\nView the list of disks:\n\n      openstack volume list --long\n\nCheck the parameters of the disk that is selected to replace the main one:\n\n   The disk is disconnected from the VM (Status: available). If not, disconnect the disk.\n   The disk is bootable (Bootable: true). If not, make it bootable.\n\nCopy the ID of the selected disk.\nRun the command to replace the main disk:\n\n   curl -g -i -X POST https://infra.mail.ru:8774/v2.1/servers//action \\\n   -H \"Accept: application/json\" \\\n   -H \"Content-Type: application/json\" \\\n   -H \"User-Agent: python-cinderclient\" \\\n   -H \"X-Auth-Token: \" \\\n   -d '{\"replaceRoot\": {\"volume_id\": \"\"}}'\n\n\nTransfer disks between projects\n\n\n\n\nOpenStack CLI\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to it (to both projects).\n\nSign in to the project you want to move the disk from.\nDisconnect from VM the disk to be moved.\nView the list of disks:\n\n      openstack volume list --long\n\nMake sure that the disk is disconnected from the VM (Status: available).\nCopy the disk ID.\nCreate a request to move the disk:\n\n      openstack volume transfer request create\n\nCopy the auth_key and id values.\nReview the list of disks and make sure that the status of the disk being moved has changed to awaiting-transfer:\n\n      openstack volume list\n\nSign in to the project you want to move the disk to.\nMove the disk:\n\n            openstack volume transfer request accept --auth-key\n\nMake sure that the disk appears in the project:\n\n            openstack volume show\n\nAdditional commands for working with disk transfer requests\n\nView the list of transfer requests:\n\n      openstack volume transfer request list\n\nDelete a transfer request:\n\n      openstack volume transfer request delete\n\n\nDeleting a disk\n\nBefore deleting disconnect the disk from the VM.\n\n\n\nWhen you delete a disk, all its snapshots will be deleted.\n\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nOpen the page with the required list of disks.\n\n   All disks: go to Cloud Computing → Disks.\n\n   Disks of a specific virtual machine:\n\n      Go to Cloud Computing → Virtual machines.\n      In the list of virtual machines, click on the name of the VM whose disk you want to delete.\n      On the VM page, go to the Disks tab.\n\nDelete the disk in one of the ways.\n\n   Using group operations — for multiple disks:\n\n      Select the disks you want to delete with the checkboxes.\n      Above the list of disks, click the Delete disk button.\n\n   Via the disk context menu:\n\n      Click   for the disk.\n      Select Delete disk.\n\n   On the disk page:\n\n      Click on the name of the disk you want to delete.\n      On the disk page, go to the General Information tab.\n      To the right above the table with the disk parameters, click on the trash icon.\n\nIn the window that opens, check the disk name and click Confirm.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nOutput a list of disks using the openstack volume list command and check its status: if the disk is connected to the VM (Status: in-use), disconnect it.\n\nCopy the disk ID.\n\nDelete the disk.\n\n         openstack volume delete\n\n\nDisk snapshots\n\nA disk snapshot is a file that stores a copy of a disk taken at a certain point in time. The snapshot can be used to create a new VM or a new disk.\n\nCreating a snapshot\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nOpen the page with the required list of disks.\n\n   All disks: go to Cloud Computing → Disks.\n\n   Disks of a specific virtual machine:\n\n      Go to Cloud Computing → Virtual machines.\n      In the list of virtual machines, click on the name of the VM that you plan to create a disk snapshot for.\n      On the VM page, go to the Disks tab.\n\nUse one of the methods to open the disk snapshot creation window.\n\n   Via the disk context menu:\n\n      Click   for the disk.\n      Select Create snapshot.\n\n   On the disk page:\n\n      Click on the name of the disk for which you want to create a snapshot.\n      On the disk page, go to the Snapshots tab.\n      Above the list of snapshots, click Create snapshot.\n\nIn the window that opens, specify the parameters:\n\n   Name snapshot.\n   Source disk.\n\nClick Create snapshot.\n\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nOutput a list of disks:\n\n      openstack volume list\n\nCopy the ID of the required disk.\n\nCreate a disk snapshot.\n\n   If the disk is disconnected from the VM (Status: available):\n\n               openstack volume snapshot create --volume\n\n   If the disk is connected to the VM (Status: in-use):\n\n               openstack volume snapshot create --force --volume\n\n\nUsing snapshots\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\nGo to personal account VK Cloud.\nOpen the page with the required list of disks.\n\n   All disks: go to Cloud Computing → Disks.\n\n   Disks of a specific virtual machine:\n\n      Go to Cloud Computing → Virtual machines.\n      In the list of virtual machines, click on the VM name.\n      On the VM page, go to the Disks tab.\n\nOpen the page with the list of snapshots using one of the following methods:\n\n   Via the disk context menu:\n\n      Click   for the disk.\n      Select List of snapshots.\n\n   On the disk page:\n\n      Click on the name of the disk whose snapshots you want to view.\n      On the disk page, go to the Snapshots tab or on the General Information tab, click the More button and select List of snapshots.\n\nClick   for the snapshot and select the required option:\n\n   Create VM instance — to go to the virtual machine creation page.\n   Create disk — to go to the disk creation page.\n   Delete disk snapshot — to delete a snapshot.\n\n\n      To delete multiple snapshots, select the snapshots with the checkboxes and click the Delete snapshot button.\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nRun the appropriate command.\n\n   Display a list of disk snapshots:\n\n            openstack volume snapshot list --volume\n   Display a list of disk snapshots of the entire project:\n\n            openstack volume snapshot list --project\n\n   Change snapshot properties:\n\n            openstack volume snapshot set\n\n      Available properties:\n\n     --name — snapshot name;\n     --description — snapshot description;\n     --property — data in the key=value format;\n     --no-property — remove additional values.\n\n   Delete a disk snapshot:\n\n            openstack volume snapshot delete\n\n","url":"/en/computing/iaas/service-management/volumes","description":"Creating, deleting, and configuring disks. Connecting and disconnecting disks, moving between projects. Creating and using disk snapshots.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"11d9fa14-1277-472e-8e0c-82b5132be8c4","title":"Images management","data":"An image is a file that contains a virtual disk with an installed operating system or other data. Images are used to create virtual machines in the cloud.\n\nCreating an image\n\nThe VK Cloud service allows you to create an image from the disk of an existing virtual machine.\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud computing → Images.\nClick the Create button.\nIn the window that appears:\n\n   Source: choose Disk.\n   Select disk: select the disk of the existing VM. The VM must be stopped.\n   Name of the image: specify the name of the image.\n   Click the Create image button.\n\n\n\n\n\n\nThe disk to create the image must be disconnected from the VM and have the status available.\n\n\n\nGet the ID of the disk:\n\n      openstack volume list\n\nCreate an image:\n\n      openstack image create --volume\n\nCheck the image creation:\n\n      openstack image list --name\n\n   The successfully created image must have the status active.\n\nImporting an image\n\nThe VK Cloud service supports uploading your own virtual machine images with some restrictions by operating systems.\n\n\n\nOnly RAW images are supported. If your image is in a different format, convert it.\n\n\n\n\n\nThe recommended way to import an image is using the CLI.\n\nWhen downloading through your Personal account, the size of the downloaded file is limited — no more than 100 GB.\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud computing → Images.\nClick the Create button.\nIn the window that appears:\n\n   Source: select File.\n   Selected file: upload the image file in RAW format.\n   Name of the image: specify the name of the image.\n   Click the Create image button.\n\n\n\n\nThe parameters of the image import command depend on the need for backup support:\n\nIf support is not needed, run the command:\n\n      openstack image create --private --container-format bare --disk-format raw --property store=s3 --file\n\nIf support is needed, add parameters --property hw_qemu_guest_agent=yes --property os_require_quiesce=yes to the command above.\n\nExporting an image\n\n\n\nOpenStack CLI\ncURL\n\n\n\nGet the ID of the image from the list:\n\n      openstack image list\n\nExport the image:\n\n      openstack image save --file .raw\n\n\nGet the access token X-Auth-Token.\nRun the command:\n\n      curl -H \"X-Auth-Token:{token}\" \"https://infra.mail.ru:9292/v2/images/{image ID}/file\" --output .raw\n\n\nChanging the visibility status of an image\n\nChanging the visibility status of an image allows you to access an image from multiple projects. Sharing an image across multiple projects allows you to speed up the deployment of virtual machines.\n\nThe VK Cloud service provides the following image visibility statuses for users:\n\n| Status      | Description                                         |\n|-------------|-----------------------------------------------------|\n| private   | The image is for personal access only               |\n| shared    | The image can be used in several projects           |\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\n\nThrough your personal account, you can allow access to the image only for certain projects.\n\n\n\nTo change the visibility status of an image:\n\nGo to personal account VK Cloud.\nGo to Cloud computing → Images.\nClick   for the required image and select Share image.\nIn the window that appears:\n\n   Choose one of the options:\n\n      My projects: select the required projects from the drop-down list.\n      Other projects: specify the project ID.\n\n   Click the Allow access button.\n\n\n\n\nTo change the visibility status of an image:\n\nGet the ID of the image from the list:\n\n      openstack image list\n\nDisplay detailed information about the individual image:\n\n      openstack image show\n\n   The visibility status of the image is displayed in the visibility line.\n\nChange the image status:\n\n      openstack image set --\n\nTo share an image in the status shared:\n\nAdd an image to the project:\n\n      openstack image add project\n\nConfirm adding the image to the project:\n\n      openstack image set --accept\n\nTo view the projects that have access to the image, run the command:\n\nopenstack image member list\n\nDeleting an image\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud computing → Images.\nDo one of the following for the required image:\n\n   Select the image using the checkbox, then click Delete.\n   Click   for the required image and select Remove image.\n\nClick the Confirm button.\n\n\n\n\nTo delete an image that is not attached to projects:\n\nopenstack image delete\n\nTo delete an image from a project:\n\nopenstack image remove project\n","url":"/en/computing/iaas/service-management/images/images-manage","description":"Creating, importing and exporting images.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"87876f2a-95b5-476f-9324-35bc1006cca4","title":"Images meta tags","data":"The image may have meta tags that affect the configuration of the virtual machine and the guest operating system. Meta tags are set when editing the image.\n\nSupported image meta tags\n\nVM image tags are divided into:\n\nos_*: guest operating system (OS) settings;\nhw_*: virtual machine settings;\nmcs_*: VM properties used for sorting in VK Cloud personal account;\nother tags that affect the work with the image.\n\n| Name | Description | Default value |\n| --- | -------- | --- |\n| os_type | The type of guest OS created from the image inside the VM. Affects the VM configuration. For example, special hypervisor features will be enabled for Windows to improve the stability and performance of the guest OS | Depends on the OS |\n| mcs_os_type | The type of guest OS inside the VM. Used for sorting in the personal account | |\n| os_distro | The distribution of the guest OS created from the image inside the virtual machine. Affects VM configuration, for example, types of virtual network devices | |\n| mcs_os_distro | The distribution of the guest OS inside the VM. Used for sorting in the personal account | |\n| os_version | Version of the guest OS inside the VM | |\n| mcs_os_version | The version of the guest OS inside the VM. Used for sorting in the personal account | |\n| os_admin_user | The user is inside the guest OS with administrator rights; the password can be set via personal account | If os_type: windows then value is Administrator, root otherwise |\n| os_require_quiesce | The flag responsible for freezing the file system before taking snapshots of attached disks. If yes, then if the file system freezes unsuccessfully, the snapshot creation operation will be canceled | no |\n| hw_qemu_guest_agent | A sign of the presence of a QEMU agent inside the guest OS. If True, it makes available the possibility of changing the user's password and freezing the file system for backup | False |\n| mcs_name | The name of the image that is displayed to the user of the personal account | |\n\nFilling os_distro and os_version tags\n\nUse the value Short ID to define the meta tag os_distro and version for the meta tag os_version:\n\n\n   Table with values\n\n   | Short ID             | Name                                      | Version  | ID |\n   |----------------------|-------------------------------------------|----------|----------------------------------------|\n   | alpinelinux3.5       | Alpine Linux 3.5                          | 3.5      | http://alpinelinux.org/alpinelinux/3.5 |\n   | alpinelinux3.6       | Alpine Linux 3.6                          | 3.6      | http://alpinelinux.org/alpinelinux/3.6 |\n   | alpinelinux3.7       | Alpine Linux 3.7                          | 3.7      | http://alpinelinux.org/alpinelinux/3.7 |\n   | altlinux1.0          | Mandrake RE Spring 2001                   | 1.0      | http://altlinux.org/altlinux/1.0 |\n   | altlinux2.0          | ALT Linux 2.0                             | 2.0      | http://altlinux.org/altlinux/2.0 |\n   | altlinux2.2          | ALT Linux 2.2                             | 2.2      | http://altlinux.org/altlinux/2.2 |\n   | altlinux2.4          | ALT Linux 2.4                             | 2.4      | http://altlinux.org/altlinux/2.4 |\n   | altlinux3.0          | ALT Linux 3.0                             | 3.0      | http://altlinux.org/altlinux/3.0 |\n   | altlinux4.0          | ALT Linux 4.0                             | 4.0      | http://altlinux.org/altlinux/4.0 |\n   | altlinux4.1          | ALT Linux 4.1                             | 4.1      | http://altlinux.org/altlinux/4.1 |\n   | altlinux5.0          | ALT Linux 5.0                             | 5.0      | http://altlinux.org/altlinux/5.0 |\n   | altlinux6.0          | ALT Linux 6.0                             | 6.0      | http://altlinux.org/altlinux/6.0 |\n   | altlinux7.0          | ALT Linux 7.0                             | 7.0      | http://altlinux.org/altlinux/7.0 |\n   | asianux-unknown      | Asianux unknown                           | unknown  | http://asianux.com/asianux/unknown |\n   | asianux4.6           | Asianux Server 4 SP6                      |          | http://asianux.com/asianux/4.6 |\n   | asianux4.7           | Asianux Server 4 SP7                      |          | http://asianux.com/asianux/4.7 |\n   | asianux7.0           | Asianux Server 7                          | 7.0      | http://asianux.com/asianux/7.0 |\n   | asianux7.1           | Asianux Server 7 SP1                      | 7.1      | http://asianux.com/asianux/7.1 |\n   | asianux7.2           | Asianux Server 7 SP2                      | 7.2      | http://asianux.com/asianux/7.2 |\n   | centos6.0            | CentOS 6.0                                | 6.0      | http://centos.org/centos/6.0 |\n   | centos6.1            | CentOS 6.1                                | 6.1      | http://centos.org/centos/6.1 |\n   | centos6.2            | CentOS 6.2                                | 6.2      | http://centos.org/centos/6.2 |\n   | centos6.3            | CentOS 6.3                                | 6.3      | http://centos.org/centos/6.3 |\n   | centos6.4            | CentOS 6.4                                | 6.4      | http://centos.org/centos/6.4 |\n   | centos6.5            | CentOS 6.5                                | 6.5      | http://centos.org/centos/6.5 |\n   | centos6.6            | CentOS 6.6                                | 6.6      | http://centos.org/centos/6.6 |\n   | centos6.7            | CentOS 6.7                                | 6.7      | http://centos.org/centos/6.7 |\n   | centos6.8            | CentOS 6.8                                | 6.8      | http://centos.org/centos/6.8 |\n   | centos6.9            | CentOS 6.9                                | 6.9      | http://centos.org/centos/6.9 |\n   | centos7.0            | CentOS 7.0                                | 7.0      | http://centos.org/centos/7.0 |\n   | debian1.1            | Debian Buzz                               | 1.1      | http://debian.org/debian/1.1 |\n   | debian1.2            | Debian Rex                                | 1.2      | http://debian.org/debian/1.2 |\n   | debian1.3            | Debian Bo                                 | 1.3      | http://debian.org/debian/1.3 |\n   | debian2.0            | Debian Hamm                               | 2.0      | http://debian.org/debian/2.0 |\n   | debian2.1            | Debian Slink                              | 2.1      | http://debian.org/debian/2.1 |\n   | debian2.2            | Debian Potato                             | 2.2      | http://debian.org/debian/2.2 |\n   | debian3              | Debian Woody                              | 3        | http://debian.org/debian/3 |\n   | debian3.1            | Debian Sarge                              | 3.1      | http://debian.org/debian/3.1 |\n   | debian4              | Debian Etch                               | 4        | http://debian.org/debian/4 |\n   | debian5              | Debian Lenny                              | 5        | http://debian.org/debian/5 |\n   | debian6              | Debian Squeeze                            | 6        | http://debian.org/debian/6 |\n   | debian7              | Debian Wheezy                             | 7        | http://debian.org/debian/7 |\n   | debian8              | Debian Jessie                             | 8        | http://debian.org/debian/8 |\n   | debian9              | Debian Stretch                            | 9        | http://debian.org/debian/9 |\n   | debiantesting        | Debian Testing                            | testing  | http://debian.org/debian/testing |\n   | eos3.3               | Endless OS 3.3                            | 3.3      | http://endlessos.com/eos/3.3 |\n   | fedora-unknown       | Fedora                                    | unknown  | http://fedoraproject.org/fedora/unknown |\n   | fedora1              | Fedora Core 1                             | 1        | http://fedoraproject.org/fedora/1 |\n   | fedora10             | Fedora 10                                 | 10       | http://fedoraproject.org/fedora/10 |\n   | fedora11             | Fedora 11                                 | 11       | http://fedoraproject.org/fedora/11 |\n   | fedora12             | Fedora 12                                 | 12       | http://fedoraproject.org/fedora/12 |\n   | fedora13             | Fedora 13                                 | 13       | http://fedoraproject.org/fedora/13 |\n   | fedora14             | Fedora 14                                 | 14       | http://fedoraproject.org/fedora/14 |\n   | fedora15             | Fedora 15                                 | 15       | http://fedoraproject.org/fedora/15 |\n   | fedora16             | Fedora 16                                 | 16       | http://fedoraproject.org/fedora/16 |\n   | fedora17             | Fedora 17                                 | 17       | http://fedoraproject.org/fedora/17 |\n   | fedora18             | Fedora 18                                 | 18       | http://fedoraproject.org/fedora/18 |\n   | fedora19             | Fedora 19                                 | 19       | http://fedoraproject.org/fedora/19 |\n   | fedora2              | Fedora Core 2                             | 2        | http://fedoraproject.org/fedora/2 |\n   | fedora20             | Fedora 20                                 | 20       | http://fedoraproject.org/fedora/20 |\n   | fedora21             | Fedora 21                                 | 21       | http://fedoraproject.org/fedora/21 |\n   | fedora22             | Fedora 22                                 | 22       | http://fedoraproject.org/fedora/22 |\n   | fedora23             | Fedora 23                                 | 23       | http://fedoraproject.org/fedora/23 |\n   | fedora24             | Fedora 24                                 | 24       | http://fedoraproject.org/fedora/24 |\n   | fedora25             | Fedora 25                                 | 25       | http://fedoraproject.org/fedora/25 |\n   | fedora26             | Fedora 26                                 | 26       | http://fedoraproject.org/fedora/26 |\n   | fedora27             | Fedora 27                                 | 27       | http://fedoraproject.org/fedora/27 |\n   | fedora28             | Fedora 28                                 | 28       | http://fedoraproject.org/fedora/28 |\n   | fedora3              | Fedora Core 3                             | 3        | http://fedoraproject.org/fedora/3 |\n   | fedora4              | Fedora Core 4                             | 4        | http://fedoraproject.org/fedora/4 |\n   | fedora5              | Fedora Core 5                             | 5        | http://fedoraproject.org/fedora/5 |\n   | fedora6              | Fedora Core 6                             | 6        | http://fedoraproject.org/fedora/6 |\n   | fedora7              | Fedora 7                                  | 7        | http://fedoraproject.org/fedora/7 |\n   | fedora8              | Fedora 8                                  | 8        | http://fedoraproject.org/fedora/8 |\n   | fedora9              | Fedora 9                                  | 9        | http://fedoraproject.org/fedora/9 |\n   | freebsd1.0           | FreeBSD 1.0                               | 1.0      | http://freebsd.org/freebsd/1.0 |\n   | freebsd10.0          | FreeBSD 10.0                              | 10.0     | http://freebsd.org/freebsd/10.0 |\n   | freebsd10.1          | FreeBSD 10.1                              | 10.1     | http://freebsd.org/freebsd/10.1 |\n   | freebsd10.2          | FreeBSD 10.2                              | 10.2     | http://freebsd.org/freebsd/10.2 |\n   | freebsd10.3          | FreeBSD 10.3                              | 10.3     | http://freebsd.org/freebsd/10.3 |\n   | freebsd10.4          | FreeBSD 10.4                              | 10.4     | http://freebsd.org/freebsd/10.4 |\n   | freebsd11.0          | FreeBSD 11.0                              | 11.0     | http://freebsd.org/freebsd/11.0 |\n   | freebsd11.1          | FreeBSD 11.1                              | 11.1     | http://freebsd.org/freebsd/11.1 |\n   | freebsd2.0           | FreeBSD 2.0                               | 2.0      | http://freebsd.org/freebsd/2.0 |\n   | freebsd2.0.5         | FreeBSD 2.0.5                             | 2.0.5    | http://freebsd.org/freebsd/2.0.5 |\n   | freebsd2.2.8         | FreeBSD 2.2.8                             | 2.2.8    | http://freebsd.org/freebsd/2.2.8 |\n   | freebsd2.2.9         | FreeBSD 2.2.9                             | 2.2.9    | http://freebsd.org/freebsd/2.2.9 |\n   | freebsd3.0           | FreeBSD 3.0                               | 3.0      | http://freebsd.org/freebsd/3.0 |\n   | freebsd3.2           | FreeBSD 3.2                               | 3.2      | http://freebsd.org/freebsd/3.2 |\n   | freebsd4.0           | FreeBSD 4.0                               | 4.0      | http://freebsd.org/freebsd/4.0 |\n   | freebsd4.1           | FreeBSD 4.1                               | 4.1      | http://freebsd.org/freebsd/4.1 |\n   | freebsd4.10          | FreeBSD 4.10                              | 4.10     | http://freebsd.org/freebsd/4.10 |\n   | freebsd4.11          | FreeBSD 4.11                              | 4.11     | http://freebsd.org/freebsd/4.11 |\n   | freebsd4.2           | FreeBSD 4.2                               | 4.2      | http://freebsd.org/freebsd/4.2 |\n   | freebsd4.3           | FreeBSD 4.3                               | 4.3      | http://freebsd.org/freebsd/4.3 |\n   | freebsd4.4           | FreeBSD 4.4                               | 4.4      | http://freebsd.org/freebsd/4.4 |\n   | freebsd4.5           | FreeBSD 4.5                               | 4.5      | http://freebsd.org/freebsd/4.5 |\n   | freebsd4.6           | FreeBSD 4.6                               | 4.6      | http://freebsd.org/freebsd/4.6 |\n   | freebsd4.7           | FreeBSD 4.7                               | 4.7      | http://freebsd.org/freebsd/4.7 |\n   | freebsd4.8           | FreeBSD 4.8                               | 4.8      | http://freebsd.org/freebsd/4.8 |\n   | freebsd4.9           | FreeBSD 4.9                               | 4.9      | http://freebsd.org/freebsd/4.9 |\n   | freebsd5.0           | FreeBSD 5.0                               | 5.0      | http://freebsd.org/freebsd/5.0 |\n   | freebsd5.1           | FreeBSD 5.1                               | 5.1      | http://freebsd.org/freebsd/5.1 |\n   | freebsd5.2           | FreeBSD 5.2                               | 5.2      | http://freebsd.org/freebsd/5.2 |\n   | freebsd5.2.1         | FreeBSD 5.2.1                             | 5.2.1    | http://freebsd.org/freebsd/5.2.1 |\n   | freebsd5.3           | FreeBSD 5.3                               | 5.3      | http://freebsd.org/freebsd/5.3 |\n   | freebsd5.4           | FreeBSD 5.4                               | 5.4      | http://freebsd.org/freebsd/5.4 |\n   | freebsd5.5           | FreeBSD 5.5                               | 5.5      | http://freebsd.org/freebsd/5.5 |\n   | freebsd6.0           | FreeBSD 6.0                               | 6.0      | http://freebsd.org/freebsd/6.0 |\n   | freebsd6.1           | FreeBSD 6.1                               | 6.1      | http://freebsd.org/freebsd/6.1 |\n   | freebsd6.2           | FreeBSD 6.2                               | 6.2      | http://freebsd.org/freebsd/6.2 |\n   | freebsd6.3           | FreeBSD 6.3                               | 6.3      | http://freebsd.org/freebsd/6.3 |\n   | freebsd6.4           | FreeBSD 6.4                               | 6.4      | http://freebsd.org/freebsd/6.4 |\n   | freebsd7.0           | FreeBSD 7.0                               | 7.0      | http://freebsd.org/freebsd/7.0 |\n   | freebsd7.1           | FreeBSD 7.1                               | 7.1      | http://freebsd.org/freebsd/7.1 |\n   | freebsd7.2           | FreeBSD 7.2                               | 7.2      | http://freebsd.org/freebsd/7.2 |\n   | freebsd7.3           | FreeBSD 7.3                               | 7.3      | http://freebsd.org/freebsd/7.3 |\n   | freebsd7.4           | FreeBSD 7.4                               | 7.4      | http://freebsd.org/freebsd/7.4 |\n   | freebsd8.0           | FreeBSD 8.0                               | 8.0      | http://freebsd.org/freebsd/8.0 |\n   | freebsd8.1           | FreeBSD 8.1                               | 8.1      | http://freebsd.org/freebsd/8.1 |\n   | freebsd8.2           | FreeBSD 8.2                               | 8.2      | http://freebsd.org/freebsd/8.2 |\n   | freebsd8.3           | FreeBSD 8.3                               | 8.3      | http://freebsd.org/freebsd/8.3 |\n   | freebsd8.4           | FreeBSD 8.4                               | 8.4      | http://freebsd.org/freebsd/8.4 |\n   | freebsd9.0           | FreeBSD 9.0                               | 9.0      | http://freebsd.org/freebsd/9.0 |\n   | freebsd9.1           | FreeBSD 9.1                               | 9.1      | http://freebsd.org/freebsd/9.1 |\n   | freebsd9.2           | FreeBSD 9.2                               | 9.2      | http://freebsd.org/freebsd/9.2 |\n   | freebsd9.3           | FreeBSD 9.3                               | 9.3      | http://freebsd.org/freebsd/9.3 |\n   | freedos1.2           | FreeDOS 1.2                               | 1.2      | http://freedos.org/freedos/1.2 |\n   | gnome-continuous-3.10 | GNOME 3.10                                | 3.10     | http://gnome.org/gnome-continuous/3.10 |\n   | gnome-continuous-3.12 | GNOME 3.12                                | 3.12     | http://gnome.org/gnome-continuous/3.12 |\n   | gnome-continuous-3.14 | GNOME 3.14                                | 3.14     | http://gnome.org/gnome-continuous/3.14 |\n   | gnome3.6             | GNOME 3.6                                 | 3.6      | http://gnome.org/gnome/3.6 |\n   | gnome3.8             | GNOME 3.8                                 | 3.8      | http://gnome.org/gnome/3.8 |\n   | mbs1.0               | Mandriva Business Server 1.0              | 1.0      | http://mandriva.com/mbs/1.0 |\n   | mes5                 | Mandriva Enterprise Server 5.0            | 5.0      | http://mandriva.com/mes/5.0 |\n   | mes5.1               | Mandriva Enterprise Server 5.1            | 5.1      | http://mandriva.com/mes/5.1 |\n   | msdos6.22            | Microsoft MS-DOS 6.22                     | 6.22     | http://microsoft.com/msdos/6.22 |\n   | netbsd0.8            | NetBSD 0.8                                | 0.8      | http://netbsd.org/netbsd/0.8 |\n   | netbsd0.9            | NetBSD 0.9                                | 0.9      | http://netbsd.org/netbsd/0.9 |\n   | netbsd1.0            | NetBSD 1.0                                | 1.0      | http://netbsd.org/netbsd/1.0 |\n   | netbsd1.1            | NetBSD 1.1                                | 1.1      | http://netbsd.org/netbsd/1.1 |\n   | netbsd1.2            | NetBSD 1.2                                | 1.2      | http://netbsd.org/netbsd/1.2 |\n   | netbsd1.3            | NetBSD 1.3                                | 1.3      | http://netbsd.org/netbsd/1.3 |\n   | netbsd1.4            | NetBSD 1.4                                | 1.4      | http://netbsd.org/netbsd/1.4 |\n   | netbsd1.5            | NetBSD 1.5                                | 1.5      | http://netbsd.org/netbsd/1.5 |\n   | netbsd1.6            | NetBSD 1.6                                | 1.6      | http://netbsd.org/netbsd/1.6 |\n   | netbsd2.0            | NetBSD 2.0                                | 2.0      | http://netbsd.org/netbsd/2.0 |\n   | netbsd3.0            | NetBSD 3.0                                | 3.0      | http://netbsd.org/netbsd/3.0 |\n   | netbsd4.0            | NetBSD 4.0                                | 4.0      | http://netbsd.org/netbsd/4.0 |\n   | netbsd5.0            | NetBSD 5.0                                | 5.0      | http://netbsd.org/netbsd/5.0 |\n   | netbsd5.1            | NetBSD 5.1                                | 5.1      | http://netbsd.org/netbsd/5.1 |\n   | netbsd6.0            | NetBSD 6.0                                | 6.0      | http://netbsd.org/netbsd/6.0 |\n   | netbsd6.1            | NetBSD 6.1                                | 6.1      | http://netbsd.org/netbsd/6.1 |\n   | netbsd7.0            | NetBSD 7.0                                | 7.0      | http://netbsd.org/netbsd/7.0 |\n   | netbsd7.1            | NetBSD 7.1                                | 7.1      | http://netbsd.org/netbsd/7.1 |\n   | netbsd7.1.1          | NetBSD 7.1.1                              | 7.1.1    | http://netbsd.org/netbsd/7.1.1 |\n   | netbsd7.1.2          | NetBSD 7.1.2                              | 7.1.2    | http://netbsd.org/netbsd/7.1.2 |\n   | netware4             | Novell Netware 4                          | 4        | http://novell.com/netware/4 |\n   | netware5             | Novell Netware 5                          | 5        | http://novell.com/netware/5 |\n   | netware6             | Novell Netware 6                          | 6        | http://novell.com/netware/6 |\n   | openbsd4.2           | OpenBSD 4.2                               | 4.2      | http://openbsd.org/openbsd/4.2 |\n   | openbsd4.3           | OpenBSD 4.3                               | 4.3      | http://openbsd.org/openbsd/4.3 |\n   | openbsd4.4           | OpenBSD 4.4                               | 4.4      | http://openbsd.org/openbsd/4.4 |\n   | openbsd4.5           | OpenBSD 4.5                               | 4.5      | http://openbsd.org/openbsd/4.5 |\n   | openbsd4.8           | OpenBSD 4.8                               | 4.8      | http://openbsd.org/openbsd/4.8 |\n   | openbsd4.9           | OpenBSD 4.9                               | 4.9      | http://openbsd.org/openbsd/4.9 |\n   | openbsd5.0           | OpenBSD 5.0                               | 5.0      | http://openbsd.org/openbsd/5.0 |\n   | openbsd5.1           | OpenBSD 5.1                               | 5.1      | http://openbsd.org/openbsd/5.1 |\n   | openbsd5.2           | OpenBSD 5.2                               | 5.2      | http://openbsd.org/openbsd/5.2 |\n   | openbsd5.3           | OpenBSD 5.3                               | 5.3      | http://openbsd.org/openbsd/5.3 |\n   | openbsd5.4           | OpenBSD 5.4                               | 5.4      | http://openbsd.org/openbsd/5.4 |\n   | openbsd5.5           | OpenBSD 5.5                               | 5.5      | http://openbsd.org/openbsd/5.5 |\n   | openbsd5.6           | OpenBSD 5.6                               | 5.6      | http://openbsd.org/openbsd/5.6 |\n   | openbsd5.7           | OpenBSD 5.7                               | 5.7      | http://openbsd.org/openbsd/5.7 |\n   | openbsd5.8           | OpenBSD 5.8                               | 5.8      | http://openbsd.org/openbsd/5.8 |\n   | openbsd5.9           | OpenBSD 5.9                               | 5.9      | http://openbsd.org/openbsd/5.9 |\n   | openbsd6.0           | OpenBSD 6.0                               | 6.0      | http://openbsd.org/openbsd/6.0 |\n   | openbsd6.1           | OpenBSD 6.1                               | 6.1      | http://openbsd.org/openbsd/6.1 |\n   | openbsd6.2           | OpenBSD 6.2                               | 6.2      | http://openbsd.org/openbsd/6.2 |\n   | openbsd6.3           | OpenBSD 6.3                               | 6.3      | http://openbsd.org/openbsd/6.3 |\n   | opensolaris2009.06   | OpenSolaris 2009.06                       | 2009.06  | http://sun.com/opensolaris/2009.06 |\n   | opensuse-factory     | openSUSE                                  | factory  | http://opensuse.org/opensuse/factory |\n   | opensuse-unknown     | openSUSE                                  | unknown  | http://opensuse.org/opensuse/unknown |\n   | opensuse10.2         | openSUSE 10.2                             | 10.2     | http://opensuse.org/opensuse/10.2 |\n   | opensuse10.3         | openSUSE 10.3                             | 10.3     | http://opensuse.org/opensuse/10.3 |\n   | opensuse11.0         | openSUSE 11.0                             | 11.0     | http://opensuse.org/opensuse/11.0 |\n   | opensuse11.1         | openSUSE 11.1                             | 11.1     | http://opensuse.org/opensuse/11.1 |\n   | opensuse11.2         | openSUSE 11.2                             | 11.2     | http://opensuse.org/opensuse/11.2 |\n   | opensuse11.3         | openSUSE 11.3                             | 11.3     | http://opensuse.org/opensuse/11.3 |\n   | opensuse11.4         | openSUSE 11.4                             | 11.4     | http://opensuse.org/opensuse/11.4 |\n   | opensuse12.1         | openSUSE 12.1                             | 12.1     | http://opensuse.org/opensuse/12.1 |\n   | opensuse12.2         | openSUSE 12.2                             | 12.2     | http://opensuse.org/opensuse/12.2 |\n   | opensuse12.3         | openSUSE 12.3                             | 12.3     | http://opensuse.org/opensuse/12.3 |\n   | opensuse13.1         | openSUSE 13.1                             | 13.1     | http://opensuse.org/opensuse/13.1 |\n   | opensuse13.2         | openSUSE 13.2                             | 13.2     | http://opensuse.org/opensuse/13.2 |\n   | opensuse15.0         | openSUSE Leap 15.0                        | 15.0     | http://opensuse.org/opensuse/15.0 |\n   | opensuse42.1         | openSUSE Leap 42.1                        | 42.1     | http://opensuse.org/opensuse/42.1 |\n   | opensuse42.2         | openSUSE Leap 42.2                        | 42.2     | http://opensuse.org/opensuse/42.2 |\n   | opensuse42.3         | openSUSE Leap 42.3                        | 42.3     | http://opensuse.org/opensuse/42.3 |\n   | opensusetumbleweed   | openSUSE Tumbleweed                       | tumbleweed | http://opensuse.org/opensuse/tumbleweed |\n   | rhel-atomic-7.0      | Red Hat Enterprise Linux Atomic Host 7.0  | 7.0      | http://redhat.com/rhel-atomic/7.0 |\n   | rhel-atomic-7.1      | Red Hat Enterprise Linux Atomic Host 7.1  | 7.1      | http://redhat.com/rhel-atomic/7.1 |\n   | rhel-atomic-7.2      | Red Hat Enterprise Linux Atomic Host 7.2  | 7.2      | http://redhat.com/rhel-atomic/7.2 |\n   | rhel2.1              | Red Hat Enterprise Linux 2.1              | 2.1      | http://redhat.com/rhel/2.1 |\n   | rhel2.1.1            | Red Hat Enterprise Linux 2.1 Update 1     | 2.1.1    | http://redhat.com/rhel/2.1.1 |\n   | rhel2.1.2            | Red Hat Enterprise Linux 2.1 Update 2     | 2.1.2    | http://redhat.com/rhel/2.1.2 |\n   | rhel2.1.3            | Red Hat Enterprise Linux 2.1 Update 3     | 2.1.3    | http://redhat.com/rhel/2.1.3 |\n   | rhel2.1.4            | Red Hat Enterprise Linux 2.1 Update 4     | 2.1.4    | http://redhat.com/rhel/2.1.4 |\n   | rhel2.1.5            | Red Hat Enterprise Linux 2.1 Update 5     | 2.1.5    | http://redhat.com/rhel/2.1.5 |\n   | rhel2.1.6            | Red Hat Enterprise Linux 2.1 Update 6     | 2.1.6    | http://redhat.com/rhel/2.1.6 |\n   | rhel2.1.7            | Red Hat Enterprise Linux 2.1 Update 7     | 2.1.7    | http://redhat.com/rhel/2.1.7 |\n   | rhel3                | Red Hat Enterprise Linux 3                | 3        | http://redhat.com/rhel/3 |\n   | rhel3.1              | Red Hat Enterprise Linux 3 Update 1       | 3.1      | http://redhat.com/rhel/3.1 |\n   | rhel3.2              | Red Hat Enterprise Linux 3 Update 2       | 3.2      | http://redhat.com/rhel/3.2 |\n   | rhel3.3              | Red Hat Enterprise Linux 3 Update 3       | 3.3      | http://redhat.com/rhel/3.3 |\n   | rhel3.4              | Red Hat Enterprise Linux 3 Update 4       | 3.4      | http://redhat.com/rhel/3.4 |\n   | rhel3.5              | Red Hat Enterprise Linux 3 Update 5       | 3.5      | http://redhat.com/rhel/3.5 |\n   | rhel3.6              | Red Hat Enterprise Linux 3 Update 6       | 3.6      | http://redhat.com/rhel/3.6 |\n   | rhel3.7              | Red Hat Enterprise Linux 3 Update 7       | 3.7      | http://redhat.com/rhel/3.7 |\n   | rhel3.8              | Red Hat Enterprise Linux 3 Update 8       | 3.8      | http://redhat.com/rhel/3.8 |\n   | rhel3.9              | Red Hat Enterprise Linux 3 Update 9       | 3.9      | http://redhat.com/rhel/3.9 |\n   | rhel4.0              | Red Hat Enterprise Linux 4.0              | 4.0      | http://redhat.com/rhel/4.0 |\n   | rhel4.1              | Red Hat Enterprise Linux 4.1              | 4.1      | http://redhat.com/rhel/4.1 |\n   | rhel4.2              | Red Hat Enterprise Linux 4.2              | 4.2      | http://redhat.com/rhel/4.2 |\n   | rhel4.3              | Red Hat Enterprise Linux 4.3              | 4.3      | http://redhat.com/rhel/4.3 |\n   | rhel4.4              | Red Hat Enterprise Linux 4.4              | 4.4      | http://redhat.com/rhel/4.4 |\n   | rhel4.5              | Red Hat Enterprise Linux 4.5              | 4.5      | http://redhat.com/rhel/4.5 |\n   | rhel4.6              | Red Hat Enterprise Linux 4.6              | 4.6      | http://redhat.com/rhel/4.6 |\n   | rhel4.7              | Red Hat Enterprise Linux 4.7              | 4.7      | http://redhat.com/rhel/4.7 |\n   | rhel4.8              | Red Hat Enterprise Linux 4.8              | 4.8      | http://redhat.com/rhel/4.8 |\n   | rhel4.9              | Red Hat Enterprise Linux 4.9              | 4.9      | http://redhat.com/rhel/4.9 |\n   | rhel5.0              | Red Hat Enterprise Linux 5.0              | 5.0      | http://redhat.com/rhel/5.0 |\n   | rhel5.1              | Red Hat Enterprise Linux 5.1              | 5.1      | http://redhat.com/rhel/5.1 |\n   | rhel5.10             | Red Hat Enterprise Linux 5.10             | 5.10     | http://redhat.com/rhel/5.10 |\n   | rhel5.11             | Red Hat Enterprise Linux 5.11             | 5.11     | http://redhat.com/rhel/5.11 |\n   | rhel5.2              | Red Hat Enterprise Linux 5.2              | 5.2      | http://redhat.com/rhel/5.2 |\n   | rhel5.3              | Red Hat Enterprise Linux 5.3              | 5.3      | http://redhat.com/rhel/5.3 |\n   | rhel5.4              | Red Hat Enterprise Linux 5.4              | 5.4      | http://redhat.com/rhel/5.4 |\n   | rhel5.5              | Red Hat Enterprise Linux 5.5              | 5.5      | http://redhat.com/rhel/5.5 |\n   | rhel5.6              | Red Hat Enterprise Linux 5.6              | 5.6      | http://redhat.com/rhel/5.6 |\n   | rhel5.7              | Red Hat Enterprise Linux 5.7              | 5.7      | http://redhat.com/rhel/5.7 |\n   | rhel5.8              | Red Hat Enterprise Linux 5.8              | 5.8      | http://redhat.com/rhel/5.8 |\n   | rhel5.9              | Red Hat Enterprise Linux 5.9              | 5.9      | http://redhat.com/rhel/5.9 |\n   | rhel6.0              | Red Hat Enterprise Linux 6.0              | 6.0      | http://redhat.com/rhel/6.0 |\n   | rhel6.1              | Red Hat Enterprise Linux 6.1              | 6.1      | http://redhat.com/rhel/6.1 |\n   | rhel6.2              | Red Hat Enterprise Linux 6.2              | 6.2      | http://redhat.com/rhel/6.2 |\n   | rhel6.3              | Red Hat Enterprise Linux 6.3              | 6.3      | http://redhat.com/rhel/6.3 |\n   | rhel6.4              | Red Hat Enterprise Linux 6.4              | 6.4      | http://redhat.com/rhel/6.4 |\n   | rhel6.5              | Red Hat Enterprise Linux 6.5              | 6.5      | http://redhat.com/rhel/6.5 |\n   | rhel6.6              | Red Hat Enterprise Linux 6.6              | 6.6      | http://redhat.com/rhel/6.6 |\n   | rhel6.7              | Red Hat Enterprise Linux 6.7              | 6.7      | http://redhat.com/rhel/6.7 |\n   | rhel6.8              | Red Hat Enterprise Linux 6.8              | 6.8      | http://redhat.com/rhel/6.8 |\n   | rhel6.9              | Red Hat Enterprise Linux 6.9              | 6.9      | http://redhat.com/rhel/6.9 |\n   | rhel7.0              | Red Hat Enterprise Linux 7.0              | 7.0      | http://redhat.com/rhel/7.0 |\n   | rhel7.1              | Red Hat Enterprise Linux 7.1              | 7.1      | http://redhat.com/rhel/7.1 |\n   | rhel7.2              | Red Hat Enterprise Linux 7.2              | 7.2      | http://redhat.com/rhel/7.2 |\n   | rhel7.3              | Red Hat Enterprise Linux 7.3              | 7.3      | http://redhat.com/rhel/7.3 |\n   | rhel7.4              | Red Hat Enterprise Linux 7.4              | 7.4      | http://redhat.com/rhel/7.4 |\n   | rhel7.5              | Red Hat Enterprise Linux 7.5              | 7.5      | http://redhat.com/rhel/7.5 |\n   | rhel7.6              | Red Hat Enterprise Linux 7.6              | 7.6      | http://redhat.com/rhel/7.6 |\n   | rhl1.0               | Red Hat Linux 1.0                         | 1.0      | http://redhat.com/rhl/1.0 |\n   | rhl1.1               | Red Hat Linux 1.1                         | 1.1      | http://redhat.com/rhl/1.1 |\n   | rhl2.0               | Red Hat Linux 2.0                         | 2.0      | http://redhat.com/rhl/2.0 |\n   | rhl2.1               | Red Hat Linux 2.1                         | 2.1      | http://redhat.com/rhl/2.1 |\n   | rhl3.0.3             | Red Hat Linux 3.0.3                       | 3.0.3    | http://redhat.com/rhl/3.0.3 |\n   | rhl4.0               | Red Hat Linux 4.0                         | 4.0      | http://redhat.com/rhl/4.0 |\n   | rhl4.1               | Red Hat Linux 4.1                         | 4.1      | http://redhat.com/rhl/4.1 |\n   | rhl4.2               | Red Hat Linux 4.2                         | 4.2      | http://redhat.com/rhl/4.2 |\n   | rhl5.0               | Red Hat Linux 5.0                         | 5.0      | http://redhat.com/rhl/5.0 |\n   | rhl5.1               | Red Hat Linux 5.1                         | 5.1      | http://redhat.com/rhl/5.1 |\n   | rhl5.2               | Red Hat Linux 5.2                         | 5.2      | http://redhat.com/rhl/5.2 |\n   | rhl6.0               | Red Hat Linux 6.0                         | 6.0      | http://redhat.com/rhl/6.0 |\n   | rhl6.1               | Red Hat Linux 6.1                         | 6.1      | http://redhat.com/rhl/6.1 |\n   | rhl6.2               | Red Hat Linux 6.2                         | 6.2      | http://redhat.com/rhl/6.2 |\n   | rhl7                 | Red Hat Linux 7                           | 7        | http://redhat.com/rhl/7 |\n   | rhl7.1               | Red Hat Linux 7.1                         | 7.1      | http://redhat.com/rhl/7.1 |\n   | rhl7.2               | Red Hat Linux 7.2                         | 7.2      | http://redhat.com/rhl/7.2 |\n   | rhl7.3               | Red Hat Linux 7.3                         | 7.3      | http://redhat.com/rhl/7.3 |\n   | rhl8.0               | Red Hat Linux 8.0                         | 8.0      | http://redhat.com/rhl/8.0 |\n   | rhl9                 | Red Hat Linux 9                           | 9        | http://redhat.com/rhl/9 |\n   | sled10               | SUSE Linux Enterprise Desktop 10          | 10       | http://suse.com/sled/10 |\n   | sled10sp1            | SUSE Linux Enterprise Desktop 10 SP1      | 10.1     | http://suse.com/sled/10.1 |\n   | sled10sp2            | SUSE Linux Enterprise Desktop 10 SP2      | 10.2     | http://suse.com/sled/10.2 |\n   | sled10sp3            | SUSE Linux Enterprise Desktop 10 SP3      | 10.3     | http://suse.com/sled/10.3 |\n   | sled10sp4            | SUSE Linux Enterprise Desktop 10 SP4      | 10.4     | http://suse.com/sled/10.4 |\n   | sled11               | SUSE Linux Enterprise Desktop 11          | 11       | http://suse.com/sled/11 |\n   | sled11sp1            | SUSE Linux Enterprise Desktop 11 SP1      | 11.1     | http://suse.com/sled/11.1 |\n   | sled11sp2            | SUSE Linux Enterprise Desktop 11 SP2      | 11.2     | http://suse.com/sled/11.2 |\n   | sled11sp3            | SUSE Linux Enterprise Desktop 11 SP3      | 11.3     | http://suse.com/sled/11.3 |\n   | sled11sp4            | SUSE Linux Enterprise Desktop 11 SP4      | 11.4     | http://suse.com/sled/11.4 |\n   | sled12               | SUSE Linux Enterprise Desktop 12          | 12       | http://suse.com/sled/12 |\n   | sled12sp1            | SUSE Linux Enterprise Desktop 12 SP1      | 12.1     | http://suse.com/sled/12.1 |\n   | sled12sp2            | SUSE Linux Enterprise Desktop 12 SP2      | 12.2     | http://suse.com/sled/12.2 |\n   | sled12sp3            | SUSE Linux Enterprise Desktop 12 SP3      | 12.3     | http://suse.com/sled/12.3 |\n   | sled9                | SUSE Linux Enterprise Desktop 9           | 9        | http://suse.com/sled/9 |\n   | sles10               | SUSE Linux Enterprise Server 10           | 10       | http://suse.com/sles/10 |\n   | sles10sp1            | SUSE Linux Enterprise Server 10 SP1       | 10.1     | http://suse.com/sles/10.1 |\n   | sles10sp2            | SUSE Linux Enterprise Server 10 SP2       | 10.2     | http://suse.com/sles/10.2 |\n   | sles10sp3            | SUSE Linux Enterprise Server 10 SP3       | 10.3     | http://suse.com/sles/10.3 |\n   | sles10sp4            | SUSE Linux Enterprise Server 10 SP4       | 10.4     | http://suse.com/sles/10.4 |\n   | sles11               | SUSE Linux Enterprise Server 11           | 11       | http://suse.com/sles/11 |\n   | sles11sp1            | SUSE Linux Enterprise Server 11 SP1       | 11.1     | http://suse.com/sles/11.1 |\n   | sles11sp2            | SUSE Linux Enterprise Server 11 SP2       | 11.2     | http://suse.com/sles/11.2 |\n   | sles11sp3            | SUSE Linux Enterprise Server 11 SP3       | 11.3     | http://suse.com/sles/11.3 |\n   | sles11sp4            | SUSE Linux Enterprise Server 11 SP4       | 11.4     | http://suse.com/sles/11.4 |\n   | sles12               | SUSE Linux Enterprise Server 12           | 12       | http://suse.com/sles/12 |\n   | sles12sp1            | SUSE Linux Enterprise Server 12 SP1       | 12.1     | http://suse.com/sles/12.1 |\n   | sles12sp2            | SUSE Linux Enterprise Server 12 SP2       | 12.2     | http://suse.com/sles/12.2 |\n   | sles12sp3            | SUSE Linux Enterprise Server 12 SP3       | 12.3     | http://suse.com/sles/12.3 |\n   | sles9                | SUSE Linux Enterprise Server 9            | 9        | http://suse.com/sles/9 |\n   | solaris10            | Solaris 10                                | 10       | http://sun.com/solaris/10 |\n   | solaris11            | Oracle Solaris 11                         | 11       | http://oracle.com/solaris/11 |\n   | solaris9             | Solaris 9                                 | 9        | http://sun.com/solaris/9 |\n   | ubuntu10.04          | Ubuntu 10.04 LTS                          | 10.04    | http://ubuntu.com/ubuntu/10.04 |\n   | ubuntu10.10          | Ubuntu 10.10                              | 10.10    | http://ubuntu.com/ubuntu/10.10 |\n   | ubuntu11.04          | Ubuntu 11.04                              | 11.04    | http://ubuntu.com/ubuntu/11.04 |\n   | ubuntu11.10          | Ubuntu 11.10                              | 11.10    | http://ubuntu.com/ubuntu/11.10 |\n   | ubuntu12.04          | Ubuntu 12.04 LTS                          | 12.04    | http://ubuntu.com/ubuntu/12.04 |\n   | ubuntu12.10          | Ubuntu 12.10                              | 12.10    | http://ubuntu.com/ubuntu/12.10 |\n   | ubuntu13.04          | Ubuntu 13.04                              | 13.04    | http://ubuntu.com/ubuntu/13.04 |\n   | ubuntu13.10          | Ubuntu 13.10                              | 13.10    | http://ubuntu.com/ubuntu/13.10 |\n   | ubuntu14.04          | Ubuntu 14.04 LTS                          | 14.04    | http://ubuntu.com/ubuntu/14.04 |\n   | ubuntu14.10          | Ubuntu 14.10                              | 14.10    | http://ubuntu.com/ubuntu/14.10 |\n   | ubuntu15.04          | Ubuntu 15.04                              | 15.04    | http://ubuntu.com/ubuntu/15.04 |\n   | ubuntu15.10          | Ubuntu 15.10                              | 15.10    | http://ubuntu.com/ubuntu/15.10 |\n   | ubuntu16.04          | Ubuntu 16.04                              | 16.04    | http://ubuntu.com/ubuntu/16.04 |\n   | ubuntu16.10          | Ubuntu 16.10                              | 16.10    | http://ubuntu.com/ubuntu/16.10 |\n   | ubuntu17.04          | Ubuntu 17.04                              | 17.04    | http://ubuntu.com/ubuntu/17.04 |\n   | ubuntu17.10          | Ubuntu 17.10                              | 17.10    | http://ubuntu.com/ubuntu/17.10 |\n   | ubuntu4.10           | Ubuntu 4.10                               | 4.10     | http://ubuntu.com/ubuntu/4.10 |\n   | ubuntu5.04           | Ubuntu 5.04                               | 5.04     | http://ubuntu.com/ubuntu/5.04 |\n   | ubuntu5.10           | Ubuntu 5.10                               | 5.10     | http://ubuntu.com/ubuntu/5.10 |\n   | ubuntu6.06           | Ubuntu 6.06 LTS                           | 6.06     | http://ubuntu.com/ubuntu/6.06 |\n   | ubuntu6.10           | Ubuntu 6.10                               | 6.10     | http://ubuntu.com/ubuntu/6.10 |\n   | ubuntu7.04           | Ubuntu 7.04                               | 7.04     | http://ubuntu.com/ubuntu/7.04 |\n   | ubuntu7.10           | Ubuntu 7.10                               | 7.10     | http://ubuntu.com/ubuntu/7.10 |\n   | ubuntu8.04           | Ubuntu 8.04 LTS                           | 8.04     | http://ubuntu.com/ubuntu/8.04 |\n   | ubuntu8.10           | Ubuntu 8.10                               | 8.10     | http://ubuntu.com/ubuntu/8.10 |\n   | ubuntu9.04           | Ubuntu 9.04                               | 9.04     | http://ubuntu.com/ubuntu/9.04 |\n   | ubuntu9.10           | Ubuntu 9.10                               | 9.10     | http://ubuntu.com/ubuntu/9.10 |\n   | win1.0               | Microsoft Windows 1.0                     | 1.0      | http://microsoft.com/win/1.0 |\n   | win10                | Microsoft Windows 10                      | 10.0     | http://microsoft.com/win/10 |\n   | win2.0               | Microsoft Windows 2.0                     | 2.0      | http://microsoft.com/win/2.0 |\n   | win2.1               | Microsoft Windows 2.1                     | 2.1      | http://microsoft.com/win/2.1 |\n   | win2k                | Microsoft Windows 2000                    | 5.0      | http://microsoft.com/win/2k |\n   | win2k12              | Microsoft Windows Server 2012             | 6.3      | http://microsoft.com/win/2k12 |\n   | win2k12r2            | Microsoft Windows Server 2012 R2          | 6.3      | http://microsoft.com/win/2k12r2 |\n   | win2k16              | Microsoft Windows Server 2016             | 10.0     | http://microsoft.com/win/2k16 |\n   | win2k3               | Microsoft Windows Server 2003             | 5.2      | http://microsoft.com/win/2k3 |\n   | win2k3r2             | Microsoft Windows Server 2003 R2          | 5.2      | http://microsoft.com/win/2k3r2 |\n   | win2k8               | Microsoft Windows Server 2008             | 6.0      | http://microsoft.com/win/2k8 |\n   | win2k8r2             | Microsoft Windows Server 2008 R2          | 6.1      | http://microsoft.com/win/2k8r2 |\n   | win3.1               | Microsoft Windows 3.1                     | 3.1      | http://microsoft.com/win/3.1 |\n   | win7                 | Microsoft Windows 7                       | 6.1      | http://microsoft.com/win/7 |\n   | win8                 | Microsoft Windows 8                       | 6.2      | http://microsoft.com/win/8 |\n   | win8.1               | Microsoft Windows 8.1                     | 6.3      | http://microsoft.com/win/8.1 |\n   | win95                | Microsoft Windows 95                      | 4.0      | http://microsoft.com/win/95 |\n   | win98                | Microsoft Windows 98                      | 4.1      | http://microsoft.com/win/98 |\n   | winme                | Microsoft Windows Millennium Edition      | 4.9      | http://microsoft.com/win/me |\n   | winnt3.1             | Microsoft Windows NT Server 3.1           | 3.1      | http://microsoft.com/winnt/3.1 |\n   | winnt3.5             | Microsoft Windows NT Server 3.5           | 3.5      | http://microsoft.com/winnt/3.5 |\n   | winnt3.51            | Microsoft Windows NT Server 3.51          | 3.51     | http://microsoft.com/winnt/3.51 |\n   | winnt4.0             | Microsoft Windows NT Server 4.0           | 4.0      | http://microsoft.com/winnt/4.0 |\n   | winvista             | Microsoft Windows Vista                   | 6.0      | http://microsoft.com/win/vista |\n   | winxp                | Microsoft Windows XP                      | 5.1      | http://microsoft.com/win/xp |\n\n\n\n\n\nIf the required OS is not in the table, use the library libosinfo.\n\nInstalling a meta tag\n\n\n\nOpenStack CLI\n\n\n\nGet the image ID:\n\n      openstack image show\n\n   The assigned meta tags are displayed in the line properties.\n\nSet a meta tag:\n\n      openstack image set --property\n\n\n  Example of installing meta tags for a Windows VM image with drivers virtio\n\n  Determine the Windows OS version:\n\n          systeminfo\n\n  Select the required Short ID value for the os_distro meta tag from the table above.\n  Create the image.\n  Set meta tags:\n\n          openstack image set --property\n\n     Example:\n\n          openstack image set --property os_type=\"windows\" --property os_distro=\"win2k16\" --property os_require_quiesce=\"yes\" --property hw_vif_model=\"virtio\" 7c81ffd7-199d-4428-8767-8120fa1b3aae\n\n  Check the image information:\n\n          openstack image show\n\n     Example of the result:\n\n          | properties | hw_vif_model=virtio, os_distro=win2k16, os_require_quiesce=yes, os_type=windows |\n\n\nDeleting a meta tag\n\n\n\nOpenStack CLI\n\n\n\nGet the image ID:\n\n      openstack image show\n\n   The assigned meta tags are displayed in the line properties.\n\nDelete a meta tag:\n\n      openstack image unset --property\n\n","url":"/en/computing/iaas/service-management/images/image-metadata","description":"Clarifying infrastructure settings","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"a8a4975a-156f-4c36-8275-6ffe95546c1e","title":"File share","data":"You can:\n\ncreate file storages,\nconnect (mount) them to virtual machines inside the VK Cloud project using the CIFS or NFS protocols,\nwrite data to connected file storages and read data from them,\ncreate snapshots of the current state of the file storage.\n\nFile storages are created in your VK Cloud personal account. Connecting file storages, writing and reading data are only available in VK Cloud virtual machines. Other file storage functions are available in your VK Cloud personal account or in the OpenStack CLI using the openstack share commands.\n\nCreating a file storage\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where you want to create a file storage.\nGo to Cloud Servers → File share.\nClick the Create button.\nSet the file storage parameters:\n\n   Name of file storage: it can consist only of Latin letters, numbers, and special characters -, _, and ..\n   Storage size: storage size in GB. Must be within the quota, not less than 10 GB and not more than 10000 GB.\n   Protocol: to access the storage from Windows, select the CIFS protocol, from Linux — NFS.\n   Network: choose from the suggested ones or create a new one. For the new network, specify also Subnet address.\n   File storage network: choose from the suggested ones or create a new one.\n\nClick the Next step button.\n(Optional) Add file storage access rules:\n    Click Add new rule.\n    Enter the IP or subnet address of the source.\n    Select the access mode.\n    Click the Add rule button.\n\n\n   You can also add access rules after creating the file storage.\n\n\nClick Add file server.\n\nConnecting file storage\n\nThe way to connect the file storage depends on the operating system, Windows or Linux, and the protocol selected when you created the storage.\n\nWindows\n\n\n\nInstructions are provided for Windows Server 2012 R2. For information about connecting to other versions of Windows, see the developer documentation.\n\n\n\n\n\nNFS protocol\nCIFS protocol\n\n\n\nIn Windows, you can connect a file storage via the NFS protocol using the Windows Server component — Client for NFS.\n\nInstall Client for NFS from the Server Manager interface or using PowerShell:\n\n    Server Manager\n    PowerShell\n\n\n    Open Server Manager and choose Add Roles and Features.\n    Go to the Installation Type section, select the Add Roles and Features option and click Next.\n    Go to the Components section, select the Client fo NFS option from the list.\n    In the same list, expand the Remote Server Administration Tools → Role Administration Tools → File Services Tools options and choose Services for NFS Administration tools. Click Next.\n    Make sure that all the necessary components are selected and click Install.\n    Wait for the installation to complete and restart the server.\n\n\n    Install:\n\n    Client for NFS;\n    services for NFS Management Tools.\n\n        Run the Powershell command:\n\n                Install-WindowsFeature NFS-Client, RSAT-NFS-Admin\n\n    Wait for the installation to finish and restart the server.\n\n\nChange the client settings using the Server Manager:\n\n   In the Tools menu, select Services for NFS.\n   Select Client for NFS and click the Display the properties icon.\n   Set the required settings.\n\nConnect the file storage using the command specified in its properties.\n\n    The command to connect the storage looks like this:\n\n        mount  :\n\n    Here:\n\n    `` — the address of the file storage specified in its description.\n    `` — the uppercase Latin letter not used as the name of other disks.\n\n\n\n\nTo connect the file storage, run the command specified in its properties.\n\nThe command to connect the storage looks like this:\n\nnet use :\n\nHere:\n\n`` — the address of the file storage specified in its description.\n`` — the uppercase Latin letter not used as the name of other disks.\n\nLinux\n\n\n\nThe instructions are given for Ubuntu. For information about connecting to other Unix-like operating systems, read the developer's documentation.\n\n\n\n\n\nNFS protocol\nCIFS protocol\n\n\n\nInstall the nfs-common package using the command:\n\n        sudo apt-get install nfs-common\n\nCreate a directory to mount the storage:\n\n        mkdir\n\nUse the command specified in properties of the file storage:\n\n        mount -t nfs  ./\n\n    Here:\n\n    `` — the address of the file storage specified in its description.\n    `` — the name of the directory created earlier.\n\n\n\n\nInstall cifs-utils using the command:\n\n        sudo apt install -y cifs-utils\n\nCreate a directory to mount the storage:\n\n        mkdir\n\nUse the command specified in the properties of the file storage:\n\n        sudo mount -o user=,password= -t cifs  ./\n\n    Here:\n\n    `` — the address of the file storage specified in its description.\n    `` — the name of the directory created earlier.\n\nViewing a list of file storages\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the required file storage is located.\nGo to Cloud Servers → File share. A list of file storages will be displayed.\n\n\n\n\nMake sure that the OpenStack client is installed and authenticate to the project.\n\nMake sure that the Manila CLI is installed.\n\nRun the command:\n\n        openstack share list\n\n\nViewing information about file storage\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the required file storage is located.\nGo to Cloud Servers → File share.\nClick the name of the required file storage. Information about it will be displayed.\n\n\n\n\nMake sure that the OpenStack client is installed and authenticate to the project.\n\nMake sure that the Manila CLI is installed.\n\nRun the command:\n\n        openstack share show\n\n\nIncreasing the file storage size\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the required file storage is located.\nGo to Cloud Servers → File share.\nClick   for the required storage and select Change size.\nEnter a new value and click Confirm.\n\n\n\n\nMake sure that the OpenStack client is installed and authenticate to the project.\n\nMake sure that the Manila CLI is installed.\n\nRun the command:\n\n        openstack share extend\n\n\nThe size of the file storage cannot be reduced.\n\nCreating a snapshot\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the required file storage is located.\nGo to Cloud Servers → File share.\nClick   for the required storage and select Create snapshot.\n(Optional) Change the name of the snapshot and add a description.\nClick Create snapshot.\n\n\n\n\nMake sure that the OpenStack client is installed and authenticate to the project.\n\nMake sure that the Manila CLI is installed.\n\nRun the command:\n\n        openstack share snapshot create --name\n\n\nGetting snapshots list\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the required file storage is located.\nGo to Cloud Servers → File share.\nClick   for the required storage and select List of snapshots. Information about the snapshots will be displayed.\n\n\n\n\nMake sure that the OpenStack client is installed and authenticate to the project.\n\nMake sure that the Manila CLI is installed.\n\nRun the command:\n\n        openstack share snapshot list --share\n\n\nRestoring a storage from its snapshot\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the required file storage is located.\nGo to Cloud Servers → File share.\nClick   for the required storage and select List of snapshots.\nClick   for the required snapshot and select Restore file storage.\nClick Confirm. The process of creating a new storage from the snapshot will begin.\n\n\n\n\nMake sure that the OpenStack client is installed and authenticate to the project.\n\nMake sure that the Manila CLI is installed.\n\nRun the command:\n\n        openstack share create --snapshot-id  --share-type  --name\n\n\nDeleting a snapshot\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the required file storage is located.\nGo to Cloud Servers → File share.\nClick   for the required storage and select List of snapshots.\nClick   for the required snapshot and select Delete snapshot.\nClick Confirm.\n\n\n\n\nMake sure that the OpenStack client is installed and authenticate to the project.\n\nMake sure that the Manila CLI is installed.\n\nRun the command:\n\n        openstack share snapshot delete\n\n\nAdding an access rule\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the required file storage is located.\nGo to Cloud Servers → File share.\nClick the name of the required file storage.\nGo to the Access rules tab.\nClick Add new rule.\nSpecify the source IP or subnet address and select an access mode.\nClick the Add rule button.\n\n\n\n\nMake sure that the OpenStack client is installed and authenticate to the project.\n\nMake sure that the Manila CLI is installed.\n\nAdd an access rule using the command:\n\n        openstack share access create  ip  --access-level\n\n    The ` argument can take values rw (read and write) and ro` (read only).\n\nCheck that the rule was created successfully by requesting the list of access rules:\n\n        openstack share access list\n\n\nDeleting an access rule\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the required file storage is located.\nGo to Cloud Servers → File share.\nClick the name of the required file storage.\nGo to the Access rules tab.\nClick   for the required rule and select Delete.\nClick Confirm.\n\n\n\n\nMake sure that the OpenStack client is installed and authenticate to the project.\n\nMake sure that the Manila CLI is installed.\n\nGet the ID of the required rule by requesting the list of access rules:\n\n        openstack share access list\n\nDelete the access rule using the command:\n\n        openstack share access delete\n\n\nDeleting a file storage and its network\n\nTo delete a file storage, you must first disconnect it from virtual machines and delete all its snapshots.\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the required file storage is located.\nGo to Cloud Servers → File share.\nClick   for the required storage and select Delete.\nClick Confirm.\n\nSimultaneously with the file storage, the network created for it will be deleted.\n\n\n\n\nMake sure that the OpenStack client is installed and authenticate to the project.\n\nMake sure that the Manila CLI is installed.\n\nTo delete the file storage, run the command:\n\n        openstack share delete\n\nTo delete the file storage network, run the command:\n\n        openstack share network delete\n\n","url":"/en/computing/iaas/service-management/fs-manage","description":"Creating and deleting file storages, connecting them to instances, configuring access rules and take storage snapshots.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"78cb7096-b494-4f7d-834f-418e44561c43","title":"Disk encryption using cryptsetup","data":"\n\nVK Cloud is not responsible for the correct operation of third-party software.\n\nMake a backup copy of the VM before encrypting the disk.\n\n\n\nConfigure virtual machine disk encryption using cryptsetup for Linux.\n\nPreparatory steps\n\nFollow the preparatory steps to create a test Linux VM running CentOS 7.9, as well as an additional disk that will be encrypted.\n\nCreate the Linux VM from CentOS 7.9 image.\nCreate and connect disk to VM.\nConnect to VM.\nOutput a list of disks and find the name of the required disk (for example, /dev/vdb):\n\n      sudo fdisk -l\n\nIf there is no file system on the disk, format it.\n\n   Check that there is no file system on the disk:\n\n            lsblk -f\n\n   Format the disk:\n\n            sudo mkfs.ext4 /dev/vdb\n\n   Check the formatting result:\n\n            lsblk -f\n\nConfigure disk mounting.\n\n   Create a directory to mount the disk /volumes/disk1:\n\n            sudo mkdir /volumes\n      sudo mkdir /volumes/disk1\n\n   Add a line with the disk mount parameters to the /etc/fstab file:\n\n            sudo sed -i '$a /dev/vdb /volumes/disk1 auto defaults 0 0' /etc/fstab\n\n   Print the contents of the file and make sure that the line is added:\n\n            cat /etc/fstab\n\nRestart the VM:\n\n      sudo reboot\n\nMake sure that the disk is mounted in the specified directory:\n\n      lsblk\n\n1. Install cryptsetup\n\nRun the command to install:\n\nsudo yum install -y cryptsetup cryptsetup-reencrypt\n\n2. Encrypt the disk\n\n\n\nRestarting the VM before bootloader configuration is completed will result in loss of access to the VM.\n\n\n\nMake the disk non-bootable.\nStop all processes using the disk:\n\n      sudo lsof /volumes/disk1\n   sudo systemctl stop volumes-disk1.mount\n\nSee the size of the current file system:\n\n      sudo e2fsck -f /dev/vdb\n\n   Example of the result of executing the command:\n\n      e2fsck 1.42.9 (28-Dec-2013)\n   Pass 1: Checking inodes, blocks, and sizes\n   Pass 2: Checking directory structure\n   Pass 3: Checking directory connectivity\n   Pass 4: Checking reference counts\n   Pass 5: Checking group summary information\n   /dev/vdb: 88/655360 files (3.4% non-contiguous), 60910/2621440 blocks\n\nChange the file system size to the minimum possible:\n\n      sudo resize2fs -M /dev/vdb\n\n   Example of the result of executing the command:\n\n      resize2fs 1.42.9 (28-Dec-2013)\n   Resizing the filesystem on /dev/vdb to 24971 (4k) blocks.\n   The filesystem on /dev/vdb is now 24971 blocks long.\n\nStart disk encryption:\n\n      sudo cryptsetup-reencrypt /dev/vdb --new --reduce-device-size 4096S\n\nEnter and confirm the keyword:\n\n      Enter new passphrase:\n   Verify passphrase:\n\n\n   Remember the key phrase. Without it, it is impossible to decrypt the disk and use the VM.\n\n\nWait for the encryption process to complete:\n\n         Finished, time 00:23.401, 3875 MiB written, speed 165.6 MiB/s\n\nCheck the work with the encrypted disk:\n\n   Run the command:\n\n            sudo cryptsetup open /dev/vdb vdb_crypt\n\n   Enter the passphrase and press Enter.\n\nExpand the file system to disk size:\n\n      sudo resize2fs /dev/mapper/vdb_crypt\n\nChange the name of the device to mount:\n\n      sudo sed 's#/dev/vdb#/dev/mapper/vdb_crypt#' -i /etc/fstab\n\nOutput the contents of the fstab file and make sure that the entry has been changed:\n\n            cat /etc/fstab\n\nMount the disk:\n\n            sudo mount /volumes/disk1\n\nAdd information about the encrypted partition to /etc/crypttab:\n\n      Get root access:\n\n                  sudo -s\n\n      Run the command:\n\n                  UUID=$(blkid -s UUID -o value /dev/vdb)\n         echo \"vdb_crypt UUID=${UUID} none luks,discard\" >> /etc/crypttab\n         exit\n\n3. Change the loader parameters\n\n\n\nRestarting the VM before the bootloader configuration is completed will result in the loss of access to the VM.\n\n\n\nConfigure the bootloader so that the passphrase for decrypting the disk is requested when the system boots. To enter a passphrase, use the VNC console of the VM.\n\nChange the settings of the grub loader. Remove the console=ttyS0,115200 setting in the bootloader parameters:\n\n      sudo sed 's#console=ttyS0,115200 ##' -i /etc/default/grub\n\nReview the grub file and make sure that the setting is removed:\n\n      cat /etc/default/grub\n\nConfigure the bootloader:\n\n      sudo grub2-mkconfig -o /boot/grub2/grub.cfg\n\nRestart the VM.\n\n4. Get access to the VM\n\nGo to the VNC Console virtual machine. In the console output, when the operating system boots, you will be prompted to enter a keyword:\n\n      Please enter passphrase for disk vdb_crypt on /volumes/disk1:\n\nEnter the passphrase and press Enter.\n\nThe passphrase for decrypting the disk will be requested every time the OS boots. After entering the passphrase, the disk will be mounted and you can work with the file system as before encryption.\n","url":"/en/computing/iaas/how-to-guides/disk-encrypt","description":"Disk encryption using cryptsetup.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"88f8656a-c5f7-45a9-b469-aa5f846ea5dc","title":"Server group binding policy","data":"The distribution of server groups of a single service or application on a cluster depends on the tasks being solved:\n\nIf the service has several groups of servers (VMs, DB instances) with the same role, for example, several application servers, distribute them across different physical servers to increase fault tolerance.\nIf there are groups of servers with different roles in the service for which communication latency is critical (for example, an application server and a database server), place them on the same computing node so that traffic passes within the same physical server.\n\nVK Cloud supports four server group allocation policies, for soft policies, support for the compute-api version above 2.15 is required.\n\n\n\nValid operations for server groups:\n\ncreate new servers in the group;\ndelete existing servers in the group.\n\nInvalid operations for server groups:\n\nadd an existing server to the group;\ndelete a group of servers without deleting all its servers.\n\n\n\nTo define the rules for distributing server groups, specify the group in the instance properties:\n\nOutput a list of server groups using the command:\n\n        openstack server group list\n\nFind a group of servers with a policy like affinity or anti-affinity. If no server groups are found, create one using the command:\n\n        ​openstack server group create  --policy\n\nCreate bootable disks for future VMs. Example for three disks:\n\n      for i in 1 2 3;do openstack volume create --size 10 --image 98af6254-XXXX-XXXX-XXXX-81858ce9302a --availability-zone MS1 --bootable root-volume-$i;done\n\nCreate the necessary virtual machines specifying the server group. Example for three VMs:\n\n        for i in 1 2 3;do openstack --insecure server create --flavor std1-1 --volume root-volume-$i --hint group= --nic net-id= vm-affinity-$i;done\n\nWait for the VM creation to finish. Make sure that the deployed VMs have been successfully created and are in the status ACTIVE.\n\n\n\nIf no compute nodes are found that meet the constraints, resource creation requests will fail.\n\n\n","url":"/en/computing/iaas/how-to-guides/managing-node","description":"Placing servers in different groups for high availability or in the same group to speed up interaction between servers.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"af045b12-cc39-4584-8702-f2057e9b0ab1","title":"Enabling multiple queues (multiqueue) on Linux","data":"VK Cloud supports multiple queues (multiqueue) for a VM image and a separate VM for Linux operational systems.\n\nConstraints\n\nThe virtio-net multiqueue function provides performance improvement, but it has some limitations:\n\nGuest OS is limited to ~200 MSI vectors. Each NIC queue requires a MSI vector, as well as any virtio device or assigned PCI device. Defining an instance with multiple virtio NICs and vCPUs might lead to a possibility of hitting the guest MSI limit.\nVirtio-net multiqueue works well for incoming traffic, but can occasionally cause a performance degradation, for outgoing traffic.\nEnabling virtio-net multiqueue increases the total network throughput, but in parallel it also increases the CPU consumption.\nEnabling virtio-net multiqueue in the host QEMU config, does not enable the functionality in the guest OS. The guest OS administrator needs to manually turn it on for each guest NIC that requires this feature, using ethtool.\nMSI vectors would still be consumed (wasted), if multiqueue was enabled in the host, but has not been enabled in the guest OS by the administrator.\nIn case the number of vNICs in a guest instance is proportional to the number of vCPUs, enabling the multiqueue feature is less important.\n\n\n\nOn the VK Cloud platform, it is not enough to enable multiqueue only at the image level (in the QEMU configuration). The OS administrator needs to manually enable the functionality using ethtool for VMs that were created before enabling multiqueue.\n\n1. Enable multiqueue\n\n\n\nFor all created VMs\nFor a separate VM\n\n\n\nThe option includes multiqueue at the image level and will work for all VMs created after executing the instruction.\n\nCreate image VM.\nGet a list of available images:\n\n        openstack image list\n\nCopy the ID of the required image.\nEnable multiqueue:\n\n        openstack image set  --property hw_vif_multiqueue_enabled=true\n\n\nThis option is used when the VM has already been created at the time of enabling multiqueue.\n\nGet the VM UUID через CLI или личный кабинет VK Cloud.\nContact technical support with a request to connect multiqueue and specifying the UUID of the target VM.\nAfter successfully processing the request from technical support, perform a forced VM reboot.\n\n2. Check the multiqueue connection\n\nCreate a VM with more than one CPU, and connect to it.\nView all network interfaces:\n\n        sudo ip link show\n\n     Example output\n\n        ubuntu@dm-test:~$ sudo ip link show\n    1: lo:  mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    2: ens3:  mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000\n        link/ether fa:16:3e:1d:3e:08 brd ff:ff:ff:ff:ff:ff\n        altname enp0s3\n\n    Here ens3 is the name of the network interface for which you need to check the multiqueue connection.\n\n\nSee the current number of queues:\n\n        ethtool -l\n\n     Example output\n\n        ubuntu@dm-test:~$ ethtool -l ens3\n    Channel parameters for ens3:\n    Pre-set maximums:\n    RX:             n/a\n    TX:             n/a\n    Other:          n/a\n    Combined:       1\n    Current hardware settings:\n    RX:             n/a\n    TX:             n/a\n    Other:          n/a\n    Combined:       1\n\n\n3. Set the required number of queues for the VM\n\n\n\nThe number of queues cannot be greater than the number of virtual CPUs.\n\n\n\nCreate a VM and connect to it.\nRun the command:\n\n        sudo ethtool -L  combined\n    Check the new number of queues (parameter Combined):\n\n        ethtool -l\n\n\n  Example of installing queues\n\nubuntu@dm-test:~$ sudo ethtool -L ens3 combined 2\nubuntu@dm-test:~$ ethtool -l ens3\nChannel parameters for ens3:\nPre-set maximums:\nRX:             n/a\nTX:             n/a\nOther:          n/a\nCombined:       4\nCurrent hardware settings:\nRX:             n/a\nTX:             n/a\nOther:          n/a\nCombined:       2\n\n\n","url":"/en/computing/iaas/how-to-guides/vm-multiqueue","description":"Enabling multiple queues (multiqueue) on Linux.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"877e667d-161d-4b48-a5ea-803a1f84111b","title":"Preparing a Windows OS image to create a VM","data":"\n\nThe list of supported Windows OS for migration is listed in the section Operating system.\n\n\n\nThe Windows Server 2016 CORE edition image is used as an example, the commands are given in PowerShell.\n\nPreparatory steps\n\nMake sure that you have installed and configured Git.\nClone a repository with automated build scripts windows-imaging-tools.\nClone the repository to update the system image WindowsUpdateCLI.\nSet up the toolkit:\n\n   Install the drivers VirtIO (KVM).\n   Configure Hyper-V in a way that is suitable for you (example), if this has not been done before.\n   Download and install Windows ADK.\n\nDownload the ISO image of the operating system for which you plan to migrate to VK Cloud. It is recommended to use the en-US version of the image.\nMake sure that OpenStack client is installed and authenticate to the project.\n\n1. Prepare the OS installation WIM file\n\n\n\nThe installation image may contain several editions of the operating system. Since the installation takes place automatically, select the required edition in advance and export it to a separate wim file.\n\n\n\nConnect the downloaded OS ISO image.\nOutput a list of all Windows versions using the command executed on behalf of the administrator:\n\n        Get-WindowsImage -ImagePath E:\\sources\\Install.wim\n\n    Here E:\\sources\\Install.wim — full access to the system's WIM file on the mounted disk.\n\n    A list of editions will appear indicating the ImageIndex of its number:\n\n        ImageIndex      : 1\n    ImageName       : Windows Server 2016 Standard\n    ImageDescription: This is the recommended option. It reduces management and maintenance by installing only what is required for most applications and server roles. It does not include a graphical user interface, but you can fully manage the server locally or remotely using Windows PowerShell or other tools. See the section \"Windows Server Installation Options\".\n    ImageSize       : 9 146 079 566 bytes\n\n    ImageIndex      : 2\n    ImageName       : Windows Server 2016 Standard (desktop features)\n    ImageDescription: This option is suitable if you need a graphical user interface (for example, to ensure backward compatibility of an application that cannot work when installing the main server components). All server roles and components are supported. More detailed: \"Windows Server Installation Options\".\n    ImageSize       : 15 219 002 744 bytes\n\nExport the revision with ImageIndex = 1 using the command:\n\n        dism `\n    /export-image `\n    /SourceImageFile:E:\\sources\\Install.wim `\n    /SourceIndex:1 `\n    /DestinationImageFile:D:\\Temp\\install.wim `\n    /Compress:max `\n    /CheckIntegrity\n\n    Here:\n\n    D:\\Temp\\install.wim — the full path on the local disk where the exported image will be saved;\n    SourceIndex:1 — index number of the required revision.\n\n2. Configure the external switch in Hyper-V\n\nCreate virtual switch external with an Internet connection.\n\n3. Build the image locally\n\nGo to the windows-imaging-tools directory and import the modules:\n\n        pushd windows-openstack-imaging-tools\n    Import-Module .\\WinImageBuilder.psm1\n    Import-Module .\\Config.psm1\n    Import-Module .\\UnattendResources\\ini.psm1\n\nTransfer the contents of the directory WindowsUpdateCLI to windows-openstack-imaging-tools\\UnattendResources\\WindowsUpdates.\nCreate a configuration file config.ini:\n\n        $ConfigFilePath = \".\\config.ini\"\n    New-WindowsImageConfig -ConfigFilePath $ConfigFilePath\n\nOpen the created file and check the parameters:\n\n        wim_file_path=D:\\Temp\\install.wim\n    image_name=Windows Server 2016 SERVERSTANDARDCORE\n    image_path=D:\\Win_Server_2016_img.qcow2\n    virtual_disk_format=QCOW2\n    image_type=KVM\n    external_switch=external\n    virtio_iso_path=\"D:\\Drivers\\virtio.iso\"\n    time_zone=\"Russian Standard Time\"\n    install_qemu_ga=True\n    install_updates=True\n    purge_updates=False\n    compress_qcow2=True\n\n    Here:\n\n    external_switch=external — name of the created switch;\n    virtio_iso_path=\"D:\\Drivers\\virtio.iso\" — full path to the ISO file with VirtIO drivers;\n    time_zone=\"Russian Standard Time\" — the time zone, you can find out using the command tzutil /l;\n    purge_updates=False — do not clear the WinSxS directory after installing updates.\n\nRun a local build of the image using the command:\n\n        New-WindowsOnlineImage -ConfigFilePath $ConfigFilePath\n\nWait for the operation to complete and make sure that the file D:\\Win_Server_2016_img.qcow2 is created.\n\n4. Import the image to the VK Cloud\n\nUse the CLI to import an image:\n\nopenstack image create --private --container-format bare --disk-format qcow2 --file D:\\Win_Server_2016_img.qcow2 --property hw_qemu_guest_agent=yes --property store=s3 --property min_ram=2048 --property os_require_quiesce=yes --property min_disk=40 --property os_type=windows --property os_admin_user=Administrator --property mcs:lic:mswinsrv=true --property mcs_name='Windows Server 2016 Standard (en)' --property mcs_os_distro='server' --property mcs_os_edition='std' --property mcs_os_type='windows' --property mcs_os_lang='en' --property mcs_os_type='windows' --property mcs_os_version='2016' --property os_distro='win2k16'\n\nWait for the operation to complete. After downloading the image, you will be able to create a VM by standard means of the VK Cloud platform.\n\nDelete unused resources\n\nIf you no longer need the imported image, delete it.\n","url":"/en/computing/iaas/how-to-guides/win-image","description":"Downloading Windows images 2008 / 2008 R2, 2012 / 2012 R2, 2016, 2019, 2022 to VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"333fcebe-51de-49ac-a2c4-a8f3b4cdb91f","title":"Creating an image using Packer","data":"Packer allows you to create virtual machine images with the necessary parameters using a configuration file. The Alt Linux P9 OS image in QCOW format will be used as an example.\n\nPreparatory steps\n\nInstall the latest version of Packer.\n\n\n   You can download Packer from the VK Cloud mirror.\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\nLoad the image OS Alt Linux P9 to your computer (file alt-p9-cloud-x86_64.qcow2).\n\n1. Convert image to RAW format\n\nUse the utility qemu-img:\n\nInstall 'qemu-img' if not done before:\n\n    RHLE/Centos\n    Ubuntu\n\n\n        sudo yum install qemu-img\n\n\n        sudo apt install qemu-utils\n\n\nRun the file conversion using the command:\n\n        qemu-img convert -f qcow2 -O raw alt-p9-cloud-x86_64.qcow2 alt-p9-cloud-x86_64.raw\n\n    The syntax of the conversion command is given in the official QEMU documentation.\n\n2. Upload the base image to the cloud\n\nImport the image according to instructions.\n\n3. Create a Packer configuration file\n\nDetermine the details of the network and the downloaded image:\n\n    Get id of the external network to which the virtual machine being created will be connected.\n    Copy the name of the downloaded image, getting a list of images using the command openstack image list.\n    Write the received values into variables:\n\n                export SOURCE_IMAGE=8b64c09b-7141-41ad-XXXX-9f5a8dbbd87e\n        export NETWORK_ID=f19e1e54-bce9-4c25-XXXX-e0f40e2cff14\n\nCreate the file altlinux.pkr.hcl:\n\n        altlinux.pkr.hcl\n\n            variable \"network_id\" {\n        type = string\n        default = \"${env(\"NETWORK_ID\")}\"\n        validation {\n          condition     = length(var.network_id) > 0\n          error_message =  0\n          error_message =\n\n      When creating a VM, specify the availability zone in which the disk should be created. Detailed information about the syntax of the configuration file in the official Packer documentation.\n\n\nCheck the created configuration using the command:\n\n        packer validate altlinux.pkr.hcl\n\n4. Upload the prepared image to the cloud\n\nRun the image creation using the command:\n\n        packer build altlinux.pkr.hcl\n\nWait for the successful download message to appear:\n\n        ==> Builds finished. The artifacts of successful builds are:\n    --> openstack.altlinux: An image was created: c6320138-035f-40d8-XXXX-e814edb2ce5f\n\nWrite down the ID c6320138-035f-40d8-XXXX-e814edb2ce5f — you will need it in the next step.\n\n5. Complete the image setup\n\nSet meta tags to the created image using the command:\n\n        openstack image set \\\n    --property hw_video_model='qxl' \\\n    --property hw_watchdog_action='reset' \\\n    --property hw_vif_multiqueue_enabled='true' \\\n    --property hw_qemu_guest_agent='yes' \\\n    --property os_require_quiesce='yes' \\\n    --property mcs_name='Alt Linux P9 Starter Kit' \\\n    --property mcs_os_distro='debian' \\\n    --property mcs_os_type='linux' \\\n    --property mcs_os_version='P9' \\\n    --property os_admin_user='altlinux' \\\n    --property os_distro='altlinux-p9' \\\n    --property os_type='linux' \\\n    --property os_version='p9' \\\n    c6320138-035f-40d8-XXXX-e814edb2ce5f\n\nMake sure that the image is displayed correctly.\n\n    Personal account\n    OpenStack CLI\n\n\n    Go to personal account VK Cloud.\n    Go to Cloud computing → Images.\n    Find the image in the list and click on it. The image page opens.\n\n      The image will also become available when creating a VM.\n\n\n        openstack image show c6320138-035f-40d8-XXXX-e814edb2ce5f\n\n    The result of the command execution:\n\n        +------------------+------------------------------------------------------+\n    | Field            | Value                                                |\n    +------------------+------------------------------------------------------+\n    | checksum         | 896ea37e28d82a548cedf1e0aa92XXXX                     |\n    | container_format | bare                                                 |\n    | created_at       | 2023-03-29T14:06:44Z                                 |\n    | disk_format      | raw                                                  |\n    | file             | /v2/images/c6320138-035f-40d8-XXXX-e814edb2ce5f/file |\n    | id               | c6320138-035f-40d8-XXXX-e814edb2ce5f                 |\n    | min_disk         | 0                                                    |\n    | min_ram          | 0                                                    |\n    | name             | Alt-Linux-P9-Starter-Kit                             |\n    | owner            | b5b7ffd4ef0547e5b222f44555dfXXXX                     |\n    | properties       | base_image_ref='1a8aa332-d8ef-4c40-XXXX-cade8b59aea3', boot_roles='mcs_owner', direct_url='s3://user:key@h-int.icebox.q/images-b5b7ffd4ef0547e5b222f44555dfXXXX/c6320138-035f-40d8-XXXX-e814edb2ce5f', hw_qemu_guest_agent='True', hw_video_model='qxl', hw_vif_multiqueue_enabled='True', hw_watchdog_action='reset', image_location='snapshot', image_state='available', image_type='image', instance_uuid='f19e1e54-bce9-4c25-XXXX-e0f40e2cff14', is_ephemeral_root='True', locations='[{'url': 's3://user:key@h-int.icebox.q/images-b5b7ffd4ef0547e5b222f44555dfXXXX/c6320138-035f-40d8-XXXX-e814edb2ce5f', 'metadata': {}}]', mcs_name='Alt Linux P9 Starter Kit', mcs_os_distro='debian', mcs_os_type='linux', mcs_os_version='P9', os_admin_user='altlinux', os_distro='altlinux-p9', os_require_quiesce='True', os_type='linux', os_version='p9', owner_project_name='mcsXXXX', owner_specified.openstack.md5='XXXX', owner_specified.openstack.object='images/alt-p9-cloud-x86_64', owner_specified.openstack.sha256='XXXX', owner_user_name='test@vk.team', self='/v2/images/c6320138-035f-40d8-XXXX-e814edb2ce5f', store='s3', user_id='5f48556ef89444dbab8fa82669dXXXX' |\n    | protected        | False                                                |\n    | schema           | /v2/schemas/image                                    |\n    | size             | 1653604352                                           |\n    | status           | active                                               |\n    | tags             |                                                      |\n    | updated_at       | 2023-03-29T14:08:15Z                                 |\n    | visibility       | private                                              |\n    +------------------+------------------------------------------------------+\n\n\nDelete unused resources\n\nIf you no longer need the image, delete it.\n","url":"/en/computing/iaas/how-to-guides/packer","description":"Creating a virtual machine image with the necessary parameters using the Packer configuration file.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"d0c06f19-6e2a-45c2-8680-bd6e365b7cf2","title":"Tariffication","data":"Tariffication is based on the “pay as you go” principle: you are charged only for the resources you consume to the nearest minute, the fee for using paid OS is charged once a day.\n\nThe cost of network services is given in price list. You can use calculator to calculate the total cost of other platform services that use services. See Billing for how the tools related to paying for platform services work.\n\nWhat is charged\n\nFunds for using virtual machines are debited while they are running. If the VM is stopped, the funds are debited only for the use of licenses and disk space.\n\nCharged:\n\nCPU (vCPU) — for each core. 1 vCPU corresponds to 1 physical core of the virtualization server.\nRandom access memory (RAM) — for each 1 GB of RAM.\nLicenses (for example, on Windows OS) — for each of license, regardless of whether the license is activated or not.\nDisks — for each 1 GB of disk space, the price depends on the type of disk (SSD, HDD, High-IOPS, Low Latency NVME).\nFile storage (NFS/CIFS) — for every 1 GB of disk space.\nDisk snapshot (backup) — for every 1 GB of snapshot size.\nDisk images — for each 1 GB of image size (for the price of an HDD).\nIP addresses — for more information, see Virtual networks.\n\nWhat is not charged\n\nIncoming and outgoing traffic.\nMonitoring.\n\nExample of price calculation\n\nFor example, the price of a virtual machine with Ubuntu Linux OS, 4 CPU, 16 GB RAM, 100 GB SSD disk and public IP will be calculated as follows:\n\n(4 x cost for 1 vCPU) + (16 x cost for 1 GB RAM) + (100 x cost for 1 GB for SSD) + (1 x cost for public IP)\n","url":"/en/computing/iaas/tariffication","description":"How the Cloud Servers is charged, cost calculation.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"39505792-9082-4c48-b2dc-00301b2d219d","title":"Network in a Linux VM","data":"This guide provides recommendations for troubleshooting network configuration issues on Linux VMs.\n\nAs an example, it is assumed that:\n\na virtual machine has only one network interface;\nthe problem of missing access to the virtual machine via SSH is being solved.\n\nThe described approaches can also be used when access is lost to another application (for example, NGINX web server) or when a virtual machine has multiple network interfaces.\n\nExample of a problem\n\nIt is not possible to connect to a VM via SSH at its private IP address or floating IP address (if there is one).\nSome time ago there were no problems with the connection.\n\nBefore starting work\n\nPerform a forced reboot.\n\n   This operation can help if the VM is not responding or its network interface is initialized incorrectly:\n   Stop VM.\n   For a stopped VM run forced reboot.\n\n   If this does not solve the problem, follow the remaining steps and proceed to diagnostics.\n\nMake sure, that the VM is running.\n\nMake sure,that you can access the virtual machine console and sign in to it using a pre-configured login.\n\n   If necessary, restore password for authorization.\n\nGet information about the configuration of the virtual machine network interface:\n\n   Go to personal account VK Cloud.\n   Select the project where the necessary VM is located.\n   Go to Cloud Computing → Virtual machines.\n   Click on the name of the required VM.\n   Go to the tab Networks.\n   Write down the following network information:\n\n      network and subnet names;\n      gateway and subnet CIDR;\n      IP address: private and floating (if any);\n      MAC address;\n      Firewall settings (list of security groups).\n\n      For example, the following values will be used:\n\n      | Parameter                                           | Value                 |\n      | --------------------------------------------------- | --------------------- |\n      | Network name                                        | demoNetwork         |\n      | Name of subnet                                      | demoSubnet          |\n      | Gateway                                             | 10.0.0.1            |\n      | CIDR                                                | 10.0.0.0/24         |\n      | Private IP address                                  | 10.0.0.5            |\n      | Private IP address in combination with a prefix from CIDR | 10.0.0.5/24         |\n      | Floating IP                                         | 192.0.2.22          |\n      | MAC address                                         | fa:16:3e:aa:bb:cc   |\n      | Firewall settings                                   | default             |\n\n\n1. Check the network interface settings\n\nSometimes the connection is hindered by incorrect initialization of the network interface of the virtual machine or its incorrect settings.\n\nCheck that the network interface is configured correctly:\n\nConnect to the virtual machine console and sign in.\n\nRun the command:\n\n        ip link show\n\n    Example output:\n\n        1: lo:  mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT                   group default qlen 1000\n        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    2: ens3:  mtu 1500 qdisc fq_codel state UP mode                   DEFAULT group default qlen 1000\n        link/ether fa:16:3e:aa:bb:cc brd ff:ff:ff:ff:ff:ff\n        altname enp0s3\n\n    Find in the output of the command the name of the interface for which the link\\ether parameter matches the MAC address received earlier.\n    In this example, it will be ens3.\n\n\n    The interface name will vary for different Linux distributions.\n\n\nExecute the command by substituting the interface name received in the previous step into it:\n\n        ip address show ens3\n\n    Example output:\n\n        2: ens3:  mtu 1500 qdisc fq_codel state UP group default qlen 1000\n        link/ether fa:16:3e:aa:bb:cc brd ff:ff:ff:ff:ff:ff\n        altname enp0s3\n        inet 10.0.0.5/24 metric 100 brd 10.0.0.255 scope global dynamic ens3\n           valid_lft 603373sec preferred_lft 603373sec\n        inet6 fe80::f816:3eff:feb4:d70f/64 scope link\n           valid_lft forever preferred_lft forever\n\n    The output should contain:\n\n    Interface status information: state UP.\n    The private IP address of the virtual machine, combined with the prefix /24 from the subnet CIDR, in the parameter inet (10.0.0.5/24).\n\nRun the command:\n\n        ip route show default\n\n    Example output:\n\n        default via 10.0.0.1 dev ens3 proto dhcp src 10.0.0.5 metric 100\n\n    Make sure that the output contains:\n\n    Gateway IP address (via 10.0.0.1).\n    Interface name received earlier (dev ens3).\n    The private IP address of the virtual machine (src 10.0.0.5).\n\n    If the output of the commands ip address show and ip route show contains the given information, then the network interface settings are correct. Go to application verification.\n\n    If the output of the commands ip address show and ip route show if it does not contain the above information, then the network interface settings are incorrect.\n\nConfigure the network interface manually:\n\n    Ubuntu\n    Debian\n    AlmaLinux, CentOS\n    openSUSE\n\n\n    Edit the file /etc/netplan/50-cloud-init.yaml and bring it to the following form:\n\n                network:\n            ethernets:\n                ens3: # Interface name\n                    dhcp4: false\n                    addresses:\n                        10.0.0.5/24 # Private IP address + prefix from CIDR\n                    routes:\n                        to: 0.0.0.0/0\n                          via: 10.0.0.1 # Gateway address\n                    nameservers:\n                        addresses:\n                            5.61.237.120\n                            5.61.237.127\n                    match:\n                        macaddress: fa:16:3e:aa:bb:cc # MAC address\n                    set-name: ens3\n            version: 2\n\n        If necessary, specify other DNS servers in the parameter networks.ethernets.ens3.nameservers.addresses.\n\n    Run the command:\n\n                sudo netplan apply\n\n\n    Edit the file /etc/network/interfaces.d/50-cloud-init and bring it to the following form:\n\nThis file is generated from information provided by the datasource.  Changes\nto it will not persist across an instance reboot.  To disable cloud-init's\nnetwork configuration capabilities, write a file\n/etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:\nnetwork: {config: disabled}\n        auto lo\n        iface lo inet loopback\n\n        auto eth0 # Interface name\n        iface eth0 inet static\n        address 10.0.0.5/24 # Private IP address + prefix from CIDR\n        gateway 10.0.0.1 # Gateway address\n        dns-nameservers 5.61.237.120 5.61.237.127\n\n        If necessary, specify other DNS servers in the parameter dns-nameservers.\n\n    Restart the network connection by running the command:\n\n                sudo systemctl restart networking\n\n\n    Edit the file /etc/sysconfig/network-scripts/ifcfg- and bring it to the following form:\n\nCreated by cloud-init on instance boot automatically, do not edit.\n\n        BOOTPROTO=none\n        DEVICE=eth0 # Interface name\n        HWADDR=FA:16:3E:AA:BB:CC # MAC address\n        MTU=1500\n        ONBOOT=yes\n        TYPE=Ethernet\n        USERCTL=no\n        PROXY_METHOD=none\n        BROWSER_ONLY=no\n        IPADDR=10.0.0.5 # Private IP address\n        PREFIX=24 # Prefix from CIDR\n        DEFROUTE=yes\n        IPV4_FAILURE_FATAL=no\n        IPV6INIT=no\n        NAME=\"System eth0\" # Use this name later to restart the network connection\n        UUID=5fb06bd0-aaaa-bbbb-cccc-d6edd65f3e03 # For your VM, the UUID will be different\n        GATEWAY=10.0.0.1 # Gateway address\n        DNS1=5.61.237.120 # DNS server 1\n        DNS2=5.61.237.127 # DNS server 2\n\n        If necessary, specify other DNS servers in the parameters DNS1 and DNS2.\n\n    Restart the network connection by running the command:\n\n                sudo nmcli con up \"System eth0\"\n\n\n    Edit the /etc/sysconfig/network/ifcfg- file and make it look like:\n\n              IPADDR='10.0.0.5/24' # Private IP address + prefix from CIDR\n       BOOTPROTO='static'\n       STARTMODE='hotplug'\n\n    Edit the /etc/sysconfig/network/routes file and specify the gateway address in it:\n\n              default 10.0.0.1 - -\n\n    Edit the /etc/sysconfig/network/config and specify the DNS servers' adresses in it:\n\n              NETCONFIG_DNS_STATIC_SERVERS=\"5.61.237.120 5.61.237.127\"\n\n       If necessary, specify other DNS servers.\n\n    Apply the DNS servers' settings:\n\n              sudo netconfig update\n\n    Restart the network connection by running the command:\n\n              sudo systemctl restart network\n\n\nProhibit making automatic changes to the edited configuration file:\n\n        echo 'network: {config: disabled}' | sudo tee /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg\n\nCheck for SSH access to the VM. If access does not appear, go to the application check.\n\n2. Make sure that the necessary applications are running on the VM\n\nThe network interface may be fine, but applications and services may not be running or running on a non-standard port.\n\nCheck the SSH operation:\n\nConnect to the virtual machine console and sign in.\n\nRun the command:\n\n        sudo systemctl status ssh\n\n    By the output of the command, determine whether the service is running:\n\n    Active: active (running): SSH service is running.\n    Active: inactive (dead): SSH service is not running.\n\nDepending on the SSH service status, follow these steps:\n\n    If SSH service is running\n    If SSH service is not running\n\n\n    Determine which port the SSH service is running on:\n\n              sudo cat /etc/ssh/sshd_config | grep -w Port\n\n       The output will contain the port number:\n\n              Port 22\n\n        If the service is running on a standard port 22 — go to checking the settings of the VM firewall. Otherwise, proceed to the next step.\n\n    Connect using a non-standard port number. For example, if the SSH service is running on a port 222:\n\n                ssh -i /path/to/private_key_file username@192.0.2.22 -p 222\n\n    Check for SSH access to the VM. If access has not appeared, proceed to checking the settings of the VM firewall.\n\n\n    Determine which port the SSH service is running on:\n\n                sudo cat /etc/ssh/sshd_config | grep -w Port\n\n        The output will contain the port number:\n\n                Port 22\n\n    Make sure that other processes are not using this port. View SSH service logs:\n\n                sudo journalctl -xeu ssh\n\n        If there is a similar string in the logs, then the SSH service port is used by another process:\n\n                error: Bind to port 22 on 0.0.0.0 failed: Address already in use.\n\n        If there is no such line in the logs, proceed to checking the settings of the VM firewall.\n\n    Determine which process took up the port:\n\n        Install the netstat utility:\n\n            Debian, Ubuntu\n            AlmaLinux, CentOS\n\n\n                        sudo apt install net-tools -y\n\n\n                        sudo yum install net-tools -y\n\n\n        Run the command:\n\n                        sudo netstat -plntu | grep :22\n\n            In this output example, port 22 is used by the some-other-service process with the PID 1234:\n\n                        tcp   0   0 0.0.0.0:22   0.0.0.0:*  LISTEN   1234/some-other-service\n            tcp6  0   0 :::22        :::*       LISTEN   1234/some-other-service\n\n    Stop the process using the SSH port:\n\n        or using systemctl:\n\n                        sudo systemctl stop some-other-service\n\n        or by forcibly shutting down the process:\n\n                        sudo kill 1234\n\n    Change the settings of the service corresponding to the stopped process so that the service uses a port other than 22.\n\n    Restart the service:\n\n                sudo systemctl restart some-other-service\n\n    Restart SSH service:\n\n                sudo systemctl restart sshd\n\n    Make sure that the SSH service is running successfully:\n\n                sudo systemctl status sshd\n\n        Example of the output part:\n\n                Active: active (running)\n\n        If the SSH service does not start, check the service settings in the configuration file /etc/ssh/sshd_config.\n        For more information about problems with the service, see the logs:\n\n                sudo journalctl -xeu ssh\n\n    Check for SSH access to the VM. If access has not appeared, proceed to checking the settings of the VM firewall.\n\n\n3. Check the settings of the virtual machine firewall\n\nIf a firewall is configured on a virtual machine (for example, iptables, ufw, firewalld), it may prevent connection, even if the IP address of the virtual machine is correct, and the SSH service is configured and running.\n\nNext, it will be shown how to temporarily disable all firewall rules, allowing all traffic. This will help to make sure that the problem is in the firewall.\n\n\n\nAfter the SSH connection problem is localized, enable the firewall rules back (with the necessary adjustments). If all traffic is allowed, the security of the VM will decrease.\n\n\n\nTo check the firewall settings:\n\nConnect to the virtual machine console and sign in.\n\nDisable the firewall:\n\n    ufw\n    firewalld\n    iptables\n\n\n        sudo ufw disable\n\n\n        sudo systemctl stop firewalld\n\n\n    Save the existing rules iptables:\n\n                sudo iptables-save | sudo tee ~/iptables.rules\n\n    Run the commands:\n\n                sudo iptables -P INPUT ACCEPT\n        sudo iptables -P FORWARD ACCEPT\n        sudo iptables -P OUTPUT ACCEPT\n        sudo iptables -t nat -F\n        sudo iptables -t mangle -F\n        sudo iptables -F\n        sudo iptables -X\n\n\nCheck for SSH access to the VM.\n\n   If access has appeared, adjust the firewall rules and enable it again.\n\n   If access does not appear, turn on the firewall again and check the settings of the VK Cloud firewall security groups.\n\nTo turn on the firewall again:\n\nConnect to the virtual machine console and sign in.\n\nRun the command:\n\n    ufw\n    firewalld\n    iptables\n\n\n        sudo ufw enable\n\n\n        sudo systemctl start firewalld\n\n\n        sudo iptables-restore\n\n\n4. Check the settings of the VK Cloud firewall security groups\n\nIncorrectly configured security groups can prevent SSH connection, even if there are no obstacles at the VM level.\n\nNext, it will be shown how to temporarily configure the firewall rules so as to allow all traffic. This will help to make sure that the problem is in the firewall.\n\n\n\nAfter the SSH connection problem is localized, re-configure the firewall rules (with the necessary adjustments). If all traffic is allowed, the security of the VM will decrease.\n\n\n\nTo check the firewall settings:\n\nGo to personal account VK Cloud.\nSelect the project where the necessary VM is located.\nGo to Cloud Computing → Virtual machines.\nClick on the name of the required VM.\nGo to the tab Networks.\nClick   for the required network connection and select Edit connection.\nIn the Firewall settings parameter:\n\n    Delete all selected security groups.\n    Select security groups from the drop-down list default and all («All allowed»).\n\n       The default security group allows any outgoing traffic. The all security group allows any incoming traffic.\n\nClick the Save button.\n\nCheck for SSH access to the VM.\n\n    If access has appeared, adjust the firewall security groups and add them again instead of the all group.\n\n    If access does not appear, go back to the original firewall settings and contact technical support.\n\nTo configure firewall rules again:\n\nGo to personal account VK Cloud.\nSelect the project where the necessary VM is located.\nGo to Cloud Computing → Virtual machines.\nClick on the name of the required VM.\nGo to the tab Networks.\nClick   for the required network connection and select Edit connection.\nIn the Firewall settings parameter:\n\n    Delete the security group all.\n    Select the necessary security groups from the drop-down list.\n\n        If the selected security groups do not contain rules that allow outgoing traffic, also select the default security group. This group allows outgoing traffic. Otherwise, the virtual machine will not have access to the network.\n\n\n    In order for the firewall to pass the traffic of an SSH service running on the standard port 22, it is enough to select the rules default and ssh («only ssh allowed»).\n\n\n5. Contact technical support\n\nIf the diagnostics did not help in solving the problem, contact technical support, by providing the information obtained during the diagnosis.\n","url":"/en/computing/iaas/troubleshooting/linux-vm-network","description":"Diagnostics and elimination of typical network problems on Linux virtual machines.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"fc25d210-12e4-45c7-ba98-e8f22b0c4863","title":"FAQ","data":"Virtual machines\n\n\n\nWhere are the servers located?\n\nThe servers are located in several VK Cloud data centers both on the territory and outside the Russian Federation.\n\n\n\n\n\nWhat is the guaranteed connection speed?\n\nVK Cloud provides virtual machines with an incoming and outgoing Internet connection channel with a bandwidth of 1 Gbit/s, without traffic restrictions.\n\n\n\n\n\nHow do I set a new password for a VM?\n\nUse the instructions.\n\n\n\n\n\nWhen I change my VM password, I get an error in my personal account\n\nThe password to the virtual machine is set through the guest agent. If the agent is unavailable, there may be problems with setting the password.\n\nIn this case, it is recommended to install and configure qemu-guest-agent by running the command in the terminal:\n\nsudo sh -c \"apt update; apt install -y qemu-guest-agent; systemctl enable qemu-guest-agent; systemctl start qemu-guest-agent\"\n\n\n\n\n\nNo access to high-performance CPUs\n\nTo get access to high-performance CPUs, contact technical support.\n\n\n\n\n\nHow much does an external IP address cost?\n\nThe current prices for floating IP addresses and IP addresses on virtual machine ports (ext-net) are posted in price list.\n\n\n\n\n\nI want to create an instance, but there is no suitable configuration\n\nIf you did not find a suitable VM configuration when creating a VM, contact technical support.\n\nIt is not recommended to use configurations in which the ratio of CPU and RAM is 1:1 or less than this value. Such configurations have performance bottlenecks and can be used to perform specific tasks, for example, for machine learning or object recognition.\n\n\n\n\n\nAn instance is not being created\n\nIf an error occurred during the VM creation process, pay attention to the pop-up window in the upper right corner of the VK Cloud panel, which displays an error message.\n\nIf the message does not appear, and the creation wizard reports an error, contact technical support.\n\n\n\n\n\nThere are not enough quotas when creating a VM\n\nFree up resources on the project or contact technical support, by informing the project, account details, as well as the amount of resources needed to add to the project.\n\n\n\n\n\nI can't create a VM with Windows 7/8/10\n\nClient operating systems of the Windows family, such as Windows 7/8/10, cannot be used in VK Cloud. This restriction is set for all projects and cannot be lifted.\n\n\n\n\n\nGraphic elements are poorly processed on the instance\n\nIn the virtualization system, CPU resources are used for graphics processing, which are not intended for processing graphic elements that require a video driver, so the quality may differ from similar local devices.\n\n\n\n\n\nHow do I recover my private key?\n\nIf you lose the private key that was used to access the VM over SSH, you need to create a new key pair and add the public key to the VM manually. For more information, see the article VM Management.\n\n\n\n\n\nOpenstack CLI does not connect\n\nYou can connect to the Openstack CLI using the configuration file. Information about installation, configuration and connection parameters is given in CLI chapter.\n\n\n\n\n\nHow do I go to the virtual server console?\n\nThe VNC console is available on the virtual machine page in the section Cloud Computing → Virtual Machines. For more information, see the article VM diagnostics.\n\n\n\n\n\nThe instance console is not displayed in the VK Cloud panel\n\nMake sure you are using the latest version of the browser. Clear the cache if necessary.\n\n\n\nThe usual keyboard shortcuts, audio transmission and clipboard are not available in the console.\n\n\n\n\n\n\n\nIs it possible to increase CPU or RAM?\n\nYes. If the machine has already been created, change its type.\n\n\n\n\n\nHow to track VM performance?\n\nOn the tab Monitoring on the page of the created VM.\n\n\n\n\n\nHow does VM scaling work?\n\nScaling a VK Cloud virtual machine goes through the steps:\n\nVM stops.\nCPU, RAM, or HDD are added to the virtual machine.\nThe VM is being restarted.\n\nBilling iteration occurs once an hour — during this time, the calculation of the cost of resources will change.\n\n\n\n\n\nCan I set a floating IP address for a VM?\n\nYou can assign an existing floating IP address to the VM, or add a new address manually.\n\n\n\nThe assignment of a new floating IP address occurs randomly.\n\n\n\n\n\n\n\nWhat is the difference between VPS and VDS?\n\nThere is no visible difference.\n\nProviders offer one service in a complex — the rental of a virtual VPS/VDS server. Choose how many and what resources are needed for the operation of web services: the number of processors, storage sizes, type of drives, operating systems and other parameters. The provider will make sure that you receive them in full upon request.\n\n\n\n\n\nHow is virtualization implemented?\n\nThe provider deploys a virtualization environment on physical servers, in which there are many VM clients. VMs are isolated from each other, clients access them remotely through encrypted connections. VK Cloud implements virtualization based on KVM + OpenStack with its own improvements.\n\n\n\n\n\nIs it possible to change the configuration of the cloud server after connection?\n\nYes, you can. This process is accompanied by a reboot of the virtual machine.\n\n\n\n\n\nIs it possible to place a VM and a disk in different data centers to increase fault tolerance?\n\nIt is highly not recommended to place a virtual machine and disks to it in different data centers, as this may affect the stability and performance of the VM as a whole. When creating a VM, place the disk and VM in the same availability zone.\n\n\n\n\n\nWhy do write-offs continue, even though the VM is stopped?\n\nIf the VM is stopped, then write-offs continue for the following services:\n\nuse of licenses (Windows and RDS, if activated);\ndisk space rental;\nstorage of existing backups.\n\n\n\n\n\nHow do VPS/VDS cloud servers differ from dedicated servers?\n\nFrom the user's point of view, servers in the cloud are no different from a dedicated physical one: you also get root rights, access to network settings, can perform any actions on files, install and configure any necessary software.\n\nTo get full control over the cost of the service, it is better to rent a VPS/VDS. You can create or destroy VMs in minutes, depending on current needs, increase or decrease their power without stopping (without downtime). Dedicated servers do not have the flexibility that a cloud VPS/VDS has, which leads to underutilization of resources and the risk of applications crashing at peak loads.\n\n\n\n\n\nWhy can't my VMs in VK Cloud see my network outside the cloud?\n\nNetwork connection (VPN) must be configured between the networks. Learn more about creating a VPN between the VK Cloud network and an external network in the article Setting up a VPN tunnel.\n\n\n\n\n\nHow many times can I create and delete virtual machines?\n\nThe operation of creating and deleting resources can be performed an unlimited number of times.\n\nDisks and images\n\n\n\nHow do I increase the disk?\n\nYou can enlarge the disk using the VK Cloud panel in the Virtual Machines or Disks section of the Cloud Computing service.\n\nFull information is available in the article about disk expansion.\n\n\n\n\n\nUnable to shrink disk\n\nIn the VK Cloud platform, only an increase in the disk size is available.\n\n\n\n\n\nWhy is my disk running slowly?\n\nDisk performance may be affected by factors:\n\nbackground processes of the operating system;\nmechanisms of backup operation;\nautomatic updates (relevant for Windows);\nrunning third-party software.\n\nIf these factors are missing, collect disk performance statistics using one of the following methods:\n\ndisk utilization — using utility iostat;\nload average — using utility top.\n\nCompare the obtained indicators with the guaranteed disk performance provided by the VK Cloud platform. If there are significant deviations, contact technical support.\n\n\n\nTo increase performance, you can increase the size or change disk type.\n\n\n\n\n\n\n\nDoes the migration process affect disk performance?\n\nAt the time of migration, there may be a decrease in read-only performance, but usually there is a sufficient margin for reading performance, and the decrease is imperceptible.\n\n\n\n\n\nI can't delete a disk\n\nMake sure that the disk is disabled from VM — after that, delete the disk in the Cloud Computing → Disks section.\n\n\n\n\n\nHow do I transfer a VM disk to another project?\n\nUse the instructions.\n\n\n\n\n\nHow do I upload my OS image?\n\nThe VK Cloud platform allows the creation of virtual machines from previously prepared and uploaded images. Image preparation consists of installing the necessary set of software components and drivers to work in cloud provider services, for more details in the articles Hyper-V VM Migration to VK Cloud and VMware VM Migration to VK Cloud.\n\n\n\n\n\nHow do I change the VM's root disk?\n\nReplacement of the root disk is possible only if another disk has already been created in the VK Cloud project. It can be either an empty disk or a boot disk containing the operating system. Instructions are given in the article about replacing the root disk.\n\n\n\n\n\nIs it possible to create a disk snapshot?\n\nCreating a disk snapshot is available from VK Cloud personal account or OpenStack CLI. The created snapshot will be stored until the disk itself is deleted.\n\n\n\n\n\nHow do I restore a disk from a snapshot?\n\nUse the instructions.\n\n\n\n\n\nHow fast does the disk type change?\n\nThe disk type changes at the following rate:\n\n70MB/s: if the disk is connected to the VM.\n250MB/s: if the disk is disconnected from the VM.\n\nYou cannot change the type of disk attached to a disabled VM.\n\n\n\n\n\nAre copies of disks stored after conversion to another data center?\n\nWhen converting a disk to another data center, a mirror is created into which the data of the original disk is loaded. After conversion, such mirrors are not saved.\n\n\n\n\n\nIs it possible to expand all disks at once, or does each need to be expanded separately?\n\nYou can expand all disks at the same time.\n\n\n\n\n\nWill the VM be rebooted when the disk size changes?\n\nThe VM disk size changes during operation, without restarting the VM.\n\nFile storage\n\n\n\nWhat is file storage intended for?\n\nNetwork file storage that operates over the NFS or CIFS protocol is designed for sharing resources.\n\nThis service allows you to create a remote file system, mount the file system on virtual machines, and then read and write data from instances to and from the file system.\n\n\n\n\n\nIs it possible to use an SSD drive instead of HDD when creating an NFS file storage?\n\nNo, such possibility is not provided.\n\n\n\n\n\nIs it possible to use file storage between projects?\n\nNo, such functionality is not provided.\n\n\n\n\n\nIs it possible to restore file storage from a snapshot?\n\nYes, there is such a possibility. In this case, the storage will be restored to a separate VM, this will require additional quotas.\n\nLearn more about creating snapshots in the article File Storage management.\n\n\n\n\n\nIs it possible to create snapshots of the file storage on a schedule?\n\nNo, there is no such possibility.\n\nA snapshot of the file storage can be created manually via personal account VK Cloud or using the API.\n\n\n\n\n\nDo file storages have any agents to control free space?\n\nNo, there are no such agents.\n\n\n\n\n\nIs it possible to use file storage over the SCSI protocol?\n\nWork with this protocol is not provided.\n\n\n\n\n\nIs it possible to connect file storage via VPN in another cloud?\n\nThere is no such possibility.\n\n\n\n\n\nWhat is the maximum amount of file storage?\n\nThe maximum amount of file storage is 50TB.\n\n\n\n\n\nIn which availability zone are file storages created?\n\nRepositories are created in availability zones GZ1 (Moscow region) and QAZ (Kazakhstan region).\nAvailability zones of repositories depend on the region of the project:\n\nGZ1 for Moscow region;\nQAZ for Kazakhstan region.\n\n\n\n\n\nIs it possible to configure simultaneous access from different VMs to the file storage?\n\nYes, you can, for more information, see the article File Storage management.\n\nLicensing\n\n\n\nHow do I get a Windows Server license?\n\nA licensed copy of the Windows Server operating system is preinstalled in the Windows-based VM being created. Activation of the licensed copy occurs automatically after the VM is created. If an OS activation error occurs, contact technical support with the virtual machine ID.\n\n\n\n\n\nWhat licenses can I get?\n\nThe list of standard licenses provided is limited, you can see the list available in the price list.\n\n\n\n\n\nWhat types of licensing are there?\n\nMicrosoft provides several licensing models that allow you to make the most optimal use of your budget:\n\nTo the core. This licensing model provides access to an unlimited number of users or devices.\nPer user. It is designed to provide access to the license to one user for an unlimited number of servers.\n\nAccording to Microsoft licensing rules, the “per core” option implies the need to cover each VM virtual core with a license. Regardless of the number of VM cores, licenses are subject to every 2 virtual CPUs, for more information, see the article Microsoft.\n\n\n\n\n\nCan I get a discount?\n\nThere are no discounts on licensing.\n\n\n\n\n\nCan I use my license?\n\nYes, learn more about using your own licenses.\n\n\n\n\n\nCan I buy a license on an indefinite basis?\n\nVK Cloud provides license rentals on a monthly basis. The purchase of a license for permanent use, both on the VK Cloud platform and outside it, is not possible.\n\n\n\n\n\nHow much does an RDS license cost?\n\nThe cost of the license is given in the price list.\n\n\n","url":"/en/computing/iaas/faq","description":"Answers to frequently asked questions about the Cloud Servers service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Servers","link":"/en/computing/iaas","isActive":false,"evenSimpler":true}]},{"uuid":"b2a6e660-59c6-4de3-be83-e09aac1fb14e","title":"About service","data":"What tasks does\n\nMachine learning\n\nNVIDIA® Tesla® GPUs are great for training and inferencing deep neural networks with high performance.\n\nThe available GPU:\n\nA100 40GB;\nA100 80GB;\nV100 16GB;\nV100S 32GB.\n\nHigh Performance Computing (HPC)\n\nThe solution of tasks that require large computing power is easier to carry out using cloud computing on the GPU. This reduces the time for research and waiting for results.\n\nRender graphics on the server side\n\nServer-side graphics rendering speed with the most advanced NVIDIA® Tesla®, far exceeds the rendering speed of a conventional graphics card.\n\nThe time to solve problems in deep learning, graphical visualization and high-performance computing can be reduced from days to hours. Whether training neural networks, solving complex design problems or cutting-edge research, get the best performance with NVIDIA® Tesla® and VK Cloud.\n\nGPU-accelerated computing in the cloud\n\nNew generations of scientific applications and tasks related to artificial intelligence and machine learning require an enormous amount of computing power that even modern CPU servers cannot cope with. To successfully solve such problems, GPU-accelerated servers are used.\n\nDepending on the type of tasks, 1 modern GPU server can replace up to 100 conventional CPU servers. This is achieved through a different way of performing tasks. The CPU consists of several cores optimized for sequential processing, while the GPU uses thousands of smaller cores to efficiently process parallel tasks.\n\nGPU-based cloud computing will take you to a new level of performance and significantly reduce data processing time, while saving your company money.\n\nVK Cloud provides NVIDIA® Tesla® graphics cards with 16 GB of VRAM.\n\nSoftware\n\nVK Cloud recommends using the following GPU-enabled development software:\n\nCUDA Toolkit — its documentation is available on the official resource.\nDeepo — documentation and distribution are available on its official resource.\nNVIDIA GRID — Datasheet is available on the official resource.\n\nBilling\n\nBilling per second. Rent and deploy computing power in minutes and turn it off as soon as you complete your tasks. Pay only for the actual use of resources with per-second billing.\n\nCurrent tariffs can be seen here.\n","url":"/en/computing/gpu/about","description":"Basic operations for Cloud GPU virtual machines.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud GPU","link":"/en/computing/gpu","isActive":false,"evenSimpler":true}]},{"uuid":"1c885b81-01fb-4939-a28c-26f19383fb15","title":"Connecting the service","data":"Before using, you must leave a request on the website. Click \"Apply\". Fill out the form and click Submit.\n\nUpon successful submission of the application, a message will appear informing you that the application has been submitted successfully.\n\nAfter reviewing the application and conducting the necessary consultations, the manager will connect the service.\n","url":"/en/computing/gpu/connect","description":"Instructions for connecting the service","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud GPU","link":"/en/computing/gpu","isActive":false,"evenSimpler":true}]},{"uuid":"57df1ccf-3daa-4bfa-85fc-9d94d3d0c076","title":"Quick start","data":"This quick start will help you get started with the service and get acquainted with its capabilities.\n\nAfter completing all the steps of the quick start, you will:\n\nSet up a connection to an existing OpenLDAP user directory.\nGet acquainted with Cloud Desktop and its main features:\n\n    Create a pool of desktops.\n    Verify that two desktops are successfully deployed in the pool.\n\n1. Preparatory steps\n\nCreate and configure the OpenLDAP user directory service (an example).\nFind out the details of the configured directory:\n\n   IP address or FQDN of the OpenLDAP server\n   Unique account name\n   Account password\n\n(Optional) Set up a VPN on the router that will be used for the service.\nCheck that the Cloud Desktop service is connected to your personal account: the Cloud Desktop section is available in the menu on the left.\n\n2. Set up an external infrastructure\n\nGo to your VK Cloud personal account.\nSelect the project where the desktops will be placed.\nGo to Cloud Desktop → Service settings.\nOn the Network settings tab, specify:\n\n    IP address space: a range of IP addresses for the subnets where the service resources will be deployed.\n    Router: a router with a VPN network.\n    DNS: the IP address of a DNS server.\n    Availability zone: Moscow (MS1).\n\nClick the Save button.\nOn the Directory settings tab, specify:\n\n    Host: the IP address or FQDN of the OpenLDAP server.\n    Port: 389.\n    Base DN: the root directory to search for objects in AD or LDAP.\n    User DN: the full path to the account in the OpenLDAP database which will be used for synchronization with the accounts directory.\n    Password: the password of the OpenLDAP account specified in User DN.\n    Selection field: UID.\n\nClick the Save button.\n\n3. Create a pool of desktops\n\nGo to Cloud Desktop → Desktop pools.\nClick the Add button.\nAt the “Configuration” step, specify:\n\n    Pool name: for example, vk-cloud-desktops-pool-quickstart.\n    Pool type: Sessional.\n    User group: specify one group from the OpenLDAP directory.\n    Naming mask: vk_desktop-.\n    Min number of desktops: 1.\n    Max number of desktops: 2.\n    Hot reserve: 1.\n    User inactivity timeout: 5.\n\nClick the Next step button.\nAt the “Virtual machine settings” step, specify:\n\n    Instance type: STD1-1.\n    Disk size: 10.\n    Disk Type: HDD.\n    Image: Astra Linux SE 1.7.2 Орел GUI.\n    Security group vk-cloud-desktops-pool-quickstart: default.\n\nClick the Next step button.\nAt the “Peripherals setup” step, leave the parameters unchanged.\nClick the Create a pool button.\n\n   The creation of the pool of desktops will begin. This process can take a long time.\n\n4. Make sure that desktops are deployed in the pool\n\nGo to Cloud Desktop → Desktop pools.\nIn the list of pools, click the pool name vk-cloud-desktops-pool-quickstart.\nGo to the Desktops tab.\n\n   The value 2 should be displayed in the Deployed block.\n\n5. Check desktop connectivity\n\nWrite down the external IP address for connecting to the pool:\n\n    Go to Cloud Desktop → Desktop pools.\n    In the list of pools, click Connection IP address.\n    In the list that opens, copy the External IP address value.\n\n    In the example, the IP address is 99.166.240.100.\n\nDownload and install the Cloud Desktop client on your local computer.\nConnect to the pools:\n\n   Open the installed client.\n   Click the Add Server button.\n   In the window that opens, enter the 99.166.240.100 address and click the Ok button.\n   Specify the AD user login and password and click the Ok button. If the connection is successful, a list of pools will be displayed.\n\nConnect to the vk-cloud-desktops-pool-quickstart pool.\nIn the XRDP connection window that opens, re-enter the AD user login and password.\n\n   Upon successful connection to the desktop, the Astra Linux operating system interface will open.\n\nDelete unused resources\n\nA running service consumes computing resources. If you don't need it anymore, delete the desktop pool.\n\nWhat's next?\n\nLearn about the concepts of the virtual desktop service.\nRead the instructions for working with virtual desktops.\n","url":"/en/computing/cloud-desktops/quick-start","description":"Getting started with the Cloud Desktop service and getting to know its main features.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"ff0745b9-87ee-4947-875e-716048df330e","title":"About service","data":"The Cloud Desktop service is based on the VDI (Virtual Desktop Infrastructure) technology and is designed to provide users with desktops created on a remote server and functioning in a virtualized environment.\n\nWhat tasks is the service suitable for?\n\nThe Cloud Desktop service is suitable for creating or upgrading the communication infrastructure of an enterprise or a network of enterprises where different people can use one workplace.\n\nTasks that the service solves:\n\nDeployment of desktops and their further administration through pools.\nManaging desktop pools and load balancing in them.\nOrganization of remote desktops for users and ensuring continuous access to them.\nProtection of desktops from unauthorized access.\n\nService features\n\nSupport for guest operating systems on which cloud desktops will be launched: Windows OS, Linux OS.\nDirectory services support: AD and LDAP.\nSupport for data transmission over the RDP protocol.\nSupport for two types of workplaces: personalized and sessional.\nMonitoring the use of peripheral devices.\nMaintenance of more than 1000 simultaneous user connections.\nCreating and configuring VMs on the virtualization platform from a pre-prepared master image.\nSupport of fault tolerance mechanisms for servers that provide management, licensing, storage and accounting for configuration changes.\n\nRole model\n\nThe following roles are provided in the service:\n\nAdministrator.\n\n  Manages the service through the VK Cloud personal account. Configures integration with the directory service, creates desktop pools, and assigns desktops to users.\n\nUser.\n\n  End user of cloud desktops. Independently installs a client application to access the desktop.\n\nAvailable peripherals\n\n| Peripheral device | Windows OS | Linux OS |\n| --- | --- | --- |\n| Print | + | + |\n| Files | + | + |\n| Microphone | + | + |\n| Clipboard | + | + |\n\nThe available peripherals are set when adding a desktop pool.\n\nWhat's next?\n\nGet acquainted with the basic concepts of the service.\nGo through the Quick start.\n","url":"/en/computing/cloud-desktops/concepts/about","description":"Overview of the Cloud Desktop VK Cloud service: capabilities, role model, available peripherals.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"57bd6b7a-10bb-4d21-8012-4d45e33ef94a","title":"Terminology","data":"Basic concepts of the Cloud Desktop service:\n\nDesktop** — a virtual workplace with software installed on it. The desktop is created within the desktop pool and is assigned to a user by the service administrator.\nDesktop pool** — a collection of multiple desktops that share computing resources. Through the desktop pool, you can set general settings for desktops. Desktops in this pool can be created from different templates depending on the tasks being performed.\nDesktop template** — a template for creating VMs and desktops. In the template, the desktop OS and the virtualization service component must be installed and configured.\nDirectory service** — a software package designed for hierarchical storage, organization and provision of access to information. Interaction with such services is carried out through protocols, the most common is LDAP. The Cloud Desktop service supports integration with Active Directory (AD) and OpenLDAP directory services.\n","url":"/en/computing/cloud-desktops/concepts/glossary","description":"Basic concepts and objects of the Cloud Desktop service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"25a7cf4b-f248-44ab-a7d4-aa73e6fe3eb3","title":"Custom desktop images","data":"A desktop image is an image of a virtual machine on which, in addition to standard program components, software is installed to support VDI technology, as well as applications required for the end user's work. The Cloud Desktop service provides ready-to-use images for deploying desktops. You can also upload your own images and use them in the service.\n\nYou can prepare a custom desktop image in any available way, for example, using one of the How-to guides of the Cloud Servers service.\n\nRequirements for a custom desktop image\n\nThe following software must be installed on the image:\n\nOS:\n\n  Windows of any version that supports connection to the Active Directory service.\n  Astra Linux «Orel» with graphical interface installed.\n\n  To use other operating systems, please contact technical support.\n\nQEMU guest agent.\nSoftware for cloud-based virtual machine initialization:\n\n  For Astra Linux — the cloud-init package.\n  For Windows — the Cloudbase-Init application.\n\nApplications required for your users' work.\n\nRecommendations for preparing a custom desktop image\n\nIt is recommended to install on the image:\n\n(For Astra Linux) additional software for supporting the RDP protocol and the AD service. This will speed up the deployment of desktops in a pool.\nComponents of the Termidesk software, which speeds up the connection of users to pool desktops.\n\n    Windows\n    Astra Linux\n\n\n    To install Termidesk, run the commands in Windows PowerShell:\n\n        Invoke-WebRequest -Uri https://repos.termidesk.ru/windows/windows_x86_64/termidesk-agent_4.3.2.24030_x64.msi -OutFile $env:TEMP\\termidesk-agent.msi\n    Start-Process msiexec -ArgumentList \"/i \"$env:TEMP\\termidesk-agent.msi\" /qn\" -Wait -NoNewWindow\n    Remove-Item $env:TEMP\\termidesk-agent.msi\n\n\n    To install Termidesk, run the commands in the terminal:\n\n        apt update && apt install -y curl lsb-release spice-vdagent xserver-xorg-video-qxl xrdp\n    echo \"deb https://repos.termidesk.ru/astra $(lsb_release -cs) non-free\" > /etc/apt/sources.list.d/termidesk.list\n    curl https://repos.termidesk.ru/astra/GPG-KEY-PUBLIC | apt-key add -\n    apt update && apt install -y 'python3-termidesk-agent=4.*' termidesk-pcsc-vscard termidesk-video-agent astra-ad-sssd-client\n\n\nWhat's next?\n\nLearn how to check your own desktop image for compatibility with the Cloud Desktop service.\n","url":"/en/computing/cloud-desktops/concepts/desktop-image","description":"Features of preparing your own desktop image","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"75f4f2e3-57f2-4803-a20d-98a94d2e7846","title":"Setting up the service","data":"Configuring the network for the service infrastructure\n\nGo to your VK Cloud personal account.\nGo to Cloud Desktop → Service settings.\nOn the Network settings tab, set the parameters:\n\n   IP address space: specify a range of IP addresses for the subnets where the service resources will be deployed. Parameter requirements: format — CIDR, minimum network prefix — /7, maximum — /22.\n   Router: select a router from the list.\n   DNS: specify the IP address of a DNS server. To add more addresses, click Add DNS and specify an additional IP address.\n   Availability zone: select the availability zone of the pool virtual machines from the list.\n\nCLick the Save button.\n\nConfiguring the connection to the user directory\n\nThe connection to the user directory is configured separately for each project. Only one connection can be configured for one project.\n\n\n\nCloud Desktop cannot work without the AD/LDAP user directory service. If your network does not have a directory service, create one yourself.\n\n\n\nTo set up a connection to the user directory:\n\nGo to your VK Cloud personal account.\nGo to Cloud Desktop → Service settings.\nOn the Directory settings tab, set the parameters:\n\n   Host: specify the IP address or FQDN of the external account directory server.\n   Port: specify the port number for protocol access (389 for LDAP).\n   Base DN: set the root directory to search for objects in AD or LDAP.\n   User DN: specify the full path to the account in the OpenLDAP database which will be used for synchronization with the account directory.\n   Password: set the password for the AD account specified in User DN through which the synchronization will take place.\n   Selection field: set a unique user attribute in the LDAP directory by which the user will be selected from the database (for example, UID or sAMAccountName).\n\nCLick the Save button.\n\n\n\nWhen working with the service, you can change the settings for connecting to the user directory.\n\nAll existing pools will switch to the new connection settings automatically.\n\nChecking network settings\n\nTo check the network settings, it is not necessary to save the changes in the settings.\n\n\n\nA virtual machine will be deployed for verification, its name begins with vdi-checker. The use of this VM is charged.\n\n\n\nTo check if the service network settings are correct:\n\nGo to your VK Cloud personal account.\nGo to Cloud Desktop → Service settings.\nOn the Network settings tab, enable the Network settings check option.\nClick Check connection.\nWait for the operation to complete.\n\n   To see the detailed result of the check, click Details.\n\nChecking the directory connection settings\n\nTo check the connection settings to the user directory, it is not necessary to save the changes in the settings.\n\n\n\nA virtual machine will be deployed for verification, its name begins with vdi-checker. The use of this VM is charged.\n\n\n\nTo check if the service catalog settings are correct:\n\nGo to your VK Cloud personal account.\nGo to Cloud Desktop → Service settings.\nOn the Directory settings tab, enable the Directory settings check option.\nClick Check connection.\nWait for the operation to complete.\n\n   To see the detailed result of the check, click Details.\n","url":"/en/computing/cloud-desktops/service-management/config","description":"Configuring the service connection to the AD or LDAP user directory.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"a4eb68cd-055b-484c-8417-a12e88807a09","title":"Creating a pool","data":"\n\nChanging created virtual resources (VMs, load balancers, DB instances) outside the service can lead to a failure of individual desktops or the entire pool as a whole.\n\nPreparatory steps\n\nMake sure that your connection to the service is configured.\n(Optional) If you want to use your own image to create desktops, prepare and check it.\nRun the pool creation wizard:\n   Go to your VK Cloud personal account.\n   Go to Cloud Desktop → Desktop pools.\n   Click the Add button.\n\n   The wizard for creating a new pool opens.\n\n1. Set up the pool configuration\n\nSet the pool configuration parameters:\n\n    Pool name: set a name that is unique within the project. Field requirements: from 3 to 128 characters, only Latin letters, characters -, _, spaces, and numbers are allowed.\n    Pool type: choose one of the options:\n\n        Sessional — in such a pool, the user is automatically assigned a desktop upon connection. Use the session pool if you need to deploy a large number of desktops of the same configuration with the same set of software. The session pool has hot reserve of desktops.\n        Personalized — in such a pool, the administrator assigns desktops to specific users. Suitable for deploying desktops with individual configuration for individual users. This type of pool is used most often.\n\n    Description: the pool description, up to 250 characters.\n    User group: specify the user group of the AD or LDAP directory. The users in this group will be assigned desktops from the pool. Field requirements: from 3 to 100 characters, only Latin letters, characters -, _, ., and numbers are allowed.\n    OU: specify the organizational unit of the AD or LDAP directory. For example, OU=Workstations,DC=domain,DC=com.\n    Naming mask: the suffix with which the desktops names in this pool will begin. Field requirements: from 3 to 15 characters, only lowercase Latin letters and the - character at the end are allowed.\n    Min number of desktops — determines how many desktops will be deployed immediately after the pool is created. The default is 0.\n\n      This parameter cannot exceed the value of the parameter Max number of desktops.\n\n    Max number of desktops — determines how many desktops in the pool can be used at the same time.\n    Hot reserve — available only for session pools. Hot reserve desktops become available only at the moment when the user connects and return to the reserve when the user session ends.\n\n      There will be no available desktops in the hot reserve if the value 0 is selected or the parameters Min number of desktops and Max number of desktops are equal.\n\n    User inactivity timeout: the time after which the user will be disconnected from the desktop if there is no activity on the desktop. The timeout works for the RDP and RX protocols.\n\n      If the value is 0, the user session will not be interrupted by time-out.\n\nClick the Next step button.\n\n2. Configure the pool VMs\n\nSpecify the settings of the virtual machines on which the pool desktops will be created:\n\n    Instance type: select the VM type from the list of types available in the project.\n    Disk size: specify the disk size in GB. The value cannot be less than the size of the OS image.\n    Disk Type: choose one of the values — HDD, SSD or High-IOPS SSD.\n    Image: select an image to create the VMs from the list of available images. If necessary, click Upload your image and download the previously prepared and checked image following the import instructions.\n\n\n      The pool cannot be created until the image download is complete.\n\n\n    *Security group *: specify the security groups for the VMs. If you do not specify a value, the group will not be assigned.\n\nClick the Next step button.\n\n3. Configure the peripherals\n\nSelect peripheral devices that will be available on the pool desktops.\nClick the Create a pool button.\n\n   The pool will be created within an hour.\n\nIf the pool is created successfully, a list of created pools will open.\n","url":"/en/computing/cloud-desktops/service-management/desktops-pool/add","description":"Creating a new pool of desktops in the Cloud Desktop service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"88c3b9cf-e41f-45be-a8a0-edff404981a3","title":"Managing a desktop pool","data":"\n\nYou can also manage all desktops created in the system for all pools.\n\nViewing a list of desktop pools\n\nGo to VK Cloud personal account.\nGo to Cloud Desktop → Desktop pools.\n\n   A list of previously created desktop pools is displayed.\n\n   The status of the pool is displayed to the left of its name:\n\n   Green — Available for connecting new desktops.\n   Gray — Unavailable for connecting desktops.\n\nManaging pool desktops\n\nReboot\n\n\n\nAvailable for desktops, all VMs of which are in an emergency state.\n\n\n\nThis is a group operation: if necessary, you can manage multiple pool desktops by selecting them using checkboxes.\n\nGo to VK Cloud personal account.\nGo to Cloud Desktop → Desktop pools.\nIn the list of pools, click the name of the required pool.\nGo to the Desktops tab.\nCheck the box for the required desktop.\nClick More and select the option Restart.\nConfirm the action.\n\nLogging out of a session\n\nIt is used to force the termination of the user's session.\n\n\n\nThe VM remains turned on at the end of the session.\n\n\n\nThis is a group operation: if necessary, you can manage multiple pool desktops by selecting them using checkboxes.\n\nGo to VK Cloud personal account.\nGo to Cloud Desktop → Desktop pools.\nIn the list of pools, click the name of the required pool.\nGo to the Desktops tab.\nCheck the box for the required desktop.\nClick More and select the option End session.\nConfirm the action.\n\nSending a message to pool users\n\nThis is a group operation: if necessary, you can send a message to users of several pools by selecting them using checkboxes.\n\nGo to VK Cloud personal account.\nGo to Cloud Desktop → Desktop pools.\nCheck the box for the required pool.\nClick the Message to users button.\nIn the window that appears, fill in the fields:\n\n   Message type: select one of the options: Warning, Informational or Error.\n   Message text: specify the information you want to convey to the pool users.\n\nClick the Send message button.\n\nDeleting a desktop pools\n\n\n\nWhen deleting a desktop pool, pay attention to the following features:\n\nThe deleted pool cannot be restored.\nIf there are desktops in the pool, they will also be deleted when the pool is deleted.\nWhen deleting the last pool, all the settings for connecting to the user directory will also be deleted.\n\n\n\nThis is a group operation: if necessary, you can delete several pools at once by selecting them using the checkboxes.\n\nGo to VK Cloud personal account.\nGo to Cloud Desktop → Desktop pools.\nDelete the pool using one of the following methods:\n\n   Click   for the pool and select Delete.\n   Via the pool page:\n\n     In the list of pools, select the pool you want to delete and click its name.\n     Go to the Parameters tab.\n     Click the icon Trash in the upper right corner of the tab.\n\nConfirm the deletion.\n","url":"/en/computing/cloud-desktops/service-management/desktops-pool/manage","description":"Managing a created desktop pool in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"61305639-bf12-4684-9827-1200de0f59d9","title":"Desktops management","data":"When a pool is successfully created, desktops are created in it automatically.\n\nViewing desktop information\n\nGo to your VK Cloud personal account.\nOpen the list of desktops using one of the following methods:\n\n   Through the general list of desktops: go to Cloud Desktop → Desktop list.\n   Through the list of pool desktops:\n\n     Go to Cloud Desktop → Desktop pools.\n     In the list of pools, click the name of the required pool.\n     Go to the Desktops tab.\n\nThe desktop status is displayed in the Availability column. Column Connection (Active/Inactive) shows whether the user is currently connected to the desktop.\n\nStarting or stopping the desktop\n\nThis is a group operation: if necessary, you can manage multiple pool desktops by selecting them using checkboxes.\n\nGo to your VK Cloud personal account.\nOpen the list of desktops using one of the following methods:\n\n   Through the general list of desktops:\n\n     Go to Cloud Desktop → Desktop list.\n     Check the box for the required desktop.\n\n   Through the list of pool desktops:\n\n     Go to Cloud Desktop → Desktop pools.\n     In the list of pools, click the name of the required pool.\n     Go to the Desktops tab.\n     Check the box for the required desktop.\n\nClick Start or Stop.\nConfirm the action.\n\n\n\nThe Start button is active only if all VMs for the selected tables are disabled. The Stop button is active only if all VMs for the selected tables are enabled.\n\nEnding the user session\n\nThe state of the desktop after the end of the session is determined by the Actions at the end of session option, set when creating the pool.\n\nTo end the user session, log out of the system:\n\n\n\nAstra Linux\nWindows\n\n\n\nOn the desktop, go to Пуск.\nClick the Завершение работы....\nIn the window that opens, click Выход из сессии.\n\n\n\n\nOn the desktop, go to Пуск.\nClick the name of the current user and select Выход.\n\n\n\nYou can also use the logoff command to end the session.\n\n\n\n\n\n\n\n\nIf the session has been terminated in another way, the disconnection will occur according to the time-out set when creating the pool.\n\nDeleting the desktop\n\nThis is a group operation: if necessary, you can delete multiple pool desktops by selecting them using checkboxes.\n\nGo to your VK Cloud personal account.\nOpen the list of desktops using one of the following methods:\n\n   Through the general list of desktops:\n\n     Go to Cloud Desktop → Desktop list.\n     Check the box for the required desktop.\n\n   Through the list of pool desktops:\n\n     Go to Cloud Desktop → Desktop pools.\n     In the list of pools, click the name of the required pool.\n     Go to the Desktops tab.\n     Check the box for the required desktop.\n\nClick the Delete button.\nConfirm the action.\n","url":"/en/computing/cloud-desktops/service-management/manage-desktops","description":"Starting, stopping and deleting cloud desktops, working with a user session.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"6add46ec-e129-43a3-bff7-c0a96bb6bf64","title":"Removing the service infrastructure","data":"Will be deleted:\n\nservice infrastructure: virtual machines, load balancers;\nservice connection settings;\nvirtual machines deployed to check settings.\n\nWill not be deleted:\n\nVDI service network connected to VPN;\nnetworks created by the user.\n\n\n\nIt is impossible to cancel the deletion operation!\n\n\n\nTo remove infrastructure and settings:\n\nGo to your VK Cloud personal account.\nGo to Cloud Desktop → Service settings.\nClick Delete VDI.\nIn the window that opens, mark the consent and click Delete.\n","url":"/en/computing/cloud-desktops/service-management/delete-vdi","description":"Deleting the infrastructure and connection settings of the Cloud Desktop service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"49281805-2600-414d-9002-e868c3394d54","title":"IPsec for VDI","data":"To organize a VPN tunnel based on VK Cloud, you need to specify one or more existing subnets located behind the router.\nThis does not allow you to configure a VPN connection for VDI, since subnets for VDI are created dynamically in a predefined address space. For successful integration with a VDI environment, a VPN tunnel can be organized using a dedicated VPN server based on an IaaS VM (virtual machine). This will allow you to organize secure connectivity up to the CIDR specified in the VDI settings.\n\nTo show how to configure a secure connection with virtual desktops:\n\nA VPN tunnel will be created between the LDAP environment and the VDI environment in VK Cloud.\nOn the LDAP side, the built-in VK Cloud VPN service will be used as a VPN solution. In a real-life scenario, you can use any hardware or software with the ability to create a VPN connection using IPsec Site-2-Site technology.\nOn the VDI side, a VM based on Debian 11 OS and additional strongSwan packages will be used as a VPN solution.\nA transit network will be created that will connect the Debian 11-based VPN gateway to the platform router, behind which VDI networks will be formed.\nTo simulate VDI, a network with a VM will be added. The VM will ping the LDAP server to check network connectivity.\n\nPreparation steps\n\nMake sure the OpenStack client is installed and authenticate in the project.\n\nCreate an LDAP environment:\n\n    Select or create a virtual network with the Internet access in VK Cloud. Use an existing router with a connection to an external network or create a new one.\n\n        Write down the following:\n\n        The subnet IP address.\n        The router name and IP address.\n\n    Create a virtual machine running Windows Server 2019 on the selected network.\n\n         Write down the name and IP address of the VM.\n\nCreate a VDI environment:\n\n    In VK Cloud, select or create a virtual network with the Internet access, which will serve as a transit network. Use an existing router with a connection to an external network or create a new one.\n\n        Write down the following:\n\n        The subnet IP address.\n        The router name and IP address.\n\n    Pre-allocate a CIDR for the network simulating VDI. CIDR will be used to configure the VDI environment.\n\n        Write down the subnet IP address (CIDR).\n\nCreate a virtual machine connected to the external network ext-net. This will allow you to avoid using a floating IP address during further configuration.\n\n    The VM parameters:\n\n    OS: Debian 11;\n    Recommended VM type: STD3-2-4.\n\n    Write down the subnet name and IP address.\n\nUpdate the OS on the Debian VM:\n\n    Connect to the Debian-IPsec-Gate virtual machine via SSH and get root user rights (sudo bash command).\n\n    Update the OS before further setup:\n\n                apt update && apt upgrade -y\n\n    Reboot the VM using the reboot command.\n\nMake sure that you have collected all the information necessary for further work.\n\n    The following data is used in the case:\n\n    AD/LDAP environment:\n\n        Subnet IP address: 10.10.2.0/24;\n        Router name and IP address: priv-router, 146.185.241.42;\n        AD/LDAP server name and IP address: LDAP, 10.10.2.14.\n\n    VDI environment:\n\n        Subnet IP address: 10.56.0.0/24;\n        Subnet IP address for VDI (CIDR): 10.55.4.0/22;\n        Router name and IP address: vdi-router, 146.185.241.242.\n\n    Name and IP address of the virtual machine on the external network: Debian-IPsec-Gate, 212.233.72.226.\n\nScheme of preliminary preparation of networks:\n\nScheme of preliminary preparation of networks\n\n1. Set up a VPN tunnel on the AD/LDAP side\n\nIn your personal account create a VPN with the following parameters:\n\nAt the “IKE Configuration” step, specify the algorithms for the primary IPsec connection:\n\n    IKE-policy:\tNew IKE-policy;\n    Policy name:\td11-gate-ike;\n    Key lifetime: 28800;\n    Authorization algorithm:\tsha256;\n    Encryption algorithm:\taes-256;\n    IKE version: v2;\n    Diffie-Hellman group: group14.\n\nAt the “Configure IPsec” step, specify algorithms to protect payload traffic between nodes:\n\n    IPsec Policy:\tNew IPsec-policy;\n    Policy name:\td11-gate-ike;\n    Key lifetime: 28800;\n    Authorization algorithm:\tsha256;\n    Encryption algorithm:\taes-256;\n    IKE version: v2;\n    Diffie-Hellman group: group14.\n\nAt the “Creating Endpoint Groups” step, specify the networks the traffic between which will be protected by VPN:\n\n    Router: priv-router;\n    Local Endpoint: New endpoint group;\n    Name:\td11-tunnel-local-acl;\n    Subnets: priv_subnet_demo (10.10.2.0/24);\n    Remote Endpoint: New endpoint group;\n    Group name: d11-tunnel-remote-acl;\n    Subnet address: 10.55.4.0/22.\n\nAt the “Configure a tunnel” step, specify the address of the remote gateway with which the IPsec connection will be established and the pre-shared key (PSK):\n\n    Settings: Advanced;\n    Tunnel name: d11-ipsec-tun;\n    Peer IP: 212.233.72.226;\n    Pre-sared key (PSK): click the Generate button or enter the key;\n    Peer ID: 212.233.72.226;\n    Initiator state:\tbi-directional;\n    When a peer is unavailable: restart;\n    Peer unreachable detection interval: 15 seconds;\n    Time to detect peer unavailability: 60 seconds.\n\n2. Connect an additional network interface to the Debian VM\n\nTo configure a VPN tunnel on the side of the VDI environment, add an extra network interface for the Debian server to the vdi-transit-vsubnet subnet and assign the 10.56.0.4 address to it:\n\nIn your personal account go to the Cloud computing section → Virtual machines.\nSelect the Debian-IPsec-Gate VM and go to the Networks tab.\nClick the Add connection button.\nSpecify the following connection settings:\n\n    Name: vdi-transit;\n    Network for connect:\tvdi-transit-vsubnet;\n    DNS-имя:\tdebian-ipsec-gate;\n    Set IP-address: turn on the option;\n    IP address:\t10.56.0.4;\n    Firewall settings: clear all the rules.\nClick the Save button.\n\n3. Configure an additional network interface in a Debian VM\n\nConnect to the Debian-IPsec-Gate virtual machine via SSH and get root user rights (sudo bash command) .\nCreate the eth1 file using the command:\n\n        vim /etc/network/interfaces.d/eth1\n\nAdd the following to the created file:\n\n        auto eth1\n    iface eth1 inet static\n    address 10.56.0.4/24\n    mtu 1500\n    post-up ip route add 10.55.4.0/22 via 10.56.0.1 || true\n    pre-down ip route del 10.55.4.0/22 via 10.56.0.1 || true\n\n\n    The post-up и pre-down commands automate the route management to the future VDI network when connecting and disconnecting the eth1 interface.\n\n\nRun the command to apply the new network settings:\n\n        systemctl restart networking\n\nCheck that the eth1 interface is configured correctly:\n\n      ip a | grep 10.56\n\n   The interface is configured correctly if the response is returned:\n\n      inet 10.56.0.4/24 brd 10.56.0.7 scope global eth1\n   Check that the route to the 10.55.4.0/22 network is configured correctly:\n\n      ip r | grep 10.55\n\n   The route to the future VDI network is correct and is added automatically if the response is returned:\n\n      10.55.4.0/22 via 10.56.0.1 dev eth1\n\n4. Disable Port Security on the port towards the transit network\n\nDisable IP Source Guard on the VPN gateway port so it can forward any traffic:\n\nOpen a new terminal session and run the command:\n\n        openstack port list --server Debian-IPsec-Gate\n\n    The response will return a list of Debian-IPsec-Gate ports. Find the port facing the transit network:\n\n        +--------------------------------------+-------------+-------------------+-------------------------------------------------------------------------------+--------+\n    | ID                                   | Name        | MAC Address       | Fixed IP Addresses                                                            | Status |\n    +--------------------------------------+-------------+-------------------+-------------------------------------------------------------------------------+--------+\n    | 4d75dafe-d562-462a-afe9-31ede945a196 |             | fa:16:3e:34:e1:3a | ip_address='212.233.72.226', subnet_id='9ec13002-fb52-4e00-ac69-84d86a75d807' | ACTIVE |\n    | f00c7678-47c0-4d88-9be2-b5592de9112f | vdi-transit | fa:16:3e:fe:e2:26 | ip_address='10.56.0.4', subnet_id='2f50371c-4e91-4f05-aff1-33bef1388fdf'      | ACTIVE |\n    +--------------------------------------+-------------+-------------------+-------------------------------------------------------------------------------+--------+\n\nTurn off Port Security:\n\n        openstack port set --disable-port-security f00c7678-47c0-4d88-9be2-b5592de9112f\n\n5. Enable packet forwarding to the VM\n\nEnable IP Forwarding so that the virtual machine can route traffic from the transit network to the VPN tunnel:\n\nOpen a terminal session with the connected Debian-IPsec-Gate VM.\n\nRun the command:\n\n        echo 'net.ipv4.ip_forward = 1' | sudo tee -a /etc/sysctl.conf\n\nRun the command to apply the settings without rebooting the OS:\n\n        sysctl -p\n\nCheck that the settings are applied:\n\n        cat /proc/sys/net/ipv4/ip_forward\n\n    If the response returns 1, packet forwarding is enabled.\n\n6. Install packages to configure a VPN tunnel on the Debian VM side\n\nRun the command:\n\n        apt install vim strongswan strongswan-swanctl iptables iptables-persistent netfilter-persistent conntrack bmon -y\n\n    The iptables-persistent package will be used to write the configuration of the active iptables rules to a file and load a list of rules for processing traffic when the OS is restarted.\n\nEnable automatic launch of the strongswan and netfilter services:\n\n        systemctl enable strongswan-starter\n    systemctl start strongswan-starter\n    systemctl enable netfilter-persistent\n\n7. Add IPsec configuration and automate its launch\n\nCreate a swanctl configuration file to configure the VPN connection:\n\n        vim /etc/swanctl/conf.d/vkcloud.conf\n\nAdd the following content to the swanctl file:\n\n        connections {\n        vkcloud-ikev2 {\n            remote_addrs = 146.185.241.42\n            local_addrs = 212.233.72.226\n            version = 2\n            proposals = aes256-sha256-modp2048\n            dpd_delay = 15s\n            dpd_timeout = 60\n            rekey_time = 28800s\n            local-1 {\n                auth = psk\n                id = 212.233.72.226\n            }\n            remote-1 {\n                auth = psk\n                id = 146.185.241.42\n            }\n            children {\n                vkcloud-sa {\n                    mode = tunnel\n                    local_ts = 10.55.4.0/22\n                    remote_ts = 10.10.2.0/24\n                    esp_proposals = aes256-sha256-modp2048\n                    dpd_action = restart\n                    rekey_time = 14400s\n                    start_action = start\n                }\n            }\n        }\n    }\n    secrets {\n        ike-vkcloud {\n            id = 146.185.241.42\n            secret =\n        }\n    }\n\n    Here `` is the pre-shared key (PSK) created earlier.\n\nAutomate the launch of a VPN connection when the OS is rebooted:\n\n    Run the command:\n\n                vim /etc/strongswan.d/charon.conf\n\n    Find the line # Section containing a list of scripts and add the swanctl configuration command to it:\n\n                start-scripts {\n            swanctl = /usr/sbin/swanctl --load-all\n        }\n\n8. Start IPsec and check the status of the VPN tunnel\n\nRun the command to apply the new configuration settings and start the VPN connection:\n\n        swanctl --load-all\n\nCheck the loading of the VPN connection configuration:\n\n        swanctl --list-conns\n\n    Expected response:\n\n        vkcloud-ikev2: IKEv2, no reauthentication, rekeying every 28800s, dpd delay 15s\n    local:  212.233.72.226\n    remote: 146.185.241.42\n    local pre-shared key authentication:\n        id: 212.233.72.226\n    remote pre-shared key authentication:\n        id: 146.185.241.42\n    vkcloud-sa: TUNNEL, rekeying every 14400s, dpd action is restart\n        local:  10.55.4.0/22\n        remote: 10.10.2.0/24\n\nCheck the IKE/SA tunnel settings:\n\n        swanctl --list-sas\n\n    Expected response:\n\n        vkcloud-ikev2: #1, ESTABLISHED, IKEv2, e462fc2edaae6649_i* e9f38c18ddd4f0ef_r\n    local  '212.233.72.226' @ 212.233.72.226[4500]\n    remote '146.185.241.42' @ 146.185.241.42[4500]\n    AES_CBC-256/HMAC_SHA2_256_128/PRF_HMAC_SHA2_256/MODP_2048\n    established 3053s ago, rekeying in 23290s, reauth in 22543s\n    vkcloud-sa: #1, reqid 1, INSTALLED, TUNNEL, ESP:AES_CBC-256/HMAC_SHA2_256_128\n        installed 3053s ago, rekeying in 10951s, expires in 12787s\n        in  ca12c3fa,      0 bytes,     0 packets\n        out cce2ec61,      0 bytes,     0 packets\n        local  10.55.4.0/22\n        remote 10.10.2.0/24\n\n9. Set up iptables rules\n\nTo ensure traffic passes correctly through the VPN tunnel, add a number of settings to the iptables chains:\n\nFor correct working of IPsec VPN tunnel, create a rule to exclude targeted traffic (from Source to Destination). Place the rule in the NAT Postrouting chain before the rule that imposes PAT translation when sending traffic to the Internet.\n\n    Add the rule to the NAT table:\n\n        iptables -t nat -A POSTROUTING -s 10.55.4.0/22 -d 10.10.2.0/24 -j ACCEPT\n\nTo prevent data packet fragmentation, optimize TCP MSS. To do this, add rules to the FORWARD chains. The MSS (Maximum Segment Size) value is selected based on the individual characteristics of each tunnel and the Internet connection.\n\n    Add the rules to the MANGLE table:\n\n        iptables -t mangle -A FORWARD -s 10.10.2.0/24 -d 10.55.4.0/22 -p tcp -m tcp --tcp-flags SYN,RST SYN -m tcpmss --mss 1321:65495 -j TCPMSS --set-mss 1320\n    iptables -t mangle -A FORWARD -s 10.55.4.0/22 -d 10.10.2.0/24 -p tcp -m tcp --tcp-flags SYN,RST SYN -m tcpmss --mss 1321:65495 -j TCPMSS --set-mss 1320\n\nSave settings:\n\n        service netfilter-persistent save\n\n10. Configure a static route from a VDI environment to AD/LDAP\n\nIn your personal account go to the Virtual networks section → Routers.\nSelect the vdi-router router and go to the Static routes tab.\nClick the Add static route button.\nSpecify the route parameters:\n\n    Destination network (CIDR): 10.10.2.0/24;\n    Intermediate node (Next HOP): 10.56.0.4.\n\n\n    The Next HOP IP is the IP address of the internal network interface with a connection to the transit network of the VM running Debian 11.\n\n\nClick the Add route button.\n\n11. Check if VDI is able to connect to LDAP\n\nCreate a virtual network with CIDR 10.55.4.0/22 and router vdi-router. This network simulates a VDI network and is needed to test the network connection. When you deploy VDI, this network is created automatically.\n\nCreate a test virtual machine on the 10.55.4.0/22 network and connect to it via SSH.\n\nPing the LDAP server:\n\n        ping 10.10.2.14\n\n    IP address must respond.\n\nScheme of the built interaction between networks and machines within networks:\n\nScheme of the built interaction between networks and machines within networks:\n\nDelete unused resources\n\nIf you no longer need the created resources, delete them:\n\nDelete the virtual machines.\nDelete the VPN tunnel.\nDelete the routers.\nDelete the subnet and the network in which the VM is located.\n","url":"/en/computing/cloud-desktops/how-to-guides/ipsec","description":"How to set up a secure connection between VDI and LDAP in the VK Cloud infrastructure using IPsec Site-2-Site","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"db455752-0075-408f-8de9-66ec5fb63671","title":"Checking custom desktop images","data":"With Cloud Desktop, you can use built-in images to deploy desktops, as well as upload your own desktop images. Custom desktop images must meet the Cloud Desktop service requirements.\r\n\r\nThe following shows how to check a custom desktop image for compatibility with the Cloud Desktop service.\r\nPreparatory steps\r\n\r\nIf the image has AD support software installed, ask your system administrator for:\r\n\r\n     the IP address of the DNS server where the IP address and name of your LDAP server are registered;\r\n     the IP address or FQDN of your LDAP server;\r\n     the username and password of a LDAP user who has rights to add a VM to the domain.\r\n\r\nGo to your VK Cloud personal account.\r\nCreate a network and a subnet:\r\n\r\n     Enable the Access to internet option.\r\n     If the image has AD support software installed:\r\n\r\n         In the network settings, select an existing router with connection to an external network or create a new one.\r\n         In the subnet settings, disable the Private DNS option and add the DNS server IP address obtained from your system administrator to the DNS servers field.\r\n\r\n     Select other network and subnet parameters at your discretion.\r\n\r\nIf the image has AD support software installed, organize a VPN tunnel between the LDAP server and the created subnet.\r\n1. Check whether you can create a VM from the image prepared\r\n\r\nCheck that the image is visible in the VM creation wizard:\r\n\r\n     Go to your VK Cloud personal account.\r\n     Upload the prepared image to VK Cloud by following the instructions.\r\n     Go to Cloud Servers → Virtual machines and click Add.\r\n     In the Operating System list, find the image you uploaded and select it.\r\n\r\nCheck that when creating a VM you cannot specify parameters for it that are less than those of the uploaded image:\r\n\r\n     If in the Type of virtual machine list there is an option with a RAM size smaller than that in the uploaded image:\r\n\r\n         Select it and click Next step.\r\n         Verify that the next step of the wizard is blocked.\r\n\r\n     In the Type of virtual machine list, select an option with a RAM size no less than that of the uploaded image.\r\n     Specify a value in the Disk size field less than the disk size of the uploaded image and click Next step.\r\n     Verify that the next step of the wizard is blocked.\r\n\r\nCheck the possibility of creating a VM from the image:\r\n\r\n     Specify a value in the Disk size field no less than the disk size of the uploaded image.\r\n     Select other parameters on this page at your discretion and click Next step.\r\n     In the Network list, select the previously created network and subnet.\r\n     Add a security group to the Firewall settings field, if it has not already been added:\r\n\r\n        rdp for the Windows VM,\r\n        ssh for the Astra Linux VM.\r\n\r\n     Enable the Assign external IP option.\r\n     Click Next step and then click Create instance.\r\n     Verify that the VM has been created successfully.\r\n2. Test different ways to connect to the VM created from the image\r\n\r\n\r\n\r\nConnection to the Astra Linux VM via RDP and LDAP should always be checked, regardless of whether software supporting these protocols is installed on the image. This software is used when operating the desktop and, if not present on the image, will be automatically installed on the desktop during its deployment.\r\n\r\n\r\n\r\nCheck the ability to connect to the created VM:\r\n\r\n     On the General information tab of the VM page, click Set password and set the administrator password.\r\n     Connect to the VM:\r\n\r\n         To connect to the Windows VM, use the RDP protocol.\r\n         To connect to the Astra Linux VM use the SSH protocol.\r\n\r\n     Verify that the connection has been successful.\r\n\r\n(For Astra Linux) Check the ability to connect to the VM via RDP.\r\n\r\n     If software supporting the RDP protocol has not been installed on the image, install the xorgxrdp and xrdp packages on the VM. To do this, run the command:\r\n\r\n         `shell\r\n         sudo apt install xorgxrdp xrdp\r\n         `\r\n\r\n     Terminate the current connection session to the VM.\r\n     Add the VM to the rdp security group.\r\n     Connect to the VM via RDP, following the instructions for connecting to a Windows VM.\r\n     Verify that the connection is successful and the graphical user interface opens.\r\n\r\n(For Astra Linux) Check the ability to connect to the VM using LDAP.\r\n\r\n     If software supporting the AD service has not been installed on the image, install the astra-ad-sssd-client package on the VM. To do this, run the command:\r\n\r\n         `shell\r\n         sudo apt update && apt install -y astra-ad-sssd-client\r\n         `\r\n\r\n     Connect to the VM via SSH or RDP.\r\n     Run the command in the terminal:\r\n\r\n         `shell\r\n         sudo astra-ad-sssd-client -d  -u  -p\n         `\r\n\r\n         Here:\r\n\r\n           `` — the IP address or FQDN of your LDAP server;\r\n           ` and ` — the username and password of a LDAP user who has rights to add the VM to the domain.\r\n\r\n     Reboot the VM.\r\n     Go to the VM's console.\r\n     Sign in using the username and password of the LDAP user. Verify you are successfully authorized.\r\n     Connect to the VM via RDP, following the instructions for connecting to a Windows VM.\r\n     Sign in using the username and password of the LDAP user. Verify you are successfully authorized.\r\n3. Check the functionality of your installed software and peripheral devices\r\n\r\nVerify that additionally installed applications are available and functioning.\r\n(For Astra Linux) Connect to the VM via RDP. Check the operation of the sound card and microphone of the VM using the tools available in the OS.\r\nDelete unused resources\r\n\r\nThe running VM consumes computing resources. If you no longer need it:\r\n\r\nstop the VM to use it later;\r\ndelete the VM permanently.\r\n","url":"/en/computing/cloud-desktops/how-to-guides/check-desktop-image","description":"How to check a custom desktop image for compatibility with the Cloud Desktop service","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"77cb4cb7-dd58-49d3-8f6d-d1344fe00179","title":"Tariffication","data":"Tariffication is based on the “pay as you go” principle: you are charged only for the resources consumed, accurate to the nearest minute; the fee for using a paid OS is charged once a day.\n\nThe cost of network services is given in price list. Learn how to pay for the service in the section Billing.\n\nWhat is charged\n\nVirtual servers deployed within the Cloud Desktop service.\nLicenses for deployed virtual servers.\n\nTo view the details of expenses, use the instructions. Write-offs are accounted for in the following sections:\n\nVirtual servers**.\nLicenses**.\n\nThese sections display write-offs for both system VMs and VMs for cloud desktops.\n","url":"/en/computing/cloud-desktops/tariffication","description":"The cost of the Cloud Desktop service and its billing features.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"f54edac7-4292-4d43-95cd-6c6507aa1566","title":"FAQ","data":"\n\nThere is an error in the service operation that I cannot fix. Where can I contact?\n\nContact VK Cloud technical support.\n\n\n\n\n\nWhat information should I provide when contacting technical support for the operation of the service?\n\nThe exact time when the problem occurred.\nPID of the personal account in which the problem occurred.\nThe name of the desktop pool, if there are several in the project.\nThe name of the desktop, if the problem occurred with a specific desktop.\nThe user on whose behalf you are trying to perform the actions.\nA detailed description of the problem and how to reproduce it. It is recommended to attach screenshots.\n\n\n","url":"/en/computing/cloud-desktops/faq","description":"Answers to frequently asked questions about the Cloud Desktop service","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"Cloud Desktop","link":"/en/computing/cloud-desktops","isActive":false,"evenSimpler":true}]},{"uuid":"176f87ce-82c9-4eee-a8e5-a94cb32dfd41","title":"Microsoft","data":"The VK Cloud platform allows you to create virtual machines with pre-installed licensed copies of Microsoft Windows Server operating systems, as well as use additional Microsoft software products — the full list is available in the price list.\n\n\n\nVK Cloud provides Microsoft software under the Service Provider License Agreement (SPLA). Under this agreement, Microsoft software can only be operated on virtual machines in the VK Cloud.\n\nTerms of use\n\nIf you have any questions about licensing and rights to use Microsoft software, contact your legal department or Microsoft sales representatives.\n\nBy using this product, you agree to the terms of use of the following products: Microsoft.\n\nPrice\n\nThe price for a month of using the software can be found out:\n\nin the price list;\nwhen creating a virtual machine;\nin project settings on the tab Prices.\nin the section Balance personal account.\n\nWindows Server\n\nWhen creating a VM with Microsoft Windows OS, a licensed copy of the OS of the selected edition is automatically deployed. It will be activated when the VM is turned on.\n\nThe available editions of the Microsoft Windows Server are displayed in the drop-down list Operating system when creating a VM through VK Cloud personal account.\n\nEvery 2 vCPUs of a Windows VM require the use of one license. For example, when creating a VM with 5 vCPUs, you will need 3 Windows Server licenses. When changing the type of virtual machine, the number of licenses will be changed in accordance with the number of virtual cores of the modified configuration.\n\n\n\nPayment of licenses does not require additional actions and will be debited automatically in accordance with the configuration of the created VM.\n\nRemote Desktop Services\n\nBy default, after installing Windows Server OS, only two simultaneous connections to the VM are available (console and/or RDP).\n\nThese 2 connections can only be used to configure and administer the server. To allow more connections (administrative or connections to multiple users):\n\nMake sure that the appropriate Remote Desktop Services client licenses are available.\nInstall the Remote Desktop session host role on the target VM.\n\nA license will be required for each OS user who connects using the Remote Desktop Protocol (RDP). Activation of the Remote Desktop service completely replaces the default RDP connection.\n\n\n\nLicenses for Remote Desktop Services can only be granted for a licensed copy of the MS Windows operating system purchased from VK Cloud.\n\n\n\nTo activate the license, contact technical support with information:\n\nproject ID;\nid of the VM with MS Windows;\nthe required number of licenses;\nremote desktop connection settings.\n\nSQL Server\n\nThe SQL Server database management system is available for installation as a ready-made image with a pre-installed copy of SQL Server. Tell technical support the ID of the VM on which it is used.\n\nThe licensing method is applied “per core”: the price increases for each of the next two cores added to the VM. The minimum number of cores for which a license is purchased is four.\n\n{heading(Migration previously purchased licenses to VK Cloud)[id=migrate_own_licenses]}\n\nVK Cloud allows you to deploy a number of Microsoft server applications using previously purchased licenses. This simplifies the migration of workloads to the VK Cloud, eliminating new costs for the purchase of Microsoft licenses. This advantage is available to Microsoft corporate licensing customers with licenses of the relevant applications under the current Microsoft Software Assurance (SA) agreements.\n\nChecking a previously purchased license\n\nTo transfer licenses under the Software Assurance agreement, you must go through the license verification process, and Microsoft must confirm that you have eligible licenses in accordance with the current Software Assurance (SA) agreement. Fill out the verification form on the corporate Licensing website and provide it to your Microsoft representative or partner who will help send it to Microsoft:\n\nemail: support@mcs.mail.ru;\nname of the partner: VK Cloud;\npartner web site: cloud.vk.com.\n\nAfter receiving the form, Microsoft will check your license and inform you and your Microsoft partner about the results of the check. Further instructions in the manual Moving licenses within Software Assurance and to the document License Mobility Verification Guide Microsoft (PDF).\n\nAccording to the requirements of the agreement, you are responsible for performing inspections and extensions. However, you can start deploying application server software without waiting for the verification to be completed. You can deploy the software ten days before submitting the form.\n\nAfter passing the verification process, let us know about it to support service, indicating the name of the project and the type of license used.\n\nLicense transfer conditions\n\nTo use the license transfer, ensure that the conditions are met:\n\nAll Microsoft Server application software products migrated to VK Cloud using Software Assurance license portability must be subject to the current Software Assurance (SA) agreement.\n\nServer applications should be included in the list of eligible products:\n\n    Exchange Server;\n    SharePoint Server;\n    SQL Server Standard Edition;\n    SQL Server Enterprise Edition;\n    SQL Server Business Intelligence Edition;\n    Skype for Business Server;\n    System Center Server;\n    Dynamics CRM Server;\n    Dynamics AX Server;\n    Project Server;\n    Visual Studio Team Foundation Server;\n    BizTalk Server;\n    Forefront Identity Manager;\n    Forefront Unified Access Gateway;\n    Remote Desktop Services.\n\n    The full list of eligible software products is available in the Microsoft Product Terms of Use.\n\nEligible corporate licensing programs include Enterprise Agreement, Enterprise Subscription Agreement and Microsoft Open Value Agreement, which include the Software Assurance agreement, as well as other Microsoft corporate licensing programs in which the Software Assurance agreement is an additional feature, for example, Microsoft Open License or Select Plus.\n\nTo access the application servers, you must have the appropriate Client Access Licenses (CAL) in the corporate licensing agreement together with the Software Assurance agreement.\n\nLicense portability does not apply to Microsoft Windows client operating systems, desktop application products (for example, Microsoft Office) and Microsoft Windows Server operating systems.\n\nIn accordance with the rules of the Microsoft Server farm, licenses deployed in a specific VK Cloud region cannot be transferred to another VK Cloud region within 90 days. For more information, see Microsoft Product Terms of Use.\n","url":"/en/computing/vm-licenses/ms-lic","description":"Licensing of Microsoft products used in VK Cloud virtual machines.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"VM Licenses","link":"/en/computing/vm-licenses","isActive":false,"evenSimpler":true}]},{"uuid":"33d32be8-3626-4794-8a8f-943178a7333c","title":"Astra Linux","data":"VK Cloud supports the installation of virtual machines with image support from the company Astra Linux.\n\nThe available editions of the operating system are displayed in the drop-down list Operating system when creating a VM through VK Cloud personal account.\n\nAstra Linux Speсial Edition 1.7 is designed for the following application scenarios:\n\nhypervisor (type 2);\nDBMS server;\ndomain controller / emulate MS ActiveDirectory;\nmail server;\nweb server;\nfile server;\nproxy server;\n\nIn Russia, the platform users have vendor support and regularly updates that fix detected vulnerabilities and improve functional features. The Astra Linux open repository contains a wide range of compatibility tested applications guaranteed by the vendor its easygoing in the scenarios and the support.\n\nAstra Linux Special Edition 1.7 “Orel”\n\nAstra Linux Special Edition 1.7 “Orel” is a licensing option for Russian full blossom Astra Linux special edition operating system. We recommend it for use in infrastructure open segments connected to public access network such as educational institutions and other IT systems which do not process sensitive information.\n\nThis reasonable price solution can serve as a server platform as it contains an optimized kernel for server duty and its repository packed with a wide range of applications for variety of server scenarions.\n\nAstra Linux Special Edition 1.7 “Voronezh”\n\nAstra Linux Special Edition 1.7 “Voronezh” is a licensing option for the Astra Linux special edition operating system with a set of built-in certified information security options, which allows to protect sensitive and confidential information, including governmental information systems, pesonal data and critical infrastructure information systems of any security class.\n\n“Voronezh” option gives the user right to use the certified features providing the data protection with following security options:\n\nuser identification and authentication;\ndiscretionary access control;\nsecurity events logging;\nprocess isolation;\nmemory protection.\n\nIn addition, Astra Linux Special Edition 1.7 “Voronezh” contains enhanced security features of Astra Group’s development providing lable integrity control, isolation of software environment and guaranteed data erasure.\n\nAstra Linux Special Edition 1.7 “Voronezh” can serve as a server platform as it contains an optimized kernel for server duty and its repository packed with a wide range of applications for a variety of server scenarions.\n\nTerms of use\n\nOnly one license can be assigned to one VM.\n\nBy using this product, you agree to the terms of use of the following products: Astra Linux.\n\nPrice\n\nThe price for a month of using the software can be found out:\n\nin the price list;\nwhen creating a virtual machine;\nin project settings on the tab Prices.\nin the section Balance personal account.\n","url":"/en/computing/vm-licenses/astra-linux","description":"Licensing of products from the domestic company Astra Linux.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"VM Licenses","link":"/en/computing/vm-licenses","isActive":false,"evenSimpler":true}]},{"uuid":"bedc7124-7160-4473-9c7e-7ee0fe98f906","title":"ALT Linux","data":"VK Cloud supports the installation of virtual machines with image support from the company Alt Linux.\n\nThe available editions of the operating system are displayed in the drop-down list Operating system when creating a VM through VK Cloud personal account.\n\nALT Server is a server image based on the Linux kernel with a wide functionality that allows you to support corporate infrastructure, as well as various additional equipment.\n\nALT Server is an enterprise-level solution that allows migration to import-substituting software and hardware, is included in the Unified Register of Russian Programs for Electronic Computers and Databases (software registration number: 1541).\n\nUsage examples:\n\nPostgreSQL, MariaDB database servers;\nSquid proxy server;\nApache Web Server;\nPostfix, Dovecot mail servers;\nanti-spam server Spamassassin;\nZabbix monitoring system;\nnetwork download server;\ntools for creating a repository mirror for centralized updating of workstations managed by ALT Workstation.\ngroup work server with Microsoft Exchange functionality;\na tool for installing and downloading operating systems over the network.\n\nALT SP (ALT 8 SP) is an operating system for servers and workstations with built—in information security software, certified by the FSTEC of Russia.\n\nThe ALT SP operating system is designed to perform tasks:\n\nensuring the execution of programs in a secure environment;\nworking with virtualization tools;\napplication in state information systems of the 1st security class;\napplication in automated control systems of production and technological processes of the 1st class of security;\nthe use of personal data in information systems, if necessary, to ensure the 1st level of personal data security;\napplication in public information systems of class II.\n\nAdvantages of ALT SP:\n\nAvailability of virtualization tools.\nAvailability of tools for container management.\nThe ability to install on computers with 64-bit processors with EFI support (without disabling SecureBoot).\nSupport for Samba-DC (Active Directory), FreeIPA and OpenLDAP/MIT Kerberos domains.\nSupport for group policies for integration into the Active Directory infrastructure.\nSupport for a wide range of peripheral equipment.\nA set of server services for corporate infrastructure.\nModular configuration platform with graphical and web interface (Alterator).\nThe ability to remotely control computers over the network via SSH protocol and via the web interface.\nThe ability to deploy a network installation server and an update server using a web interface.\nCompatibility with domestic software.\nPrompt release of security updates.\n\nTerms of use\n\nOnly one license can be assigned to one VM.\n\nBy using this product, you agree to the terms of use of the following products: ALT Linux.\n\nPrice\n\nThe price for a month of using the software can be found out:\n\nin the price list;\nwhen creating a virtual machine;\nin project settings on the tab Prices.\nin the section Balance personal account.\n","url":"/en/computing/vm-licenses/alt-linux","description":"Licensing of products from the domestic company ALT Linux.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"VM Licenses","link":"/en/computing/vm-licenses","isActive":false,"evenSimpler":true}]},{"uuid":"3d4b1da2-5c82-47d4-85ed-0f078826dea6","title":"RED OS","data":"VK Cloud supports the installation of virtual machines with image support from the company RED OS.\n\nThe available editions of the operating system are displayed in the drop-down list Operating system when creating a VM through VK Cloud personal account.\n\nRED OS is a completely domestic operating system, Linux distribution of RPM format. It is included in the register of Russian Software of the Ministry of Finance of Russia (No. 3751) and has a valid certificate of the FSTEC of Russia. Additional benefits:\n\nComplies with regulatory legal acts of market regulators.\nHas a full-fledged ecosystem of compatible solutions.\nHas tools for centralized management of IT infrastructure.\nHas an intuitive desktop interface and graphical environment.\nAccepted into the corpstandard of FOIV, ROIV and State Corporations.\n\nThe “Server” configuration has a Standard and Certified edition:\n\nThe standard edition has the most recent and up-to-date set of packages.\nThe certified edition is certified by the FSTEC of Russia. Possible scope of application:\n\n  ISOP (information system of the Prosecutor's office) up to and including Class II.\n  Automated process control system up to and including security class I.\n  GIS (state information systems) up to and including security class I.\n  ISPDn (personal data information system) up to and including the I level of security.\n  CII (critical information infrastructure) up to and including the I category of significance.\n\nThe “Server” configuration allows you to configure:\n\nmirror of the APM update repository;\nnetwork download server;\ndatabases;\nmonitoring system;\nproxy server;\ndomain controller;\nweb server;\nmail server;\nDNS, DHCP services;\nvirtualization systems.\n\nAll editions of the “Server” configuration support architecture types:\n\nx86_64, i686,\naarch64:\n\n  Raspberry Pi,\n  Baikal–M,\n  Kunpeng (Huawei Taishan).\n\ne2k (Elbrus).\n\nTerms of use\n\nOnly one license can be assigned to one VM.\n\nBy using this product, you agree to the terms of use of the following products: Red OS.\n\nPrice\n\nThe price for a month of using the software can be found out:\n\nin the price list;\nwhen creating a virtual machine;\nin project settings on the tab Prices.\nin the section Balance personal account.\n","url":"/en/computing/vm-licenses/red-os","description":"Licensing of products from the domestic company RED OS.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Computing","link":"/en/computing","isActive":false,"evenSimpler":true},{"title":"VM Licenses","link":"/en/computing/vm-licenses","isActive":false,"evenSimpler":true}]},{"uuid":"c59e996c-7a61-4e35-b66d-47e4a7ad65dc","title":"Quick start","data":"A quick start will help you get started with the service and get acquainted with its capabilities.\n\nAfter completing all the quick start steps, you will:\n\nSet up Internet access.\nOrganize the basic network connectivity of several virtual machines.\nLearn how to assign groups of firewall rules.\n\nStep 1: Create a network and subnet\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Networks.\nClick the Create button.\nSpecify a network name, for example, test-network.\nInternet access: included.\nRouter: Create new.\nClick Add network.\n\nStep 2: Create some virtual machines\n\nCreate a virtual machine with Internet access:\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual Machines.\nClick the Add button.\nAt the “Configuration” step:\n\n    Name the virtual machine, for example test-vm1.\n    Set other settings depending on your requirements or leave them unchanged.\n\nPress Next step.\nAt the “Network settings” step:\n\n     Networks: External network (ext-net).\n     Leave the rest of the settings unchanged.\n\nPress Next step.\nAt the “Backup settings” step, leave the settings unchanged.\nClick Create Instance.\n\nThe creation of the virtual machine may take some time, after which it will appear in the list.\n\nCreate a second virtual machine:\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual Machines.\nClick the Add button.\nAt the “Configuration” step:\n\n     Give the virtual machine a name, for example test-vm2.\n     Set other settings depending on your requirements or leave them unchanged.\n\nAt the “Network settings” step:\n\n     Networks: test-network.\n     Virtual machine key: Create a new key.\n     Firewall settings: default, ssh.\n     Assign an external IP address: disable.\n\nPress Next step.\nAt the “Backup settings” step, leave the settings unchanged.\nClick Create Instance.\n\nA file with a virtual machine key will be downloaded to your computer.\n\nCreate a third virtual machine:\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual Machines.\nClick the Add button.\nAt the «Configuration» step:\n\n     Name the virtual machine, for example test-vm3.\n     Set other settings depending on your requirements or leave them unchanged.\n\nAt the “Network settings” step:\n\n    Networks: test-network.\n    Virtual machine key: Create a new key.\n    Firewall settings: default, ssh.\n    Assign an external IP address: disable.\n\nPress Next step.\nAt the “Backup settings” step, leave the settings unchanged.\nClick Create Instance.\n\nWait for the creation of the virtual machine to complete.\n\nStep 3: Create a rule group\n\nClick Add.\nSpecify a rule group name, for example, test-icmp.\nClick Create Group.\nIn the Incoming traffic block, create rules for traffic management:\n\n     Click Add Rule.\n     Type: ICMP.\n     Remote address: All IP addresses.\n\nClick the Save Rule button.\nIn the Outgoing traffic block, create rules for traffic management:\n\n     Click Add Rule.\n     Type: ICMP.\n     Remote address: All IP addresses.\n\nClick the Save Rule button.\n\nStep 4. Assign a rule group\n\nTo send and receive traffic, assign a security group to the VM. For previously created VMs, apply the following settings:\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Virtual Machines.\nClick   for the virtual machine and select Firewall Settings.\nFind the rule group created in step 3 in the list.\nClick Apply in the line with the rule group.\nPress Confirm.\n\n\n\nSuccessful completion of this step ensures that ICMP connectivity exists between the created VMs. Network connectivity will work even if one VM in the assigned rule group does not have a floating IP address.\n\nStep 4. Connect to the VM via ssh\n\nIn the last step, you created a virtual machine key that was saved to your computer. Find the file in downloads, you will need it to connect to the VM.\n\nOpen a terminal.\nGo to downloads:\n\n        cd ~/Downloads/\n\nMake the key available only to the current user:\n\n          chmod 400\n\nConnect to the instance via SSH:\n\n          ssh -i  centos@10.0.0.6\n\nStep 5: Delete unused resources\n\nIf you no longer need the resources you created, delete them.\n","url":"/en/networks/vnet/quick-start","description":"Getting started with Cloud Networks.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"9f16e939-edb5-4c3b-9747-e1a5981820d1","title":"About service","data":"The service provides networking within the selected VK Cloud platform's projects and region for:\n\nCreating networks and subnets. Subnets use private IP addresses that are not routable on the internet.\n\nConnecting platform services to networks and subnets.\n\n  Both parts of an individual service (for example, database cluster nodes) and services as a whole (for example, a database cluster can interact with a virtual machine) are connected to the network.\n\n  For example, DNS service works on top of the networks and subnets created.\n\nConfiguring the following entities that make use of networks and subnets:\n\n  Routers to connect multiple subnets to each other.\n\n    They can be used to connect both VK Cloud subnets alone and VK Cloud subnets to subnets at a remote site.\n\n  Load balancers to distribute incoming traffic across multiple instances of platform services.\n  Firewall with rule groups to restrict traffic to specific platform services.\n  VPN to connect a third-party network to a network created within the platform.\n\nWhat's next\n\nRead about the architecture of the service.\nRead about the principles of network addressing and organizing internet access.\n","url":"/en/networks/vnet/concepts/about","description":"Overview of VK Cloud virtual network service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"388502f4-a8ea-4588-907e-03b419422690","title":"Service architecture","data":"What the service does\n\nCloud Networks service consists of two layers:\n\nSoftware-Defined Network (SDN).\nNetwork Function Virtualization (NFV).\n\nOn the SDN layer, the service allows you to work with the underlying entities:\n\nNetworks.\nSubnets.\nPorts. An OpenStack port is equivalent to a combination of a port on a virtual switch connected to a virtual NIC, and an IP address from some subnet assigned to this NIC.\n\nOn the NFV layer, the following entities are implemented with SDN:\n\nStandard routers to connect multiple private networks to each other and to provide internet access.\nLoad balancers to distribute incoming traffic across multiple instances of VK Cloud platform services.\nFirewall with rule groups to restrict traffic to specific platform services. Rule groups are assigned at the individual port level.\nVPN for connecting VK Cloud subnets with the client subnets.\n\nAdvanced routers are implemented using specialized virtual machines without SDN involvement.\n\nSDNs used\n\nThe Openstack Neutron API is used to work with SDN and NFV objects. Hosted agents receive requests via the API and create or modify objects needed for SDN or NFV.\n\nAs an SDN, the VK Cloud platform uses:\n\nNeutron** by Openstack.\n\n  It is effective for a small network, but in large networks (thousands and tens of thousands ports) there can be problems:\n\n  With synchronization of multiple agents that do not store their state.\n  With a large number of events due to overcomplicated data transmission layer (dataplane).\n  With the loss of these events in the RabbitMQ queue because of their large number. This queue acts as a transport for service events.\n\nSprut**, VK Cloud's own SDN, fully compatible with the Openstack Neutron API.\n\n  It lacks the listed disadvantages of Openstack Neutron. Sprut provides stable operation of networks and network functions on top of these networks on a large scale. Its advantages:\n\n  It uses the HTTP REST API instead of a message queue.\n  All agents store their current state. Agents receive from the SDN controller the target state they should be in, and bring their state to the required state.\n  A layered architecture was chosen to simplify the data transfer layer and allow for more flexibility.\n  Advanced router is used with support for static and dynamic routing.\n  The organization of dedicated connections between the local network of clients and VK Cloud is available — Cloud Direct Connect.\n\n  Additional materials about Sprut:\n\n  How to choose SDN for high loads video;\n  Как мы в VK Cloud SDN-ы писали article.\n\n\n\nSprut is the main SDN for all new projects. You can find out which SDN is used in your project in the project settings. To connect another SDN, contact technical support.\n\n\n","url":"/en/networks/vnet/concepts/architecture","description":"Architecture of VK Cloud virtual network service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"06c78433-5d9b-4bc8-bf88-7b294b2caffc","title":"Router","data":"\n\nThe ability to exchange traffic in subnets is affected not only by network topology, but also by firewall security groups.\n\nTo make it easier to explain how routers work, it is further assumed that the firewall lets all traffic through.\n\n\n\nRouters are used to organize traffic exchange between subnets. There are two types of routers in VK Cloud:\n\nA standard router is designed to route traffic between VK Cloud subnets, and to provide access to the external network from such subnets.\n\n  It also routes traffic between VK Cloud subnets and the remote site if a VPN tunnel is used.\n\nAn advanced router is designed to route traffic between VK Cloud subnets and subnets at the remote site.\n\n  It is used if a network junction is created between VK Cloud and the remote site via the Cloud Direct Connect service.\n\n\n  The Cloud Direct Connect service is connected on request.\n\n\n  Sprut SDN is required for an advanced router to operate. Any VK Cloud subnets can be connected to such a router, regardless of which SDN these subnets belong to.\n\nDifferent types of routers can be used within a single VK Cloud subnet.\n\nStandard router capabilities\n\nRouting traffic between VK Cloud subnets.**\n\n  A special OpenStack port for the INTERFACE_DISTRIBUTED device is used as the default gateway in any subnet connected to the router. This port is the router's interface on the subnet and is assigned the IP address specified in the subnet settings as the gateway address.\n\n  You can also configure static routing. For example, direct traffic to a virtual machine that acts as a dedicated firewall or other network device.\n\nProviding access from VK Cloud subnets to the external network.**\n\n  A router connected to the external network provides additional capabilities for subnets:\n\n  Internet access for an OpenStack port on the subnet.\n\n    For example, you can provide Internet access to a virtual machine so that it can receive updates. However, this virtual machine will not be accessible from the internet.\n\n    To allow the port to access the internet, the private IP address of the port is translated to the public IP address of the router using the Source NAT (SNAT) mechanism. The translation is performed through the router's dedicated interface on the subnet, a special SNAT port that is assigned a random IP address from the DHCP address pool.\n\n  The ability to assign a floating IP address to an OpenStack port on the subnet.\n\n    If a floating IP address is assigned to an OpenStack port, that port not only accesses the Internet, but also becomes accessible from the Internet. For example, you can assign a public IP address to a port assigned to a virtual machine to connect to it from the internet via SSH.\n\n    To provide access to and from the internet, the private IP address of the port is translated to a floating IP address using the 1-to-1 NAT mechanism.\n\n  Organization of a VPN tunnel between networks in projects with SDN Neutron.\n\n    This allows you to create a secure channel for managing the VK Cloud infrastructure or transferring data between the company network and VK Cloud networks.\n\nAdvanced router capabilities\n\nConnecting to a network junction between VK Cloud and a remote site.**\n\n  Such a junction is organized with the help of Cloud Direct Connect service. After connecting and configuring the service, a new subnet will be added to the VK Cloud project. Parameters of this subnet can be set by you or telecom provider. This subnet is the network junction, so an advanced router must have a configured interface in this subnet.\n\nRouting traffic between VK Cloud subnets and subnets on the remote site.**\n\n  Both static routing and dynamic routing using (via BGP) can be configured to route traffic.\n\n  The following rules apply when configuring BGP peering between an advanced router and a remote router at the remote site:\n\n  BGP peering must be performed over a Direct Connect network junction.\n  Only private Autonomous System Numbers (ASNs) from the range 64512-65534 may be used.\n  On the side of both the advanced and remote router, it is allowed to announce only the prefixes of private subnets that are not routable in the internet.\n\n  Also on the advanced router side, the following BGP timers values are set:\n\n  Hold timer: 240 seconds.\n  Keepalive timer: 80 seconds (hold timer / 3).\n\n\n  The mismatch of these timers values on both BGP routers does not affect the forming of neighbor relationships.\n\n\nOrganization of a VPN tunnel between remote networks and networks in projects with SDN Sprut.**\n\n   VPN tunnels in projects with SDN Sprut are created based on an advanced router.\n\nSupporting redundancy for fault tolerance.**\n\n  Advanced routers support VRRP protocol. Therefore, you can configure multiple routers in combination with VRRP to provide fault tolerance. If one or more routers fail, the remaining routers will continue to handle traffic that arrives at the IP address specified as the default gateway address when configuring VRRP.\n","url":"/en/networks/vnet/concepts/router","description":"How to use a router to connect different networks with each other.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"cb778b44-2c4c-46fe-b945-cf6fd5968fab","title":"Traffic restriction","data":"Using IP Source Guard\n\nFor OpenStack ports, you can specify a list of IP addresses to use IP Source Guard.\nOnly traffic whose source IP address is on this list will be sent through the port. This helps protect against IP spoofing attacks.\n\nFor example, you can allow:\n\nOnly traffic from the virtual machine that uses the OpenStack port.\nAll traffic that passes through the virtual machine (0.0.0.0.0\\0). This can be useful when the virtual machine is involved in processing traffic and is an intermediate network node (such as a router, firewall or VPN gateway).\n\nUse of firewall and security groups\n\nA firewall can be used to restrict traffic on virtual networks.\n\nThe firewall handles traffic according to defined security groups. These groups contain rules for handling inbound and outbound traffic and operate according to the \"anything not allowed is denied\" principle. One or more security groups can be assigned:\n\nIn VK Cloud personal account (only to OpenStack ports to which virtual machines are associated);\nvia the OpenStack CLI (to any OpenStack ports).\n\nYou can either create your own security groups or use preconfigured groups that cannot be changed.\n\nFor security groups to work correctly:\n\nEither configure not only inbound but also outbound rules for them.\nOr use them in combination with a default security group that allows any outgoing traffic. This applies to both pre-configured and custom security groups.\n\nDefault security groups:\n\ndefault — for nets in SDN Neutron;\ndefault-sprut — for nets in SDN Sprut.\n\nPreconfigured security groups\n\n\n\nOnly the default group is available in a project by default, other groups become available after creating virtual machine with these groups.\n\n\n\n\n\ndefault\nssh\nssh+www\nrdp\nrdp+www\nall\n\n\n\nDefault security group. This group is assigned to all OpenStack ports created within the network, including:\n\nports to which virtual machines and other platform services connect;\nservice ports that are created, for example, for a router or load balancer.\n\n\n\nWhen using the OpenStack CLI, it is possible to create an OpenStack port:\n\neither with security groups that are different from the default group;\nor no security group at all.\n\n\n\nAllows:\n\nany outgoing traffic;\nany incoming traffic within a security group.\n\n\n\n\nA security group that allows SSH traffic.\n\nAllows incoming traffic from any IP addresses on TCP port 22.\n\n\n\n\nA security group that allows SSH and HTTP(S)-traffic.\n\nAllows incoming traffic from any IP addresses to TCP ports:\n\n22\n80\n443\n\n\n\n\nA security group that allows RDP traffic.\n\nAllows incoming traffic from any IP addresses on TCP port 3389.\n\n\n\n\nA security group that allows RDP and HTTP(S)-traffic.\n\nAllows inbound traffic from any IP addresses to TCP ports:\n\n3389\n80\n443\n\n\n\n\nAny incoming traffic from any IP addresses is allowed.\n\n\n\n","url":"/en/networks/vnet/concepts/traffic-limiting","description":"How to use IP Source Guard and firewall to restrict traffic in virtual networks.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"69c143c1-559c-4eb5-b82d-4a6261f4adf8","title":"VPN","data":"\n\n\nThe service is only available in networks created with use of Neutron SDN.\n\n\n\nVPN allows you to organize a tunnel between one or more VK Cloud subnets and the client network. This can be useful in situations where you need to:\n\nconnect the company's network to the cloud network (for example, to have access to the VK Cloud infrastructure);\narrange a secure channel to manage the VK Cloud infrastructure (for example, to use telnet together with virtual machines).\n\nThe VPN service is based on StrongSwan and allows organizing IPsec tunnels. To link VK Cloud subnets and client subnets:\n\nConnect to router all VK Cloud subnets that you want to access via VPN. Networks in SDN Sprut can only use an advanced router to organize VPN, and networks in SDN Neutron can only use a standard one.\n\nConfigure this router to access the external network so that you can use its SNAT interfaces.\n\nSet static routes to the necessary client subnets on the remote site.\n\n   These routes are set in the VK Cloud subnet settings, which must be accessible through the VPN. This is necessary because the VK Cloud VPN accesses client subnets through the SNAT interface:\n\n       -\n\n   ...\n\n    -\n\nWhen setting up a VPN connection in VK Cloud, specify the router behind which the VK Cloud subnets you want to access via VPN are placed.\n\nConfigure the VPN on the client side, taking into account the configuration of the VK Cloud IPsec VPN server, which:\n\n   works in the main mode;\n   supports only authorization by pre-shared key;\n   supports a limited subset of Diffie-Hellman groups.\n","url":"/en/networks/vnet/concepts/vpn","description":"How to use VPN to establish a secure tunnel to VK Cloud private network.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"b225b356-34fe-4eb6-8d2c-459cf7149201","title":"Network types","data":"Cloud Networks service allows to work with several network types.\n\nStandard network\n\nStandard network provides access to the subnets with the private IP addresses and exists within a single project.\n\nThe standard network, its subnets and ports can be managed via personal account, OpenStack CLI or Terraform.\n\n{heading(Shared network)[id=shared_net]}\n\nShared network also provides access to the subnets with the private IP addresses, but exists across several projects.\n\nTo create a shared network, you have to create a standard network in one project and then share it with other projects. Only a user with the role of the project owner (hereinafter referred to as network owner) has the ability to share a network.\n\nA network owner can share the net with any project:\n\nby unique identifier (PID) with those projects where this user is also the owner.\nby project ID in OpenStack (Project ID) with any other projects.\n\nThe project user with whom the network was shared (hereinafter referred to as network user) will receive a message and can accept or decline the invitation to the shared network. The user's decision applies to the entire project: if one user declines the offer to join the network, the network will be unavailable for the entire project.\n\nNetwork users cannot manage subnets on it, but can create resources, connect services, and manage ports in their project. When connecting some resources, such as load balancers or advanced routers, service ports will be created that belong to the network owner's project and are not visible to network users. The owner of the shared network cannot control such ports; they are connected or disabled only when a service is connected or disabled. To remove a port occupied by a load balancer in a linked project, you must remove the balancer in the linked project.\n\nOnly the network owner can disconnect a project from the network. Before disconnecting, the network users must delete all VMs and PaaS services of the project located on the shared network. It is also recommended to remove all file storages, balancers and advanced routers of the project connected to the shared network. Otherwise, the network connectivity between them and the network will remain when the network is disconnected.\n\nOnly the owner of the network can delete a shared network. Before deleting, you must disable all ports, including service ports, which are visible only to the network owner. To disable service ports, network users need to remove the services connected to them in their projects.\n\nShared network access matrix:\n\n[cols=\"1,2,2\", options=\"header\"]\n|===\n|Action\n|Network owner\n|Network user\n\n|Viewing a network\n|Available\n|Available\n\n|Editing a network\n|Available\n|Not available\n\n|Providing access to other projects to a network\n|Available\n|Not available\n\n|Disconnecting a project from a network\n|Available. If the project has added VM ports or PaaS services, an error message will appear. Remove all busy ports.\n\nIf the project has file storages, balancers or advanced routers, the shutdown will be performed. At the same time, the services will remain connected to the network, that is, network connectivity between projects will be preserved. It is recommended to remove all file storages, balancers and advanced routers from the disconnected network\n|Not available\n\n|Deleting a network\n|Available. If there are busy ports on the network (both the network owner's and the network user's projects ), an error message will appear. It is necessary to delete all occupied ports, including service ones\n|Not available\n\n|Viewing subnets\n|Available\n|Available\n\n|Creating, editing, deleting subnets\n|Available\n|Not available\n\n|Viewing ports\n|Available. All ports directed to all projects are visible\n|Available. All project ports are visible. Service ports that are created for file storages, balancers and advanced routers are not visible, since they belong to the network owner’s project\n\n|Creating ports\n|Available\n|Available\n\n|Editing ports\n|Available\n|Not available\n\n|Deleting ports\n|You can only delete ports in your project\n|You can only delete ports in your project\n|===\n\nExternal network\n\nExternal network (ext-net, internet) provides access to the subnets with the public IP addresses and exists across all projects. It is used to provide services with the Internet access. See details in the Network addressing and internet access section.\n\nIt is not possible to edit or delete the subnets of that network. The following operations can be performed via personal account, OpenStack CLI or Terraform:\n\nAssigning the network to a virtual machine.\nAssigning the network for a router external network connection.\n","url":"/en/networks/vnet/concepts/net-types","description":"Types of networks you can work with in the service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"6e13dab1-b7ba-4e0f-a650-e717e73e603d","title":"Network addressing and internet access","data":"Network addressing\n\nWhen you create a subnet, you specify:\n\nsubnet name;\naddress of the subnet;\nDHCP IP address pool (even if DHCP is disabled for the subnet);\n(optional) static routes in the format  - .\n\nThe following rules apply:\n\nIf DHCP is enabled for a subnet, two DHCP servers are placed in it and take the first two addresses from the DHCP address pool.\n\n   For example, if a DHCP address pool of 192.168.0.0/24 is set to 192.168.0.11-192.168.0.254, the DHCP servers will use the ports with addresses 192.168.0.11 and 192.168.0.12.\n\nIf a router with a public IP address is connected to the subnet, the SNAT port of the router will be assigned a random address from the remaining DHCP IP address pool.\n\nOrganizing internet access\n\nIn order for objects in the subnet to have access to the Internet, you need to connect a router to the subnet with access to external network.\n\nNetwork objects that need a router with external network access:\n\nVPN tunnel.\nLoad balancer with internet access.\n\nFor virtual machine the Internet access can be provided in different ways:\n\nConnect the VM to an external network. In this way, a subnet and external IP address will be automatically assigned to it.\nConnect the VM to a private subnet that is connected to a router with access to an external network, and assign it a floating IP address. In this way, the IP address can be set manually or automatically.\n\nFloating IP adress\n\nA floating IP address (DNAT) is a static IP address that can be dynamically reassigned between resources in a cloud environment. Floating IP is used through SDN, which allows the network administrator to move IP addresses between devices without having to change the physical or virtual network configuration. This allows you to switch traffic between different servers without changing the configuration of the servers themselves.\n\nFloating IP addresses are used for:\n\nEnsuring fault tolerance — if the primary server fails, the IP address can be quickly redirected to the backup server, minimizing downtime.\n\nLoad balancing — floating IP addresses can be used to distribute traffic across multiple servers, thereby increasing system performance and reliability.\n\nFlexible network reconfiguration — in environments with frequently changing requirements, floating IP addresses allow you to quickly redistribute resources without the need to change IP addressing and associated settings.\n\nPublic IP address pool of the internet external network\n\nAll projects in SDN Sprut are connected to the external network internet.\n\nOn all subnets of the external network internet:\n\nDHCP is disabled;\nDNS servers 5.61.237.120 and 5.61.237.127 are used.\n\n\n| Subnet name  | Subnet address   | Available IP address range                         | Gateway         |\n| ------------ | ---------------- | -------------------------------------------------- | --------------- |\n| ext-sub  | 89.208.216.0/24  | 89.208.216.1 - 89.208.216.253                       | 89.208.216.254 |\n| ext-sub2  | 212.111.84.0/22  | 212.111.84.1 - 212.111.87.253                        | 212.111.87.254 |\n\nPublic IP address pool of the ext-net external network\n\nAll projects in SDN Neutron are connected to an external network ext-net.\n\nOn all subnets of the external network ext-net:\n\nDHCP is disabled;\nDNS servers 5.61.237.120 and 5.61.237.127 are used.\n\n\n| Subnet name  | Subnet address   | Available IP address range                         | Gateway         |\n| ------------ | ---------------- | -------------------------------------------------- | --------------- |\n| ext-subnet1  | 95.163.248.0/22  | 95.163.248.10—95.163.251.250                       | 95.163.251.254  |\n| ext-subnet2  | 79.137.174.0/23  | 79.137.174.5—79.137.175.253                        | 79.137.175.254  |\n| ext-subnet4  | 95.163.212.0/22  | 95.163.212.1—95.163.215.253                        | 95.163.215.254  |\n| ext-subnet5  | 95.163.208.0/22  | 95.163.208.1—95.163.211.253                        | 95.163.211.254  |\n| ext-subnet6  | 95.163.180.0/23  | 95.163.180.1—95.163.181.253                        | 95.163.181.254  |\n| ext-subnet7  | 89.208.84.0/22   | 89.208.84.1—89.208.87.253                          | 89.208.87.254   |\n| ext-subnet8  | 89.208.196.0/22  | 89.208.196.1—89.208.199.253                        | 89.208.199.254  |\n| ext-subnet9  | 95.163.182.0/23  | 95.163.182.1—95.163.183.253                        | 95.163.183.254  |\n| ext-subnet10 | 85.192.32.0/22   | 85.192.32.1—85.192.35.253                          | 85.192.35.254   |\n| ext-subnet11 | 89.208.208.0/22  | 89.208.208.1—89.208.211.253                        | 89.208.211.254  |\n| ext-subnet12 | 89.208.220.0/22  | 89.208.220.1—89.208.223.253                        | 89.208.223.254  |\n| ext-subnet13 | 213.219.212.0/22 | 213.219.212.1—213.219.215.253                      | 213.219.215.254 |\n| ext-subnet14 | 89.208.228.0/22  | 89.208.228.1—89.208.231.253                        | 89.208.231.254  |\n| ext-subnet15 | 185.241.192.0/22 | 185.241.192.1—185.241.195.253                      | 185.241.195.254 |\n| ext-subnet17 | 87.239.104.0/21  | 87.239.104.1—87.239.111.253                        | 87.239.111.254  |\n| ext-subnet18 | 185.86.144.0/22  | 185.86.144.1—185.86.147.253                        | 185.86.147.254  |\n| ext-subnet19 | 37.139.32.0/22   | 37.139.32.1—37.139.35.253                          | 37.139.35.254   |\n| ext-subnet20 | 37.139.40.0/22   | 37.139.40.1—37.139.43.253                          | 37.139.43.254   |\n| ext-subnet21 | 146.185.240.0/22 | 146.185.240.1—146.185.243.253                      | 146.185.243.254 |\n| ext-subnet22 | 5.188.140.0/22   | 5.188.140.1—5.188.143.253                          | 5.188.143.254   |\n| ext-subnet23 | 146.185.208.0/22 | 146.185.208.1—146.185.211.253                      | 146.185.211.254 |\n| ext-subnet24 | 84.23.52.0/22    | 84.23.52.1—84.23.55.253                            | 84.23.55.254    |\n| ext-subnet25 | 109.120.180.0/22 | 109.120.180.1—109.120.183.253                      | 109.120.183.254 |\n| ext-subnet26 | 109.120.188.0/22 | 109.120.188.1—109.120.191.253                      | 109.120.191.254 |\n| ext-subnet27 | 185.130.112.0/22 | 185.130.112.1—185.130.115.253                      | 185.130.115.254 |\n| ext-subnet28 | 94.139.244.0/22  | 94.139.245.0—94.139.247.253                        | 94.139.247.254  |\n| ext-subnet29 | 212.233.88.0/21  | 212.233.88.1—212.233.95.253                        | 212.233.95.254  |\n\n","url":"/en/networks/vnet/concepts/ips-and-inet","description":"How IP address management and internet access are organized.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"e7d18536-a3c8-4f2c-8189-5b25d9935a68","title":"Networks and subnets","data":"In addition to the cloud network, you can create subnets. By default, one network with several subnets is already created in the project. Once a network and subnets are created, they become available to all virtual machines in the project at once.\n\n\n\nShared networks can be managed only from the owner project.\n\nViewing the list of networks and subnets and information about them\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nIn personal account, go to Virtual networks → Networks.\n\n   A list of networks will be displayed.\n\nClick on the name of the necessary network.\n\n   A page with detailed information about the network will open. This will include a list of subnets on that network.\n\nTap on the name of the required subnet.\n\n   A page detailing the network information will be displayed.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nTo see a list of networks and their identifiers, run the command:\n\n      openstack network list\n\nTo see detailed information about the network, run the command:\n\n      openstack network show\n\nTo see a list of all subnets and their identifiers, run the command:\n\n      openstack subnet list\n\nTo see a list of all subnets belonging to a particular network and their IDs, run the command:\n\n      openstack subnet list --network\n\nTo see detailed information about a subnet, execute the command:\n\n      openstack subnet show\n\n\nCreating a network\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nIn personal account, go to Virtual networks → Networks.\nClick the Create button.\nSet the network name.\n(Optional) Give access to the Internet. This is necessary if you plan to use VPN, SNAT services.\nSelect a router from the suggested list.\nSpecify the zone for private DNS.\nThe default subnet is already created, but you can add more. If you need to add subnets later, skip this step.\nClick the Add Network button.\n\nOnce a network is created, it will appear in the list of networks.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nRun the command:\n\n      openstack network create\n\n\nEditing a network\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nIn personal account, go to Virtual networks → Networks.\nClick on the name of the cloud network.\nGo to Network settings.\nMake any changes you want.\nClick the Save changes button.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nGet ID of the network you want to edit.\n\nFamiliarize yourself with the command's help.\n\n      openstack network set --help\n\n   Below are just the basic arguments of the command.\n\nTo apply the necessary settings to the network, run the command:\n\n   Linux/macOS (bash, zsh)\n   Windows (PowerShell)\n\n\n      openstack network set  \\\n     --name  \\\n     --dns-domain\n\n\n      openstack network set  `\n     --name  `\n     --dns-domain\n\n\nDeleting a network\n\n\n\nAll the network subnets and ports will be deleted along with the network.\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n{include(/en/_includes/_delete_net.md)}\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nGet ID of the network you want to delete.\n\nRun the command:\n\n      openstack network delete\n\n\nCreating a subnet\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nIn personal account, go to Virtual networks → Networks.\nClick on the name of the cloud network.\nClick the Add subnet button.\nSpecify the name of the subnet.\nEnter the IP address and gateway of the subnet.\n(Optional) DHCP is enabled by default. The addresses issued by the DHCP server will remain constant. Disabling DHCP will cause the IP addresses issued by the DHCP service to stop being served. This can cause virtual machines to become unavailable. If necessary, disable it.\nSpecify a pool of DHCP IP addresses.\n(Optional) By default, Private DNS is enabled. If it is disabled, specify DNS servers.\n(Optional) Enable Show Static route field to specify static routes.\nClick the Create button.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nGet ID of the network in which you want to create a subnet.\n\nFamiliarize yourself with the command's help.\n\n      openstack subnet create --help\n\n   Below are just the basic arguments of the command.\n\nRun the command:\n\n   Linux/macOS (bash, zsh)\n   Windows (PowerShell)\n\n\n      openstack subnet create  \\\n     --subnet-range  \\\n     --network  \\\n     --dns-nameserver  \\\n     --gateway\n\n\n      openstack subnet create  `\n     --subnet-range  `\n     --network  `\n     --dns-nameserver  `\n     --gateway\n\n\nEditing a subnet\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nIn personal account, go to Virtual networks → Networks.\nClick on the name of the cloud network where the subnet is located.\nClick   for the subnet you want to change and select Edit subnet.\nMake the necessary changes.\nClick the Save button.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nGet ID of the subnet you want to edit.\n\nTo apply (set) the necessary settings to a subnet or to cancel them (unset):\n\n   Familiarize yourself with the commands' help.\n\n            openstack subnet set --help\n\n            openstack subnet unset --help\n\n      Below are just the basic arguments of the commands.\n\n   Run the command:\n\n      Linux/macOS (bash, zsh)\n      Windows (PowerShell)\n\n\n            openstack subnet   \\\n        --allocation-pool start=,end= \\\n        --dns-nameserver  \\\n        --host-route destination=,gateway=\n\n\n            openstack subnet   `\n        --allocation-pool start=,end= `\n        --dns-nameserver  `\n        --host-route destination=,gateway=\n\n\nDeleting a subnet\n\n\n\nThere must be at least one subnet on the cloud network.\nOnce a subnet is deleted, it cannot be restored.\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nIn personal account, go to Virtual networks → Networks.\nClick on the name of the cloud network where the subnet is located.\nClick   for the subnet you want to delete and select Delete Subnet.\nIn the window that opens, click the Confirm button.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nGet ID of the subnet you want to delete.\n\nRun the command:\n\n      openstack subnet delete\n\n","url":"/en/networks/vnet/service-management/net","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"9ddacf7c-21f9-428d-8a74-95b72780cc64","title":"Shared networks","data":"Shared networks allow you to work with one network in several projects.\n\n{heading(Sharing network)[id=share]}\n\nOnly a user with the role of the project owner can share the network with other projects.\n\nTo share a network with other projects:\n\n\n\nPersonal account\n\n\n\nGo to you VK Cloud personal account.\nSelect the project.\nGo to Cloud Networks → Networks.\nClick more-icon for the network and select sharing-icon Share network.\nIn the window that opens select a type of the project that you want to share the network with:\n\n   My projects: allows to share a network with projects where you are also the owner.\n\n     As you select this type, select the unique identifier (PID) in the ID of the project field. PID looks like mcsNNNNNNNNNN. You can select several projects here.\n\n   Other projects: allows to share a network with any project.\n\n     As you select this type, specify  the Project ID in the Project ID in OpenStack field. Project ID looks like exampled4ef0547e5b222f445, differs from unique identifier (PID). You can specify several projects here.\nClick Send.\n\n\n\n\nUsers of the specified projects will receive a notification of an invitation to the shared network and will be able to accept or decline it. The user's decision applies to the entire project: if one user declines the offer to join the network, the network will be unavailable for the entire project.\n\n{heading(Viewing projects with shared network access)[id=view_shared]}\n\n\n\nPersonal account\n\n\n\nGo to you VK Cloud personal account.\nSelect the project.\nGo to Cloud Networks → Networks.\nClick the name of the required network.\nGo to the Information tab.\n\nThe Network owner parameter specifies the PID of the project that shares its network.\n\nThe Network users parameter specifies the PIDs of projects that are allowed access to the network. The parameter is displayed only for the network owner.\n\n{heading(Unsharing network)[id=unshare]}\n\nOnly the network owner can disconnect a project from the shared network.\n\nBefore unsharing, make sure that all VMs, PaaS services, file storages, balancers and advanced routers that are connected to the shared network are deleted in the related project.\n\nTo disconnect a project from the shared network:\n\n\n\nPersonal account\n\n\n\nGo to you VK Cloud personal account.\nSelect the project.\nGo to Cloud Networks → Networks.\nClick the name of the required network.\nGo to the Information tab.\nClick Edit.\nThe Network users parameter specifies the PIDs of projects that are allowed access to the network. Click cross-icon to deny the project access to the shared network.\nA window will open to warn you that the corresponding ports will be disabled. Click Unshare.\nClick Save changes.\n\n{heading(Deleting shared network)[id=delete_shared]}\n\nOnly network owner can delete a shared network.\n\nTo delete a shared network:\n\n\n\nPersonal account\n\n\n\nDelete all occupied shared network ports in your project. To remove some ports, you need to remove or disable the services that are connected to them: VMs, PaaS services, file storages, balancers, advanced router interfaces directed to the shared network.\nMake sure that connected projects also have all services on the shared network removed or disabled.\n\n As long as there are occupied ports on the shared network, it is impossible to delete the network.\n\n\n{include(/en/_includes/_delete_net.md)}\n\n\n\n","url":"/en/networks/vnet/service-management/shared-nets","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"b372c224-4fe7-45ab-bd5d-20f0bc384c11","title":"Ports","data":"You can manage OpenStack ports: view ports, add, edit and delete them.\n\n\n\nAll of the following operations are not available on the external network.\nYou cannot manage SNAT device ports.\n\nViewing a list of ports and port information\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Networks.\nClick the name of the necessary network, then the name of the necessary subnet.\nNavigate to the Ports tab.\n\n   A list of ports will be displayed.\n\nClick the name of the necessary port.\n\n   Its information will be displayed.\n\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nTo see a list of all ports, run the command:\n\n      openstack port list\n\nTo see a list of all ports on a subnet:\n\n   Run the command to get subnet IDs and names:\n\n            openstack subnet list\n\n   Run the command:\n\n            openstack port list --fixed-ip subnet=\n\nTo see detailed port information, run the command with the port ID or name obtained earlier:\n\n      openstack port show\n\nUse one of the commands to get detailed information about the supported parameters:\n\nopenstack port list --help\n\nopenstack subnet list --help\n\nopenstack port show --help\n\nAdding a port\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Networks.\nClick the name of the necessary network, then the name of the necessary subnet.\nNavigate to the Ports tab.\nClick the Add port button.\nSet the port parameters:\n\n   port name,\n   (optional) DNS name of the port,\n   IP address of the port.\n\nClick the Create port button.\n\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nRun the command:\n\n      openstack port create  --network  --fixed-ip subnet=,ip-address=\n\nFor detailed information on supported parameters, run the command:\n\nopenstack port create --help\n\nEnabling and disabling a port\n\nEnabling a port\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nThis is a group operation: you can enable multiple disabled ports at once, if necessary, by selecting them using the checkboxes.\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Networks.\nClick the name of the necessary network, then the name of the necessary subnet.\nNavigate to the Ports tab.\nEnable the port in one of the following ways:\n\n   Using the checkboxes:\n\n     Select the necessary port using the checkbox.\n     Click the Enable port button.\n     Confirm the operation.\n\n   Using the menu:\n\n     Click   for the required port and select Enable port.\n     Confirm the operation.\n\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nGet the name or ID of the necessary port.\n\nRun the command:\n\n      openstack port set  --enable\n\nFor detailed information on supported parameters, run the command:\n\nopenstack port set --help\n\nDisabling a port\n\nA disabled port does not allow any traffic to pass through.\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nThis is a group operation: you can disable multiple enabled ports at once, if necessary, by selecting them using the checkboxes.\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Networks.\nClick the name of the necessary network, then the name of the necessary subnet.\nNavigate to the Ports tab.\nDisable the port in one of the following ways:\n\n   Using the checkboxes:\n\n     Select the necessary port using the checkbox.\n     Click the Disable port button.\n     Confirm the operation.\n\n   Using the menu:\n\n     Click   for the required port and select Disable port.\n     Confirm the operation.\n\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nGet the name or ID of the necessary port.\n\nRun the command:\n\n      openstack port set  --disable\n\nFor detailed information on supported parameters, run the command:\n\nopenstack port set --help\n\nEditing a port\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Networks.\nClick the name of the necessary network, then the name of the necessary subnet.\nNavigate to the Ports tab.\nClick   for the required port and select Edit.\nSet the parameters of the port:\n\n   port name,\n   DNS name of the port,\n   IP address of the port.\n\nClick the Save changes button.\n\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nGet the name or ID of the necessary port.\n\nChange the parameters of the port:\n\n   Port name:\n\n          openstack port set  --name\n\n   DNS name of the port:\n\n          openstack port set  --dns-name\n\n   IP address of the port:\n\n          openstack port set  --no-fixed-ip --fixed-ip subnet=,ip-address=\n\nFor detailed information on supported parameters, run the command:\n\nopenstack port set --help\n\nConfiguring IP Source Guard for a port\n\nThis mechanism allows only traffic for which the source IP address is in the allowed-address list to be allowed to leave the port.\n\n\n\nOpenStack CLI\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nGet the name or ID of the necessary port.\n\nTo add a single source IP address, run the command:\n\n      openstack port set  --allowed-address ip-address=\n\n   If you need to add more IP addresses, repeat this command for each of them.\n\nTo remove a single source IP address, run the command:\n\n      openstack port unset  --allowed-address ip-address=,mac-address=\n\n   If you need to remove more than one IP address, repeat this command for each of them.\n\nFor detailed information on supported parameters, run one of the commands:\n\nopenstack port set --help\n\nopenstack port unset --help\n\nDeleting a port\n\n\n\nIt is not possible to delete a port if it is in use by the router.\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nThis is a group operation: you can delete multiple ports at once, if necessary, by selecting them using the checkboxes.\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Networks.\nClick the name of the necessary network, then the name of the necessary subnet.\nNavigate to the Ports tab.\nDelete the port in one of the following ways:\n\n   Using the checkboxes:\n\n     Select the necessary port using the checkbox.\n     Click the Delete port button.\n     Confirm the operation.\n\n   Using the menu:\n\n     Click   for the required port and select Delete port.\n     Confirm the operation.\n\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nGet the name or ID of the necessary port.\n\nRun the command:\n\n      openstack port delete\n\nFor detailed information on supported parameters, run the command:\n\nopenstack port delete --help\n\n\n\n","url":"/en/networks/vnet/service-management/ports","description":"Managing OpenStack ports in the virtual networks service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"749d3f26-cda0-4379-a807-3f4df4e7acc3","title":"Firewall rules","data":"A security group is a set of custom traffic rules that can be assigned to instances ports.\n\nView a list of security groups and information about them\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Firewall settings.\n\nA list of security groups will be displayed.\n\nClick on the security group name.\n\nA page with detailed information about it will open.\n\n\n\n\nMake sure OpenStack CLI is installed. Also check that you can sign in the OpenStack CLI.\n\nTo view the list of security groups, run the command:\n\nopen stack security group list\n\nTo view detailed information about a security group, run the command:\n\nopenstack security group show\n\nTo view security group rules:\n\nopenstack security group rule list --long\n\nCreate a security group\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Firewall settings.\nClick the Add button.\nEnter a name for the security group.\nAdd a description.\nClick Create Group.\n\n\n\n\nMake sure OpenStack CLI is installed. Also check that you can sign in the OpenStack CLI.\n\nView the security groups in the project:\n\nopen stack security group list\n\nGet information about the security group:\n\nopenstack security group show\n\nCreate a security group:\n\nopenstack security group create --description\n\nEditing the name and description of the security group\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Firewall settings.\nClick on the security group name.\n\nA page with detailed information about it will open.\n\nClick the pencil icon next to the security group name.\n\nEdit the name and (if necessary) the name of the security group.\n\nClick the Save button.\n\n\n\n\nMake sure OpenStack CLI is installed. Also check that you can sign in the OpenStack CLI.\n\nTo change the description and name of a security group, run the command:\n\nopenstack security group set --description  --name\n\nAdding a rule\n\nA security group rule is a set of parameters that define the conditions for traffic to pass through. Rules are combined into groups, which in turn apply to instance ports.\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Firewall settings.\nClick on the security group name.\nIn the Incoming traffic and Outgoing traffic section , click + Add rule.\nSelect the type of traffic (SSH, HTTP, HTTPS, etc.)\nSelect the required protocol.\nIn the Remote address section , specify the address for which the selected type of traffic will be allowed:\n\n   All IP addresses;\n   Range of IP addresses;\n   Security team.\n\nClick Add description and in the field that appears, describe the new rule.\nClick Save Rule.\n\n\n\n\nMake sure OpenStack CLI is installed. Also check that you can sign in the OpenStack CLI.\n\nView the list of group rules:\n\nopenstack security group rule list --long\n\nCreate a rule:\n\nopenstack security group rule create\n\nAvailable arguments for the rule creation command:\n\n--remote-ip - specifies the address from which connections can be made (in CIDR format).\n--remote-group - specifies a group whose instances can be a source of traffic.\n--dst-port - destination port, required for TCP and UDP protocols.\n--protocol - protocol, it is possible to specify the name, number or permission of all protocols (any).\n--description is an arbitrary description.\n--icmp-type — ICMP type.\n--icmp-code — ICMP code.\n--ingress - apply the rule for incoming traffic.\n--egress - apply the rule for outgoing traffic.\n--ethertype - EtherType value (IPv4, IPv6).\n\nDeleting a rule\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Firewall settings.\nClick on the security group name.\nClick the trash icon in the line with the required rule.\nClick   for the rule you want to delete and select Delete button.\nPress the Confirm button.\n\n\n\n\nMake sure OpenStack CLI is installed. Also check that you can sign in the OpenStack CLI.\n\nTo view detailed information for a rule, run the command:\n\nopenstack security group rule show\n\nTo delete a rule, run the command:\n\nopenstack security group rule delete\n\nAssign a rule group to an instance\n\nTo apply a rule set to a virtual machine, the security group that contains the rule set must be applied to the virtual machine.\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Firewall settings.\nClick on the security group name.\nIn the Virtual machines with rule group section , click Add virtual machine.\nSelect the instances to which the group will be added.\nClick Add Rule Group.\n\n\n\n\nMake sure OpenStack CLI is installed. Also check that you can sign in the OpenStack CLI.\n\nRun the command:\n\nopenstack server add security group\n\nDetach a group from an instance\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Firewall settings.\nClick on the security group name.\nIn the Virtual Machines with Rule Group section , hover your mouse over the instance row.\nClick the trash can icon.\nPress the Confirm button.\n\n\n\n\nMake sure OpenStack CLI is installed. Also check that you can sign in the OpenStack CLI.\n\nRun the command:\n\nopenstack server remove security group\n\nDeleting a security group\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\n\n\nA group cannot be removed as long as there are ports that use this group. Also, the default security group and other preset groups cannot be removed.\n\n\n\nThis is a group operation: if necessary, you can delete several security groups at once by selecting them using the checkboxes.\n\nGo to personal account VK Cloud.\nSelect a project.\nGo to Virtual networks → Firewall settings.\nClick   for the security group you want to delete and select Delete button.\n\n\n\n\nMake sure OpenStack CLI is installed. Also check that you can sign in the OpenStack CLI.\n\nRun the command:\n\nopenstack security group delete\n","url":"/en/networks/vnet/service-management/secgroups","description":"A security group is a set of custom permissive traffic rules that can be assigned to instance ports. The firewall uses these groups to control traffic.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"4fd1aa43-3e2f-4d80-8541-996083376d4d","title":"Floating IP addresses","data":"You can manage floating IP addresses: view, add and remove them from a project, as well as bind and unbind these IP addresses.\n\nViewing a list of floating IP addresses\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary IP address is located.\nGo to Virtual networks → Floating IP.\n\n   A list of floating IP addresses will be displayed (External IP column).\n\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nRun the command:\n\n      openstack floating ip list\n\n\nAdding floating IP address to the project\n\n\n\nA floating IP address is randomly assigned from a shared pool.\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary IP address is located.\nGo to Virtual networks → Floating IP.\nClick the Add IP to project button.\n(Optional) Add a description.\nClick the Add IP button.\n\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nRun the command:\n\n      openstack floating ip create ext-net\n\n\nEditing floating IP address description\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary IP address is located.\nGo to Virtual networks → Floating IP.\nClick   for the required floating IP address and select Edit description.\nSet the description.\nClick the Save button.\n\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nGet list of floating IP addresses. Find in the list the ID of the floating IP address for which you want to edit the description.\n\nRun the command:\n\n      openstack floating ip set  --description \"\"\n\n\nBindind a floating IP address\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary IP address is located.\nGo to Virtual networks → Floating IP.\nClick   for the floating IP address, for which the Internal IP column shows `Unbinded', and select Bind IP.\n\n\n   Unbound IP addresses also contain a link to the binding in the adjacent column.\n\n   To bind a floating IP to another internal IP, first unbind it from the current one.\n\n\nFrom the drop-down list, select the OpenStack port with the internal IP address to which you are binding.\nClick the Confirm button.\n\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nGet list of floating IP addresses. Find in the list the ID of the floating IP address that you want to bind to the port.\nGet the list of ports. Find in the list the port ID of the port to which you want to bind the floating IP address.\nRun the command:\n\n      openstack floating ip set  --port\n\n\nUnbinding floating IP address\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary IP address is located.\nGo to Virtual networks → Floating IP.\nClick   for the floating IP address that is bound to the internal IP and select Unbind IP.\nClick the Confirm button.\n\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nGet list of floating IP addresses. Find in the list the ID of the floating IP address that you want to unbind from the port.\n\nRun the command:\n\n      openstack floating ip unset  --port\n\n\nRemoving floating IP address from the project\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary IP address is located.\nGo to Virtual networks → Floating IP.\nClick   for the required floating IP address and select Remove IP from the project. To remove multiple IPs at once, select them using the checkboxes and click Remove IP from project.\nClick the Confirm button.\n\n\n\n\nMake sure the OpenStack CLI is installed and you can authorize in it.\n\nGet list of floating IP addresses. Find in the list the ID of the floating IP address that you want to remove from the project.\n\nExecute the command:\n\n      openstack floating ip delete\n\n","url":"/en/networks/vnet/service-management/floating-ip","description":"Managing floating IP addresses in the virtual networks service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"f0ebf594-b4a4-4290-8574-bab3ef7fa9fe","title":"Standard routers","data":"You can manage routers: view, edit and delete them.\n\nViewing a list of routers and information about them\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Routers.\n\n   A list of routers will be displayed.\n\nClick on the name of the necessary router.\n\n   A page with detailed information about the router will open. You can also edit router parameters on this page.\n\n\n\n\nMake sure that:\n\n   OpenStack CLI is installed.\n   You can authorize in the OpenStack CLI.\n\nTo see a list of routers and their IDs, run the command:\n\n      openstack router list\n\nTo see detailed information about a router, run the command:\n\n      openstack router show\n\n   In the output of the command:\n\n   id: the router's ID.\n   external_gateway_info: information on connection to external network. A blank field means that connection to the external network is not configured.\n   interfaces_info: information about interfaces of the router. A blank field means that no interfaces have been added.\n   routes: information about static router routes. An empty field means no static routes were added.\n\nAdding a router\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Routers.\nClick the Add router button.\nSet the router parameters:\n\n   Name.\n   External network connection: if this option is selected, the router will have Internet access and a public IP address.\n\n     Select this option if you plan to assign floating IP addresses to ports on subnets connected to the router and provide Internet access from these subnets.\n\n   Subnet List: one or more subnets to be connected to the router.\n\nClick the Create button.\n\n\n\n\nMake sure that:\n\n    OpenStack CLI is installed.\n    You can authorize in the OpenStack CLI.\n\nCreate a router of the necessary type:\n\n   With connection to external network\n   Without connection to the external network\n\n\n      openstack router create  --external-gateway ext-net\n\n\n      openstack router create\n\n\n   A router with an external network connection will have Internet access and a public IP address. Create this type of router if you plan to assign floating IP addresses to ports on subnets connected to the router and provide Internet access from those subnets.\n\nLearn how to connect one or more interfaces to the router by either editing the router or managing its interfaces directly.\n\nEditing a router\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Routers.\nPerform one of the actions for the router you want to edit:\n\n   Click the name of the router and then in the General information tab, click Edit router.\n   Click   for the router and select Edit router.\n\nPerform one of the available actions:\n\n   Change the router name.\n   Manage the connection to the external network: you can enable or disable the corresponding option. If this option is selected, the router will have Internet access and a public IP address.\n\n     Select this option if you plan to assign floating IP addresses to ports on subnets connected to the router and provide Internet access from those subnets.\n\n   Manage the subnets connected to the router. You can add new subnets to the router by selecting them from the list, or remove already added subnets.\n\n\n     You can also add or remove subnets by changing the router interface.\n\n\nAfter editing the router, click the Save button.\n\n\n\n\nMake sure that:\n\n    OpenStack CLI is installed.\n    You can authorize in the OpenStack CLI.\nGet the ID of the necessary router.\nPerform one of the available actions:\n\n   Change the name of the router:\n\n          openstack router set  --name\n\n   Manage the connection to the external network:\n\n     Enable external network connection\n     Disable external network connection\n\n\n          openstack router set  --external-gateway ext-net\n\n\n          openstack router unset  --external-gateway\n\n\n     Connect the router to an external network if you plan to assign floating IP addresses to ports on subnets connected to the router and provide Internet access from those subnets.\n\n   Manage the subnets connected to the router. You can add new subnets to the router, or remove already added subnets.\n\n     Add a subnet\n     Remove a subnet\n\n\n     Get the ID of the necessary subnet.\n     Run the command:\n\n                openstack router add subnet\n\n     As a result:\n\n     The selected subnet will be connected to the router.\n     Interfaces INTERFACE_DISTRIBUTED and (if the router is connected to an external network) SNAT related to this subnet will appear in the list of interfaces.\n\n\n          openstack router remove subnet\n\n     As a result:\n\n     The following will be deleted:\n       The port corresponding to the interface of the subnet being removed.\n       The SNAT interface (if there is one) and its corresponding port.\n\n     The subnet, where those interfaces were located, will be removed from the router.\n\n\n     You can also add or remove a subnet by changing the interface of the router.\n\n\nManaging an interface\n\nRouter interface management is an alternative to managing subnets connected to it.\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to the router page and select tab Interfaces.\n\nTo add an interface:\n\n    Click the Add Interface button.\n    Select the required subnet from the list.\n    Click the Add Interface button.\n\n    As a result:\n\n    The selected subnet will be connected to the router.\n    The INTERFACE_DISTRIBUTED and (if Connect to external network is selected) SNAT interfaces related to this subnet will appear in the list of interfaces.\n\nTo remove an interface:\n\n    Click   for the interface and select Remove Interface.\n    Confirm the deletion.\n\n    As a result:\n\n    The corresponding SNAT interface (if present) will also be removed.\n    The subnet that this interface was on will be disconnected from the router.\n\n\n\n\nMake sure that:\n\n    OpenStack CLI is installed.\n    You can authorize in the OpenStack CLI.\n\nGet the ID of the necessary router.\n\nTo add an interface:\n\n    Get the ID of the OpenStack port located on the subnet you want to connect to the router. This port should not be used by any objects (load balancers, virtual machines, etc.) If there is no such port, create it.\n    Run the command:\n\n              openstack router add port\n\n    As a result:\n\n    The selected subnet will be connected to the router.\n    The INTERFACE_DISTRIBUTED and (if the router is connected to an external network) SNAT interfaces related to this subnet will appear in the list of interfaces.\n\n   To get a list of interfaces, see router details using the OpenStack CLI.\n\n\nTo remove an interface:\n\n    Look at the interfaces (ports) configured on the router.\n    Run the command:\n\n              openstack router remove port\n\n    As a result:\n\n    Will also be removed:\n      port corresponding to the interface to be removed.\n      SNAT interface (if any) and its corresponding port.\n\n    The subnet that these interfaces were on will be disconnected from the router.\n\nManaging Static Routes\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to the router page, then select the Static Routes tab.\n\nTo add a static route:\n\n    Click the Add Static Route or Add button.\n    Set the destination network and specify the network prefix.\n    Specify an intermediate node (next hop).\n    Click the Add Interface button.\n\nTo delete a static route, click   for the required route and select Delete Interface.\n\n\n\n\nMake sure that:\n\n    OpenStack CLI is installed.\n    You can authorize in the OpenStack CLI.\n\nGet the ID of the necessary router.\n\nTo add a static route, run the command:\n\n        openstack router set  --route destination=,gateway=\n\nTo remove a static route:\n\n    Look up the static routes configured on the router.\n    Run the command:\n\n              openstack router unset  --route destination=,gateway=\n\nTo remove all static routes, run the command:\n\n        openstack router set  --no-route\n\n\nRemoving the Router\n\n\n\nBefore deleting the router, delete the static routes and the configured interfaces (if any) one by one before deleting the router.\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nThis is a group operation: if necessary, you can delete several routers at once by selecting them using the checkboxes.\n\nTo remove a router:\n\nGo to personal account VK Cloud.\nSelect the project where the required balancer is located.\nGo to Virtual networks → Routers.\nDo one of the following for the required router:\n\n    Select the router using the checkbox, then click the Remove Router button.\n    Click   for the router and select Remove Router.\n    Click on the name of the router, then on the General Information tab, click on the Delete Router button.\n\nConfirm the removal of the router.\n\n\n\n\nMake sure that:\n\n    OpenStack CLI is installed.\n    You can authorize in the OpenStack CLI.\n\nGet the ID of the necessary router.\n\nRun the command:\n\n      openstack router delete\n\n","url":"/en/networks/vnet/service-management/router","description":"Managing standard routers in the virtual networks service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"009acd1a-1ee4-43ed-b7d3-d4103833482a","title":"Managing advanced routers","data":"You can manage advanced routers: view, edit and delete them.\n\nAdvanced routers are only available with SDN Sprut. To find out which SDN is used in your project or to connect another, contact technical support.\n\nView a list of advanced routers\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nGo to Cloud networks → Routers. A list of routers will be displayed. For advanced routers, the Router Type column indicates Advanced.\nClick the name of the router you need. A page will open with detailed information about it. You can edit router parameters on this page .\n\nAdding an advanced router\n\n\n\nPersonal account\n\n\n\nGo to your personal account VK Cloud.\nGo to Cloud networks → Routers.\nClick the Add router button.\nIn the Router type field choose Advanced. If this field is missing, SDN Sprut is not enabled in your project. Please, contact technical support, to enable SDN Sprut and advanced routers.\nSet parameters of the router:\n\n   Name. You can use numbers, Latin letters, spaces, and special characters: ?, !, ~, @, #, $, %, ^, \", &, _, -, +, *, =, ;, :, ,, ., /, \\, |, `  `, [, ], {, }, (, ), `.\n   Availability zone: choose an availability zone you need.\n   Description: an optional field.\n   SNAT: enable the option if you need to convert private IP addresses into public ones for access from the project network to the Internet. When you add the router interface directed to an external network, SNAT is enabled automatically.\n\n      SNAT is active only if the router is connected to an external network.\n\n\nClick the Add button.\n\nEditing an advanced router\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nGo to Cloud networks → Routers.\nOpen the router editing window by one of the following ways:\n\n   Click the name of the router and then click the Edit button on the General information tab.\n   Click   for the router and select Edit router.\n\nYou can do the following:\n\n   Edit the name of the router.\n   Edit the description of the router.\n   Enable or disable SNAT. Enabling the option allows you to convert private IP addresses into public ones for access from the project network to the Internet. When you add the router interface directed to an external network, SNAT is enabled automatically.\n\n      SNAT is active only if the router is connected to an external network.\n\n\nClick the Save button to complete the editing.\n\nViewing an advanced router statistics\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Statistics tab.\n\nThe tab provides statistical information on the advanced router:\n\nThe time when the last information was received from the service. Information about the service status is updated at intervals of up to 5 minutes.\nThe router's readiness for use.\nThe number of routes received in announcements from BGP neighbors and added to the routing table.\nThe number of static routes added to the routing table.\nRouter interfaces, traffic incoming and outgoing on those interfaces, and overall traffic incoming and outgoing to the router.\nInformation about BGP sessions: session status, neighbor name, incoming and outgoing BGP announcements, date and time of the last change in the BGP session.\nInformation about static routes: destination network, intermediate node and status.\n\nViewing a VPN information\n\nYou can find out information about VPN tunnels that use the advanced router.\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the VPN tab.\n\nThe tab displays statistical information on the VPN tunnels connected to the advanced router.\n\nTo find out the detailed information about a VPN tunnel, click its name. The VPN settings page will open.\n\nDeleting an advanced router\n\nYou can delete an advanced router in the same way as a standard router.\n","url":"/en/networks/vnet/service-management/advanced-router/manage-advanced-routers","description":"Adding, editing, deleting advanced routers and viewing statistics.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"51df0506-57ab-4ded-ab7f-88743d6d3f03","title":"Managing interfaces","data":"To connect the router to subnets, you need to add interfaces. To connect the router to an external network, you need to add an interface directed to the external network.\n\nAdding interfaces of advanced router\n\n\n\nPersonal account\n\n\n\n\n\nYou can also connect a subnet to the router in the Cloud networks → Networks section of your personal account.\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Interfaces tab.\nClick the Add interface button.\nFill in the Name field. You can use numbers, Latin letters, spaces, and special characters: ?, !, ~, @, #, $, %, ^, \", &, _, -, +, *, =, ;, :, ,, ., /, \\, |, `  `, [, ], {, }, (, ), `.\nSelect a subnet from the list or create a new one. If you need the Internet access to the router, select one of the external network subnets:\n\n   internet for SDN Sprut;\n   ext-net for SDN Neutron.\n\n   An external network is required for SNAT and VPN. You can add only one interface per router with a connection to an external network. When you add an interface with an external network connection, SNAT is enabled automatically, even if the option was not enabled when creating the router.\n(Optional) By default, an IP address of the interface is generated automatically. To set a custom IP address, enable the Specify interface IP address option, then enter the address in the field below. You cannot select the first subnet address, for example 10.0.0.1.\n(Optional) Add a description for the interface.\nClick the Add button.\n\nAs a result:\n\nThe added interface will appear in the list of interfaces.\nThe selected subnet will be connected to the router.\n\nRemoving interfaces of advanced router\n\n\n\nPersonal account\n\n\n\nThis is a group operation: you can remove several interfaces at once by selecting them using the checkboxes.\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Interfaces tab.\nRemove the interface in one of the following ways:\n\n    Click   for the interface and select Remove interface.\n    Set the checkbox for the required interface, then click the Remove interface button above the table.\nConfirm the removal.\n\nAs a result:\n\nThe interface will be removed.\nThe interface subnet will be disconnected form the router.\n\n\n\n","url":"/en/networks/vnet/service-management/advanced-router/manage-interfaces","description":"Adding and deleting interfaces of advanced router.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"4ed59bfa-9e19-4a9e-a2c1-9e5e2ba76baf","title":"Static routing","data":"Adding a static route\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nOpen the Static routes tab.\nClick the Add static route button.\nSet the destination network and specify the network prefix.\nSpecify the next hop.\n(Optional) Specify a metric that determines the priority of static routes in the routing table.\n(Optional) Add a route description.\nClick the Add button.\n\nDeleting a static route\n\n\n\nPersonal account\n\n\n\nThis is a group operation: you can delete several routes at once by selecting them using the checkboxes.\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nOpen the Static routes tab.\nDelete the route in one of the following ways:\n\n    Click   for the route and select Delete static route.\n    Set the checkbox for the required route, then click the Delete button above the table.\nConfirm the deletion.\n\n\n\n","url":"/en/networks/vnet/service-management/advanced-router/manage-static-routs","description":"Adding and deleting static routes of advanced router.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"8f2eb4b0-dd96-4bfe-98fd-2dfbfa6ca8e3","title":"Dynamic routing","data":"An advanced router allows you to establish connection between autonomous networks using the BGP protocol. To do this, you need to add a BGP router and specify BGP neighbors.\n\nAdding a BGP router\n\n\nYou can only add one BGP router per advanced router.\n\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Dynamic Routing tab.\nClick the Add a BGP router button.\nSpecify the name of the BGP router. You can use only numbers, Latin letters, spaces, and special characters: ?, !, ~, @, #, $, %, ^, \", &, _, -, +, *, =, ;, :, ,, ., /, \\, |, `  `, [, ], {, }, (, ), `.\nSpecify an IP address for the BGP router — the router interface directed to the transit network. Use the highest IP address on the loopback or physical interface.\nEnter an ASN from the range 64512–65534.\n(Optional) Disable the BGP router if you do not need to establish a connection at this time. The BGP router will become inactive.\n(Optional) Add a description for the BGP router.\nClick the Create button.\n\nEditing a BGP router\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Dynamic Routing tab.\nOpen the editing page of the dynamic router by one of the following ways:\n\n    Click the name of the BGP router, then click the Edit button.\n    Click   for the BGP router and select Edit.\n\nYou can do the following:\n\n    Edit the router name.\n    Edit the router description.\n    Enable the ECMP option. The router will create multiple routes to the same destination. When transferring data, if one of the routes is unavailable, another will be automatically selected. This allows for increased throughput and fault tolerance.\n    Enable the Graceful restart option. After the peer is restarted, the BGP router will retain its state and continue transmitting data.\n    Enable the Long lived graceful restart option. The BGP router will maintain its state for a longer time in the event of a BGP peer failure.\n    Disable the BGP router if you do not need to establish communication at the moment. The BGP router will become inactive.\nClick the Save changes button.\n\nDeleting a BGP router\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Dynamic Routing tab.\nDelete the router in one of the following ways:\n\n    Click   for the router and select Delete.\n    Set the checkbox for the required router, then click the Delete button above the table.\nConfirm the deletion.\n\nAdding a BGP neighbor\n\nTo connect autonomous networks using the BGP protocol, you need to specify the BGP routers which will connect to each other.\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Dynamic Routing tab.\nSelect the BGP router.\nGo to the BGP neighbors tab.\nClick Add a BGP neighbor.\n(Optional) Specify the name of the BGP neighbor. You can use only numbers, Latin letters, spaces, and special characters: ?, !, ~, @, #, $, %, ^, \", &, _, -, +, *, =, ;, :, ,, ., /, \\, |, `  `, [, ], {, }, (, ), `.\nSpecify the interface directed to the transit network of the neighboring BGP router.\nSpecify the ASN of the remote network. If the network does not have an ASN defined, use private ASNs from the range 64512–65534.\n(Optional) Disable the BGP neighbor if you do not need to establish a connection at the moment. The BGP router will not send requests to the remote router.\n(Optional) Add a description for the BGP neighbor.\nClick the Add button.\n\n\n\n\nAfter you add the BGP neighbor, the router will try to establish a connection with it. When this is done, you will see:\n\nthe mark next to the name of the BGP neighbor is green\nBFD is enabled\n\nEditing a BGP neighbor\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Dynamic Routing tab.\nSelect the BGP router and then go to the BGP neighbors tab.\nSelect the BGP neighbor.\nOpen the editing window of the BGP neighbor by one of the following ways:\n\n    Click the BGP neighbor name, then click the Edit button.\n    Click   for the BGP neighbor and select Edit.\n\nYou can do the following:\n\n    Edit the BGP neighbor name.\n    Edit the BGP neighbor description.\n    Edit the neighbor operating mode that determines the ability to exchange all routing options for each announcement. You can select the mode in the Add path field:\n\n       off: disable\n       on: enable in both directions\n       rx: enable reception only\n       tx: enable delivery only\n\n    Enable the BFD option. This allows the use of session control via BFD (руBidirectional Forwarding Detection).\n    Enable the Next Hop Self option. The BGP neighbor will get the IP adress of the router interface as the next hop.\n    Disable the BGP neighbor if the connection is not currently required. The BGP router will not send requests to the remote router.\nClick the Save changes button.\n\nDeleting a BGP neighbor\n\n\n\nPersonal account\n\n\n\nThis is a group operation: you can delete several neighbors at once by setting the checkboxes.\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Dynamic Routing tab.\nSelect the BGP router and then go to the BGP neighbors tab.\nSelect the BGP neighbor.\nDelete the BGP neighbor in one of the following ways:\n\n    Click   for the BGP neighbor and select Delete.\n    Set the checkbox for the required BGP neighbor, then click the Delete button above the table.\n    Click the name of the BGP neighbor, then click the Delete button above the table.\nConfirm the deletion.\n\nView a BGP announcement\n\nAfter setting up a BGP neighbor, an advanced router will begin transmitting to the neighbor BGP announcements of all networks to which its interfaces are directed.\n\nTo view BGP announcements:\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Dynamic Routing tab.\nSelect the BGP router.\nGo to the BGP announcements tab.\n\nThe tab displays a list of all BGP announcements. Active announcements are marked with green marks.\n\nAdding a BGP announcement\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Dynamic Routing tab.\nSelect the BGP router and then go to the BGP announcements tab.\nClick the Add a BGP announcement button.\nSelect the type of announcement:\n\n   Static\n   Connected\n\n\n   The announcement conveys information about a manually specified static route.\n\n    Specify the request parameters:\n\n       Network: the IP address with the network mask that will be announced to a BGP neighbor.\n       Default Gateway: the gateway IP address. If the gateway is available, the network will be announced in BGP.\n       (Optional) Disable the BGP announcement if this network does not need to be announced to a BGP neighbor. The BGP announcement will become inactive.\n       (Optional) Add a description for the BGP announcement.\n    Click the Add button.\n\n\n   The announcement conveys information about a subnet connected to the advanced router interface.\n\n   Select a subnet connected to the router in the Connected subnet field.\n   Click the Connect button.\n\n\nEditing a BGP announcement\n\nYou can only edit the Static announcements.\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Dynamic Routing tab.\nSelect the BGP router and then go to the BGP announcements tab.\nOpen the editing page of the BGP announcement by one of the following ways:\n\n    Click   for the announcement and select Edit.\n    Click the IP address of the announced subnet, then click the Edit button.\n\nYou can do the following:\n\n   Disable the announcement if this network does not need to be announced to a BGP neighbor.\n   Edit the description of the announcement.\n\nClick the Save changes button.\n\nDeleting a BGP announcement\n\n\n\nPersonal account\n\n\n\nThis is a group operation: you can delete several announcements at once by selecting them using the checkboxes.\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the Dynamic Routing tab.\nSelect the BGP router and then go to the BGP announcements tab.\nSelect the BGP announcement.\nDelete the BGP announcement in one of the following ways:\n\n    Click   for the BGP announcement and select Delete.\n    Set the checkbox for the required BGP announcement, then click the Delete button above the table.\n    (For Static only) Click the BGP announcement name, then click the Delete button above the table.\nConfirm the deletion.\n\n\n\n","url":"/en/networks/vnet/service-management/advanced-router/manage-bgp","description":"Adding, editing and deleting a BGP router, managing BGP neighbors и BGP announcements.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"eff338e4-5490-45a2-a503-1ac88c728cbf","title":"Setting DNAT rules","data":"To provide access to a private network from the Internet, configure DNAT— destination network address translation for an advanced router. DNAT forwards incoming packets from an external IP address or port to an IP address or port inside the private network. At the same time, from one public IP address, you can configure data transfer to many internal addresses of private networks using unique ports (TCP or UDP).\n\nAdding DNAT rule\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the DNAT tab.\nClick plus-icon Add a DNAT rule.\nSpecify a name of the DNAT rule. You can only use numbers, Latin letters, spaces and characters: ?, !, ~, @, #, $, %, ^, \", &, _, -, +, *, =, ;, :, ,, ., /, \\, |, `  `, [, ], {, }, (, ), `.\nSelect a router interface that will receive a data packet. If there is no interface you need, select Add a new interface and add a new interface.\nSelect a data transfer protocol:\n\n   All protocls and ports: the rule will apply to all ports and protocols.\n   TCP: the rule will apply to the specified port and the TCP protocol.\n   UDP: the rule will apply to the specified port and the UDP protocol.\n\nIn the IP after translation field enter IP adress that will receive data packets from the public IP adress.\n(Optional and just for TCP and UDP) In the Port before translation field enter a port that will forward data packets to the specified IP adress.\n(Optional and just for TCP and UDP) In the Port after translation field enter a port that will receive data packets from the specified IP adress.\n(Optional) Enter a description of the DNAT rule. You can only use numbers, Latin letters, spaces and characters: ?, !, ~, @, #, $, %, ^, \", &, _, -, +, *, =, ;, :, ,, ., /, \\, |, `  `, [, ], {, }, (, ), `.\nClick Add.\n\nEditing DNAT rule\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the DNAT tab.\nOpen the DNAT rule settings with one of the following ways:\n\n   Click more-icon for the rule you need and select pencil-icon Edit.\n   Click the name of the rule you need and then click pencil-icon Edit.\n\nEdit the rule settings:\n\n   Name: the name of the DNAT rule. You can only use numbers, Latin letters, spaces and characters: ?, !, ~, @, #, $, %, ^, \", &, _, -, +, *, =, ;, :, ,, ., /, \\, |, `  `, [, ], {, }, (, ), `.\n   Protocol: the data transfer protocol:\n\n     All protocls and ports: the rule will apply to all ports and protocols.\n     TCP: the rule will apply to the specified port and the TCP protocol.\n     UDP: the rule will apply to the specified port and the UDP protocol.\n   IP after translation: IP adress that will receive data packets from the public IP adress.\n   (Optional and just for TCP and UDP) Port before translation: the port that will forward data packets to the specified IP adress.\n   (Optional and just for TCP and UDP) Port after translation: the port that will receive data packets from the specified IP adress.\n   Description: the description of the DNAT rule. You can only use numbers, Latin letters, spaces and characters: ?, !, ~, @, #, $, %, ^, \", &, _, -, +, *, =, ;, :, ,, ., /, \\, |, `  `, [, ], {, }, (, ), `.\nClick Save changes.\n\nDeleting DNAT rule\n\n\n\nPersonal account\n\n\n\nThis is a group operation: you can delete several rules at once by setting the checkboxes.\n\n{include(/en/_includes/_open_advanced_router.md)}\n\nGo to the DNAT tab.\nDelete the rule with one of the following ways:\n\n   Click more-icon for the rule you need and select trash-icon Delete.\n   Set the checkbox for the required rule, then click trash-icon Delete above the table.\nConfirm the deletion.\n\n\n\n","url":"/en/networks/vnet/service-management/advanced-router/manage-dnat","description":"Adding, editing and deleting rules for redirecting data from a public IP address to an IP address inside a private network.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"834200cc-51b4-4196-9af3-3f6175872e20","title":"VPN tunnels","data":"VPN service is available in the Neutron and Sprut SDN. Management of VPN tunnels in Sprut SDN is available only through the personal account interface.\n\nYou can manage VPN tunnels: view, add or remove them from the project, as well as edit and restart tunnels.\n\nViewing a list of VPN tunnels and information about them\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project you need.\nGo to Virtual networks → VPN.\n\n   A list of VPN tunnels will appear.\n\nClick the name of a VPN tunnel.\n\n    A page with detailed information about it will open. Navigate between the page tabs to view information about IKE and IPsec settings, endpoint groups, and the tunnel. You can also edit the VPN tunnel settings on this page.\n\n\n\n\nMake sure that:\n\n   OpenStack CLI is installed.\n   You can authorize in the OpenStack CLI.\n\nTo view a list of VPN tunnels, run the command:\n\n      openstack vpn ipsec site connection list\n\nTo view detailed information about the VPN tunnel, run the command:\n\n        openstack vpn ipsec site connection show\n\n   General information about the tunnel and identifiers will be displayed:\n\n   IKE Policy — the identifier of the IKE policy. To view detailed information about the policy, run the command:\n\n           openstack vpn ike policy show\n\n   IPSec Policy — the identifier of the IPsec policy. To view detailed information about the policy, run the command:\n\n            openstack vpn ipsec policy show\n\n   Local Endpoint Group ID — the identifier of the local endpoint group. To view detailed information about the group, run the command:\n\n            openstack vpn endpoint group show\n\n   Peer Endpoint Group ID — the identifier of the remote (peer) endpoint group. To view detailed information about the group, run the command:\n\n            openstack vpn endpoint group show\n\n   VPN Service — the identifier of the VPN service that this VPN tunnel serves. To view detailed information about the service, run the command:\n\n            openstack vpn service show\n\n\nAdding a VPN Tunnel\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project you need.\nGo to Virtual networks → VPN.\nClick the Add VPN or Add button. The New VPN Tunnel Wizard opens.\nSelect the SDN in which the VPN will be created:\n\n   SDN Neutron: VPN can only be connected to a standard router of Sprut or Neutron.\n   SDN Sprut: VPN can only be connected to an advanced router.\n\nSet up IKE:\n\n   IKE Policy — select an IKE policy from the dropdown list. If the required policy does not exist, create a new one:\n\n       Select New IKE Policy from the drop-down list.\n       Set the policy settings:\n\n          Policy name.\n          Key lifetime (in seconds).\n          Authorization Algorithm — sha256 is recommended.\n          Encryption algorithm — aes256 is recommended.\n          IKE version — v2 is recommended.\n          Diffie-Hellman group — group14 is recommended.\n\n   Press the Next Step button.\n\nConfigure IPsec:\n\n    IPsec policy — select an IPsec policy from the drop-down list. If the required policy does not exist, create a new one:\n\n       Select New IPsec Policy from the drop-down list.\n       Set the policy settings:\n\n          Policy name.\n          Key lifetime (in seconds).\n          Authorization Algorithm — it is recommended to select sha256.\n          Encryption algorithm — aes256 is recommended.\n          Diffie-Hellman group — it is recommended to select the group14 group.\n\n    Press the Next Step button.\n\nSet up endpoint groups:\n\n   Router — select the router whose subnets should be accessible through the VPN tunnel. The available options depend on the selected SDN, and only include routers that are connected to an external network and have an assigned external IP address.\n\n\n       It is recommended to create no more than 500 connections for one router. With more connections, errors may occur.\n\n\nSelect New endpoint group from the drop-down list.\n\n    Set the group settings:\n\n         Neutron\n         Sprut\n\n\n         Name — the name of a local endpoint group.\n         Subnets — select one or more subnets connected to the previously selected router. These subnets will be accessible through the VPN tunnel.\n\n\n         Name — the name of a local endpoint group.\n         Subnet address — specify the IP address (CIDR) of the subnet connected to the previously selected router. This subnet will be accessible through the VPN tunnel.\n         (Optional) To connect an additional subnet, click Добавить еще один CIDR and specify the IP address of the subnet that should be accessible through the VPN tunnel.\n\n\n    Remote Endpoint — select a remote endpoint group from the dropdown list. If the required group does not exist, create a new one:\n\n       Select New endpoint group from the drop-down list.\n       Set the group settings:\n\n          Group name.\n          Subnet address — address of the remote subnet that will be accessible through the VPN tunnel.\n\n            If you need to add more subnets, click the Add subnet link.\n\n    Press the Next Step button.\n\nSet up a VPN tunnel:\n\n    Specify basic settings:\n\n       Tunnel name.\n       Public IPv4 address of the peer (Peer IP).\n       Shared Key (PSK).\n\n         If necessary, generate a key by clicking the corresponding button.\n\n\n         Valid characters:\n         uppercase and lowercase letters of the Latin alphabet;\n         numbers;\n         special characters -, +, &, !, @, #, $, %, ^, *, (, ), ,, ., :, ;, _, =, `, {, }, /`.\n\n         The key must contain at least one letter or number.\n\n\n    (Optional) Specify advanced settings:\n\n       Peer Router ID for Authentication (Peer ID) — by default matches the peer address.\n       (For VPN in Sprut SDN only) Селектор потоков траффика:\n\n         Объединить — do not split traffic selectors, that is, wrap all address prefixes in one data transmission tunnel.\n         Разделить — split traffic selectors, that is, create a separate data transmission tunnel for each pair of address prefixes.\n\n       Initiator State — behavior when establishing an IPsec connection:\n\n         bi-directional (default) — the VK Cloud platform will attempt to establish a connection with a remote peer.\n         response-only — the platform expects a VPN connection to be initiated by a remote peer and does not attempt to establish one on its own.\n\n       Settings for detecting the unavailability of a remote peer (Dead Peer Detection, DPD):\n\n         When a peer is unavailable — determines the behavior of the VK Cloud platform if a remote peer is unavailable:\n\n           hold (default) — When an unreachable IPsec connection is detected, the connection is terminated. The connection can only be re-established by a remote peer.\n           clear — When an unreachable IPsec connection is detected, the connection is terminated. The connection will not be re-established even if the remote peer attempts to do so.\n           restart — When an unreachable IPsec connection is detected, the connection is terminated. The VK Cloud platform will try to re-establish a connection with the remote peer.\n\n         Peer Downtime Detection Interval — at what interval (in seconds) to send test DPD messages.\n\n         Time to detect peer unavailable — if after this time-out (in seconds) no DPD check messages were received from a remote peer, then it is considered unavailable (dead).\n\n           The default value for this setting is four times the Peer Downtime Detection Interval.\n\nClick the Create VPN Tunnel button.\n\n\n\n\nMake sure that:\n\n    OpenStack CLI is installed.\n    You can authorize in the OpenStack CLI.\n\nSet up IKE:\n\n   Get a list of IKE policies and view policy details:\n\n              openstack vpn ike policy list\n\n              openstack vpn ike policy show\n\n      Save the policy ID (id) that will be used by the VPN tunnel.\n\n   If a suitable IKE policy was not found in the previous step, create it:\n\n       Linux/macOS (bash, zsh)\n       Windows (PowerShell)\n\n\n              openstack vpn ike policy create  \\\n         --lifetime units=,value= \\\n         --auth-algorithm  \\\n         --encryption-algorithm  \\\n         --ike-version  \\\n         --pfs\n\n\n             openstack vpn ike policy create  `\n         --lifetime units=,value= `\n         --auth-algorithm  `\n         --encryption-algorithm  `\n         --ike-version  `\n         --pfs\n\n\n      After creation, information about the created object will be displayed, including its identifier. Save the policy ID (id) that will be used by the VPN tunnel.\n\nSet up IPsec:\n\n   Get a list of IPsec policies and view policy details:\n\n              openstack vpn ipsec policy list\n\n              openstack vpn ipsec policy show\n\n       Make a note of the policy ID that will be used by the VPN tunnel.\n\n   If a suitable IPsec policy was not found in the previous step, create it:\n\n       Linux/macOS (bash, zsh)\n       Windows (PowerShell)\n\n\n              openstack vpn ipsec policy create  \\\n         --lifetime units=,value= \\\n         --auth-algorithm  \\\n         --encryption-algorithm  \\\n         --pfs\n\n\n            openstack vpn ipsec policy create  `\n        --lifetime units=,value= `\n        --auth-algorithm  `\n        --encryption-algorithm  `\n        --pfs\n\n\n      After creation, information about the created object will be displayed, including its identifier. Make a note of the policy ID that will be used by the VPN tunnel.\n\nCreate a VPN service that will serve the VPN tunnel:\n\n   Get a list of routers and view detailed information about them:\n\n              open stack router list\n\n              openstack router show\n\n       Write down the ID of the router whose subnets you want to make available through the VPN tunnel.\n\n       Such a router must have access to the Internet and an external IP address associated with it.\n\n   Create a VPN service using this router:\n\n      Linux/macOS (bash, zsh)\n      Windows (PowerShell)\n\n\n            openstack vpn service create  \\\n        --router  \\\n        --enable\n\n\n            openstack vpn service create  `\n        --router  `\n        --enable\n\n\n      After creation, information about the created object will be displayed, including its identifier. Make a note of the VPN service ID that will be used by the VPN tunnel.\n\nSet up endpoint groups:\n\n   Get a list of endpoint groups and view detailed information about them:\n\n              openstack vpn endpoint group list\n\n              openstack vpn endpoint group show\n\n       Write down:\n       ID of the group that will be used by the VPN tunnel as the local endpoint group. Such a group must be of type subnet.\n\n         The subnets belonging to the group must be connected to the router that was specified in the previous step in the VPN service settings.\n\n       ID of the group that will be used by the VPN tunnel as the remote endpoint group. Such a group must be of type cidr.\n\n   If there is no suitable local endpoint group, create it:\n\n      Linux/macOS (bash, zsh)\n      Windows (PowerShell)\n\n\n             openstack vpn endpoint group create  \\\n         --type subnet \\\n         --value  \\\n         ...\n         --value\n\n\n             openstack vpn endpoint group create  `\n         --type subnet `\n         --value  `\n         ...\n         --value\n\n\n      After creation, information about the created object will be displayed, including its identifier. Write down the ID of the local endpoint group that will be used by the VPN tunnel.\n\n   If there is no suitable remote endpoint group, create it:\n\n      Linux/macOS (bash, zsh)\n      Windows (PowerShell)\n\n\n             openstack vpn endpoint group create  \\\n         --type cidr \\\n         --value \"\" \\\n         ...\n         --value \"\"\n\n\n             openstack vpn endpoint group create  `\n         --type cidr `\n         --value \"\" `\n         ...\n         --value \"\"\n\n\n      After creation, information about the created object will be displayed, including its identifier. Write down the ID of the remote endpoint group that will be used by the VPN tunnel.\n\nCreate a VPN tunnel:\n\n   Linux/macOS (bash, zsh)\n   Windows (PowerShell)\n\n\n       openstack vpn ipsec site connection create  \\\n      --dpd action=,interval=,timeout  \\\n      --initiator  \\\n      --peer-address \"\" \\\n      --peer-id \"\" \\\n      --ikepolicy  \\\n      --ipsecpolicy  \\\n      --vpnservice  \\\n      --local-endpoint-group  \\\n      --peer-endpoint-group  \\\n      --psk \"\" \\\n      --enable\n\n\n       openstack vpn ipsec site connection create  `\n      --dpd action=,interval=,timeout  `\n      --initiator  `\n      --peer-address \"\" `\n      --peer-id \"\" `\n      --ikepolicy  `\n      --ipsecpolicy  `\n      --vpnservice  `\n      --local-endpoint-group  `\n      --peer-endpoint-group  `\n      --psk \"\" `\n      --enable\n\n\nEditing a VPN Tunnel\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project you need.\nGo to Virtual networks → VPN.\nClick   for the required VPN tunnel and select Edit VPN.\nIf necessary, edit the local or remote endpoint group:\n\n   Local Endpoint — select a local endpoint group from the drop-down list. If the required group does not exist, create a new one:\n\n       Select New endpoint group from the drop-down list.\n       Set the group settings:\n\n         Neutron\n         Sprut\n\n\n         Name — the name of a local endpoint group.\n         Subnets — select one or more subnets connected to the previously selected router. These subnets will be accessible through the VPN tunnel.\n\n\n         Name — the name of a local endpoint group.\n         Subnet address — specify the IP address (CIDR) of the subnet connected to the previously selected router. This subnet will be accessible through the VPN tunnel.\n         (Optional) To connect an additional subnet, click Добавить еще один CIDR and specify the IP address of the subnet that should be accessible through the VPN tunnel.\n\n\n   Remote Endpoint — select a remote endpoint group from the dropdown list. If the required group does not exist, create a new one:\n\n       Select New endpoint group from the drop-down list.\n       Set the group settings:\n\n          Group name.\n          Subnet address — address of the remote subnet that will be accessible through the VPN tunnel.\n\n            If you need to add more subnets, click the Add subnet link.\n\nPress the Next Step button.\nEdit the VPN tunnel settings:\n\n   Basic settings:\n\n       Tunnel name.\n       Public IPv4 address of the peer (Peer IP).\n       Shared Key (PSK).\n\n         If necessary, generate a key by clicking the corresponding button.\n\n\n         Valid characters:\n         uppercase and lowercase Latin letters;\n         numbers;\n         special characters -, +, &, !, @, #, $, %, ^, *, (, ), ,, ., :, ;, _, =, `, {, }, /`.\n\n         The key must contain at least one letter or number.\n\n\n   (Optional) Аdvanced settings:\n\n      Peer Router ID for Authentication (Peer ID) — by default matches the peer address.\n      (For VPN in Sprut SDN only) Селектор потоков траффика:\n\n         Объединить — do not split traffic selectors, that is, wrap all address prefixes in one data transmission tunnel.\n         Разделить — split traffic selectors, that is, create a separate data transmission tunnel for each pair of address prefixes.\n\n      Initiator State — behavior when establishing an IPsec connection:\n\n         bi-directional (default) — the VK Cloud platform will attempt to establish a connection with a remote peer.\n         response-only — the platform expects a VPN connection to be initiated by a remote peer and does not attempt to establish one on its own.\n\n      Settings for detecting the unavailability of a remote peer (Dead Peer Detection, DPD):\n\n         When a peer is unavailable — determines the behavior of the VK Cloud platform if a remote peer is unavailable:\n\n           hold (default) — when an unreachable IPsec connection is detected, the connection is terminated. The connection can only be re-established by a remote peer.\n           clear — when an unreachable IPsec connection is detected, the connection is terminated. The connection will not be re-established even if the remote peer attempts to do so.\n           restart — when an unreachable IPsec connection is detected, the connection is terminated. The VK Cloud platform will try to re-establish the connection with the remote peer.\n\n         Peer Downtime Detection Interval — the interval (in seconds) at which to send DPD test messages.\n\n         Time to detect peer unavailable — if during this interval (in seconds) no DPD test messages were received from a remote peer, then it is considered unavailable (dead).\n\n           The default value for this setting is four times the Peer Downtime Detection Interval.\n\nClick the Save button.\n\n\n\n\nMake sure that:\n\n    OpenStack CLI is installed.\n    You can authorize in the OpenStack CLI.\n\nView detailed information about the VPN tunnel you want to edit.\n\n(If necessary) select other endpoint groups or create new ones:\n\n   Get a list of endpoint groups and view detailed information about them:\n\n            openstack vpn endpoint group list\n\n            openstack vpn endpoint group show\n\n      Write down:\n\n      ID of the group that will be used by the VPN tunnel as the local endpoint group. Such a group must be of type subnet.\n\n         The subnets belonging to the group must be connected to the router that was specified in the previous step in the VPN service settings.\n\n      ID of the group that will be used by the VPN tunnel as the remote endpoint group. Such a group must be of type cidr.\n\n   If there is no suitable local endpoint group, create it:\n\n      Linux/macOS (bash, zsh)\n      Windows (PowerShell)\n\n\n            openstack vpn endpoint group create  \\\n      --type subnet \\\n      --value  \\\n      ...\n      --value\n\n\n             openstack vpn endpoint group create  `\n         --type subnet `\n         --value  `\n         ...\n         --value\n\n\n      After creation, information about the created object will be displayed, including its identifier. Write down the ID of the local endpoint group that will be used by the VPN tunnel.\n\n   If there is no suitable remote endpoint group, create it:\n\n      Linux/macOS (bash, zsh)\n      Windows (PowerShell)\n\n\n             openstack vpn endpoint group create  \\\n         --type cidr \\\n         --value \"\" \\\n         ...\n         --value \"\"\n\n\n             openstack vpn endpoint group create  `\n         --type cidr `\n         --value \"\" `\n         ...\n         --value \"\"\n\n\n      After creation, information about the created object will be displayed, including its identifier. Write down the ID of the remote endpoint group that will be used by the VPN tunnel.\n\nEdit the VPN tunnel settings:\n\n   Linux/macOS (bash, zsh)\n   Windows (PowerShell)\n\n\n       openstack vpn ipsec site connection set  \\\n      --name  \\\n      --dpd action=,interval=,timeout  \\\n      --initiator  \\\n      --peer-address \"\" \\\n      --peer-id \"\" \\\n      --local-endpoint-group  \\\n      --peer-endpoint-group  \\\n      --enable\n\n\n       openstack vpn ipsec site connection set  `\n      --name  `\n      --dpd action=,interval=,timeout  `\n      --initiator  `\n      --peer-address \"\" `\n      --peer-id \"\" `\n      --local-endpoint-group  `\n      --peer-endpoint-group  `\n      --enable\n\n\nRestarting a VPN tunnel\n\n\n\nRestarting the VPN tunnel is only available for VPNs in SDN Neutron. In SDN Sprut, all tunnel changes are applied automatically, no restart is required.\n\n\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project you need.\nGo to Virtual networks → VPN.\nClick   for the required VPN tunnel and select Restart VPN.\nRead the warning.\nPress the Restart button.\n\n\n\n\nMake sure that:\n\n    OpenStack CLI is installed.\n    You can authorize in the OpenStack CLI.\n\nView detailed information about the VPN tunnel that needs to be restarted.\n\nRun the command:\n\n   Linux/macOS (bash, zsh)\n   Windows (PowerShell)\n\n\n       openstack vpn service set  --disable && openstack vpn service set  --enable\n\n\n       openstack vpn service set  --disable; openstack vpn service set  --enable\n\n\nRemoving a VPN tunnel\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to your VK Cloud personal account.\nSelect the project you need.\nGo to Virtual networks → VPN.\nClick   for the required VPN tunnel and select Remove VPN.\nReview the list of objects to be deleted.\n\n   When deleting a VPN tunnel, the objects associated with it will also be deleted (if they are not used by other VPN tunnels):\n\n    VPN service serving the tunnel;\n    IKE policy and IPsec policy;\n    local and remote endpoint groups.\n\nClick the Confirm button.\n\n\n\n\nMake sure that:\n\n    OpenStack CLI is installed.\n    You can authorize in the OpenStack CLI.\n\nLook at the list of VPN tunnels and find the ID of the tunnel you want to remove.\n\nView detailed information about the tunnel you want to delete.\n\n   Write down the following IDs:\n\n   IKE Policy — the identifier of the IKE policy.\n   IPSec Policy — the identifier of the IPsec policy.\n   Local Endpoint Group ID — the identifier of the local endpoint group.\n   Peer Endpoint Group ID — the identifier of the remote (peer) endpoint group.\n   VPN Service — the identifier of the VPN service that this VPN tunnel serves.\n\nTo remove only the VPN tunnel, run the command:\n\n       openstack vpn ipsec site connection delete\n\nTo remove the VPN tunnel and all objects associated with it, run the commands:\n\n   Linux/macOS (bash, zsh)\n   Windows (PowerShell)\n\n\n      openstack vpn ipsec site connection delete\n   openstack vpn ike policy delete\n   openstack vpn ipsec policy delete\n   openstack vpn endpoint group delete\n   openstack vpn endpoint group delete\n   openstack vpn service delete\n\n\n      openstack vpn ipsec site connection delete ; `\n   openstack vpn ike policy delete ; `\n   openstack vpn ipsec policy delete ; `\n   openstack vpn endpoint group delete ; `\n   openstack vpn endpoint group delete ; `\n   openstack vpn service delete\n\n","url":"/en/networks/vnet/service-management/vpn","description":"Manage VPN tunnels in a virtual network service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"e7fdb009-3ab6-411c-bbfb-0560125bcab0","title":"Viewing Topology","data":"A network topology is a diagram of the network connectivity of virtual machines. It shows the following elements, as well as all the links between them:\n\nnetworks;\nsubnets;\nrouters;\nvirtual machines.\n\nTo view the network topology of an individual project:\n\nGo to VK Cloud personal account.\nSelect the project.\nGo to Virtual networks → Network topology.\n\nTopology management\n\nAvailable network topology operations:\n\nMove. Use the arrows in the upper right corner or hold the left mouse button and move the diagram in the required direction.\nScaling. Use + and - in the upper right corner of the diagram or scroll with the mouse wheel.\nHide or show signatures. Use the Show labels switch in the upper left corner of the diagram.\nView item details. Click on the schematic element with the left mouse button. Options will be displayed:\n\n  Name — name of the selected element.\n  ID — element identifier in the system.\n  Type — type of the selected element:\n\n    network;\n    subnet;\n    instance;\n    router;\n    balancer;\n    port.\n\n  Status — state of the selected element. Not shown for subnet.\n\n  Additional options depending on the element type.\n  Link to go to the item in your account. Not displayed for ports.\n","url":"/en/networks/vnet/service-management/topology","description":"Viewing the topology of virtual networks using VK Cloud tools.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"7d8999cb-fc52-4d53-b715-18b4c5e8e49d","title":"Setting up a VPN tunnel","data":"\n\nThe VPN service is only available in networks created with use of Neutron SDN.\n\n\n\nThroughout this guide, an VK Cloud subnet will be connected with the client subnet via a VPN tunnel. This VPN tunnel will be built to another network of the VK Cloud platform, and a virtual machine will be used as a VPN endpoint. Several virtual machines will also be created to test the functionality of the tunnel.\n\nThe guide can be adapted to work with any other VPN endpoint, such as a corporate firewall or other network equipment.\n\n1. Preparatory steps\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nCreate networks.\n\n\n   You can create a network with any parameters you choose. Adjust the next steps in this scenario as needed.\n\n   Client network\n   Virtual network\n\n\n   This network will act as the client network.\n\n   When creating a network, set the following parameters:\n\n    Network name: clientNet.\n    Internet Access: Make sure this option is selected. It will allow you to assign public floating IP addresses to virtual machines on this network.\n    Router: Create new.\n    Subnet List: edit the only subnet in the list. Set the following parameters for the subnet:\n\n      Name: clientSubnet.\n      Address: 172.16.0.0/29.\n      Gateway: 172.16.0.1.\n      Enable DHCP: Make sure this option is selected.\n      DHCP IP address pool: 172.16.0.2 - 172.16.0.6.\n      Private DNS: Make sure this option is selected.\n\n\n    This network will act as a virtual network.\n\n    When creating a network, set the following parameters:\n\n    Network name: vkcloudNet.\n    Internet Access: Make sure this option is selected. It will allow you to ensure the operation of the VPN tunnel and assign public floating IP addresses to virtual machines in this network.\n    Router: Create new.\n    Subnet List: edit the only subnet in the list. Set the following parameters for the subnet:\n\n      Name: vkcloudSubnet.\n      Address: 10.0.0.0/29.\n      Gateway: 10.0.0.1.\n      Enable DHCP: Make sure this option is selected.\n      DHCP IP address pool: 10.0.0.2 - 10.0.0.6.\n      Private DNS: Make sure this option is selected.\n\n\nDetermine which routers have been created for these networks. This information will be needed when you configure the VPN further.\n\n    It is further assumed that:\n\n    router router_1234 was created for the clientNet network;\n    router router_5678 was created for the vkcloudNet network.\n\nDetermine the IP address of the SNAT interface for the router_5678 router:\n\n    Open the page with the list of subnets for the vkcloudNet network.\n    Click on the subnet name vkcloudSubnet.\n    Click the Ports tab.\n    Find the SNAT device port in the list of ports and copy its IP address.\n\nCreate a virtual machine that will act as a VPN gateway on the clientNet client network, with the following settings:\n\n    Virtual machine name: client_vpn_gw.\n    Virtual machine type: STD2-1-2.\n    Number of machines in the configuration: one.\n    Operating system: Ubuntu 22.04.\n    Network: client network and corresponding clientNet: clientSubnet subnet.\n    Virtual Machine Key: The key that will be used to connect via SSH.\n    Firewall settings: everything is allowed (all).\n    Assign External IP: Make sure this option is selected.\n\n   Select other parameters of the virtual machine at your discretion.\n\nCollect information necessary for further work. Further, it is assumed that:\n\n   | Object                                                               | Value           |\n   | -------------------------------------------------------------------- | --------------- |\n   | The public IP address of the router router_5678                    | 192.0.2.100   |\n   | IP address of the client_vpn_gw virtual machine on the clientSubnet subnet | 172.16.0.5 |\n   | Floating public IP address of the virtual machine client_vpn_gw    | 192.0.2.200   |\n   | Client subnet from client VPN gateway client_vpn_gw                | 172.16.0.0/29 |\n   | Cloud VPN Gateway Virtual Subnet                                     | 10.0.0.0/29   |\n   | IP address of the SNAT port in the cloud subnet                    | 10.0.0.5      |\n\nExample Infrastructure\n\n2. Set up a VPN tunnel on the cloud side\n\nCreate a VPN with the following settings:\n\n\n\n1. Setting up IKE\n2. Configuring IPsec\n3. Creating Endpoint Groups\n4. Tunnel setup\n\n\n\nSelect IKE Policy - New IKE Policy, and set:\n\nPolicy name**: vkcloud-client-ike.\nKey lifetime**: 3600 seconds.\nAuthorization Algorithm**: sha256.\nEncryption algorithm**: aes-256.\nIKE version**: v2.\nDiffie-Hellman group**: group14.\n\n\n\n\nSelect IPsec Policy - New IPsec Policy, and set:\n\nPolicy name**: vkcloud-client-ipsec.\nKey lifetime**: 3600 seconds.\nAuthorization Algorithm**: sha256.\nEncryption algorithm**: aes-256.\nDiffie-Hellman group**: group14.\n\n\n\n\nChoose:\n\nRouter**: router_5678.\n\nLocal Endpoint**: New endpoint group.\n  Name: vkcloud-endpoint-group.\n  Subnets: vkcloudSubnet.\n\nRemote Endpoint**: New endpoint group.\n  Group name: client-endpoint-group.\n  Subnet address: 172.16.0.0/29.\n\n\n\n\nSelect Settings - Basic, and set:\n\nTunnel name**: vkcloud-client-vpn.\nPublic IPv4 Peer IP address**: 192.0.2.200.\nShared Key (PSK)**: Any pre-shared key of your choice that satisfies the key requirements.\n\n  The key must:\n\n  be at least 16 characters long;\n  contain at least one letter or number;\n  consist only of the following valid characters:\n    uppercase and lowercase letters of the Latin alphabet;\n    numbers;\n    special characters -, +, &, !, @, #, $, %, ^, *, (, ), ,, ., :, ;, _, =, `, {, }, /`.\n\n3. Set up a VPN tunnel on the side of the client network\n\nDisable IP Source Guard on the VPN gateway port so that it can forward any traffic:\n\n    Find the port with the private IP address 172.16.0.5 of the virtual machine client_vpn_gw. Get the ID of this port.\n\n    Allow traffic from any address through this port:\n\n              openstack port set  --allowed-address ip-address=0.0.0.0/0\n\nConnect to the client_vpn_gw virtual machine via SSH. All further actions must be performed on this virtual machine.\n\nEnable IP Forwarding so that the virtual machine can route traffic from the private network to the VPN tunnel:\n\n        echo 'net.ipv4.ip_forward = 1' | sudo tee -a /etc/sysctl.conf\n    sudo sysctl -p\n\nInstall StrongSwan - An IPsec VPN Implementation for Linux:\n\n      sudo apt update\n   sudo apt install -y strongswan libcharon-extra-plugins libcharon-extauth-plugins libstrongswan-extra-plugins\n\nAdd VPN connection settings from the client network side to the /etc/ipsec.conf file. These settings are a mirror image of the tunnel settings made in the previous step.\n\n      conn client-vkcloud-vpn\n      authby=secret\n      left=%defaultroute\n      leftid=192.0.2.200\n      leftsubnet=172.16.0.0/29\n      right=192.0.2.100\n      rightsubnet=10.0.0.0/29\n      ike=aes256-sha2_256-modp2048!\n      esp=aes256-sha2_256!\n      keyingtries=0\n      ikelifetime=3600\n      lifetime=8h\n      dpddelay=30\n      dpdtimeout=120\n      dpdaction=hold\n      auto=start\n\n\n   For the Diffie-Hellman group group14, the equivalent notation is modp_2048. The mapping of modp to group names is given in RFC 3526.\n\n\nSpecify the shared key (PSK) in the /etc/ipsec.secrets file. The key must match the key specified previously:\n\n      192.0.2.200 192.0.2.100 : PSK \"\"\n\nRestart the StrongSwan service:\n\n      sudo systemctl restart strongswan-starter\n\n4. Add static routes\n\nIn order for traffic to pass through the VPN tunnel, you need to add static routes:\n\n\n\nOn the side of the cloud network\nOn the side of the client network\n\n\n\nOpen the page with the list of subnets for the vkcloudNet network in your personal account.\nExpand the vkcloudSubnet subnet menu and select Edit Subnet.\nMake sure the Show static routes field option is selected.\nEnter a static route to the client network 172.16.0.0/29 in the field. As the next hop, you need to specify the IP address of the SNAT interface of the router router_5678 in the cloud subnet vkcloudSubnet.\n\n      172.16.0.0/29 - 10.0.0.5\n\n\nOpen the page with information about the router router_1234 in your personal account, to which the client subnet clientSubnet and the VPN gateway client_vpn_gw are connected.\nOn the Static Routes tab, click the Add Static Route button.\nWrite a static route to the cloud network 10.0.0.0/29:\n\n    Destination network: 10.0.0.0/29.\n    Intermediate host (Next HOP): 172.16.0.5 (IP address of the VPN gateway on the client subnet).\n\nClick the Add route button.\n\n5. Check if the VPN tunnel is working\n\nView the status of the VPN tunnel from the VK Cloud platform.\n\n    To do this, open the VPN page vkcloud-client-vpn in your personal account and go to the Tunnel Settings tab. The VPN must be in ACTIVE status.\n\nCreate a firewall rule group icmp to allow ICMP traffic.\n\n    In this group, create an inbound rule:\n\n    Type: ICMP.\n    Remote address: All IP addresses.\n\n    This is necessary so that the test virtual machines can ping each other.\n\nCreate two virtual machines:\n\n    clientVM:\n\n      in the network clientNet, subnet clientSubnet;\n      with a floating IP address to connect to it via SSH;\n      with firewall rule groups default, ssh, icmp.\n\n    vkcloudVM:\n      in the network vkcloudNet, subnet vkcloudSubnet;\n      with a floating IP address to connect to it via SSH;\n      with firewall rule groups default, ssh, icmp.\n\nDetermine the private IP addresses of the virtual machines on the respective subnets. Let be:\n\n    clientVM has IP address 172.16.0.4;\n    vkcloudVM has IP address 10.0.0.4.\n\nConnect to the vkcloudVM virtual machine via SSH.\n\nPing the clientVM virtual machine from the vkcloudVM virtual machine:\n\n        ping 172.16.0.4\n\n    The clientVM host must respond to the ping.\n\nDelete unused resources\n\nIf you no longer need the created resources, delete them:\n\nDelete virtual machines.\nDelete VPN tunnel.\nDelete static routes defined on the side of the client network.\n\n\n   Static routes on the side of the cloud network will be deleted along with the subnet for which they were registered.\n\n\nDelete client and cloud network routers.\nDelete the client and cloud subnets and networks.\nDelete floating IP addresses.\n","url":"/en/networks/vnet/how-to-guides/vpn-tunnel","description":"VPN allows you to connect the VK Cloud cloud network with an external (client) network. An example will be shown here on how to organize the interaction of a cloud network with a client network via a VPN.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"390f8cce-a9c2-4b65-91d2-3e0990a63a12","title":"Organizing a virtual IP address","data":"If incoming traffic is processed by a cluster of several similar virtual machines, then you can create a virtual IP address (Virtual IP, VIP) for them. This address is used to organize fault-tolerant processing of incoming traffic by these virtual machines. The technology works like this: a VIP is assigned to one of the cluster virtual machines that processes traffic from this VIP. Clients accessing this address will be directed to this virtual machine. If the virtual machine fails, then the VIP will be transferred to another virtual machine from the cluster, and traffic processing will continue. Virtual machines monitor their state and manage VIP assignment using the [VRRP] protocol (https://www.rfc-editor.org/rfc/rfc3768).\n\nTo show the virtual IP address setting:\n\ntwo virtual machines will be created for which:\n  a virtual IP address will be created;\n  VRRP will be configured in the keepalived implementation to serve this IP address.\na third virtual machine with a floating IP address will be created, from which all the necessary settings and health check will be performed.\n\nScenario Diagram\n\n1. Preparation steps\n\nMake sure OpenStack CLI is installed. Also check that you can sign in the OpenStack CLI.\n\nSelect the subnet where the virtual machines and virtual IP address will be placed. They must be on the same subnet.\n\n    If the required subnet does not exist, create it.\n\n    Write down the following information:\n    subnet name;\n    the name of the network in which the subnet is located.\n\n    The mysubnet subnet in the mynetwork network will be used as an example.\n\nDetermine which IP address will be used as the virtual IP address. This IP address must not be used by any entities (such as a virtual machine or load balancer).\n\n    The address 192.168.0.254/24 will be used as an example.\n\nMake sure that this IP address will not be used by other objects (for example, assigned to a virtual machine or load balancer port). One of the easiest ways to achieve this is to create an OpenStack port with a chosen address and then never use that port anywhere.\n\n    To create such a port, run the command in the OpenStack CLI:\n\n        openstack port create  --network mynetwork --fixed-ip subnet=mysubnet,ip-address=192.168.0.254\n\nCreate the kld-vm1, kld-vm-2 and kld-jumphost-vm virtual machines.\n\n    These VMs must be in a security group that allows all inbound and outbound traffic within the group itself.\n    For example, such a group is default.\n\n    If non-standard security groups are used, they must allow:\n    SSH traffic to kld-vm1 and kld-vm-2;\n    VRRP traffic between kld-vm1 and kld-vm-2;\n    ICMP traffic to the selected virtual IP address (for health check).\n\n   kld-vm-1\n   kld-vm-2\n   kld-jumphost-vm\n\n\n   Set the following options:\n\n   Virtual machine name: kld-vm-1.\n   Number of machines in configuration: 1.\n   Operating system: Ubuntu 22.04.\n   Network: network and subnet selected earlier.\n   DNS-name: kld-vm-1.\n   Firewall settings: default.\n   Assign external IP: make sure the option is not selected.\n\n   Select other parameters of the virtual machine at your discretion.\n\n\n   Set the following options:\n\n   Virtual machine name: kld-vm-2.\n   Number of machines in configuration: 1.\n   Operating system: Ubuntu 22.04.\n   Network: network and subnet selected earlier.\n   DNS-name: kld-vm-2.\n   Firewall settings: default.\n   Assign external IP: make sure the option is not selected.\n\n   Select other parameters of the virtual machine at your discretion.\n\n\n   Set the following options:\n\n    Virtual machine name: kld-jumphost-vm.\n    Number of machines in configuration: 1.\n    Operating system: Ubuntu 22.04.\n    Network: network and subnet selected earlier.\n    DNS-name: kld-jumphost-vm.\n    Firewall settings: default, ssh.\n    Assign external IP: make sure the option is selected. A floating IP address (Floating IP, FIP) is required to connect to a virtual machine via SSH from the Internet.\n\n    Select other parameters of the virtual machine at your discretion.\n\n\nDefine the OpenStack port through which the kld-vm-1 and kld-vm-2 virtual machines will work with the virtual IP address:\n\n    For the virtual machine kld-vm-1:\n\n       Connect to the kld-jumphost-vm virtual machine via SSH.\n       Connect to the virtual machine kld-vm-1 via SSH.\n       Run the command:\n\n                    ip route | grep default\n\n          Sample output:\n\n                    default via 192.168.0.1 dev ens3 proto dhcp src 192.168.0.11 metric 100\n\n          Record the following information from the output:\n\n          Network interface name (followed by dev): in this example ens3.\n          IP address of the network interface (followed by src): in this example 192.168.0.11.\n\n       Run the OpenStack CLI command:\n\n                  openstack port list -c ID --server kld-vm-1 --fixed-ip ip-address=\n\n         Sample output:\n\n                  +--------------------------------------+\n         | ID                                   |\n         +--------------------------------------+\n         | e1bd636a-aaaa-bbbb-cccc-a673e7cbef83 |\n         +--------------------------------------+\n\n         Record the OpenStack port ID from the output.\n\n    Follow the same steps on the kld-vm-2 virtual machine.\n\nWrite down all received data. Result for the given example:\n\n\n| Object                                                    | Value                                     |\n| --------------------------------------------------------- | ----------------------------------------- |\n| For kld-vm-1 virtual machine                                                                      |\n| Network interface name                                    | ens3                                    |\n| Network interface IP address                              | 192.168.0.11                            |\n| OpenStack port ID for the network interface               | e1bd636a-aaaa-bbbb-cccc-a673e7cbef83    |\n| For kld-vm-2 virtual machine                                                                      |\n| Network interface name                                    | ens3                                    |\n| Network interface IP address                              | 192.168.0.22                            |\n| OpenStack port ID for the network interface               | 74268d00-xxxx-yyyy-zzzz-cf9f93536d5c    |\n| Other                                                                                             |\n| Subnet for virtual machines and virtual IP address        | 192.168.0.0/24                          |\n| Subnet name                                               | mysubnet                                |\n| The name of the network where the subnet is located       | mynetwork                               |\n| Virtual IP address                                        | 192.168.0.254/24                        |\n\n2. Install and configure keepalived\n\nInstall keepalived:\n\n    For the virtual machine kld-vm-1:\n\n       Connect to the kld-jumphost-vm virtual machine via SSH.\n       Connect to the virtual machine kld-vm-1 via SSH.\n       Run the commands:\n\n                    sudo apt update\n          sudo apt install keepalived\n\n    Follow the same steps on the kld-vm-2 virtual machine.\n\nConfigure keepalived:\n\n   kld-vm-1\n   kld-vm-2\n\n\n   Connect to the kld-jumphost-vm virtual machine via SSH.\n   Connect to the virtual machine kld-vm-1 via SSH.\n   Edit the /etc/keepalived/keepalived.conf file, replacing its contents with the following:\n\n            global_defs\n      {\n        router_id KLD-VM-1\n      }\n\n      vrrp_instance VI_254\n      {\n        state MASTER\n        interface ens3\n        virtual_router_id 254\n        priority 120\n        advert_int 1\n\n        authentication\n        {\n          auth_type PASS\n          auth_pass\n        }\n\n        virtual_ipaddress\n        {\n          192.168.0.254/24\n        }\n      }\n\n\n   Connect to the kld-jumphost-vm virtual machine via SSH.\n   Connect to the virtual machine kld-vm-2 via SSH.\n   Edit the /etc/keepalived/keepalived.conf file, replacing its contents with the following:\n\n            global_defs\n      {\n        router_id KLD-VM-2\n      }\n\n      vrrp_instance VI_254\n      {\n        state BACKUP\n        interface ens3\n        virtual_router_id 254\n        priority 90\n        advert_int 1\n\n        authentication\n        {\n          auth_type PASS\n          auth_pass\n        }\n\n        virtual_ipaddress\n        {\n          192.168.0.254/24\n        }\n      }\n\n\n   In the config file:\n\n    router_id — identifier of the keepalived router, the value is different for each of the configurations.\n    vrrp_instance - VRRP instance settings. The VRRP instance name is only locally significant, so it can be the same for configurations on different virtual machines.\n\n      state is the role that keepalived starts with: MASTER or SLAVE. Then, depending on the result of the selection of the master, the role can be changed.\n      interface is the name of the interface on which VRRP is running. The interface name is only locally significant, so it can be the same for configurations on different virtual machines.\n      virtual_router_id — VRRP router ID from 0 to 255. Must match in both configurations.\n      priority - the priority according to which the VRRP master is selected. In this case, kld-vm-1 will be the master because it has the highest priority.\n      advert_int — VRRP message sending interval (in seconds).\n      authentication — authentication parameters. In this case, authentication is performed using a password (PASS). Passwords (value auth_pass) must match in all configurations.\n      virtual_ipaddress - virtual IP address. The values must match in all configurations.\n\n      Read more about the parameters in the keepalived documentation.\n\nRun keepalived:\n\n    For the virtual machine kld-vm-1:\n\n       Connect to the kld-jumphost-vm virtual machine via SSH.\n       Connect to the virtual machine kld-vm-1 via SSH.\n       Run the command:\n\n                  sudo systemctl start keepalived\n\n       Run the command to check the status of keepalived:\n\n                  sudo systemctl status keepalived\n\n         Example output:\n\n                  ● keepalived.service - Keepalive Daemon (LVS and VRRP)\n              Loaded: loaded (/lib/systemd/system/keepalived.service; enabled; vendor preset: enabled)\n              Active: active (running) since Mon 2023-01-09 10:55:35 UTC; 21h ago\n              ...\n\n         keepalived must be in the running state.\n\n    Follow the same steps on the kld-vm-2 virtual machine.\n\n3. Configure IP Source Guard\n\nThe keepalived installed on virtual machines should be able to send traffic not only from the IP addresses 192.168.0.11 and 192.168.0.22, but also from the virtual IP address 192.168.0.254. Since OpenStack ports in VK Cloud use the IP Source Guard mechanism, allow traffic from the virtual IP address for virtual machine ports:\n\nFor kld-vm-1:\n\n      openstack port set e1bd636a-aaaa-bbbb-cccc-a673e7cbef83 --allowed-address ip-address=192.168.0.254/24\n\nFor kld-vm-2:\n\n      openstack port set 74268d00-xxxx-yyyy-zzzz-cf9f93536d5c --allowed-address ip-address=192.168.0.254/24\n\n4. Check if the virtual IP address is working\n\nOpen two terminal sessions:\n\nDuring the first session:\n   Connect to the virtual machine kld-jumphost-vm via SSH.\n   Start a continuous ping of the virtual IP address:\n\n            ping 192.168.0.254\n\n      The IP address should successfully ping.\n\nIn the second session:\n\n    Connect to the kld-jumphost-vm virtual machine via SSH.\n    Connect to the virtual machine kld-vm-1 via SSH.\n    Stop the keepalived process:\n\n            sudo systemctl stop keepalived\n\n      Since this virtual machine was in the role of master and stopped processing traffic, now the processing of traffic arriving at the virtual IP address will be transferred to the virtual machine kld-vm-2.\n      The ping process started in the first terminal session should not be interrupted.\n\n   Restart the keepalived process:\n\n              sudo systemctl start keepalived\n\n       The virtual machine will again assume the role of master and begin processing traffic that arrives at the virtual IP address.\n\n   Run the command:\n\n            ip address show dev ens3\n\n      Check that the virtual IP address is present in the output:\n\n            ens3:  mtu 1500 qdisc fq_codel state UP group default qlen 1000\n          link/ether fa:16:3e:a3:82:f2 brd ff:ff:ff:ff:ff:ff\n          altname enp0s3\n          inet 192.168.0.4/24 metric 100 brd 192.168.0.255 scope global dynamic ens3\n             valid_lft 523420sec preferred_lft 523420sec\n          inet 192.168.0.254/24 scope global secondary ens3\n             valid_lft forever preferred_lft forever\n\nDelete unused resources\n\nIf you no longer need the created resources, delete them:\n\nDelete virtual machines.\nRemove the floating IP address assigned to the kld-jumphost-vm virtual machine.\nDelete the port that has been assigned a virtual IP address.\nDelete the subnet and network where the virtual cars.\n","url":"/en/networks/vnet/how-to-guides/vip-keepalived","description":"Multiple virtual machines can share the same clustered IP address using the VRRP protocol. When using a configuration with this IP address, if the virtual machine that processes traffic from this address fails, the traffic will be redirected to one of the remaining virtual machines.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"447412e4-b9e7-40b9-b8ca-34a664f27f67","title":"Using an advanced router","data":"{include(/en/networks/directconnect/how-to-guides/advanced-router/advanced-router.md)}\n","url":"/en/networks/vnet/how-to-guides/advanced-router","description":"How to organize data transfer between the on premise infrastructure and the VK Cloud platform using a direct connection and an advanced router.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"67484dfe-8bd0-4103-82f0-922e3c47e7cf","title":"Tariffication","data":"About tariffication\n\nTariffication is based on the \"pay as you go\" principle: you are charged only for the resources you consume to the nearest second.\n\nThe cost of network services is given in price list. You can use calculator to calculate the total cost of other platform services that use network services. See Billing for how the tools related to paying for platform services work.\n\nWhat is charged\n\nThe cost of these resources is displayed in your personal VK Cloud account as part of the total cost of the services that use them.\n\nCharged:\n\nPublic IP addresses:\n  The public IP address assigned to the port if the port is connected to the `ext-net' network.\n  Existing floating public IP addresses (even if not assigned to any port).\n\nStandard and service load balancers.\n\nWhat is not charged\n\nThe public IP address assigned to the router when you select the External network connection option.\n\n  This allows, for example, virtual machines to access the Internet without having to purchase floating IP addresses.\n  However, you cannot use such an IP address to access virtual machines from the Internet. A floating IP address will be required.\n\nIncoming and outgoing traffic.\nUsing a VPN.\n","url":"/en/networks/vnet/tariffication","description":"How the service is charged, cost calculation.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Networks","link":"/en/networks/vnet","isActive":false,"evenSimpler":true}]},{"uuid":"9d234375-730b-446b-b9c4-dc86bbf5f85e","title":"Load balancer","data":"General information\n\nThe load balancer allows you to distribute incoming traffic between multiple virtual backend servers and thereby ensure high availability for the services provided by those servers. If one or more servers fail, traffic will be redirected to the remaining servers. Also the separate type of load balancer is used to power other VK Cloud platform services.\n\nThe VK Cloud platform load balancer is based on OpenStack Octavia, which has HAProxy at its core. OpenStack Octavia gives the balancer the following capabilities:\n\nProxying and balancing HTTP-, HTTPS-, UDP- and TCP-connections (the latter including proxy-protocol support).\n\n  Proxy protocol when using the TCP protocol allows you to pass information about connections (such as the client's IP address) to the server. For the server to be able to process such information, it must itself support this protocol. The Proxy protocol is supported by NGINX, Apache, Envoy and other products.\n\nProxying and balancing HTTP/2 connections in addition to HTTP/1.1.\n\nTerminating SSL connections on the balancer.\n\nFine tune time-outs.\n\nDifferent balancing methods.\n\nAdjustment of weights for backends.\n\nFor fault tolerance, two instances of the balancer are supported, one in active mode and one in standby mode. State synchronization and traffic switching between these balancers takes place using the VRRP protocol. From the user's point of view this fault-tolerant configuration looks like a single balancer.\n\nTo ensure maximum balancer performance:\n\nProvide keep-alive connectivity to the backends over HTTP/1.1.\nUse a standard Ethernet MTU of 1500 bytes for incoming traffic to the balancer.\n\nUnder these conditions, the balancer can reach the peak performance, handling 1 to 1.5 gigabits of traffic per second and about 10,000 requests per second.\n\nTypes of load balancers\n\nThere are two types of load balancers in VK Cloud. You can see the type in personal account.\n\nDifferent types of balancers are charged differently.\n\n\n\nStandard balancer\nService balancer\n\n\n\nThis balancer is user-created and can be used to provide fault tolerance and flexible application scaling:\n\nWhen working with Cloud Servers service to distribute incoming traffic between virtual servers.\nWhen working with container service, such as when using Ingress controllers (including the preconfigured one) or services like Load Balancer.\n\n\n\n\nThis balancer is created automatically to power services and is applied:\n\nIn container service for each Kubernetes cluster to provide access to the Kubernetes API.\nIn database service for database instances in a cluster configuration to load-balance between instance nodes. This applies to instances:\n  MySQL;\n  PostgreSQL;\n  Postgres Pro Enterprise;\n  Postgres Pro Enterprise 1C.\n\n\n\n","url":"/en/networks/balancing/concepts/load-balancer","description":"How to use a load balancer to distribute incoming traffic between multiple servers.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Load Balancing","link":"/en/networks/balancing","isActive":false,"evenSimpler":true}]},{"uuid":"0211098f-0a9c-45df-b260-5cdeeabaa1a1","title":"Managing load balancers","data":"You can manage load balancers: view, edit and delete them, add and modify balancing rules, manipulate public IP addresses.\n\nViewing a list of load balancers and information about them\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to Virtual networks → Load balancers.\n\n   A list of load balancers will be displayed.\n\nClick on the name of the necessary balancer.\n\n   A page will open with detailed information about the balancer. In this page you can also edit the balancer parameters.\n\n\n\n\nMake sure that:\n\n   Make sure that OpenStack client is installed and authenticate to the project.\n   Install python-octaviaclient package in the OpenStack CLI client.\n\nTo see a list of load balancers and their identifiers, run the command:\n\n      openstack loadbalancer list\n\nTo see detailed information about a load balancer, run the command:\n\n      openstack loadbalancer show\n\n   General information about the load balancer and identifiers will be displayed:\n\n   vip_port_id is the identifier of the port that is used as the Virtual IP on the load balancer. You can assign a floating IP address to this port.\n\n   listeners is a list of listener object IDs. These objects listen for incoming connections to the load balancer and serve as an entry point for traffic.\n\n   pools is a list of pools identifiers. These objects serve to group the end users of traffic. Consumers act as members of the pool. Traffic from the listener object is balanced between several members of the pool configured for the listener object.\n\nTo see the listener object settings and their relations to pools, run the command:\n\n      openstack loadbalancer listener show\n\nTo see the pool settings and the list of members of that pool, run the command:\n\n      openstack loadbalancer pool show\n\nTo see the settings of an individual participant from the pool, run the command:\n\n      openstack loadbalancer member show\n\n   Information will be displayed for the member, including the traffic destination port.\n\nAdding a load balancer\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to Virtual networks → Load balancers.\nClick Add balancer or Add.\nSet the balancer parameters:\n\n   Loadbalancer name.\n   Network: the network and subnet where the balancer will be located.\n\n     The balancer will distribute incoming traffic to the selected services located in this subnet.\n\n\n     It is not possible to change this parameter later.\n\n\n   Availability zone: select the availability zone from the list. To optimize and speed up the\n     balancer connection to VMs, place them in the same zone.\n   DNS-name: (Optional) DNS name for the balancer.\n   Assign external IP: if this option is selected, the balancer will be assigned a public IP address through which it will be accessible from the Internet. Otherwise the balancer will act as an internal load balancer. Such IP address can be assigned later.\n\n     Select this option if you plan to place services behind the load balancer that must be accessible from the Internet.\n     The option can only be selected if the network selected earlier is behind a router that has access to the Internet.\n\nSet the balancer rules.\nClick the Add balancer button.\n\n\n\n\nMake sure that:\n\n   Make sure that OpenStack client is installed and authenticate to the project.\n   Install python-octaviaclient package in the OpenStack CLI client.\n\nSelect the network and subnet where the balancer will be hosted. Get ID of the subnet.\n\nSelect the availability zone to place the balancer. To get the list of availability zones, run the command:\n\n      openstack availability zone list\n\n   To optimize and speed up the balancer connection to VMs, place them in the same zone.\n\nCreate a balancer:\n\n      openstack loadbalancer create --name  --vip-subnet-id  --availability-zone\n\n(Optional) Assign the balancer an external IP address. Through this address, it\n   will be accessible from the Internet. Otherwise the load balancer will act as an internal load balancer.\n\n   The address must be assigned if you plan to place services behind the load balancer which must be accessible from the Internet. You can assign an address only if the network for the selected subnet earlier is behind a router that has access to the Internet.\n\nEditing a load balancer name\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to Virtual networks → Load balancers.\nPerform one of the actions for the load balancer you want to edit:\n\n   Click on the balancer name.\n   Click   for the balancer and select Edit balancers.\n\n   This will open a page detailing the balancer.\n\nTo change the name:\n\n   Click on the pencil icon next to the current balancer name.\n   Set the new name.\n   Click the Rename button.\n\n\n\n\nMake sure that:\n\n   Make sure that OpenStack client is installed and authenticate to the project.\n   Install python-octaviaclient package in the OpenStack CLI client.\n\nGet ID of the necessary load balancer.\n\nChange the name of the load balancer:\n\n   Linux/macOS (bash, zsh)\n   Windows (PowerShell)\n\n\n      openstack loadbalancer  \\\n     --name\n\n\n      openstack loadbalancer  `\n     --name\n\n\nManaging public IP addresses\n\nAssign a public IP address\n\nIf the balancer network is connected to a router with Internet access, you can assign a public (external) IP address to the balancer.\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to Virtual networks → Load balancers.\nDo one of the following:\n\n   Click the name of the necessary load balancer.\n\n     On the balancer details page, click the Assign external IP link in the IP address → External IP block.\n\n   Click   for the required balancer and select Assign external IP.\n\nSelect the necessary public IP address from the list, or create a new one.\nClick the Confirm button.\n\n\n\n\nMake sure that:\n\n   Make sure that OpenStack client is installed and authenticate to the project.\n   Install python-octaviaclient package in the OpenStack CLI client.\n\nGet port ID with Virtual IP for the necessary load balancer.\nAssign a floating IP address to a port with this ID.\n\nUnassign the public IP address\n\nIf the balancer network is connected to a router with Internet access, and a public (external) IP address is assigned to the balancer, this address can be unassigned.\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to Virtual Networks → Load balancers.\nDo one of the following:\n\n   Click the name of the necessary load balancer.\n\n     On the balancer details page, click the x symbol next to the IP address in the IP address → External IP section.\n\n   Click   for the required balancer and select Unlink external IP.\n\nClick Confirm.\n\n\n\n\nMake sure that:\n\n   Make sure that OpenStack client is installed and authenticate to the project.\n   Install python-octaviaclient package in the OpenStack CLI client.\n\nGet port ID with Virtual IP for the necessary load balancer.\nUnlink floating IP address from the port with this ID.\n\nManaging balancing rules\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to the balancer page to edit it.\nIn the Balancer rules section perform one of the available actions:\n\n   Add a balancing rule.\n   Edit an existing balancing rule by clicking the pencil icon next to the rule.\n   Remove an existing balancing rule by clicking on the trashcan icon next to the rule.\n\nThe following options are available when adding or editing a rule:\n\nRule options:\n\n   Protocols and Ports (only when creating a rule):\n\n     The balancing protocol and port to be used by the balancer.\n     Destination protocol and port.\n\n     If you select TCP balancing protocol, two destination protocols are available: TCP or PROXY. The Proxy protocol can be used if it is supported by the servers behind the balancer.\n\n   Balancing method:\n\n     LEAST_CONNECTIONS: use the backend to which the least number of connections are established.\n     ROUND_ROBIN: go through all backends one by one.\n     SOURCE_IP: assign a backend for handling traffic to a specific client IP address.\n\n   Allowed cidrs: IP addresses or subnet addresses from which connections to the balancer are allowed. This parameter can be used to restrict access to the balancer from trusted addresses only.\n\n     If the parameter is not set, connections are allowed from any IP addresses (which is equivalent to CIDR 0.0.0.0/0).\n\n   Timeout parameters:\n\n     Client data: client inactivity time-out.\n     Member connect: backend connection time-out.\n     Member data: backend inactive time-out.\n     TCP inspect: time-out for additional TCP segments during content inspection.\n\n     The time-out values are set in milliseconds. The minimum value is 0, the maximum value is 2073600000 (576 hours).\n\n   Send X-Forwarded-For header (for HTTP and HTTPS balancing protocols only): an option that allows you to enable the corresponding HTTP header to be sent to the backend. By default the option is disabled.\n\n   Sticky sessions: the option provides long-term connection with the server with data saved throughout the entire session.\n\n      If the option is enabled, requests are executed between servers using the selected balancing method, but after connecting, the session is assigned to the selected server. All requests for this session will be sent to the same server, regardless of the balancing method. The session will not end until the user ends it or until the server becomes unavailable. In this case, requests will be transferred to the second server according to the balancing rules.\n\n      Field Type allows you to configure the session settings:\n\n      APP-cookie: the session is determined by the cookie specified in the application code. If this option is selected, in the Cookie name field, enter the cookie that identifies the session.\n      HTTP-cookie: the session is determined by the cookie that the load balancer creates during connections to the server and attaches to the session.\n      Source IP: the client’s IP address is used to bind the session to the server. Use this type only if the client's IP address does not change during the session.\n\n   Apply to the following instances: tools to select virtual machine instances that will act as backends for the balancer. You can add an instance either by selecting it from the list or by selecting the tag assigned to the instance.\n\n\n     The security groups for the selected virtual machines must be configured to allow traffic to the destination port and protocol.\n\n\n   Certificate (HTTPS balancing protocol only): the certificate to be used by the balancer to terminate the SSL connection.\n\n     You can select an existing certificate or load a new one.\n\n     When you load a new certificate, specify:\n\n     Certificate Name.\n     Certificate and chain of certificates: a public certificate or certificate chain. You can either paste it into the field in text form or load it from a file.\n     Private key: A private certificate. Can be inserted into the field in text form or loaded from a file.\n     Password: password of the private certificate (if used). You can insert it into the field in text form or load it from a file.\n\nBackend availability check options (healthcheck):\n\n   Method of checking: TCP or HTTP.\n   Interval: check interval in seconds.\n   Number of attempts: the number of times to try before the backend is deemed inaccessible.\n   Timeout: time-out in seconds, after which it is considered that the backend does not respond to the check.\n\n   The following parameters are only available for the HTTP validation type:\n\n   HTTP method: the method to use to check availability.\n   Response status: HTTP status which will make the accessibility check a success.\n   Query path: the path to use to test accessibility.\n\n\n\n\n\n\nHere are just the basic command arguments. Read more about the commands and their arguments in the OpenStack CLI help:\n\nopenstack loadbalancer --help\nopenstack loadbalancer  --help\n\n\n\nMake sure that:\n\n   Make sure that OpenStack client is installed and authenticate to the project.\n   Install python-octaviaclient package in the OpenStack CLI client.\n\nTo create a balancing rule:\n\n   Get the load balancer ID for which to create a rule.\n\n   Create a pool in which the traffic consumers will be placed:\n\n      Linux/macOS (bash, zsh)\n      Windows (PowerShell)\n\n\n            openstack loadbalancer pool create \\\n        --loadbalancer  \\\n        --name  \\\n        --protocol  \\\n        --lb-algorithm\n\n\n            openstack loadbalancer pool create `\n        --loadbalancer  `\n        --name  `\n        --protocol  `\n        --lb-algorithm\n\n\n      Write down the identifier of the created pool (id).\n\n   Find the IP addresses of the virtual machines that will be members of the pool. Also find the ID subnet where the virtual machines are located.\n\n      These virtual machines must either be located on the subnet where the balancer for which the balancing rule is being created is located, or be accessible from that subnet.\n\n   For each such virtual machine, create a member object that will be a member of the created pool:\n\n      Linux/macOS (bash, zsh)\n      Windows (PowerShell)\n\n\n            openstack loadbalancer member create  \\\n        --name  \\\n        --address  \\\n        --subnet-id  \\\n        --protocol-port\n\n\n            openstack loadbalancer member create  `\n        --name  `\n        --address  `\n        --subnet-id  `\n        --protocol-port\n\n\n      All member objects within the same pool must use the same port.\n\n\n   Create a healthcheck object for the pool. It will check the status and availability of members in the pool.\n\n      Linux/macOS (bash, zsh)\n      Windows (PowerShell).\n\n\n            openstack loadbalancer healthmonitor create  \\\n        --name  \\\n        --delay  \\\n        --timeout  \\\n        --max-retries  \\\n        --max-retries-down  \\\n        --type\n\n\n            openstack loadbalancer healthmonitor create  `\n        --name  `\n        --delay  `\n        --timeout  `\n        --max-retries  `\n        --max-retries-down  `\n        --type\n\n\n   Create a listener object to handle incoming connections:\n\n      Linux/macOS (bash, zsh)\n      Windows (PowerShell)\n\n\n            openstack loadbalancer listener create  \\\n        --name  \\\n        --protocol  \\\n        --default-pool  \\\n        --protocol-port\n\n\n            openstack loadbalancer listener create  `\n        --name  `\n        --protocol  `\n        --default-pool  `\n        --protocol-port\n\n\nTo apply (set) the necessary settings to objects that are part of a balancing rule, or to override the settings (unset), use the appropriate commands (for example, openstack loadbalancer pool set).\n\nTo remove a balancing rule:\n\n   Find the ID of the necessary listener object.\n\n   Determine the ID of the pool used by that listener-object:\n\n      Execute the command:\n\n                  openstack loadbalancer listener show\n\n      The pool ID will be contained in the default_pool_id field.\n\n   Remove the listener object:\n\n            openstack loadbalancer listener delete\n\n   Delete the pool:\n\n            openstack loadbalancer pool delete\n\n\nRemoving the load balancer\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nThis is a group operation: if necessary, you can remove multiple load balancers at once by selecting them using the checkboxes.\n\nTo remove a load balancer:\n\nGo to personal account VK Cloud.\nSelect the project where the necessary load balancer is located.\nGo to Virtual networks → Load balancers.\nPerform one of the actions for the necessary load balancer:\n\n   Select the balancer using the checkbox, then click Delete.\n   Click   for the balancer and select Delete balancer.\n\nConfirm the removal of the balancer.\n\n\n\n\nMake sure that:\n\n   Make sure that OpenStack client is installed and authenticate to the project.\n   Install python-octaviaclient package in the OpenStack CLI client.\n\nGet ID of the load balancer.\n\nRemove the load balancer:\n\n      openstack loadbalancer delete\n\n","url":"/en/networks/balancing/service-management/manage-lb","description":"Managing load balancers in the virtual networks service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Load Balancing","link":"/en/networks/balancing","isActive":false,"evenSimpler":true}]},{"uuid":"54ed68ba-31ab-402a-8769-02351d939a84","title":"About service","data":"How CDN works\n\nCDN (Content Delivery Network) is a network of geographically distributed caching servers for content distribution.\n\nWhen using a CDN, content is delivered from the CDN server closest to the user. If the necessary content is not on this server, CDN requests it from the origin servers or neighboring CDN servers. When the CDN server gets the data, the server caches it for a while. The CDN service of VK Cloud also provides the option of preloading content onto CDN servers, which further reduces the load of origin servers.\n\nThe CDN service helps:\n\nTo increase the speed and reliability of content delivery.\n\n  Without CDN, users located far from the origin servers may have difficulties in accessing content. For example, there might be slow or unstable connection with these servers.\n\nTo reduce the load of origin servers. There is also the capability to handle increased demand when content is requested by a lot of users.\n\n  Without CDN, all users would send their requests to the origin servers, creating a significant load of these servers. CDN allows to transfer this load to the CDN servers and distribute it among them.\n\nCDN Operation Diagram\n\nCDN VK Cloud opportunities\n\nOver 46 points of presence in Europe, CIS, North and South America.\nMore than 200 caching CDN servers.\nOver 700 peering partners.\nNetwork capacity exceeding 12 terabits per second (Tbps).\n","url":"/en/networks/cdn/concepts/about","description":"Brief description and opportunities of the CDN VK Cloud service","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"ad9ee911-e7f3-458c-8c33-92bec7646d70","title":"HTTP/2 support","data":"CDN servers support data transfer over HTTP/2 by default. The current versions of most browsers are compatible with this protocol.\n\n\n\nTo utilize CDN over HTTP/2, configure SSL certificates for your CDN resource, as browsers make requests over HTTPS when operating with HTTP/2.\n\n\n\nWhen a browser requests content via HTTP/2, both CDN servers and HTTP/2-supporting origin servers will deliver the content using this protocol. If the origin servers do not support HTTP/2, they will deliver the content using a supported protocol (for example, HTTP/1.1). The browser will combine all the received content into a single page, even if it was obtained through different protocols.\n\nIf a browser does not support HTTP/2, CDN servers or origin servers deliver content using a supported protocol (for example, HTTP/1.1).\n","url":"/en/networks/cdn/concepts/http2-support","description":"Overview of HTTP/2 support features.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"703d54e8-2b8a-4773-b862-7a38b3501c2f","title":"Origin groups","data":"For a CDN resource, an origin group is always configured, even when only one origin of content is used. When content is requested, a CDN server selects an origin according to the options set at the origin group level. Depending on the responses from the selected source, another origin may be chosen.\n\nThe following options affect the order in which an origin can be selected from the group when CDN servers request content:\n\nSource type**: active or reserve.\n\n  The type of origin. This is specified at the single origin level within the group.\n\nИспользовать следующий источник из списка при 4XX и 5XX кодах на источнике**.\n\n  This is set at the overall origin group level. The option is available if more than one origin is configured in the group.\n\nThe selection of an origin is based on the following rules:\n\nIf the option Использовать следующий источник из списка при 4XX и 5XX кодах на источнике is disabled, content is firstly requested from one of the active origins, then from one of the reserve origins.\n\n  The CDN server requests content from one of the reserve origins only if the active origin returns an HTTP status from the 5XX range. If an HTTP status from the 4XX range is received from either active or reserve origins, the CDN server will return an error to the user.\n\n  The active origin is selected based on a round-robin algorithm to provide load balancing.\n\nIf the option Использовать следующий источник из списка при 4XX и 5XX кодах на источнике is enabled, the CDN server requests content by moving down the origin list.\n\n  If all origins in the list are active, the CDN server requests content from the first active origin. With HTTP status 404, 500, 502, 503 or 504, the CDN server will move down the list and request content from the remaining origins.\n\n  If there are reserve origins in the list, the CDN server requests content from the first availible origin. Upon receiving an HTTP status from the 4XX or 5XX range, the CDN server will proceed down the list and request content from the remaining origins, according to their type.\n\n  If all origins are unavailable, the CDN server will return the response of the last origin in the list.\n","url":"/en/networks/cdn/concepts/origin-groups","description":"How CDN servers deal with origin groups.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"2fe0e386-3d97-49fc-9c6f-acfdd1b86ddd","title":"Creating CDN resource","data":"You can create a CDN resource in two ways:\n\nThrough the CDN service interface. Use this option if you need to specify third-party content origins or configure SSL certificates manually.\n\nThrough the bucket interface in the Cloud Storage(/en/storage/s3) service. Use this option for the bucket to act as the content source. A corresponding CDN resource for the bucket will be automatically created, and SSL certificates will also be automatically configured.\n\nCreating resource via CDN service interface\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)[tags=resources]}\n\nClick Create resource.\nEnable the option Access to content by end users to manage availability of the CDN resource:\n\n   (By default) If the option is disabled, the CDN resource goes into Suspended state after creation. Content will not be delivered to users.\n   If the option is enabled, the CDN resource goes into Active state after creation. Сontent will be delivered to users.\n\n   You can enable or disable access to content after creating the CDN resource as well.\n\nSet up the interaction of the CDN resource with origins:\n\n   Source interaction protocol: select the protocol that CDN servers will use to request content from the origins:\n\n       {include(/en/_includes/_cdn_origin.md)[tags=http]}\n\n   Content request: select the type of origins that CDN servers will request content from:\n\n     (By default) From one origin\n     From group of origin\n\n\n     {include(/en/_includes/_cdn_origin.md)[tags=content_source]}\n\n     An origin group with a single origin will be automatically created after the CDN resource is set up.\n\n\n     Select an origin group from the drop-down list.\n\n     If the required group is not in the list, click Add source group and add new origin group.\n\n\nIn the Personal domain field, specify the personal domain that will be used for the CDN. When users request this domain, content will be delivered using the CDN. Use the Fully Qualified Domain Name (FQDN). Do not add a root domain name: you can use cdn.example.com, but not cdn.example.com..\n\n(Optional) Click plus-icon Add domain to specify additional personal domains.\n\n(Optional) Click trash-icon to remove the domain you no longer need.\n\n\n   You cannont change personal domains after creating a CDN resource.\n\n\nSave the original domain name that needs to be set in the CNAME record for the specified domains.\n\nSpecify the SSL-certificate parameter. SSL will be used to access the previously configured personal domains via HTTPS:\n\n   Do not use: the certificate is not used, personal domains are only accessible via HTTP.\n   (By default) Let's Encrypt: Let's Encrypt certificate is used. The certificate will be created after the CDN resource is created. It needs available origin servers and the DNS changes related to CNAME records for personal domains. This typically takes up to 30 minutes.\n\n\n   The option needs the Access to content by end users option to be enabled.\n\n\n   User's certificate: you can select your SSL certificate from the list. To make the certificate available for selection, add it to the certificate store.\n\nSelect whether to alter the host HTTP header value when accessing the previously configured origins.\n\n   CDN servers specify the mandatory Host header in HTTP when request content from origins. This header allows you to address the appropriate virtual host on the origins.\n\n   {include(/en/_includes/_cdn_origin.md)[tags=host]}\n\nClick Create resource.\n\n\n\n\nTo create a CDN resource, use the POST /projects/{project_id}/resources/ method.\n\nRequest example:\n\ncurl --location --request POST 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n   \"cname\": \"cdn.example.com\",\n   \"originGroup\": 132,\n   \"secondaryHostnames\": [\n      \"1.example.com\",\n      \"2.example.com\"\n   ]\n}'\n\nCreating resource via bucket interface\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the bucket you need is located. If you do not have the bucket, create it.\nGo to the Cloud Storage → Buckets section.\nClick the name of the bucket you need.\nGo to the CDN tab.\nEnable the Use CDN for this bucket option.\n\n   To make the bucket able to be an origin for a CDN resource, select the public-read ACL, then create objects in this bucket.\n\nIn the Personal domain field enter the personal domain, what should be used for CDN. When requesting this domain, content will be delivered via CDN. Use the Fully Qualified Domain Name (FQDN). Do not add a root domain name: you can use cdn.example.com, but not cdn.example.com..\n\n(Optional) Click plus-icon Add domain to specify additional personal domains.\n\n(Optional) Click trash-icon to remove the domain you no longer need.\n\n\n   You cannont change personal domains after creating a CDN resource.\n\n\nSave the original domain name, which needs to be set in the CNAME record for the specified domains.\n\nSelect the Cache lifetime from the list.\n\n   This allows cache during the specified period the following HTTP-answers: 200, 201, 204, 206, 301, 302, 303, 304, 307, 308.\n\n   Other HTTP answers are not cached.\n\n   (Optional) Select Don't cache to disable caching.\n\nClick Save changes.\n\nThe creation of the origin group and CDN resource will start for the bucket. The created items will be available in the CDN section of your VK Cloud personal account.\n\nPreparing the CDN resource for operation\n\nCreate a CNAME record for the CDN resource. This will change the URL address. For example, if you are using a CDN resource to serve images for your website, create a CNAME record like images.example.com that points to a CDN resource like cl-541e19d9.service.cdn.msk.vkcs.cloud.\n\n   If you use the DNS VK Cloud service, follow the instructions.\n\n   If you did not save the original domain when created, you can find it in the CDN resource information:\n\n   {include(/en/_includes/_open-cdn.md)}\n\nReplace the original domain in the path to static files with the personal domain.\n\nUpload content in the VK Cloud interface (if the CDN resource was not created from a bucket).\n","url":"/en/networks/cdn/service-management/create-resource","description":"How to create a CDN resource in the CDN service or in the Cloud Storage bucket.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"63d0185d-7c37-4015-b7bd-cf6beb187327","title":"Enabling and disabling CDN resource","data":"Use the Access to content by end users option to manage availability of the CDN resource:\n\nIf the option is disabled, the CDN resource goes into Suspended state. Content will not be delivered to users.\nIf the option is enabled, the CDN resource goes into Active state. Сontent will be delivered to users.\n\nTo enable a CDN resource:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the General settings tab.\nEnable the Access to content by end users option.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the active parameter in the request body:\n\ntrue — resource is active, content is delivered.\nfalse — resource is suspend, content is not delivered.\n\nExample request to suspent the CDN resource:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"active\": false\n}'\n\n\n\n","url":"/en/networks/cdn/service-management/manage-cdn/enable-cdn","description":"How to activate or suspend content delivery via CDN resource.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"f39e2f7a-3f05-449f-b131-5fbe19cd2a9f","title":"Interacting with origins","data":"Changing orogon interaction protocol\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the General settings tab\nSelect the option in the Source interaction protocol parameter:\n\n    {include(/en/_includes/_cdn_origin.md)[tags=http]}\n\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the originProtocol parameter in a request body:\n\n{include(/en/_includes/_cdn_origin.md)[tags=http-api]}\n\nExample request:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"originProtocol\": \"HTTPS\"\n}'\n\nChanging origin\n\nThe option allows you to change the origin that the CDN resource will fetch content from.\n\nIf a single origin was specified when creating the CDN resource, an origin group is automatically created, consisting of one origin with the specified parameters. Therefore, when editing a CDN resource, you can only change the origin group, and the option to select a single origin is not available.\n\nTo change an origin:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the General settings tab\nSelect a group from the Source group list.\n\n    If there is no group you need in the list:\n\n     Click Add source group.\n     Enter the name of the origin group.\n     Add the first origin:\n\n        {include(/en/_includes/_cdn_origin.md)[tags=add,4XX_5XX]}\n\n     Click Create group.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify parameters in a request body:\n\norigin: the IP address or the domain name of the origin and the port, if setted.\noriginGroup: the ID or the name of the origin group.\n\nYou can use either origin, or originGroup.\n\nExample request to change a single origin:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"origin\": \"yoursite.com\"\n}'\n\nExample request to change an origin group:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n}'\n\n\n\nTo edit or to add an origin group use methods that work with origin groups.\n\nChanging host header\n\nCDN servers specify the mandatory Host header in HTTP requests when requesting content from origins. This header allows addressing the appropriate virtual host at the origin.\n\nBy default, the value of the Host header matches the first CNAME.\n\nTo change the header:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the General settings tab\nIn the Changing the Host header parameter, select whether to change the value of the Host HTTP header when requesting from origins or not.\n\n    {include(/en/_includes/_cdn_origin.md)[tags=host]}\n\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the parameters in a request body:\n\nTo have the first resource CNAME as the Host header (by default), set forward_host_header and hostHeader parameters to \"enabled\": false.\nTo have the name of the first configured personal domain as the Host header, set the parameters:\n\n        \"forward_host_header\": {\n        \"enabled\": true,\n        \"value\": true\n    },\n    \"hostHeader\": {\n        \"enabled\": false\n    }\n\nTo have a custom domain or IP address as the Host header, set the parameters:\n\n        \"forward_host_header\": {\n        \"enabled\": false,\n        \"value\": false\n    },\n    \"hostHeader\": {\n        \"enabled\": true,\n        \"value\": \"\"\n    }\n\nExample request to set a custom header in host:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"options\": {\n        \"forward_host_header\": {\n            \"enabled\": false,\n            \"value\": false\n        },\n        \"hostHeader\": {\n            \"enabled\": true,\n            \"value\": \"content-source.example.org\"\n        }\n    }\n}'\n\n\n\n\n\n\nAfter making changes to the option, clear the CDN resource cache.\n\nActivating shielding\n\nOrigin shielding (or precache server) helps to protect the origin from high loads. The precache server collects all user requests and sends them one by one to the origin if the required content is not in the cache of the precache server itself.\n\n\n\nActivating origin shielding increases the consumption of total CDN traffic.\n\n\n\nTo enable origin shielding:\n\n\n\nPersonal account\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the General settings tab\nEnable the Activate origin shielding option.\nRead the warning and confirm the enabling.\nSelect the data center, located closer to your origin server.\nClick Save changes.\n\n\n\n","url":"/en/networks/cdn/service-management/manage-cdn/origin-settings","description":"How to change origins, an interaction protocol, a host header and how to protect the origins from high loads.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"0dd523dd-2d6a-4bfe-8506-d1de233a0e9d","title":"Using SSL certificate","data":"\n\nHere is the article about applying SSL sertificates. To know how to create, edit and delete SSL certificates, read Managing SSL certificates.\n\n\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the General settings tab.\nSelect how to apply SSL certificate in the SSL certificate parameter:\n\n    Do not use — a certificate will not be used: personal domains can only be accessed via HTTP.\n    (By default) Let's Encrypt — a free Let's Encrypt certificate will be used. The certificate will be created after the CDN resource is established, once the origin servers are available and DNS changes involving the CNAME records for personal domains have propagated. This usually takes up to 30 minutes. Choosing this option also requires enabling the Access to content by end users option.\n    User's certificate —  a certificate selected from the list will be used. To make the certificate available for selection, add it to the certificate store.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the following parameters in a request body:\n\nTo not use SSL certificate and access to personal domains via HTTP only, specify the \"sslEnabled\": false parameter.\nTo use a free Let's Encrypt certificate, specify the \"sslEnabled\": true parameter and the parameters to update the certificate:\n\n  \"ssl_automated\": true — the Let’s Encrypt certificate will be automatically renewed upon expiry.\n  \"ssl_automated\": false — the Let’s Encrypt certificate will not be automatically renewed.\n\n  To create or configure a Let's Encrypt certificate, use the methods of the Let's Encrypt certificates section.\n\nTo use your SSL certificate, specify the \"sslEnabled\": true, and also specify the certificate identifier in the sslData parameter. To create or configure your certificate, use the methods of the SSL certificates section.\n\nExample request to use SSL custom certificate:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"sslEnabled\": true,\n    \"sslData\": 42,\n}'\n\n\n\n","url":"/en/networks/cdn/service-management/manage-cdn/ssl","description":"How to apply Let's Encrypt or custom SSL certificate to a CDN resource.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"330016db-057b-4420-9a49-f1461e6634b1","title":"Caching","data":"CDN resource сaching configurations\n\nThe option allows to define content storage parameters in the cache of a CDN resource. When the time expires, the CDN resource checks the file on the origin:\n\nIf the ETag of the file on the CDN server matches the ETag of the file on the origin, the CDN resource continues to store and serve the cached file to users.\nIf the ETag of the file on the CDN server does not match the ETag of the file on the origin, the CDN resource caches the new file.\n\nTo configure the caching for the CDN resource:\n\n\n\nPersonal accaunt\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the Caching tab.\nEnable the CDN caching on option.\nSelect a configuration option:\n\n   Origin settings\n   Settings\n\n\n      The CDN resource will cache content for the duration set on the origin in the Cache-Control header.\n\n      If no Cache-Control is specified on the origin, the default time (4 days) is used. To change this time, select a value in the Default cache lifetime parameter. The chosen value will apply for responses with codes 200, 201, 204, 206, 301, 302, 303, 304, 307, 308. Responses with other codes will not be cached.\n\n\n      The CDN resource will cache the content for the time specified in its settings:\n\n      Specify the cache lifetime. This value will only apply to responses with codes 200, 206, 301, 302. Responses with 4XX, 5XX codes will not be cached.\n      (Optional) Customize the caching time for different responses:\n\n         Click Add rule.\n         Select the response code. For all types of responses, select the any value.\n         Select the caching time for this response.\n\n\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the edge_cache_settings parameter in the request body in the options block.\n\nExample request:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"edge_cache_settings\": {\n            \"custom_values\": {\n                \"100\": \"3600s\",\n                \"404\": \"0s\"\n            },\n            \"enabled\": true,\n            \"value\": \"345600s\"\n        }\n    }\n}'\n\nThe example sets the caching settings for the CDN resource:\n\nby default, content is cached for 4 days\nfor 100 responses, content is cached for 1 hour\nfor 404 responses, content is not cached\n\nBrowser сaching сonfigurations\n\nThe option allows to set parameters for storing content in the cache of end-users' browsers.\n\nTo set up user's browser caching:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the Caching tab.\nEnable the Browser caching option.\nSelect a configuration option:\n\n   Origin settings\n   Settings\n\n\n      The browser will cache content for the duration set on the origin in the Cache-Control header.\n\n      If no Cache-Control is specified on the origin, content is not cached.\n\n\n      Caching settings for user browsers are specified in the CDN resource settings.\n\n      Specify the cache lifetime. This value will apply to responses with codes 200, 201, 204, 206, 301, 302, 303, 304, 307, 308 if caching headers are not configured at the origin. Responses with other codes will not be cached.\n\n\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the browser_cache_settings parameter in the request body in the options block.\n\nExample request to configure browser caching by source:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"browser_cache_settings\": {\n            \"enabled\": true,\n            \"value\": \"\"\n        }\n    }\n}'\n\nIgnoring Set-Cookie header when caching\n\nRequests to a CDN server to get the same file may contain one path to the file but different values in the Set-Cookie HTTP header. By default, the CDN resource considers such requests different and redirects them to the origin server. This decreases data transfer speed. Ignoring the Set-Cookie header allows the CDN resource to use its own cache instead of sending a request to the origin.\n\nTo have the CDN resource ignore Set-Cookie headers in HTTP requests:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the Caching tab.\nEnable the Ignore Set-Cookie option.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the ignore_cookie parameter in the request body in the options block.\n\nExample request to ignore the Set-Cookie header:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"ignore_cookie\": {\n            \"enabled\": true,\n            \"value\": true\n        }\n    }\n}'\n\nIgnoring query parameters when caching\n\nRequests to a CDN server to get the same file may contain one path to the file but different query parameters. By default, the CDN resource treats such requests as different and redirects them to the origin server, which reduces the speed of data transfer. Ignoring all or some parameters allows the CDN resource to use its cache instead of sending a request to the origin.\n\nTo have the CDN resource ignore parameters in HTTP requests:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the Caching tab.\nEnable the Ignore request parameters option.\nSelect the type of ignoring:\n\n   Ignore everything — files with any query parameters are cached as one object.\n   Ignore everything except — files with specified parameters are cached as different objects, files with other parameters — as one.\n   Ignore only — files with specified parameters are cached as one object, files with other parameters — as different.\n\nFor types Ignore everything except and Ignore only specify the parameters. Enter each parameter in a new line.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the following parameters in the request body in the options block:\n\nTo ignore all parameters — ignoreQueryString (value=true).\nTo ignore specified parameters — ignoreQueryString (value=false), query_params_blacklist.\nTo ignore all parameters except specified — ignoreQueryString (value=false), query_params_whitelist.\n\n\nExample request to ignore all query parameters in HTTP requests\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"ignoreQueryString\": {\n            \"enabled\": true,\n            \"value\": true\n        }\n    }\n}'\n\n\n\nExample request to ignore specified query parameters in HTTP requests\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"ignoreQueryString\": {\n            \"enabled\": true,\n            \"value\": false\n        },\n        \"query_params_blacklist\": {\n                \"enabled\": true,\n                \"value\": [\n                    \"color\",\n                    \"type\"\n                ]\n         }\n    }\n}'\n\n\n\nExample request to ignore all query parameters in HTTP requests except specified\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"ignoreQueryString\": {\n            \"enabled\": true,\n            \"value\": false\n        },\n        \"query_params_whitelist\": {\n                \"enabled\": true,\n                \"value\": [\n                    \"color\",\n                    \"type\"\n                ]\n         }\n    }\n}'\n\n\n\n\n\n","url":"/en/networks/cdn/service-management/manage-cdn/caching","description":"How to set up caching on a CDN resource and in users browsers, ignoring request parameters.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"67e65739-5547-4b3b-baba-c3c8bae197c8","title":"HTTP headers setting","data":"Adding request headers\n\nThe Add request titles option allows you to set custom HTTP headers that the CDN server will add to the client request to the origin server.\n\nTo add an HTTP header to a request:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the HTTP-headers tab.\nEnable the Add request titles option.\nEnter the title name and the value. The following characters are allowed:\n\n    Title name: Latin letters, numbers, special characters _ and -.\n    Value: Latin letters, numbers, special characters _, ., /, :, -, =, and spaces. The value cannot start with a special character and be composed only of special characters.\n(Optional) Click plus-icon Add title to add one more request header. Specify the name and value of the header. You can set up to 50 request headers.\n(Optional) Click trash-icon to delete a request header.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the staticRequestHeaders parameter in the options block of a request body:\n\nTo enable the option specify \"enabled\": true, to disable  — \"enabled\": false.\nIn the value parameter, write the headers and values in the name: value format. You can specify up to 50 request headers. The following characters are allowed:\n\n  name: Latin letters, numbers, special characters _ and -.\n  value: Latin letters, numbers, special characters _, ., /, :, -, =, and spaces. The value cannot start with a special character and be composed only of special characters.\n\nExample request:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"staticRequestHeaders\": {\n            \"enabled\": true,\n            \"value\": {\n                \"Header-One\": \"Value 1\",\n                \"Header-Two\": \"Value 2\"\n            }\n        },\n    }\n}'\n\nAdding response headers\n\nThe Add response titles option allows you to set custom HTTP headers that the CDN server will add to the response to a user request.\n\nTo add an HTTP header to a response:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the HTTP-headers tab.\nEnable the Add response titles option.\nEnter the title name and the value. The following characters are allowed:\n\n    Title name: Latin letters, numbers, special characters _ and -.\n    Value: Latin letters, numbers, special characters `  `, ~, !, @, #, %, ^, &, *, (, ), -, _, =, +, /, |, \", ;, :, ?, ., >, \n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the staticHeaders parameter in the options block of a request body:\n\nTo enable the option specify \"enabled\": true, to disable  — \"enabled\": false.\nIn the value parameter, write the headers and values in the name: value format. You can specify up to 50 request headers. The following characters are allowed:\n\n  name: Latin letters, numbers, characters _ and -.\n  value: Latin letters, numbers, characters `  ` ~ ! @ # % ^ & * ( ) - _ = + / | \" ; : ? . >\n\nEnabling CORS headers support\n\nThe Cross-Origin Resource Sharing (CORS) mechanism uses the Access-Control-Allow-Origin response header to grant users' browsers permissions to access resources located on different domains from the origin.\n\nTo configure the Access-Control-Allow-Origin HTTP header:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the HTTP-headers tab.\nEnable the HTTP header Access-Control-Allow-Origin option.\nSelect the header configuration option:\n\n    *, for all domains: allows content to be displayed for all domains.\n    \"$http_origin\", for all domains: allows content to be displayed for all domains, but the requesting domain will be sent in the Access-Control-Allow-Origin response header.\n    \"$http_origin\", if the source is below: allows content to be displayed for the specified domains:\n\n        Specify the domain for which content should be displayed.\n        (Optional) Click plus-icon Add domain to add one more domain.\n        (Optional) Click trash-icon to remove a domain form the list.\n\n        The requesting domain will be sent in the response header.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the cors parameter in the options block of a request body:\n\nTo disable the option and not provide any values, use cors: null.\nTo enable the option, specify \"enabled\": true; to disable it, specify \"enabled\": false.\nIn the value parameter, specify the domains for which content display will be permitted.\n\nExample request:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"cors\": {\n            \"enabled\": true,\n            \"value\": [\n                \"first.example.com\",\n                \"second.example.com\"\n            ]\n        }\n    }\n}'\n\n\n\n","url":"/en/networks/cdn/service-management/manage-cdn/http-headers","description":"How to add HTTP headers to requests and answers, configure CORS support.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"bffbf18f-cf65-4b64-817e-100caf56d1e2","title":"Content managing","data":"Clearing cache\n\nWhen a CDN resource is not functioning correctly or if origin data has been updated, it may be necessary to partially or fully clear a cache.\n\nTo clear a cache of a CDN resource:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nClear the cache with one of the following ways:\n\n    Click   for the required resource and select **Clear\n      CDN cache**.\n    Click the name of the personal domain you need and go to the Content tab.\n\nIn the Clear cache section, select the type of clearance:\n\n    Full — completely removes all files from the cache of the CDN resource. Full cache clearance will cause CDN servers to request content from origin servers that increases the load on origins and may destabilize the service. Use selective cache clearance if you need to remove a large volume of content from the cache.\n    Selective — allows you to remove individual files from the cache of the CDN resource. In the input field, specify the path or pattern of the path to the files you want to delete. Use the following characters for the pattern:\n\n       Use / or * at the beginning of the path.\n       Use * to replace any number of characters in the path.\n\nClick Clear cache.\n\n\n\n\nUse the POST /projects/{project_id}/resources/{resources_id}/purge method.\n\nIn the request body specify the following:\n\n[] — to delete all files on the CDN resource.\nThe file path without the domain name to clear the cache of a selected file.\nThe path pattern to delete certain files. Use the following characters for the pattern:\n\n  Use / or * at the beginning of the path.\n  Use * to replace any number of characters in the path.\n\nExample request for clearing the cache of a CDN resource:\n\ncurl --location --request POST 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281/purge'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"paths\": [\n        \"/test.jpg\",\n        \"/static/*\",\n        \"*.png\",\n        \"/images/\",\n        \"/pictures/.jpg\"\n    ]\n}'\n\nPreloading cache\n\nYou can upload content in a cache of a CDN resource in advance, before end users request it. It can help reduce the load of origin servers. Preloading is relevant for files larger than 200 MB.\n\n\n\nTo update files on a CDN, first clear a cache of the CDN resource, and then upload new.\n\n\n\nTo preload a cache of a CDN resource:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nUpload the content with one of the following ways:\n\n    Click   for the required resource and select Uploading to CDN.\n    Click the name of the personal domain you need and go to the Content tab.\n\nIn the Filling the cache specify the path to the files on the origin without the domain name and one per line.\nClick Fill cache.\n\n\n\n\nUse the POST /projects/{project_id}/resources/{resources_id}/prefetch method.\n\nIn the body of the request, specify paths to the files that need to be loaded, without the domain name.\n\nExample request for cache preloading of a CDN resource:\n\ncurl --location --request POST 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281/prefetch'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"paths\": [\n        \"/video.mp4\",\n        \"video.mp4\"\n        ]\n    }'\n\nConfiguring HTTP response codes\n\nThe Enable returning HTTP response code option allows you to set the HTTP response code for content hosted on a CDN resource. For example, you can configure a redirection to another URL or send the 403 code to end-users when requesting certain files.\n\nTo configure the HTTP response code:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the Content tab.\nEnable the Enable returning HTTP response code option.\nSpecify a responce code. For instance, 403.\nEnter the URL for redirection or text for the response code.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the force_return parameters in the options block of a request body:\n\nTo enable the option, specify \"enabled\": true; to disable it — \"enabled\": false.\nSpecify the HTTP response code in the code parameter. For example, \"code\": 403.\nSpecify the URL or response code text in the body parameter.\n\nExample request with settings for the HTTP 403 response:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"force_return\": {\n            \"enabled\": true,\n            \"code\": 403,\n            \"body\": \"Access denied\"\n        }\n    }\n}'\n\nGZip compression\n\nFiles arriving at the CDN resource can be compressed using the GZip method. The average compression ratio if this method is 70%, sometimes it can reach 90%. The minimum file size for compression is 128 bytes.\n\nWhen you use the GZip compression method, uncompressed files are requested from the origin, so the option does not work in conjunction with origin compression and large file delivery optimization.\n\nTo enable GZip compression:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the Content tab.\nEnable the GZip compression option.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the gzipOn parameters in the options block of a request body.\n\nExample request to enable GZip compression:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"gzipOn\": {\n            \"enabled\": true,\n            \"value\": true\n        }\n    }\n}'\n\nBrotli compression\n\nFiles arriving at a CDN resource can be compressed using the Brotli method. This method provides stronger compression than GZip. For instance, for text files, Brotli compression exceeds GZip by an average of 20%. The minimum file size for compression is 128 bytes.\n\nWhen using the Brotli compression method, uncompressed files are requested from the origin, and compression occurs on a special pre-cache server. The pre-cache server stands between the origin server and the CDN servers, protecting the origin server from high load. Origin server protection using a pre-cache server (shielding) is a paid option. To enable it, contact technical support.\n\nThe option does not work in conjunction with origin compression and large file delivery optimization.\n\nTo enable Brotli compression:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the Content tab.\nEnable the Enable Brotli compression option.\nSelect the data types of the content to be compressed. The type text/html is selected by default and cannot be removed from the list.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the brotli_compression parameters in the options block of a request body.\n\nExample request to set up Brotli compression:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"brotli_compression\": {\n            \"enabled\": true,\n            \"value\": [\n                \"text/html\",\n                \"application/xml+rss\",\n                \"application/xml\"\n            ]\n        }\n    }\n}'\n\nOrigin compression\n\nContent compression at the origin accelerates content delivery: the content is transferred to the CDN server in compressed form, reducing data transmission time. For the option to work correctly, the origin must support compression.\n\nThe option does not work in conjunction with GZip compression, Brotli compression and large file delivery optimization.\n\nTo have a CDN resource request compressed content from an origin:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the Content tab.\nEnable the Request compression at source option.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the fetch_compressed parameters in the options block of a request body.\n\nExample request to set up compression at an origin:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"fetch\": {\n            \"enabled\": true,\n            \"value\": true\n        }\n    }\n}'\n\nLarge file delivery optimization\n\nTo optimize delivery, large files are transferred from an origin to a CDN resource and stored in a cache not in full, but in parts of 10 MB each. The CDN resource will start transferring the file to users sooner and will also be able to transfer the file in parts to multiple users simultaneously.\n\nFor the option to work correctly, the origin must support HTTP Range requests. If an origin group is used for the resource, the files on each of the origins used must contain identical Content-Length and ETag headers.\n\nAfter enabling or disabling the option, caching keys will change, and content will be requested from the origin again.\n\n\n\nTo reduce the load on the origin, manage the option during off-peak hours and contact technical support to protect the origin with a pre-cache server.\n\n\n\nThe option does not work in conjunction with GZip compression, Brotli compression and  origin compression.\n\nTo enable the optimization of large file delivery:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the Content tab.\nEnable the Enable Large File Delivery Optimization option.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the slice parameters in the options block of a request body.\n\nExample request to set up large file delivery optimization:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"slice\": {\n            \"enabled\": true,\n            \"value\": true\n        }\n    }\n}'\n\n\n\n","url":"/en/networks/cdn/service-management/manage-cdn/content-settings","description":"How to clear or preload a CDN resource cache, configure compression, optimize content delivery, configure response codes.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"c63e3589-b838-4a0a-8421-8cf636a7f9a9","title":"Access setting","data":"Configuring Allowed HTTP Methods\n\nThe Allowed HTTP Methods option enables control over permitted HTTP request methods to a CDN resource. By default, the following methods are allowed: GET, POST, HEAD, OPTIONS, PUT, PATCH, DELETE.\n\nTo configure the allowed methods:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the HTTP-headers tab.\nEnable the Allowed HTTP Methods tab.\nSelect the methods that CDN resource has to process.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the allowedHttpMethods parameter in the options block of a request body:\n\nto enable the option specify \"enabled\": true; to disable — \"enabled\": false.\nlist methods that should be available in the value parameter.\n\nExample request:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"allowedHttpMethods\": {\n            \"enabled\": true,\n            \"value\": [\n                \"GET\",\n                \"HEAD\"\n            ]\n        },\n    }\n}'\n\n\n\n\nIf the origin server does not recognize the specified method, a response with the code 501 (Not Implemented) will be returned. If the origin server knows the method but it is not permitted on the CDN resource, a response with the code 405 (Method Not Allowed) will be returned.\n\nConfiguring country access policy\n\nThis option allows you to protect content from unauthorized access from certain countries.\n\nTo configure the country-based security policy:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the Security tab\nEnable the Set up access policy by country option.\nSelect the type of policy:\n\n   Permissive — allows access to content from all countries except selected.\n   Blocking —  blocks access to content from all countries except selected.\n\nSelect the countries to which you want to deny or allow access to content. Countries are listed in the format ISO 3166-1 alpha-2. Multiple selections are available.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the country_acl parameter in the options block of a request body.\n\nExample request to allow access to content only from the UK and France:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"country_acl\": {\n            \"enabled\": true,\n            \"excepted_values\": [\n                \"UK\",\n                \"FR\"\n            ],\n            \"policy_type\": \"deny\"\n        }\n    }\n}'\n\nConfiguring domain access policy\n\nThe option allows you to protect content from being published on other sites.\n\nTo configure the domain access policy:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the Security tab\nEnable the Configure domain access policy option.\nSelect the type of policy:\n\n   Permissive — allows access to content from all domains except selected.\n   Blocking —  blocks access to content from all domains except selected.\n\nEnter the domain or domain mask without http:// or https://. For instance, example.com, *.example.com. You can enter several domains.\n(Optional) Enable the Contact via direct link option to allow or deny the specified sites from accessing the CDN resource via a direct link.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the referrer_acl parameter in the options block of a request body.\n\nExample request to allow access to content only for subdomains of the domain vk.com:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"referrer_acl\": {\n            \"enabled\": true,\n            \"excepted_values\": [\n                \"*.vk.com\"\n            ],\n            \"policy_type\": \"deny\"\n        }\n      }\n}'\n\nConfiguring IP address access policy\n\nThe option allows you to deny access to content for specific IP addresses.\n\nTo configure IP address-based security policy:\n\n\n\nPersonal account\nAPI\n\n\n\n{include(/en/_includes/_open-cdn.md)}\n\nGo to the Security tab\nEnable the Configure access policy by IP addresses option.\nSelect the type of policy:\n\n   Permissive — allows access to content from all IP addresses except selected.\n   Blocking —  blocks access to content from all IP addresses except selected.\n\nEnter the IP addresses with the subnet mask. For example, 192.168.3.2/32 or 2a03:d000:2980:7::8/128. The access policy works on network addresses calculated based on the specified IP addresses. If two or more IP addresses belong to the same network, it is sufficient to specify only one of those IP addresses.\nClick Save changes.\n\n\n\n\n{include(/en/_includes/_api_cdn_create_change.md)}\n\nSpecify the ip_address_acl parameter in the options block of a request body.\n\nThe access policy works on network addresses calculated based on the specified IP addresses. If two or more IP addresses belong to the same network, it is sufficient to specify only one of those IP addresses.\n\nExample request to deny access to content from IP addresses 192.168.1.100/32 or 10.10.10.10/24:\n\ncurl --location --request PUT 'https://msk.cloud.vk.com/api/cdn/api/v1/projects/examplef8f67/resources/175281'\\\n--header 'X-Auth-Token: example6UjMOd'\\\n--header 'Content-Type: application/json'\\\n--data '{\n    \"originGroup\": 267760,\n    \"originGroup_name\": \"exampleorigin\",\n    \"secondaryHostnames\": [],\n    \"options\":\n        {\n        \"ip_address_acl\": {\n            \"enabled\": true,\n            \"excepted_values\": [\n                \"192.168.1.100/32\",\n                \"10.10.10.10/24\"\n            ],\n            \"policy_type\": \"allow\"\n        }\n    }\n}'\n","url":"/en/networks/cdn/service-management/manage-cdn/security","description":"How to restrict HTTP methods of requests to a CDN resource, how to restrict access to content by country, domain and IP address.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"25c34379-7886-4614-aec5-9b8795b5739c","title":"Managing origin groups","data":"Viewing list of groups\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nSelect the project.\nGo to the CDN → Source groups section.\n\nA list of origin groups will be displayed.\n\nCreating group\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nSelect the project.\nGo to the CDN → Source groups section.\nClick   Create source group.\nEnter the origin group name.\nAdd one or several origins to group.\n(Optional) Edit the list of origins in the group.\nMake sure, that DNS records are configured for the domains of all added sources (including disabled ones).\n\n\n   If VK Cloud cannot check the availability of these domains using a DNS query, the origin group will not be created.\n\n\n{include(/en/_includes/_cdn_origin.md)[tags=4XX_5XX]}\n\nClick Create group.\n\nAdding origin to group\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nSelect the project.\nGo to the CDN → Source groups section.\nOpen the group editing page by one of the following ways:\n\n   Click the name of the group you need.\n   Click   for the required group and select Group settings.\n\n{include(/en/_includes/_cdn_origin.md)[tags=add]}\n\n{include(/en/_includes/_cdn_origin.md)[tags=4XX_5XX]}\n\nEditing origins in group\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nSelect the project.\nGo to the CDN → Source groups section.\nOpen the group editing page by one of the following ways:\n\n   Click the name of the group you need.\n   Click   for the required group and select Group settings.\n\n(Optional) Use the toggle to the right from the origin to enable or disable it. You cannot disable Active origin if it is the only enabled origin in the group. The disabled origin will not answer requests of CDN servers.\n\n(Optional) Edit the URL of the origin:\n\n   Click pencil-icon to the right from the origin.\n\n   {include(/en/_includes/_cdn_origin.md)[tags=edit_url]}\n\n   Click Save changes.\n\n(Optional) Edit the origin type:\n\n   Click pencil-icon to the right from the origin.\n   Change the origin type in the Source type parameter. You cannot change the origin type if it is the only enabled Active origin in the group.\n   Click Save changes.\n\n(Optional) Remove the origin. Click trash-icon to the right from the origin. You cannot remove the origin type if it is the only enabled Active origin in the group.\n\nClick Save changes.\n\nEditing group\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nSelect the project.\nGo to the CDN → Source groups section.\nOpen the group editing page by one of the following ways:\n\n   Click the name of the group you need.\n   Click   for the required group and select Group settings.\n\n(Optional) Edit the group name.\n(Optional) Add one or more origins.\n(Optional) Edit origins in the group.\nMake sure, that DNS records are configured for the domains of all added sources (including disabled ones).\n\n\n   If VK Cloud cannot check the availability of these domains using a DNS query, the origin group will not be created.\n\n\n{include(/en/_includes/_cdn_origin.md)[tags=4XX_5XX]}\n\nClick Create group.\n\nDeleting group\n\n\n\nYou cannot delete the origin group if CDN servers use it. To delete such group, firstly link another origin group with the CDN servers or delete CDN servers.\n\n\n\n\n\nPersonal account\n\n\n\nThis is a group operation: you can delete several groups at once by setting the checkboxes.\n\nGo to your VK Cloud personal account.\nSelect the project.\nGo to the CDN → Source groups section.\nDelete the orgin group with one of the following ways:\n\n   Set the checkbox for the required group, then click Delete above the table.\n   Click   for the required group and select Delete group.\nConfirm the deletion.\n\n\n\n","url":"/en/networks/cdn/service-management/manage-origin-groups","description":"How to operate origin groups in CDN.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"6c0e065c-3c46-4e0a-8ded-f41b5eb27edc","title":"Managing SSL certificates","data":"\n\nTo apply an SSL certificate to a CDN resource, follow the instructions.\n\nCreating an SSL certificate\n\n\n\nPersonal account\n\n\n\nGo to you VK Cloud personal account.\nSelect the project.\nGo to CDN → SSL certificates section.\nClick Create SSL-certificate or Create.\nIn the window that opens specify:\n\n   SSL certificate name: the name of the creating certificate.\n   Certificate: the public part of the certificate key in PEM format.\n   Private key: the private part of the certificate key in PEM format.\n\nClick Add SSL certificate.\n\nEditing SSL certificate\n\n\n\nYou can edit the certificate's name only. To edit public and private key, create a new one.\n\n\n\n\n\nPersonal account\n\n\n\nGo to you VK Cloud personal account.\nSelect the project.\nGo to CDN → SSL certificates section.\nClick   for the required certificate and select Certificate settings.\nIn the SSL certificate name field edit the certificate name.\nClick Save changes.\n\nDeleting SSL certificate\n\n\n\nPersonal account\n\n\n\nGo to you VK Cloud personal account.\nSelect the project.\nGo to CDN → SSL certificates section.\nClick   for the required certificate and select Delete certificate.\nConfirm the deletion.\n\n\n\n","url":"/en/networks/cdn/service-management/manage-certificates","description":"How to create, edit and delete SSL certificates via VK Cloud personal account.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"a69db7bb-9c2c-4518-98ee-5e85358a57e3","title":"Deleting CDN resource","data":"You can delete a CDN resource in two ways:\n\nThrough the CDN service interface.\n\nThrough the bucket interface in the Cloud Storage service. Use this if the CDN resource was created trough the bucket interface.\n\nDeleting resource via CDN service interface\n\n\n\nPersonal account\n\n\n\nThis is a group operation: you can delete several CDN resources at once by setting the checkboxes.\n\nTo delete a CDN resource:\n\n{include(/en/_includes/_open-cdn.md)[tags=resources]}\n\nDelete the resource in one of the following ways:\n\n   Set the checkbox for the required resource, then click Delete above the table.\n   Click   for the required resource and select Delete.\n\nConfirm the deletion.\n\nDeleting resource via bucket interface\n\n\n\nPersonal account\n\n\n\nGo to you VK Cloud personal account.\nSelect the project with the bucket you need.\nGo to the Cloud Storage → Buckets section.\nClick the name of the bucket you need.\nGo to the CDN tab.\nDisable the Use CDN for this bucket option.\n\n\n\n","url":"/en/networks/cdn/service-management/delete-resource","description":"How to delete a CDN resource in the CDN service or from the Cloud Storage bucket.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"d38ebc64-d6de-4e51-9583-6ad7fcc0cf68","title":"Statistics","data":"How to get monitoring data\n\nGo to your VK Cloud personal account.\nGo to the CDN → Statistics section.\nSelect the metric you need to display statistics for.\nSelect the time interval.\nSelect the CDN resources you need.\n\nA chart will display the change in the selected metric over the specified time period.\n\nAvailable monitoring metrics\n\nTotal traffic** — the volume of all traffic from selected CDN resources for the specified time period.\nResponse codes** — the response codes for requests to the selected CDN resources for the specified time period. A table below the chart shows statistics for each type of responses. You can select one or several types of responses to filter the data in the chart.\nCached traffic** — the percentage of caching for each selected resource for the specified time period.\nRequests** — the number of requests to the selected CDN resources for the specified time period.\nTraffic statistics by regions** — the volume and percentage of traffic from the selected CDN resources for the specified time period in various regions. A table below the chart shows statistics for each region. You can select one or several regions to filter the data in the chart.\nTraffic Statistics by countries** — the volume and percentage of traffic from the selected CDN resources for the specified time period in various countries. A table below the chart shows statistics for each country. You can select one or several countries to filter the data in the chart.\n","url":"/en/networks/cdn/monitoring","description":"How to get statistics about CDN resource traffic and available metrics","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"a7db4aff-180f-4e05-b1af-bd2aaa65feb8","title":"Tariffication","data":"About pricing\n\nCDN is based on the “pay as you go” principle: the fee is charged only for the resources down to the minute.\n\nThe cost of the service is listed in the price list. To learn how to pay for the service, see the Billing section.\n\nWhat is charged\n\nIncoming and outgoing traffic of caching CDN servers.\n\n\n\nIf you need to distribute more than 100 TB per month, contact technical support.\n\n\n","url":"/en/networks/cdn/tariffication","description":"Service tariffication rules and cost calculation.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"97b0ce4e-1d17-4ac1-8220-4f1866453fc5","title":"FAQ","data":"\n\nWhere are the CDN points of presence?\n\n\n\n  The CDN points of presence in Russia\n\nAksai\nAngarsk\nVoronezh\nEkaterinburg\nKazan\nKrasnodar\nKrasnoyarsk\nMoscow\nNizhny Novgorod\nNovosibirsk\nPetrozavodsk\nPskov\nSaint Petersburg\nSamara\nKhabarovsk\nChelyabinsk\n\n\n\n\n  The CDN points of presence outside Russia\n\nAlmaty\nAmsterdam\nAshburn\nAshgabat\nBishkek\nHong Kong\nDushanbe\nMinsk\nSao Paulo\nSingapore\nSukhum\nTashkent\nFrankfurt\n\n\n\n\n\n\n\nIs it possible to enable WAF and DDoS protection?\n\nYes, it is. Contact technical support to make an applications for connection.\n\n\n\n\n\nCan I use HTTP/2 protocol?\n\nCDN servers have HTTP/2 enabled by default. If the origin servers do not support HTTP/2, they will deliver the content using a supported protocol (for example, HTTP/1.1). In this case, the browser combines all received content into a single page, even if it is received via different protocols. Read more about HTTP/2 support.\n\n\n\n\n\nCan I deal with CDN via API?\n\nYou can find endpoints for managing and monitoring CDN resources in the API help section.\n\n\n\n\n\nIs it possible to add rewrite rules and provide access to raw logs?\n\nUnfortunately, this option is not provided.\n\n\n\n\n\nCan I get CDN statistics?\n\nRead about getting statistics in the [Statistics] section(/en/networks/cdn/monitoring).\n\n\n","url":"/en/networks/cdn/faq","description":"Frequently asked questions about CDN service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"CDN","link":"/en/networks/cdn","isActive":false,"evenSimpler":true}]},{"uuid":"0cc58071-0258-4726-89e5-2d3986a51f66","title":"Public DNS","data":"Setting up a role model\n\nTo work with public DNS, configure roles for users of personal account VK Cloud:\n\nRole for viewing DNS zones and resource records:\n\n  Viewer.\n\nRoles for editing DNS zones and resource records:\n\n  Project administrator.\n  Network administrator.\n  Superadministrator.\n\nQuotas and limits\n\nThe maximum number of DNS zones within the project is 100.\nThe maximum number of resource records of each type for one DNS zone is 500.\n\nViewing a list of DNS zones\n\n\n\nPersonal account\nAPI\n\n\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to DNS → DNS zones.\n\n\n\n\nUse the method GET /v2/dns/ from the specification in API documentation.\n\nCreating a zone\n\nDNS zone is a logical association of domain names of your resources, containing their resource records.\n\n\n\nPersonal account\nAPI\n\n\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to DNS → DNS zones.\nClick the button Add zone.\nSet DNS zone parameters:\n\n   DNS zone: the name of the zone being created, for example, the domain that was previously purchased.\n\n\n     The DNS zone name must contain at least one dot, must not end with a dot or digits.\n\n\n   Contact email: the mail of the zone administrator.\n   Time to expire: the time (in seconds) after which the secondary NS server stops responding to requests for this zone if the primary NS server does not respond. The value must be greater than the sum in the fields Time to refresh and Time to retry.\n   Time to refresh: the time (in seconds) after which the secondary NS server must request an SOA record from the primary in order to support changes in the zone.\n   Time to retry: the time (in seconds) after which the secondary NS server will again request the SOA record from the primary if the primary NS server has not responded. The value must be less than the one specified in Time to refresh.\n   Time to live (TTL): the lifetime of the cache in case of a negative response to a request in the zone.\n\nClick the button Add zone.\nContact the owner of the specified domain to delegate zone management to VK Cloud DNS servers.\n\n\n\n\nUse the method POST /v2/dns/ from the specification in API documentation.\n\n\n\n\n\n\nFor most providers, it is possible to delegate zone management independently. If you have any questions about how to do this, contact the owner of specified domain.\n\nEditing a zone\n\n\n\nPersonal account\nAPI\n\n\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to DNS → DNS zones.\nClick   for the required zone and select Edit.\nMake the changes and click Save changes.\n\n\n\n\nUse the method PUT /v2/dns/ from the specification in API documentation.\n\nCreating a subzone\n\nA subzone is a DNS zone that is a level below the current one. For example, for the domain example.com the subzone will be subzone.example.com.\n\nA subzone can be created:\n\nIn the same project where the main zone is located. This approach is used to separate the resource records of the subzone from the records of the main zone.\nFrom a third-party DNS provider.\n\n\n\nYou cannot create a sub-zone for a zone in another project.\n\n\n\nTo create a subzone in the VK Cloud project, create two NS resource records with the name of the subzone, re-delegating the subzone to the VK Cloud DNS servers.\n\n\n\nIf you want to create a subzone from a third-party provider, then the created NS records will have to point to the DNS servers of the third-party provider.\n\n\n\nAfter creating NS records, you can create a zone for a delegated subdomain.\n\nDeleting a zone\n\n\n\nPersonal account\nAPI\n\n\n\nThis is a group operation: if necessary, you can delete several zones at once by selecting them using the checkboxes.\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to DNS → DNS zones.\nClick   for the required zone and select Delete.\nConfirm the action.\n\n\n\n\nUse the method DELETE /v2/dns/ from the specification in API documentation.\n\nAdding resource records\n\nA resource record is a DNS record of a domain in the domain name system. With their help, you determine where to send requests that come to domain names, as well as provide additional information about the domain.\n\nVK Cloud supports resource record types:\n\nA is a DNS record that maps a domain name to an IPv4 address.\nAAAA is a DNS record that maps a domain name to an IPv6 address.\nNS is a DNS record that contains the address of the name server serving this zone or subzone. By default, two NS entries will be set in the zone. These records are installed on the side of the domain name owner in order to transfer domain management rights to the VK Cloud name server.\nCNAME is a DNS record that binds an alias to a domain name. It is usually used to bind a subdomain (for example, www) to the domain where the content of this subdomain is hosted.\nMX is a DNS record that tells the address of the server that processes email.\nSRV is a DNS record that defines the host name and server port for some network services.\nTXT is a DNS record that contains text information for sources outside the domain.\n\nTo add a resource record:\n\n\n\nPersonal account\nAPI\n\n\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to DNS → DNS zones.\nClick on the name of the zone for which you want to add a resource record.\nClick the button Add record.\nFill in the fields depending on the value Record type:\n\n    CNAME\n\n   Name (the alias being added):\n\n     @, example.com or an empty value — indicates the zone itself;\n     subzone or subzone.example.com — indicates a subzone subzone;\n     \\*.dns.zone — indicates any name in the domain request.\n\n   Time to live (TTL): cache lifetime in seconds.\n   Value: FQDN is the destination address (where the alias being added points to). The maximum length is 255 characters. Must consist of two or more subdomains. The maximum length of a subdomain is 63 characters. Only numbers, Latin letters, special characters . and - are allowed.\n\n   Example: you need to create a CNAME record www.example.com pointing to example.com. To do this, enter www in the Name field, and in the Value — example.com.\n\n    A\n\n   Name:\n\n     @, example.com or an empty value — indicates the zone itself;\n     subzone or subzone.example.com — indicates a subzone subzone;\n     \\*.dns.zone — indicates any name in the domain request.\n\n   Time to live (TTL): cache lifetime in seconds.\n   IP address: IP address (IPv4). Select from the list of existing VMs or specify a new one.\n\n    MX\n\n   Name:\n\n     @, example.com or an empty value — indicates the zone itself;\n     subzone or subzone.example.com — indicates a subzone subzone.\n\n   Priority: host priority. The lower the value, the more preferred the host.\n   Time to live (TTL): cache lifetime in seconds.\n   Value: FQDN is the address of the mail server. The maximum length is 255 characters. Must consist of two or more subdomains. The maximum length of a subdomain is 63 characters. Only numbers, Latin letters, special characters . and - are allowed.\n\n    AAAA\n\n   Name:\n\n     @, example.com or an empty value — indicates the zone itself;\n     subzone or subzone.example.com — indicates a subzone subzone;\n     \\*.dns.zone — indicates any name in the domain request.\n\n   Time to live (TTL): cache lifetime in seconds.\n   IP address: IP address (IPv6).\n\n    SRV\n\n   Name:\n\n     @, example.com or an empty value — indicates the zone itself;\n     subzone or subzone.example.com — indicates a subzone subzone;\n     \\*.dns.zone — indicates any name in the domain request.\n\n   Service: the symbolic name of the service (for example, _sip).\n   Protocol: the symbolic name of the protocol (for example, _tcp or _udp).\n   Priority: host priority. The lower the value, the more preferred the host.\n   Weight: weight for hosts with the same priority. The closer this value is to 0, the less likely it is that the host will be selected.\n   Port: the port number that the SRV service uses.\n   Time to live (TTL): cache lifetime in seconds.\n   Host: FQDN of the host hosting the service. The maximum length is 255 characters. Must consist of two or more subdomains. The maximum length of a subdomain is 63 characters. Only numbers, Latin letters, special characters . and - are allowed.\n\n    TXT\n\n   Name:\n\n     @, example.com or an empty value — indicates the zone itself;\n     subzone or subzone.example.com — indicates a subzone subzone;\n     \\*.dns.zone — indicates any name in the domain request.\n\n   Time to live (TTL): cache lifetime in seconds.\n   Value: the text value of the resource record.\n\n    NS\n\n   Name:\n\n     @, example.com or an empty value — indicates the zone itself;\n     subzone or subzone.example.com — indicates a subzone subzone.\n\n   Time to live (TTL): cache lifetime in seconds.\n   Value: the address of the NS server, for example, ns1.mcs.mail.ru or ns2.mcs.mail.ru.\n\n\nClick the button Add record.\n\n\n\n\nUse the methods from the specification in API documentation.\n\nEditing resource records\n\n\n\nPersonal account\nAPI\n\n\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to DNS → DNS zones.\nClick on the name of the zone for which you want to change the resource record.\nClick   for the required entry and select Edit.\nMake the changes and click Save changes.\n\n\n\n\nUse the methods from the specification in API documentation.\n\nDeleting resource records\n\n\n\nPersonal account\nAPI\n\n\n\nThis is a group operation: if necessary, you can delete several records at once by selecting them using the checkboxes.\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to DNS → DNS zones.\nClick on the name of the zone for which you want to delete the resource record.\nClick   for the required entry and select Delete.\nConfirm the action.\n\n\n\n\nUse the methods from the specification in API documentation.\n\n\n\n","url":"/en/networks/dns/publicdns","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"DNS","link":"/en/networks/dns","isActive":false,"evenSimpler":true}]},{"uuid":"ee5db9e5-1514-49f8-941e-b803b57b6b89","title":"Private DNS","data":"Private DNS in VK Cloud is the functionality of a DNS server running in the project networks of the platform. Allows you to access instances by DNS names.\n\nThe service supports configuring the private zone and port names of virtual machines. The DNS server responds to the same addresses as the DHCP ports on the network. For private DNS to work on the network, a DHCP server must be enabled.\n\n\n\nAt the moment, the private DNS request forwarding servers are 8.8.8.8, 8.8.4.4. Changing these addresses is not supported.\n\nEditing a zone name for a network\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to Virtual networks → Networks.\nOpen the network page by clicking on its name in the general list.\nGo to tab Network settings.\nEnter the zone name in the Zone field.\nClick the button Save changes.\n\n\n\nThe maximum length of the zone name is 253 characters. Consists of blocks of the form [a-z0-9-]+\\\\.. The maximum block length is 63 characters. A block cannot start and end with -.\n\nConfiguring the DNS name\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nThere are several ways to configure the DNS name:\n\nVia VM:\n\nGo to personal account VK Cloud.\nSelect the project.\nStart to create new virtual machine. In the “Network Settings” step, enter the name in the DNS-name field.\n\nVia port settings:\n\nGo to personal account VK Cloud.\nSelect the project.\nGo to Virtual networks → Networks.\nOpen the network card by clicking on its name in the general list.\nOpen the subnet card by clicking on its name in the general list.\nGo to the Ports tab.\nClick   for the required port and select Edit port.\nEnter the name in the field DNS-name.\nClick the button Save Changes.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nGet a list of instance ports by running the command:\n\n      openstack port list --server\n\nRun the command:\n\n      openstack port set --dns-name\n\n\nThe maximum length of the name is 63 characters. Only numbers, small Latin letters and dashes - are allowed.\n\n\n","url":"/en/networks/dns/private-dns","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"DNS","link":"/en/networks/dns","isActive":false,"evenSimpler":true}]},{"uuid":"a0d46301-3354-48d9-af6d-5c88e961c3c9","title":"Using an advanced router","data":"Advanced routers can connect your local network with VK Cloud virtual networks without using a dedicated virtual machine as a router. This simplifies data transfer by using dynamic routing capabilities over the BGP protocol.\n\nTo show the configuration of an advanced router, two independent networks will be connected using the BGP protocol:\n\nClient network — the customer’s internal network, which may not have access to the Internet.\nVirtual network — located in VK Cloud and connected to the platform router.\n\nPreparation steps\n\nMake sure the OpenStack client is installed and authenticate in the project.\n\nSelect a client network in your local infrastructure:\n\n   The network must be connected to a router that:\n       supports connection via the BGP protocol;\n       (optional) can support the BFD protocol: this will reduce the convergence time of routing protocols;\n       can be a device or virtual machine on the client network.\n   The network may not have access to the Internet.\n\n   If there is no appropriate network, create one.\n\n   Note the following:\n\n    the name and the IP address of the subnet;\n    the name of the network in which the subnet is located;\n    the IP address of the machine in the subnet that will be used to test connection between networks;\n    the name of the BGP router.\n\n   In this case, we will use a network with a virtual machine Router OS 7.10 (MikroTik), performing the functions of a BGP router.\n\n\nSelect or create a virtual network with Internet access in VK Cloud. Use an existing router with an external network connection or create a new one.\n\n   Note the following:\n\n   the name and the IP address of the subnet;\n   the name of the network in which the subnet is located;\n   the router name.\n\nCreate a virtual machine on the vkcloud-net network. Add SSH access and disable backup.\n\n    Note the VM's IP address.\n\nContact the technical support to add a network to your VK Cloud project to organize a direct connection.\n\n    When the network is provided, note its name, subnet name, and IP address.\n\nMake sure you have collected all the data you need.\n\nIn this case the following data is used:\n\n| Object | Client network | Virtual network| Direct connection network\n| :---| :--- | :--- | :---\n| Netwok                                  | customer-net                                    | vkcloud-net | x-net |\n| Subnet and IP address                              | customer-subnet, 10.0.0.0/24                            | vkcloud-subnet, 172.17.0.0/24| x-subnet, 172.20.2.0/24|\n| Machine IP address    | 10.0.0.5    | 172.17.0.8|\n| BGP router | MikroTik |\n\nThe scheme for preparation of networks looks like this:\n\nScheme for preparation of networks\n\n1. Creating an advanced router\n\nIn your personal account, go to Virtual networks → Routers.\nClick Add router.\nSelect the Advanced router type. If there is no advanced type in your project, contact the technical support.\nEnter the name. In this case — advanced-router.\nClick Add.\n\n2. Configure the network interfaces of the advanced router\n\nTo organize data transfer between independent networks, you need to add interfaces to the advanced router:\n\nTo the vkcloud-net network where the router is located. These interfaces are used to connect to VMs within the network. The number of such interfaces depends on the network structure.\nTo the direct connection network x-net. This interface will help to organize connection between VK Cloud and the local network.\n\nTo add the interfaces:\n\nGo to the router's Interfaces tab.\nAdd an interface directed to the virtual network:\n   Click Add interface.\n   Configure parameters:\n\n      Name: vkcloud-net-iface;\n      Subnet: vkcloud-subnet;\n      Interface IP address: 172.17.0.100.\n\n   Click Create.\nAdd an interface directed to the direct connection network:\n   Click Add interface.\n   Configure parameters:\n\n      Name: x-net-iface;\n      Subnet: x-subnet;\n      Interface IP address: 172.20.2.215.\n   Click Create.\n\n3. Configure the network interfaces of the client network's BGP router\n\nAdd network interfaces directed to:\n\n   To the direct connection network x-net. This interface will help to organize connection between VK Cloud and the local network.\n   To the customer-net network where the BGP router is located. These interfaces are used to connect to machines within the network. The number of such interfaces depends on the network structure.\n\nConfigure interfaces using DHCP.\n\nConfigure system ID.\n\nAdd networks for BGP announcement.\n\n(Optional) If the router supports BFD, configure the BFD protocol.\n\n\n    Example setup for MikroTik\n\n   To add network interfaces, connect to MikroTik via SSH and run the command:\n\n             /ip address add address=172.20.2.204/24 interface ether1\n       /ip address add address=10.0.0.15/24 interface ether2\n\n   Configure interfaces using DHCP:\n\n             /ip dhcp-client\n       add add-default-route=no interface=ether1\n       add add-default-route=no interface=ether2\n\n   Configure system ID:\n\n             /system identity\n       set name=bgp-customer\n\n   Add networks for BGP announcement:\n\n             /ip firewall address-list\n       add address=10.0.0.0/24 list=bgp_networks\n\n   Configure the BFD protocol:\n\n             /routing bfd configuration\n       add disabled=no interfaces=ether1\n\n\n4. Configure BGP neighbors for the advanced router\n\nTo configure the connection using the BGP protocol, you need to add dynamic routes and specify BGP neighbors. Dynamic routing requires Autonomous System Numbers (ASNs). If your network is assigned an ASN, use it. If no ASN is assigned, use a number from the range 64512–65534 — these numbers can be assigned for private autonomous networks. In the case the following numbers will be used:\n\n65512 for customer-net;\n64512 for vkcloud-net.\n\nTo configure dynamic routes for advanced-router:\n\nIn your personal account, go to Virtual networks → Routers.\nClick the added advanced router and go to the Dynamic routing tab.\nClick Create a BGP router.\nFill in parameters:\n\n   Name: to-MikroTik;\n   Router ID: 172.20.2.215;\n   ASN: 64512.\n\nClick Create.\nClick the added BGP router and go to the BGP neighbors tab.\nAdd a BGP neighbor. Fill in parameters:\n\n   Name: MikroTik;\n   Remote neighbor: 172.20.2.204;\n   Remote ASN: 65512.\n\nClick Create.\n\nMake sure the router has established a connection with its neighbor: the marker next to the name is green and BFD is enabled.\n\nThe advanced router will start broadcasting BGP announcements to its neighbor. Go to the BGP Announcements tab and make sure that the router broadcasts announcements for all networks to which its interfaces are directed:\n\n172.17.0.0/24;\n172.20.2.0/24.\n\nBoth announcements must have green markers.\n\n5. Configure BGP neighbors for the client network router\n\nConnect to a router on your local network.\nSpecify parameters for connecting via the BGP protocol:\n\n   The client network ASN: 65512;\n   Router ID: 172.20.2.204;\n   The virtual network ASN: 64512;\n   BGP router ID: 172.20.2.215;\n   Use BFD.\n(Optional) Check that BFD connection is established.\nCheck that a connection with the BGP neighbor is established. If the BGP connection is established, the response must have keepalive-time and uptime values more than zero.\nView all available BGP routes. The list of routes should include the networks 172.17.0.0/24 and 172.20.2.0/24.\n\n\n Example setup for MikroTik\n\nConnect to MikroTik via SSH and run the command::\n\n        /routing bgp connection\n    add address-families=ip as=65512 local.address=172.20.2.204 .role=ebgp name=bgp-customer output.network=bgp_networks remote.address=172.20.2.215 .as=64512 router-id=172.20.2.204 use-bfd=yes\n    Check the connection is established using the BFD protocol. Run the command:\n\n      /routing bfd session print\n\n   Response example:\n\n         Flags: U - up, I - inactive\n   0 U multihop=no vrf=main remote-address=172.20.2.215%ether1 local-address=172.20.2.204 state=up state-changes=1 uptime=3h27m12s desired-tx-interval=200ms actual-tx-interval=100ms\n     required-min-rx=200ms remote-min-rx=10ms multiplier=5 hold-time=1s packets-rx=75343 packets-tx=72203\n\nCheck the connection with the BGP neighbor is established in MikroTik. Run the command:\n\n      /routing bgp session print\n\n   Response example:\n\n         Flags: E - established\n   0 E name=\"tw-bgp-mikrotik-1\"\n        remote.address=172.20.2.215 .as=64512 .id=172.20.2.215 .capabilities=mp,rr,gr,as4,ap,err,llgr .hold-time=4m\n       .messages=5 .bytes=131 .gr-time=120 .eor=ip\n      local.address=172.20.2.204 .as=65512 .id=172.20.2.204 .capabilities=mp,rr,gr,as4 .messages=4 .bytes=105 .eor=\"\"\n        output.procid=20 .network=bgp_networks\n       input.procid=20 ebgp\n      hold-time=3m keepalive-time=1m uptime=2m51s380ms last-started=aug/28/2023 07:27:15\n   Run the command to check all MikroTik's routes:\n\n      /ip route print where bgp\n\n   Response example:\n\n      Flags: D - DYNAMIC; A - ACTIVE; b, y - BGP-MPLS-VPN\n   Columns: DST-ADDRESS, GATEWAY, DISTANCE\n       DST-ADDRESS    GATEWAY       DISTANCE\n   DAb 172.17.0.0/24  172.20.2.215        20\n   D b 172.20.2.0/24  172.20.2.215        20\n\n\n6. Configure static routes between networks\n\nConfigure a static route from the virtual network vkcloud-net to the client network customer-net via an advanced router:\n\n   In your personal account, go to Virtual networks → Networks..\n   Select the vkcloud-net network and open its subnets's settings.\n   Select Show static route field.\n   Enter the route: 10.0.0.0/24 - 172.17.0.100.\n   Click Save.\n\nConfigure a static route from the client network customer-net to the virtual network vkcloud-net via the BGP router of the client network. The route must be to 172.17.0.0/24 via 10.0.0.15.\n\nReboot the 172.17.0.8 and 10.0.0.5 machines so that the routes are included in their route network.\n\nCheck that static routes are registered on the VM 172.17.0.8, which is located in the virtual network vkcloud-net. Connect to the vkcloud-vm VM via SSH and run the command:\n\n       ip route\n\n   The list of routes should include the networks 10.0.0.0/24 and 172.20.2.0/24.\nCheck the list of routes for machine 10.0.0.5, which is located on the client network customer-net. The list of routes should include the networks 172.17.0.0/24 and 172.20.2.0/24.\n\n7. Check the direct connection\n\nPing or traceout a machine on the opposite network. If a response comes from another network, then the network connection has been configured correctly.\n\nFor instance, ping the 10.0.0.5 machine on the client network from the 172.17.0.8 machine on the virtual network:\n\nConnect to the vkcloud-vm VM via SSH.\nPing the internal IP address of a machine on the client network:\n\n            ping 10.0.0.5\n\nThe IP address must respond to ping.\n\nThe structured interaction between the networks and the VMs within networks looks like this:\n\nThe interaction between networks scheme\n\nDelete unused resources\n\nIf you no longer need the created resources, delete them:\n\nDelete the virtual machine.\nDelete routers.\nDelete the subnet and the network.\n","url":"/en/networks/directconnect/how-to-guides/advanced-router","description":"How to organize data transfer between the on premise infrastructure and the VK Cloud platform using a direct connection and an advanced router.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Networks and content delivery","link":"/en/networks","isActive":false,"evenSimpler":true},{"title":"Cloud Direct Connect","link":"/en/networks/directconnect","isActive":false,"evenSimpler":true}]},{"uuid":"421ea5fc-afe0-4cdb-b1ab-535c1c648e77","title":"Quick start","data":"Быстрый старт поможет вам начать работу с сервисом и познакомиться с его возможностями.\n\nПройдя все шаги быстрого старта, вы научитесь загружать объекты в бакет и предоставлять доступ к загруженным объектам по ссылке.\n\n\n\nЗагруженные в бакет объекты тарифицируются. После прохождения быстрого старта удалите эти объекты и бакет, если они вам больше не нужны.\n\nПодготовительные шаги\n\nУбедитесь, что AWS CLI установлен.\n\nСоздайте аккаунт для сервиса Cloud Storage:\n\n   Перейдите в личный кабинет VK Cloud.\n   Выберите проект.\n   Перейдите в раздел Объектное хранилище → Аккаунты.\n   Нажмите кнопку Добавить аккаунт или Добавить.\n   Задайте имя аккаунта.\n   Нажмите кнопку Создать.\n   В окне скопируйте и сохраните значения Access Key ID и Secret Key.\n\n\n      После закрытия окна значение Secret Key нельзя будет восстановить. Если значение утеряно, создайте новый аккаунт.\n\n\nНастройте AWS CLI на работу с созданным аккаунтом:\n\n   Выполните команду:\n\n            aws configure\n\n   Задайте необходимые настройки:\n\n      AWS Access Key ID: введите сохраненное ранее значение Access Key ID.\n      AWS Secret Access Key: введите сохраненное ранее значение Secret Key.\n      Default region name: введите ru-msk.\n      Default output format: введите json.\n\n1. Создайте бакет\n\nПерейдите в личный кабинет VK Cloud.\nВыберите проект.\nПерейдите в раздел Объектное хранилище → Бакеты.\nНажмите кнопку Создать бакет или Добавить.\nЗадайте название бакета.\nВыберите класс хранения Hotbox.\nВыберите настройку ACL по умолчанию private.\nНажмите кнопку Добавить бакет.\n\n2. Добавьте объект с приватными настройками ACL и предоставьте доступ к нему\n\nДобавьте объект:\n\n   Перейдите в личный кабинет VK Cloud.\n   Выберите проект, где находится созданный бакет.\n   Перейдите в раздел Объектное хранилище → Бакеты.\n   Нажмите на имя созданного бакета.\n   Нажмите кнопку Добавить файл.\n   Убедитесь, что выбраны настройки ACL private.\n   Нажмите кнопку Выбрать файлы и выберите файл, который нужно загрузить в бакет.\n\nПредоставьте доступ к загруженному объекту по временной подписанной ссылке:\n\n   Сгенерируйте временную ссылку для доступа к объекту с приватными настройками ACL, выполнив команду:\n\n            aws s3 presign s3:/// --endpoint-url https://hb.vkcs.cloud\n\n      Пример\n\n      Пусть в бакет my-cloud-bucket был загружен объект cat_image_private_acl.png.\n\n      Тогда команда будет иметь вид:\n\n            aws s3 presign s3://my-cloud-bucket/cat_image_private_acl.png --endpoint-url https://hb.vkcs.cloud\n\n\n   Сохраните сгенерированную временную ссылку.\n\n      Ссылка имеет следующий вид:\n\n            https://hb.vkcs.cloud//...?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=...%2F...%2Fru-msk%2Fs3%2Faws4_request&X-Amz-Date=...&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=...\n\n\n      Такая ссылка действует ограниченное время (по умолчанию 1 час). По истечении этого времени нужно будет сгенерировать новую ссылку на объект.\n\n\n3. Добавьте объект с публичными настройками ACL и предоставьте доступ к нему\n\nДобавьте объект:\n\n   Перейдите в личный кабинет VK Cloud.\n   Выберите проект, где находится созданный бакет.\n   Перейдите в раздел Объектное хранилище → Бакеты.\n   Нажмите на имя созданного бакета.\n   Нажмите кнопку Добавить файл.\n   Убедитесь, что выбраны настройки ACL public-read.\n   Нажмите кнопку Выбрать файлы и выберите файл, который нужно загрузить в бакет.\n\nПредоставьте доступ к загруженному объекту по прямой ссылке:\n\n   Перейдите в личный кабинет VK Cloud.\n   Выберите проект, где находится созданный бакет.\n   Перейдите в раздел Объектное хранилище → Бакеты.\n   Нажмите на имя созданного бакета.\n   Раскройте меню загруженного ранее объекта и выберите пункт Доступ к файлу.\n   Сохраните сгенерированную прямую ссылку.\n\n      Ссылка имеет следующий вид:\n\n            https://.hb.vkcs.cloud/...\n\n4. Проверьте наличие доступа к объектам\n\nПерейдите в браузере по сгенерированной временной ссылке. Должен загрузиться объект, добавленный ранее с приватными настройками ACL.\nПерейдите в браузере по сгенерированной прямой ссылке. Должен загрузиться объект, добавленный ранее с публичными настройками ACL.\n\nУдалите неиспользуемые ресурсы\n\nЗагруженные в бакет объекты тарифицируются. Если они вам больше не нужны, удалите их.\nЕсли бакет вам больше не нужен, удалите его.\n","url":"/en/storage/s3/quick-start","description":"Getting started with the service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"5c3782e9-d62e-4b75-876e-e5b29e72afd1","title":"About the service","data":"Description\n\nObject Storage can be used to store and retrieve any amount of data anytime from anywhere on the Internet. These tasks can be accomplished using the VK Cloud Control Panel.\n\nCloud Storage stores data as objects in buckets. An object is a file and any additional metadata describing the file. To save a file to Cloud Storage, it must be loaded into a bucket. When the file is loaded as an object, you can set permissions on the object and any metadata.\n\nBuckets are containers for objects. A project can have one or more buckets. Access to each bucket can be controlled by deciding who can create, delete, and enumerate objects in it. Additionally, you can view the access logs for the bucket and its objects.\n\nThe following tasks can be accomplished using the VK Cloud Control Panel:\n\nBucket creation\nLoading an object into a bucket\nObject Access Control\nObject lifecycle management\nCopying an item to a directory\nRemoving objects and buckets\n","url":"/en/storage/s3/concepts/about","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"cbb6aafe-ab11-47de-a5f2-9f2d2cee8cf4","title":"Cloud Storage API","data":"VK Cloud provides a RESTful XML API for programmatically manipulating stored data using standard HTTP requests. The API is compatible with the Amazon AWS S3 API, allowing you to interact with the service using familiar tools.\n\n\n\nSOAP over HTTP support has been deprecated but is still available over HTTPS. However, new Amazon S3 features will not be supported for SOAP. REST API or AWS SDK is recommended.\n\nS3 compatible\n\nThe Cloud Storage API is designed to interact with the Amazon AWS S3 API. In most cases, when using the client library, setting the \"endpoint\" or \"base\" URL hb.vkcs.cloud and creating the Cloud Storage key pair will allow the Cloud Storage service.\n\nCloud Storage provides support for create, read, update, and delete operations for both buckets and objects, as well as the ability to define access control lists (ACLs). Some S3 features are not supported, as shown in the table below:\n\nFunctionSupportNoteBucket Create, Read, Update, DeleteYesObject Create, Read, Update, DeleteYesMultipart UploadsYesPre-Signed URLsYesBoth v2 and v4 signature types are supportedBucket ACLsYesObject ACLsYesIdentity and Access Management (IAM)NoSecurity Token Service (STS)NoMulti-factor AuthenticationNoPublic Access BlockNoBucket PoliciesNoObject PoliciesNoBucket VersioningNoBucket ReplicationNoBucket NotificationsNoBucket TaggingNoObject TaggingYesRequest PaymentNoBucket LifecycleYesObject expiration and removal of incomplete compound downloads are supported. Lifecycle policies based on tagging objects are not supported.Bucket InventoryNoBucket Access LoggingNoBucket MetricsNoBucket AnalyticsNoBucket AccelerateNoBucket Encryption ConfigurationNoBucket WebsitesNoObject TorrentNoObject LockNo\n\nAPI requests for S3 functions that are not currently supported by VK Cloud will receive an S3-compliant NotImplemented error response in XML format.\n\nPython example\n\nimport boto3\nfrom botocore.client import Config\nInitialize session to Cloud Storage.\nsession = boto3.session.Session ()\nclient = session.client ('s3',\nregion_name = 'ru-msk',\nendpoint_url = 'https: //hb.vkcs.cloud',\naws_access_key_id = 'urvt4LXPwoSL9s6ieGTLT5',\naws_secret_access_key = '5JogfQUsWzzBE9xG1mbBkMkgW7pxY4TGyHgefSC9n2Xx')\nCreate a new bucket.\nclient.create_bucket (Bucket = 'my-test-bucket1')\nView the list of buckets in the project.\nresponse = client.list_buckets ()\nbuckets = [bucket ['Name'] for bucket in response ['Buckets']]\nprint (\"Bucket List:% s\"% buckets)\n\nGo example\n\npackage main\nimport (\n\"context\"\n    \"fmt\"\n\"log\"\n\"os\"\n\"github.com/minio/minio-go\"\n\"github.com/minio/minio-go/pkg/credentials\"\n)\nfunc main () {\naccessKey: = os.Getenv (\"VK Cloud_KEY\")\nsecKey: = os.Getenv (\"VK Cloud_SECRET\")\nendpoint: = \"hb.vkcs.cloud\"\nbucketName: = \"my-test-bucket1\" // Bucket names must be unique for all VK Cloud projects\nssl: = true\nif accessKey == \"\" || secKey == \"\" {\nlog.Fatal (\"Must provide VK Cloud_KEY and VK Cloud_SECRET environment variables!\")\n}\n// Connect to Cloud Storage\nclient, err: = minio.New (endpoint, & minio.Options {\nCreds: credentials.NewStaticV4 (accessKey, secKey, \"\"),\nSecure: ssl,\n})\nif err! = nil {\nlog.Fatal (err)\n}\n// Create a new bucket.\nerr = client.MakeBucket (context.TODO (), bucketName, minio.MakeBucketOptions {Region: \"ru-msk\"})\nif err! = nil {\nlog.Fatal (err)\n}\n// Show a list of all buckets.\nbuckets, err: = client.ListBuckets (context.TODO ())\nif err! = nil {\nlog.Fatal (err)\n}\nfmt.Println (\"List of all buckets for this access key:\")\nfor _, bucket: = range buckets {\nfmt.Println (bucket.Name)\n}\n}\n\nAuthentication\n\nRequests to the Cloud Storage APIs must include the HTTP-Authorization header. The AWS v4 signature type is supported, as well as the AWS v2 signature type for compatibility with legacy customers. The examples below use v4 signatures. When using the client library, signatures will be generated automatically.\n\nYou can create the required access key and secret key in the \"Accounts\" menu of the \"Object Storage\" service of the graphical interface of the VK Cloud panel.\n\nWhen creating an account, an Access Key ID and Secret Key values will be generated that are required for use.\n\nThe v4 signature consists of several parts. The table below describes each part of the example separately:\n\nParameterDescriptionAWS4-HMAC-SHA256AWS Version 4 Signature (AWS4) and Signature Algorithm (HMAC-SHA256)CredentialContains an access key and request information in the format: $ {ACESS_KEY} / $ {YYYMMDD} / $ {REGION_SLUG} / s3 / aws4_requestSignedHeadersA lowercase list of request header names used in signature computation, for example: host; x-amz-acl; x-amz-content-sha256; x-amz-dateSignatureA signed hash consisting of the hash of the request body, the secret key, and information about the request (canonical request). To demonstrate how this is calculated, a \"pseudo-code\" example is provided.\n\nAuthorization header example\n\nAuthorization: AWS4-HMAC-SHA256\nCredential = urvt4LXPwoSL9s6ieGTLT5 / 20200831 / ru-msk / s3 / aws4_request,\nSignedHeaders = host; x-amz-acl; x-amz-content-sha256; x-amz-date,\nSignature = 6cab03bef74a80a0441ab7fd33c829a2cdb46bba07e82da518cdb78ac238fda5\n\nSignature example (pseudo code)\n\ncanonicalRequest = \\`\n{HTTPMethod} \\ n\n{canonicalURI} \\ n\n{canonicalQueryString} \\ n\n{canonicalHeaders} \\ n\n{signedHeaders} \\ n\n{hashedPayload}\n\\`\nstringToSign = \"AWS4-HMAC-SHA256\" + \"\\ n\" +\ndate (format = ISO08601) + \"\\ n\" +\ndate (format = YYYYMMDD) + \"/\" + \"ru-msk\" + \"/\" + \"s3 / aws4_request\" + \"\\ n\" +\nHex (SHA256Hash (canonicalRequest))\ndateKey = HMAC-SHA256 (\"AWS4\" + {SECRET_KEY}, date (format = YYYYMMDD))\ndateRegionKey = HMAC-SHA256 (dateKey, \"ru-msk\")\ndateRegionServiceKey = HMAC-SHA256 (dateRegionKey, \"s3\")\nsigningKey = HMAC-SHA256 (dateRegionServiceKey, \"aws4_request\")\nsignature = Hex (HMAC-SHA256 (signingKey, stringToSign))\n\nThe canonical request included in the signature consists of:\n\nThe HTTP request method to use.\nPath component of the request URI.\nQuery string parameters included in the request.\nA list of request headers and their values, separated by a newline, in lowercase and without spaces.\nA list of header names without values, sorted alphabetically, in lowercase, and separated by semicolons.\nThe SHA256 hash of the request body.\n\nFor example, for the following query:\n\nGET /? Acl HTTP / 1.1\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T221549Z\n\nThis is the canonical query:\n\nGET\n/\nacl =\nhost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T221549Z\nhost; x-amz-content-sha256; x-amz-date\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n\nGeneral headers\n\nCommon headers that can be used in most requests:\n\nNameDescriptionAuthorizationAuthorization details for request in AWS Signature Version 4 or AWS Signature Version 2 formatContent-LengthThe length of the request body in bytes. Required for PUT requests containing an XML body.Content-TypeThe MIME type of the request body (e.g. text / plain)DateCurrent date and time in Coordinated Universal Time (UTC) format in RFC 2822 format. Example: Mon, 10 Jul 2017 19:05:09 +0000HostThe target host for the request (for example, my-test-bucket1.hb.vkcs.cloud).x-amz-content-sha256The SHA256 hash of the request payload. Required when using AWS Signature Version 4 for authentication.x-amz-dateThe current date and time in Coordinated Universal Time (UTC) using the ISO 8601 format:% Y% m% dT% H% M% SZ (for example 20200831T172753Z). If provided, it takes precedence over the Date heading.\n\nThese general headers can also be found in most of the answers:\n\nNameDescriptionContent-LengthResponse body length in bytesContent-TypeThe MIME type of the request body (e.g. text / plain)ConnectionIndicator of whether the connection to the server is open or closedDateDate and time of response, in Coordinated Universal Time (UTC)EtagObject tag containing the MD5 hash of the objectx-amz-request-idUnique request ID\n","url":"/en/storage/s3/concepts/about-s3-api","description":"Information about the service API: compatibility with the Amazon AWS S3 API, authentication, common request headers.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"a7f311a0-c1a7-4fe0-b1e9-1d20e540176e","title":"S3 CLI","data":"The AWS CLI, or AWS Command Line Interface, is a single tool for managing Cloud Storage service based on the AWS S3 toolkit. With just one tool downloaded, it is possible to control many Cloud Storage services from the command line and automate them with scripts.\n\nThe AWS CLI introduces a new set of simple commands to efficiently receive and send files to Cloud Storage.\n\nWith minimal configuration, the AWS CLI allows you to run commands from the command line in a terminal program:\n\nLinux shells are common shell programs such as bash, zsh, and tcsh for executing commands on Linux or macOS.\nWindows Command Prompt - On Windows, you run commands from the Windows Command Prompt or PowerShell.\n\nThe AWS CLI is available in two versions, and the information in this guide applies to both, unless otherwise noted.\n\nVersion 2.x is the current generally available version of the AWS CLI for use in production environments.\nVersion 1.x is the previous version of the AWS CLI available for backward compatibility.\n\nFull information about the set of commands and additional CLI settings is available on the developer's website.\n\nCLI installation\n\nTo install AWS CLI v2 in the operating system, you must install the appropriate package:\n\nLinux\n\nPrerequisites:\n\nAbility to extract or \"unzip\" a downloaded package. If the operating system does not have a built-in unzip command, use the equivalent.\n\nAWS CLI version 2 uses glibc, groff, and less. They are included by default in most major Linux distributions.\n\nFor Linux x86 (64-bit):\n\n curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n\nFor Linux ARM\n\n curl \"https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n\nCheck the correctness of the installation:\n\n aws --version\n\nMacOS\n\nInstallation for MacOS is done using the standard MacOS user interface and browser:\n\nLoad the macOS pkg file in your browser.\nDouble-click the downloaded file to launch the installer.\nFollow the instructions on the screen.\n\nYou can check the correctness of the installation in the terminal:\n\n aws --version\n\nWindows\n\nBefore installing AWS CLI v2 on Windows, you must ensure that you have the following:\n\nWindows XP 64-bit or newer.\nAdministrator rights to install software\n\nFor installation:\n\nDownload the MSI AWS CLI Installer for Windows (64-bit).\n\nRun the downloaded MSI installer and follow the instructions on the screen. By default, the AWS CLI is installed to C: \\\\ Program Files \\\\ Amazon \\\\ AWSCLIV2.\n\nYou can use the Windows Standard Command Line Interface (cmd) to confirm the installation:\n\n aws --version\n\nAccessing the CLI\n\nAccess keys consist of an access key identifier and a secret access key, which are used to sign software requests sent to VK Cloud. If access keys are missing, they can be created in the VK Cloud Control Panel.\n\nThe only time the private access key can be viewed or downloaded is when the keys are generated. It will be impossible to restore them later. However, you can create new access keys at any time.\n\nIn the VK Cloud panel, in the \"Accounts\" menu of the \"Object Storage\" service, you need to add an account, and save the received keys for future use.\n\nCLI setup\n\nThe fastest way to customize your AWS CLI installation is with the command:\n\n aws configure\n\nWhen you enter this command, the AWS CLI prompts for four pieces of information:\n\nAccess key identifier - the received key identifier data is used when adding an account.\nSecret access key - the received secret key data is used when adding an account.\nAWS region - the region where the Cloud Storage service is located, by default it is ru-msk.\nOutput Format - Determines how to format the output of the command being used. If no output format is specified, it will use json by default. Available options:\n\n  json – The output is formatted as a JSON string.\n  yaml – The output is formatted as a YAML string.\n  yaml-stream – The output is streamed and formatted as a YAML string. Streaming allows for faster handling of large data types.\n  text – The output is formatted as multiple lines of tab-separated string values. This can be useful to pass the output to a text processor, like grep, sed, or awk.\n  table – The output is formatted as a table using the characters +|- to form the cell borders. It typically presents the information in a \"human-friendly\" format that is much easier to read than the others, but not as programmatically useful.\n\nThe AWS CLI stores this information in a profile (set of settings) called default in the credentials file. By default, the information in this profile is used when you run an AWS CLI command that does not explicitly specify the profile to use.\n\nFeatures:\n\nWhen using the AWS CLI to work with Object Storage, there are a few things to keep in mind:\n\nThe AWS CLI treats Cloud Storage as a hierarchical file system and object keys are in the form of a file path.\nWhen running the aws command to work with Cloud Storage, the --endpoint-url parameter is required because the client is configured by default to work with Amazon servers.\nBucket creation should be done using the appropriate --endpoint-url - http://hb.vkcs.cloud.\nAny operations cannot be performed using the CLI with the Backup bucket class.\nWhen using the storage classes --storage-class, the STANDARD values for Hotbox and STANDARD_IA for Icebox apply.\nWhen working in MacOS, in some cases it is required to run the following view:\n\n export PYTHONPATH =/Library/Python/2.7/site-packages; aws s3  --endpoint-url=http://hb.vkcs.cloud\n\nExamples of using\n\nCreate a bucket:\n\n aws s3 mb s3: // --endpoint-url http://hb.vkcs.cloud\n\nChanging the bucket storage class:\n\n aws s3api create-bucket --bucket  --endpoint-url  --cli-input-json \"{\\\" Bucket \\ \": {\\\" storage-class \\ \": \\\"  \\ \"}}\"\n\nFile upload\n\n aws s3 cp  s3: //  --endpoint-url http://hb.vkcs.cloud\n\nDownloading an object\n\n aws s3 cp s3: //  /   --endpoint-url http://hb.vkcs.cloud\n\nSynchronizing a local directory with a bucket\n\n aws s3 sync  s3: //  --endpoint-url http://hb.vkcs.cloud\n\nMoving an object\n\n aws s3 mv s3: //  /  s3: //  /  --endpoint-url http://hb.vkcs.cloud\n\nGetting a list of objects\n\n aws s3 ls s3: //  --endpoint-url http://hb.vkcs.cloud\n\nDeleting an Object\n\n aws s3 rm s3: //  /  --endpoint-url http://hb.vkcs.cloud\n\nRemoving a multi-component object\n\n aws s3api abort-multipart-upload --bucket  --endpoint-url http://hb.vkcs.cloud --key large_test_file --upload-id\n","url":"/en/storage/s3/connect/s3-cli","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"f598e9bb-7761-4b67-be7a-d02ef39081fe","title":"File managers","data":"Graphical tools such as file managers allow you to view objects in a bucket by simulating a hierarchical directory structure.\n\nBefore starting work\n\nBefore you start working with file managers, you need to create an authorized account or make sure it exists, as well as the availability of data on the key ID and secret key.\n\nYou can create an account in the \"Accounts\" menu of the \"Object Storage\" service using the \"Add account\" button.\n\nWhen you create an account, you will be provided with data that you want to save. After closing the window, it will be impossible to restore the Secret Key, however, if you lose it, you can create a new account or add a new secret key to an existing account.\n\nCyberDuck\n\nCyberduck is a free cloud storage browser for Mac and Windows with support for Cloud Storage as well as many other cloud platforms.\n\nInstallation is performed using the product distribution kit from the official website, as well as the classic user interface of the operating system.\n\nConnection\n\nTo connect, specify:\n\nConnection type: Amazon S3\nServer and Port: hb.vkcs.cloud:443\nAccess Key ID: Key identifier obtained when creating an Object Storage account\nSecret access key: the secret key obtained when creating an Object Storage account\n\n\n\nCyberDuck works with Cloud Storage as a hierarchical file system. This means that the keys of objects uploaded via CyberDuck will look like a path to a file, for example: prefix / subprefix / image.png.\n\nWinSCP\n\nWinSCP is an open source graphical SFTP (SSH File Transfer Protocol) client for Windows. It also supports SCP (Secure Copy Protocol). Designed for secure copying of files between a computer and servers that support these protocols.\n\nTo configure, enter the following data:\n\nTransfer protocol: Amazon S3\nHostname: hb.vkcs.cloud\nPort: 443\nAccess Key ID: Key ID obtained when creating an Object Storage account\nSecret Access Key: The secret key obtained when creating an Object Storage account\n\n\n\nCyberDuck works with Cloud Storage as a hierarchical file system. This means that the keys of objects uploaded via CyberDuck will look like a path to a file, for example: prefix / subprefix / image.png.\n\nS3FS\n\ns3fs allows Linux and macOS to mount an Cloud Storage bucket via FUSE. s3fs maintains its own object format for files, allowing you to use other tools such as the AWS CLI.\n\nInstallation\n\nMany systems provide ready-made packages:\n\nAmazon Linux via EPEL:\n\nsudo amazon-linux-extras install epel\nsudo yum install s3fs-fuse\n\nArch Linux:\n\nsudo pacman -S s3fs-fuse\n\nDebian 9 and Ubuntu 16.04 or newer:\n\nsudo apt install s3fs\n\nFedora 27 or newer:\n\nsudo dnf install s3fs-fuse\n\nGentoo:\n\nsudo emerge net-fs / s3fs\n\nRHEL and CentOS 7 or newer via EPEL:\n\nsudo yum install epel-release\nsudo yum install s3fs-fuse\n\nSUSE 12 and openSUSE 42.1 or newer:\n\nsudo zypper install s3fs\n\nmacOS via Homebrew:\n\nbrew cask install osxfuse\nbrew install s3fs\n\nConnection\n\nTo mount a bucket in an operating system, you should:\n\nSave the key ID and private key in the /.passwd-s3fs file in the format : , and also restrict access to the /.passwd-s3fs file:\n\n        echo : > ~ / .passwd-s3fs\n    chmod 600 ~ / .passwd-s3fs\n\nSelect the directory where the bucket will be mounted and make sure you have permission for the mount operation.\nRun a command of the form:\n\n        s3fs  / mount /  -o passwd_file = ~ / .passwd-s3fs -o url = http: //hb.vkcs.cloud -o use_path_request_style\n\nDisk-O:\n\nDisk-O: - a program that allows you to work with clouds in one place - on your computer. At the same time, files do not take up space on the device, because they are stored only in the clouds. The program works with Cloud Storage, as well as platforms such as: Cloud Mail.ru, Yandex.Disk, Google Drive, Dropbox and other storages.\n\nThe program should be installed using the standard user interface of the operating system, as well as the installation file from the official website .\n\nConnection\n\nTo connect, add the Cloud Storage service, then enter the account and bucket data:\n\n\n\nWhere:\n\nStorage: predefined storage class of the Object Storage service - Cold data or Hot data\nBucket: the name of the existing bucket\nAccess Key: Key identifier obtained when creating an Object Storage account\nSecret Key: the secret key obtained when creating an Object Storage account\n\nAs a result, the bucket will be mounted as an additional disk in the operating system and will not use local space.\n\n\n\nOnly one service is available in the free version of Disk-O: When connecting additional drives, you need to purchase the paid version.\n\n\n","url":"/en/storage/s3/connect/s3-file-managers","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"483b4c17-d93f-4c69-936b-4cf65ea1d4a1","title":"SDK","data":"To connect to the object storage using the SDK:\n\nInstall the necessary tools or SDK:\n\n   Tools for PowerShell.\n   SDK for Java.\n   SDK for .NET.\n   SDK for JavaScript.\n   SDK for Ruby.\n   SDK for Python (Boto).\n   SDK for PHP.\n   SDK for Go.\n   Mobile SDK for iOS and Android.\n\nCreate account and bucket if this has not been done before. Save the connection details:\n\n   Moscow region\n   Kazakhstan region\n\n\n   Endpoint URL: https://hb.vkcs.cloud or https://hb.ru-msk.vkcs.cloud.\n   Access Key ID: the key ID received when creating the account.\n   Secret Key: the secret key received when creating the account.\n   Default region name: ru-msk.\n\n\n   Endpoint url: https://hb.kz-ast.vkcs.cloud.\n   Access Key ID: the key ID received when creating the account.\n   Secret Key: the secret key received when creating the account.\n   Default region name: kz-ast.\n\n\nConfigure the connection parameters to the object storage using one of the following methods:\n\n   Add the details to the ~/.aws/credentials configuration file.\n\n     For a complete list of tools and SDKs that support this method, see official AWS documentation.\n\n   Specify the details in the environment variables:\n\n     Linux (bash) / MacOS (zsh)\n     Windows (PowerShell)\n\n\n          export AWS_ACCESS_KEY_ID=\n     export AWS_SECRET_ACCESS_KEY=\n     export AWS_DEFAULT_REGION=\n\n\n          $Env:AWS_ACCESS_KEY_ID=\"\"\n     $Env:AWS_SECRET_ACCESS_KEY=\"\"\n     $Env:AWS_DEFAULT_REGION=\"\"\n\n\n     Some tools and SDKs may not read the AWS_DEFAULT_REGION variable and require specifying the region in a different way — check the documentation of the required tool.\n\n\n   Add the details directly to the source code.\n\nConnect to the storage via the installed SDK.\n\n   In the examples below, upon successful connection, a list of storage buckets for the Moscow region is displayed.\n\n     Example on Python\n\n     All parameters of connection to the object storage are specified in the source code.\n\n          import boto3\n     session = boto3.session.Session()\n     s3_client = session.client(\n         service_name = 's3',\n         endpoint_url = 'https://hb.vkcs.cloud',\n         aws_access_key_id = '',\n         aws_secret_access_key = '',\n         region_name='ru-msk'\n     )\n\n     response = s3_client.list_buckets()\n\n     for key in response['Buckets']:\n         print(key['Name'])\n\n     Example on Go\n\n     The parameters AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are added to the environment variables, the rest are specified in the source code.\n\n          package main\n\n     import (\n         \"github.com/aws/aws-sdk-go/aws\"\n         \"github.com/aws/aws-sdk-go/aws/session\"\n         \"github.com/aws/aws-sdk-go/service/s3\"\n         \"log\"\n     )\n\n     const (\n         vkCloudHotboxEndpoint = \"https://hb.vkcs.cloud\"\n         defaultRegion = \"ru-msk\"\n     )\n\n     func main() {\n     \tsess, _ := session.NewSession()\n\n     \tsvc := s3.New(sess, aws.NewConfig().WithEndpoint(vkCloudHotboxEndpoint).WithRegion(defaultRegion))\n\n     \tif res, err := svc.ListBuckets(nil); err != nil {\n     \t\tlog.Fatalf(\"Unable to list buckets, %v\", err)\n     \t} else {\n     \t\tfor _, b := range res.Buckets {\n     \t\t\tlog.Printf(\"* %s created on %s \\n\", aws.StringValue(b.Name), aws.TimeValue(b.CreationDate))\n     \t\t}\n     \t}\n     }\n\n","url":"/en/storage/s3/connect/s3-sdk","description":"Connecting to S3-compatible Cloud Storage using the SDK.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"e0c725c3-a177-4def-9703-3c07123044f7","title":"Creating and managing an account","data":"Access to the resources of the \"Object Storage\" service when using the VK Cloud Panel is carried out without hindrance, if you have a registered and active account and VK Cloud project.\n\nHowever, if you need to access resources through tools such as AWS CLI, S3 API, as well as any other software, you must create an additional account in the service and use the account key for access. This mechanism allows you to delimit access for the most comfortable and safe work with the service.\n\n\n\nThe number of created accounts is not limited and their names should not be unique\n\nCreate an account\n\nTo create an account, go to the \"Accounts\" tab of the \"Object Storage\" service and click the \"Add account\" button. In the window that appears, enter the account name and confirm the entry.\n\n\n\nThe account name must start with a letter or number, it can only consist of Latin letters, numbers, and characters: ., -, _.\n\n\n\nWhen you create an account, you will be provided with data that you want to save. After closing the window, it will be impossible to restore the Secret Key, but if you lose it, you can create a new account.\n","url":"/en/storage/s3/service-management/account-management","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"facede94-4c31-4f99-b096-6c4037b59074","title":"Creating an bucket","data":"When registering on the VK Cloud platform and activating the account, the user gets full access to the Object Storage service.\n\nBefore uploading an object to the storage, you should create a bucket for its placement.\n\n\n\nThere is no charge for creating a bucket. The fee is charged only for storing objects in the bucket and for moving objects to and from the bucket. Additional information about the cost of storage and operations can be obtained at website.\n\n\n\nThere are several types of buckets that differ in both the purpose and the amount of payment for the objects placed in them:\n\nHotbox — designed for storing and quickly distributing a large number of files for media services, online media, sites with multi-user content and mobile applications\nIcebox — cloud storage of rarely used data: backups, logs, media content, scientific, statistical data, as well as working archives\nBackup — placement of backup copies of instances created both automatically and manually. A bucket of this type is not subject to self-creation or deletion, but is managed by the backup service.\n\nYou can create a bucket both in the VK Cloud Panel and using the AWS CLI.\n\n\n\nThere is a limit on the number of buckets in one project, for more information, see the article Quotas and limits\n\n\n\n\n\nVK Cloud panel\nS3 CLI\n\nCreation via VK Cloud Panel\n\nTo create it, you should:\n\nGo to the \"Buckets\" tab of the Object Storage service in the VK Cloud panel.\nClick the \"Add\" button.\nSelect the type of bucket to be created and enter a DNS-compatible name.\n\n\n\nThe bucket name must meet the conditions:\n\nBe unique for the entire VK Cloud platform\nContain from 4 to 63 characters\nDo not contain uppercase characters (uppercase)\nStart with a lowercase character (lowercase) or digits\n\nNot recommended in the name:\n\nFormatting similar to the IP address (i.e. 192.168.5.4)\nUsing the underscore character (\\_), because it is not DNS-compatible and such a bucket cannot be linked to a DNS name\nStart with the characters xn--\n\nIt is recommended to avoid using personal information, such as the project number or user account in the bucket name.\n\nAfter creating a bucket, its name cannot be changed.\n\nCreation via AWS CLI\n\nCreate an authorized account.\n\nBefore creating a bucket, you need to create a user who will be granted access to manage operations in the AWS CLI.\n\nTo do this, on the \"Accounts\" tab of the \"Object Storage\" service, create an account by clicking the \"Add Account\" button, specify any required name and save the API keys received.\n\nSign in to AWS CLI\n\nRun the AWS configuration\n\naws configure\n\nUse the following data in the configurator:\n\nAccess Key ID: the key received when creating the account\nSecret Key: the key received when creating the account\nDefault region name: ru-msk\nDefault output format: json\n\nCreate a bucket\n\nThe bucket is created using the command\n\naws s3 mb s3:// --endpoint-url\n\nWhere endpoint is the url:\n\nCommon domain (points to the Moscow region): https://hb.vkcs.cloud\nMoscow domain: https://hb.ru-msk.vkcs.cloud\nKazakhstan domain: https://hb.kz-ast.vkcs.cloud\n\nThe following output will appear as a result of the correct execution of the command:\n\nmake_bucket:\n\nA bucket with the appropriate storage type will be created. You can change its type in the VK Cloud panel.\n\n\n\n","url":"/en/storage/s3/service-management/buckets/create-bucket","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"7655459d-49fc-455a-b0e0-6fdd04f97da9","title":"Bucket","data":"Buckets are containers for objects. A project can have one or more buckets. Access to each bucket can be controlled: specify which users can create and delete objects in the bucket, and display a list of objects.\n\nBucket name\n\nThe bucket name is used to access data and will be seen by other users when granting access.\n\nBucket names must be unique for all projects of the Object Storage service, ie. you cannot create two buckets with the same names even in different projects.\n\nThe bucket name must meet the following conditions:\n\nContain between 4 and 63 characters\nDo not contain uppercase characters (uppercase)\nStart with a lowercase character (lowercase) or numbers\n\nNot recommended in name:\n\nThe format is similar to the IP address (i.e. 192.168.5.4)\nUsing the underscore (\\_) character, because it is not DNS-compatible and such a bucket cannot be bound to a DNS name\nStart with characters xn--\n\nIt is recommended to avoid using personal information such as project number or user account in the bucket name.\n\nFeatures of use\n\nBucket cannot be renamed\nBuckets cannot be nested\nYou can delete a bucket only if there are no objects in it\nIt is impossible to delete a bucket with the \"Backup\" type on your own, this type of storage is managed by the instance backup service\nAfter deleting a bucket, it may take some time before creating a new bucket with the same name becomes possible.\n\nBucket creation\n\nTo create a bucket in the VK Cloud panel:\n\nGo to the \"Buckets\" tab of the Object Storage service in the VK Cloud panel.\nPress the \"Add\" button.\nSelect the type of bucket to create and enter a DNS-compatible name.\n\nRemoving a bucket\n\nDeleting a bucket after cleaning it is possible in the general list of buckets of the Object Storage service. To delete one bucket, you can use the delete button against the bucket, and to delete several elements, you can select using the checkbox to the left of the buckets.\n\n\n\nThe bucket with the Backup storage class cannot be deleted. You can delete the objects it contains through the backup service. Deletion of backups can be delayed, therefore, if there are backup objects in it, you should wait until the delete backup operation is completed.\n\n\n\nAfter deleting backups, the Backup bucket may be empty. It is required by Object Storage for further work with backup plans in the project.\n","url":"/en/storage/s3/service-management/buckets/bucket","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"918bf226-3af1-45ed-a4c8-6b8c0479fafe","title":"Webhooks","data":"The Cloud Storage webhook feature allows you to receive notifications when certain events occur in the bucket. To enable notifications, you must first add a notification configuration that specifies the events that Cloud Storage should publish and the destinations where Cloud Storage should send notifications. This configuration is saved in the Webhooks menu, which is tied to the bucket.\n\n\n\nCloud Storage event notifications are designed to be delivered at least once. Usually, event notifications are delivered in seconds, but sometimes it can take a minute or more.\n\nList of notifications\n\nCurrently Cloud Storage can publish notifications for the following events:\n\nNew Object Creation Events - Cloud Storage supports several APIs for object creation. You can request notification when only a specific API is used (for example, s3: ObjectCreated: Put), or you can use a wildcard (for example, s3: ObjectCreated: \\*) to request notification when an object is created, regardless of the API used.\nObject removal events - support for object removal. You can request notification when an object has been deleted using the s3: ObjectRemoved: Delete event type. You can also use the s3: ObjectRemoved: \\* wildcard to request a notification whenever an object is deleted.\n\nTurn on notification\n\nEnabling notifications is a bucket-level operation; that is, the notification configuration information is stored in the notification subresources associated with the bucket. You can use any of the following methods to manage notification configuration:\n\nUsing the VK Cloud control panel\n\nThe VK Cloud dashboard interface allows you to customize the notification configuration for the bucket without writing any code. To create a notification, use the \"Add\" button in the \"Wehooks\" tab of the selected bucket.\n\nIn the window that appears, enter the required parameters and confirm with the \"Add hook\" button.\n\nUsing the AWS SDK Programmatically\n\nInternally, both the VK Cloud panel and the SDK call the Cloud Storage REST API to manage the notification subresources associated with the bucket.\n\nRegardless of which method is used, Cloud Storage stores the notification configuration as XML on the notification subresources associated with the bucket.\n\nBy default, notifications are not enabled for any events. Thus, initially, the notification sub-resource stores an empty configuration.\n\n  .hb.vkcs.cloud / image / 01.jpg /\"> \n\n\nTo enable notifications for certain types of events, replace the XML with the appropriate configuration that defines the types of events that Cloud Storage should publish and the destination where the events should be published.\n","url":"/en/storage/s3/service-management/buckets/s3-webhooks","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"0ee46304-942f-485d-a0a3-8ea55dac5173","title":"Uploading objects to bucket","data":"The created bucket can be loaded with files that are saved as an Cloud Storage object. Objects are composed of file data and metadata describing the object. The bucket can contain an unlimited number of objects.\n\nYou can upload any type of file - images, backups, data, movies, etc. - to the bucket. The maximum size of a regular file that can be uploaded to a bucket is 32 GB. For files that exceed this size, you must use the multipart upload and store method. For a comfortable download of a file larger than 1 GB, it is recommended to use the AWS S3 CLI or Cloud Storage REST API.\n\nLoading from VK Cloud Panel\n\nTo load objects, go to the created bucket in the \"Buckets\" section of the \"Object Storage\" service.\n\n\n\nTo load folders through the VK Cloud Panel interface, they must be dragged into the loading window using the Drag & Drop function. You can drag and drop files to upload files or select them using the Select Files button. The drag and drop functionality is only supported for Chrome and Firefox browsers.\n\n\n\nWhen loading a folder, Cloud Storage loads all files and subfolders from the specified folder into the bucket. It then assigns a key object name, which is a combination of the downloaded file name and the folder name. For example, if you download a folder named / images that contains two files, sample1.jpg and sample2.jpg, Cloud Storage downloads the files and then assigns the appropriate key names, images / sample1.jpg and images / sample2.jpg. Key names include the folder name as a prefix. The VK Cloud panel displays only the part of the key name that follows the last \"/\". For example, in the images folder, the images / sample1.jpg and images / sample2.jpg objects appear as sample1.jpg and sample2.jpg.\n\nIf an object with a key name that already exists in the bucket is loaded, Cloud Storage replaces the existing object.\n\nBooting from AWS CLI\n\nS3 CLI provides several options for loading objects:\n\nThe following command copies the file to the specified bucket and sets the key:\n\n aws s3 cp test.txt s3: //mybucket/test2.txt --endpoint-url https://hb.vkcs.cloud\n\nFor the convenience of loading files from a local directory, you can apply synchronization of objects, the keys of which will be automatically generated after the completion of loading objects into the specified bucket.\n\nIf objects already exist in the bucket, then the files are subject to synchronization:\n\nThe size of which is different from the size of the object Cloud Storage\nLocal file last modified time is newer than Cloud Storage object last modified time\nLocal file does not exist in the specified bucket\n\n aws s3 sync  s3: //  --endpoint-url https://hb.vkcs.cloud\n\nA complete description of copying and moving objects and files is available in the official S3 CLI documentation.\n","url":"/en/storage/s3/service-management/objects/upload-object","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"1a089ded-2908-4cce-a8bd-5e6d0775ee4a","title":"Managing objects in a bucket","data":"Просмотр списка объектов\n\n\n\nЛичный кабинет\nAWS CLI\nGolang SDK\nPython SDK\n\n\n\nПерейдите в личный кабинет VK Cloud.\nВыберите проект, где находится нужный бакет.\nПерейдите в раздел Объектное хранилище → Бакеты.\nНажмите на имя нужного бакета.\n\n\n\n\nУстановите и настройте AWS CLI, если он еще не установлен.\n\nОткройте консоль и выполните команду:\n\n            aws s3 ls s3:// --endpoint-url\n\n      Здесь:\n\n      `` — домен сервиса Cloud Storage, должен соответствовать региону аккаунта:\n         https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n         https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n      Пример команды просмотра списка объектов\n\n      Пример команды:\n\n            aws s3 ls s3://my-bucket --endpoint-url https://hb.ru-msk.vkcs.cloud\n\n      Пример ответа:\n\n                                          PRE folder/\n      2023-09-27 11:45:05     421326 picture-1.jpg\n      2023-09-27 11:47:37       2713 picture-2.png\n      2023-09-27 11:48:37       2662 picture-3.png\n      2023-09-27 10:31:02      48314 picture-4.png\n      2023-09-27 11:48:56        361 delete-picture.png\n\n\nПолное описание операций просмотра объектов доступно в официальной документации AWS CLI.\n\n\n\n\nУстановите и настройте SDK для Go, если он еще не установлен.\n\nДобавьте реквизиты подключения к Cloud Storage в переменные окружения или конфигурационный файл, если этого не было сделано ранее.\n\nДобавьте код в свой проект:\n\n      package main\n\n   import (\n\t   \"github.com/aws/aws-sdk-go/aws\"\n\t   \"github.com/aws/aws-sdk-go/aws/session\"\n\t   \"github.com/aws/aws-sdk-go/service/s3\"\n\t   \"log\"\n   )\n\n   const vkCloudHotboxEndpoint = \"https://hb.ru-msk.vkcs.cloud\"\n\n   const defaultRegion = \"us-east-1\"\n\n   func main() {\n\t   // Создание сессии\n\t   sess, _ := session.NewSession()\n\n\t   // Подключение к сервису Cloud Storage\n\t   svc := s3.New(sess, aws.NewConfig().WithEndpoint(vkCloudHotboxEndpoint).WithRegion(defaultRegion))\n\n\t   bucket := \"gobucket\"\n\t   // получение списка объектов в бакете\n\t   result, err := svc.ListObjectsV2(&s3.ListObjectsV2Input{\n\t\t   Bucket: aws.String(bucket),\n\t   })\n\t   if err != nil {\n\t\t   log.Fatalf(\"Unable to list items in bucket %q, %v\", bucket, err)\n\t   } else {\n         // итерирование по объектам\n\t\t   for _, item := range result.Contents {\n\t\t\tlog.Printf(\"Object: %s, size: %d\\n\", aws.StringValue(item.Key), aws.Int64Value(item.Size))\n\t\t   }\n\t   }\n   }\n\nЗначение переменной vkCloudHotboxEndpoint должно соответствовать региону аккаунта:\n\nhttps://hb.ru-msk.vkcs.cloud — домен региона Москва;\nhttps://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\nКоманда ListObjectsV2 подробно описана в официальной документации к библиотеке aws-sdk-go.\n\n\n\n\nУстановите и настройте SDK для Python, если он еще не установлен.\n\nДобавьте реквизиты подключения к Cloud Storage в переменные окружения или конфигурационный файл, если этого не было сделано ранее.\n\nДобавьте код в свой проект:\n\n      import boto3\n   session = boto3.session.Session()\n   s3_client = session.client(service_name='s3',endpoint_url='https://hb.ru-msk.vkcs.cloud')\n\n   test_bucket_name = 'boto3-test-bucket-name'\n\n   for key in s3_client.list_objects(Bucket=test_bucket_name) ['Contents']:\n   print(key['Key'])\n\nЗначение переменной endpoint_url должно соответствовать региону аккаунта:\n\nhttps://hb.ru-msk.vkcs.cloud — домен региона Москва;\nhttps://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\nКоманда list_object подробно описана в официальной документации к библиотеке boto3.\n\nСкачивание объекта\n\n\n\nЛичный кабинет\nAWS CLI\nGolang SDK\nPython SDK\n\n\n\nПерейдите в личный кабинет VK Cloud.\nВыберите проект, где находится нужный бакет.\nПерейдите в раздел Объектное хранилище → Бакеты.\nНажмите на имя нужного бакета.\nВыполните одно из действий для нужного объекта:\n\n   Выберите с помощью флажка объект, затем нажмите кнопку Скачать.\n   Раскройте меню объекта и выберите пункт Скачать файл.\n\n\n\n\nУстановите и настройте AWS CLI, если он еще не установлен.\n\nОткройте консоль и перейдите в директорию, в которую нужно скачать объект.\n\nВыполните команду:\n\n      aws s3api get-object --bucket  --key   --endpoint-url\n\n   Здесь:\n\n   `` — имя объекта и путь до него, включая директории, если они есть.\n   `` — имя, которое будет присвоено скаченному объекту.\n   `` — домен сервиса Cloud Storage, должен соответствовать региону аккаунта:\n       https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n       https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n      Пример команды скачивания объекта\n\n      Пример команды:\n\n            aws s3api get-object --bucket my-bucket --key folder/my-object.exe uploaded-file.exe --endpoint-url   https://hb.ru-msk.vkcs.cloud\n\n      Пример ответа:\n\n            {\n       \"LastModified\": \"2023-10-05T14:38:16+00:00\",\n       \"ContentLength\": 13204976,\n       \"ETag\": \"\\\"ab5083fd8cd77246da821f42f90a5761\\\"\",\n       \"ContentType\": \"application/x-msdownload\",\n       \"Metadata\": {}\n       }\n\n\nУстановите и настройте SDK для Go, если он еще не установлен.\n\nДобавьте реквизиты подключения к Cloud Storage в переменные окружения или конфигурационный файл, если этого не было сделано ранее.\n\nДобавьте код в свой проект:\n\n      package main\n\n   import (\n\t   \"github.com/aws/aws-sdk-go/aws\"\n\t   \"github.com/aws/aws-sdk-go/aws/session\"\n\t   \"github.com/aws/aws-sdk-go/service/s3\"\n\t   \"log\"\n\t   \"strings\"\n   )\n\n   const (\n\t   vkCloudHotboxEndpoint = \"https://hb.ru-msk.vkcs.cloud\"\n\t   defaultRegion         = \"us-east-1\"\n   )\n\n   func main() {\n\t   // Создание сессии\n\t   sess, _ := session.NewSession()\n\n\t   // Подключение к сервису Cloud Storage\n\t   svc := s3.New(sess, aws.NewConfig().WithEndpoint(vkCloudHotboxEndpoint).WithRegion(defaultRegion))\n\n\t   bucket := \"gobucket\"\n\t   key := \"test_string.txt\"\n\n\t   // Извлечение объекта из бакета\n\t   if result, err := svc.GetObject(&s3.GetObjectInput{\n\t\t   Bucket: aws.String(bucket),\n\t\t   Key:    aws.String(key),\n\t   }); err != nil {\n\t\t   log.Fatalf(\"Unable to get object %q from bucket %q, %v\\n\", key, bucket, err)\n\t   } else {\n\t\t   data := make([]byte, *result.ContentLength)\n\t\t   result.Body.Read(data)\n\t\t   log.Printf(\"File with data %q downloaded from bucket %q\", data, bucket)\n\t   }\n   }\n\n   Значение переменной vkCloudHotboxEndpoint должно соответствовать региону аккаунта:\n\n   https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n   https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n   Команда GetObject подробно описана в официальной документации к библиотеке aws-sdk-go.\n\n\n\n\nУстановите и настройте SDK для Python, если он еще не установлен.\n\nДобавьте реквизиты подключения к Cloud Storage в переменные окружения или конфигурационный файл, если этого не было сделано ранее.\n\nДобавьте код в свой проект:\n\n      import boto3\n   session = boto3.session.Session()\n   s3_client = session.client(service_name='s3', endpoint_url='https://hb.ru-msk.vkcs.cloud')\n\n   response = s3_client.get_object(Bucket='boto3-bucket-name-test', Key='object_name.txt')\n   print(response)\n   print(response['Body'].read())\n\n   Значение переменной endpoint_url должно соответствовать региону аккаунта:\n\n   https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n   https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n   Команда get_object подробно описана в официальной документации к библиотеке boto3.\n\nПредоставление доступа к объекту\n\nЗагруженные в бакет объекты по умолчанию имеют уровень доступа private, то есть их можно скачать только:\n\nв личном кабинете VK Cloud;\nчерез CLI, API, SDK и файловые менеджеры, если есть ключи доступа к бакету или аккаунту.\n\nВы можете изменить уровень доступа объекта, чтобы сделать его доступным сторонним пользователям. Уровни доступа соответствуют фиксированным ACL. По умолчанию доступны уровни:\n\nprivate — полные права доступа при наличии ключей доступа к бакету или аккаунту, всем остальным объект не доступен;\npublic-read — полные права доступа при наличии ключей, остальным пользователям VK Cloud и сторонним пользователям объект доступен только для чтения;\nauthenticated-read — полные права доступа при наличии ключей, остальным пользователям VK Cloud объект доступен только для чтения.\n\nЧтобы изменить уровень доступа к объекту:\n\n\n\nЛичный кабинет\nAWS CLI\n\n\n\nЭто групповая операция: при необходимости можно изменить настройки доступа сразу для нескольких объектов, выбрав их с помощью флажков.\n\nПерейдите в личный кабинет VK Cloud.\nВыберите проект, где находится нужный бакет.\nПерейдите в раздел Объектное хранилище → Бакеты.\nНажмите на имя нужного бакета.\nРаскройте меню нужного объекта и выберите Доступ к файлу.\nВ поле Настройка ACL выберите нужную настройку доступа и нажмите Сохранить изменения. При выборе public-read или authenticated-read после сохранения изменений появится ссылка на доступ к объекту.\nСкопируйте ссылку и закройте окно настройки доступа.\nОтправьте ссылку пользователям или разместите ее на стороннем ресурсе для прямого доступа к объекту.\n\n\n\n\nУстановите и настройте AWS CLI, если он еще не установлен.\n\nОткройте консоль и выполните команду:\n\n            aws s3api put-object-acl --bucket  --key  --acl  --endpoint-url\n\n      Здесь:\n\n      `` — имя объекта и путь до него, включая директории, если они есть.\n      ` — уровень доступа ACL. Если указанный уровень доступа не настроен в проекте, то объекту будет присвоен уровень доступа private`.\n      `` — домен сервиса Cloud Storage, должен соответствовать региону аккаунта:\n         https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n         https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n      Пример команды просмотра списка объектов\n\n      Пример команды:\n\n            aws s3api put-object-acl --bucket my-bucket --key picture.png --acl public-read --endpoint-url https://hb.ru-msk.vkcs.cloud\n\n\nПолное описание операции изменения ACL объекта доступно в официальной документации AWS CLI.\n\nПредоставление доступа к объекту по подписанному URL\n\nПодписанный URL предоставляет полный доступ к объекту стороннему пользователю, но ограничен по времени.\n\nДля генерации подписанного URL:\n\n\n\nAWS CLI\nGolang SDK\nPython SDK\n\n\n\nУстановите и настройте AWS CLI, если он еще не установлен.\n\nОткройте консоль и выполните команду:\n\n      aws s3 presign s3:/// --endpoint-url  --expires-in\n\n   Здесь:\n\n   `` — имя объекта и путь до него, включая директории, если они есть.\n   `` — домен сервиса Cloud Storage, должен соответствовать региону аккаунта:\n       https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n       https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n   `` — время действия доступа в секундах. Если не указано, ссылка будет действовать 3600 секунд.\n\n      Пример команды формирования подписанного URL\n\n   Пример команды :\n\n      aws s3 presign s3://my-bucket/my-object.png --endpoint-url https://hb.ru-msk.vkcs.cloud --expires-in 300000\n\n   Пример ответа:\n\n      https://hb.vkcs.cloud/dd-winscp/usetech.ico?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=4TucX9HhP8RZveR9Cb3vGr%2F20231113%2Fru-msk%2Fs3%2Faws4_request&X-Amz-Date=20231113T104239Z&X-Amz-Expires=300000&X-Amz-SignedHeaders=host&X-Amz-Signature=5a7c4d87d12dd12f8f420ffaed84328f1d6a79050818fd4c615b219ce7bc18e9\n\n\nУстановите и настройте SDK для Go, если он еще не установлен.\n\nДобавьте реквизиты подключения к Cloud Storage в переменные окружения или конфигурационный файл, если этого не было сделано ранее.\n\nДобавьте код в свой проект:\n\n      package main\n\n   import (\n\t   \"context\"\n\t   \"fmt\"\n\t   \"github.com/aws/aws-sdk-go-v2/aws\"\n\t   \"github.com/aws/aws-sdk-go-v2/config\"\n\t   \"github.com/aws/aws-sdk-go-v2/service/s3\"\n\t   \"log\"\n\t   \"os\"\n\t   \"time\"\n   )\n\n   const (\n\t   vkCloudHotboxEndpoint = \"https://hb.ru-msk.vkcs.cloud\"\n\t   defaultRegion         = \"ru-msk\"\n   )\n\n   func main() {\n\t   cfg, err := config.LoadDefaultConfig(context.TODO())\n\t   if err != nil {\n\t\t   fmt.Printf(\"Error loading default config: %v\", err)\n\t\t   os.Exit(0)\n\t   }\n\n\t   client := s3.NewFromConfig(cfg, func(o *s3.Options) {\n\t\t   o.BaseEndpoint = aws.String(vkCloudHotboxEndpoint)\n\t\t   o.Region = defaultRegion\n\t   })\n\n\t   presigner := s3.NewPresignClient(client)\n\t   // имя бакета, где находится объект\n\t   bucketName := \"my-bucket\"\n\t   // имя объекта\n\t   objectKey := \"picture.png\"\n\t   lifeTimeSeconds := int64(60)\n\n\t   request, err := presigner.PresignGetObject(context.TODO(), &s3.GetObjectInput{\n\t\t   Bucket: aws.String(bucketName),\n\t\t   Key:    aws.String(objectKey),\n\t   }, func(opts *s3.PresignOptions) {\n\t\t   opts.Expires = time.Duration(lifeTimeSeconds * int64(time.Second))\n\t   })\n\n\t   if err != nil {\n\t\t   log.Printf(\"Couldn't get a presigned request to get %v:%v. Error: %v\\n\",\n\t\t\t   bucketName, objectKey, err)\n\t   }\n\n\t   fmt.Printf(\"%s\", request.URL)\n   }\n      Значение переменной vkCloudHotboxEndpoint должно соответствовать региону аккаунта:\n\n   https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n   https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n\n\n\nУстановите и настройте SDK для Python, если он еще не установлен.\n\nДобавьте реквизиты подключения к Cloud Storage в переменные окружения или конфигурационный файл, если этого не было сделано ранее.\n\nДобавьте код в свой проект:\n\n      import argparse\n   import logging\n   import boto3\n   from botocore.exceptions import ClientError\n   import requests\n\n   def create_presigned_url(s3_client, bucket_name, object_name, expiration=60):\n      try:\n         response = s3_client.generate_presigned_url('get_object',\n                                                      Params={'Bucket': bucket_name,\n                                                            'Key': object_name},\n                                                      ExpiresIn=expiration)\n      except ClientError as e:\n         logging.error(e)\n         return None\n\n      return response\n\n   logger = logging.getLogger(name)\n   session = boto3.session.Session()\n   s3_client = session.client(service_name='s3', endpoint_url='https://hb.ru-msk.vkcs.cloud')\n\n   request = create_presigned_url(s3_client, \"my-bucket\", \"picture.png\")\n\n   if request is not None:\n      url = requests.get(request)\n      print(\"url is \", request)\n   else:\n      print(\"Error creating link\")\n\n   Значение переменной endpoint_url должно соответствовать региону аккаунта:\n\n   https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n   https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\nКопирование объекта\n\nИнструкция подходит для копирования объекта в пределах одного бакета или между бакетами одного проекта.\n\n\n\nAWS CLI\nGolang SDK\nPython SDK\n\n\n\nУстановите и настройте AWS CLI, если он еще не установлен.\n\nОткройте консоль и выполните команду:\n\n      aws s3 cp s3:/// s3:/// --endpoint-url=\n\n   Здесь:\n\n   `` — имя бакета, из которого копируется объект.\n   `` — имя бакета, в который копируется объект. Если имена совпадают, объект копируется в тот же бакет.\n   `` — имя объекта и путь до него, включая директории, если они есть.\n   `` — домен сервиса Cloud Storage, должен соответствовать региону аккаунта:\n       https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n       https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n      Пример команды копирования объекта в другой бакет\n\n   Пример команды:\n\n      aws s3 cp s3://my-bucket/my-picture.png s3://my-another-bucket/my-picture.png --endpoint-url=https://hb.ru-msk.vkcs.cloud\n\n   Пример ответа:\n\n      copy: s3://my-bucket/my-picture.png to s3://my-another-bucket/my-picture.png\n\n\nУстановите и настройте SDK для Go, если он еще не установлен.\n\nДобавьте реквизиты подключения к Cloud Storage в переменные окружения или конфигурационный файл, если этого не было сделано ранее.\n\nДобавьте код в свой проект:\n\n      package main\n\n   import (\n\t   \"fmt\"\n\t   \"github.com/aws/aws-sdk-go/aws\"\n\t   \"github.com/aws/aws-sdk-go/aws/session\"\n\t   \"github.com/aws/aws-sdk-go/service/s3\"\n\t   \"log\"\n\t   \"strings\"\n   )\n\n   const (\n\t   vkCloudHotboxEndpoint = \"https://hb.ru-msk.vkcs.cloud\"\n\t   defaultRegion         = \"us-east-1\"\n   )\n\n   func main() {\n\t   // Создание сессии\n\t   sess, _ := session.NewSession()\n\n\t   // Подключение к сервису Cloud Storage\n\t   svc := s3.New(sess, aws.NewConfig().WithEndpoint(vkCloudHotboxEndpoint).WithRegion(defaultRegion))\n\n\t   // Копирование объекта из одного бакета в другой\n\t\n\t   sourceBucket := \"gobucket\"\n\t   sourceKey := \"test_string.txt\"\n\t   destBucket := \"gobucket2\"\n\t   destKey := \"test_string.txt\"\n\n\t   if _, err := svc.CopyObject(&s3.CopyObjectInput{\n\t\t   Bucket:     aws.String(destBucket),\n\t\t   Key:        aws.String(destKey),\n\t\t   CopySource: aws.String(fmt.Sprintf(\"%s/%s\", sourceBucket, sourceKey)),\n\t   }); err != nil {\n\t\t   log.Fatalf(\"Unable to copy object from %q to %q, %v\\n\", sourceBucket, destBucket, err)\n\t   } else {\n\t\t   fmt.Printf(\"Object copied from %q to %q\\n\", sourceBucket, destBucket)\n\t   }\n   }\n\n   Значение переменной vkCloudHotboxEndpoint должно соответствовать региону аккаунта:\n\n   https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n   https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n   Команда CopyObject подробно описана в официальной документации к библиотеке aws-sdk-go.\n\n\n\n\nУстановите и настройте SDK для Python, если он еще не установлен.\n\nДобавьте реквизиты подключения к Cloud Storage в переменные окружения или конфигурационный файл, если этого не было сделано ранее.\n\nДобавьте код в свой проект:\n\n   import boto3\n   session = boto3.session.Session()\n   s3_client = session.client(service_name='s3', endpoint_url='https://hb.ru-msk.vkcs.cloud')\n\n   source_bucket_name = 'boto3-source-bucket-name'\n   source_path = 'object_key1.txt'\n   target_bucket_name = 'boto3-target-bucket-name'\n   target_path = 'backup/copy_object_key1.txt'\n\n\n   copy_source = {\n      'Bucket': source_bucket_name,\n      'Key': source_path\n   }\n\n   s3_client.copy(copy_source, target_bucket_name, target_path)\n      Значение переменной endpoint_url должно соответствовать региону аккаунта:\n\n   https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n   https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n   Команда copy подробно описана в официальной документации к библиотеке boto3.\n\nКопирование всех объектов бакета\n\n\n\nAWS CLI\n\n\n\nУстановите и настройте AWS CLI, если он еще не установлен.\n\nОткройте консоль и выполните команду:\n\n      aws s3 cp --recursive s3:// s3:// --endpoint-url=\n\n   Здесь:\n\n   `` — имя бакета, из которого копируется объект.\n   `` — имя бакета, в который копируется объект. Если имена совпадают, объект копируется в тот же бакет.\n   `` — домен сервиса Cloud Storage, должен соответствовать региону аккаунта:\n       https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n       https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n      Пример команды копирования всех объектов бакета в другой бакет\n\n      Пример команды:\n\n            aws s3 cp --recursive s3://my-bucket s3://my-another-bucket --endpoint-url=https://hb.ru-msk.vkcs.cloud\n\n      Пример ответа:\n\n            copy: s3://my-bucket/video.mp4 to s3://my-another-bucket/video.mp4\n      copy: s3://my-bucket/pre/scheme.svg to s3://my-another-bucket/pre/scheme.svg\n      copy: s3://my-bucket/picture.png to s3://my-another-bucket/picture.png\n      copy: s3://my-bucket/example.txt to s3://my-another-bucket/example.txt\n\n\nУдаление объекта\n\n\n\nЛичный кабинет\nAWS CLI\nGolang SDK\nPython SDK\n\n\n\nЭто групповая операция: при необходимости можно удалить сразу несколько объектов, выбрав их с помощью флажков.\n\nДля удаления объекта:\n\nПерейдите в личный кабинет VK Cloud.\nВыберите проект, где находится нужный бакет.\nПерейдите в раздел Объектное хранилище → Бакеты.\nНажмите на имя нужного бакета. Откроется страница с информацией о бакете на вкладке Папки и файлы.\nВыполните одно из действий для нужного объекта:\n\n   Выберите с помощью флажка объект, затем нажмите кнопку Удалить.\n   Раскройте меню объекта и выберите пункт Удалить файл.\n\nПодтвердите удаление.\n\n\n\n\nУстановите и настройте AWS CLI, если он еще не установлен.\n\nОткройте консоль и выполните команду:\n\n      aws s3 rm s3:/// --endpoint-url=\n\n   Здесь:\n\n   `` — имя объекта и путь до него, включая директории, если они есть.\n   `` — домен сервиса Cloud Storage, должен соответствовать региону аккаунта:\n       https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n       https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n      Пример команды удаления объекта\n\n      Пример команды:\n\n            aws s3 rm s3://my-bucket/my-picture.png --endpoint-url=https://hb.ru-msk.vkcs.cloud\n\n      Пример ответа:\n\n            delete: s3://my-bucket/my-picture.png\n\n\nУстановите и настройте SDK для Go, если он еще не установлен.\n\nДобавьте реквизиты подключения к Cloud Storage в переменные окружения или конфигурационный файл, если этого не было сделано ранее.\n\nДобавьте код в свой проект:\n\n      package main\n\n   import (\n\t   \"github.com/aws/aws-sdk-go/aws\"\n\t   \"github.com/aws/aws-sdk-go/aws/session\"\n\t   \"github.com/aws/aws-sdk-go/service/s3\"\n\t   \"log\"\n\t   \"strings\"\n   )\n\n   const (\n\t   vkCloudHotboxEndpoint = \"https://hb.ru-msk.vkcs.cloud\"\n\t   defaultRegion = \"us-east-1\"\n   )\n\n   func main() {\n\t   // Создание сессии\n\t   sess, err := session.NewSession()\n\t   if err != nil {\n\t\t   log.Fatalf(\"Unable to create session, %v\", err)\n\t   }\n\t   // Подключение к сервису Cloud Storage\n\t   svc := s3.New(sess, aws.NewConfig().WithEndpoint(vkCloudHotboxEndpoint).WithRegion(defaultRegion))\n\n\t   // Удаление объекта из бакета\n      bucket := \"gobucket\"\n\t   key := \"test_string.txt\"\n\n\t   if _, err := svc.DeleteObject(&s3.DeleteObjectInput{\n\t\t   Bucket: aws.String(bucket),\n\t\t   Key:    aws.String(key),\n\t   }); err != nil {\n\t\t   log.Fatalf(\"Unable to delete object %q from bucket %q, %v\\n\", key, bucket, err)\n\t   } else {\n\t\t   log.Printf(\"Object %q deleted from bucket %q\\n\", key, bucket)\n\t   }\n\n      // Удаление множества объектов\n\t   if _, err := svc.DeleteObjects(&s3.DeleteObjectsInput{\n\t\t   Bucket: aws.String(bucket),\n\t\t   Delete: &s3.Delete{\n\t\t\t   Objects: []*s3.ObjectIdentifier{\n\t\t\t\t   {\n\t\t\t\t\t   Key: aws.String(\"test_string1.txt\"),\n\t\t\t\t   },\n\t\t\t\t   {\n\t\t\t\t\t   Key: aws.String(\"test_string2.txt\"),\n\t\t\t\t   },\n\t\t\t   },\n\t\t   },\n\t   }); err != nil {\n\t\t   log.Fatalf(\"Unable to delete objects from bucket %q, %v\\n\", bucket, err)\n\t   } else {\n\t\t   log.Printf(\"Objects deleted from bucket %q\\n\", bucket)\n\t   }\n   }\n\n      Значение переменной vkCloudHotboxEndpoint должно соответствовать региону аккаунта:\n\n   https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n   https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n   Команды DeleteObject и DeleteObjects подробно описаны в официальной документации к библиотеке aws-sdk-go.\n\n\n\n\nУстановите и настройте SDK для Python, если он еще не установлен.\n\nДобавьте реквизиты подключения к Cloud Storage в переменные окружения или конфигурационный файл, если этого не было сделано ранее.\n\nДобавьте код в свой проект:\n\n      import boto3\n   session = boto3.session.Session()\n   s3_client = session.client(service_name='s3', endpoint_url='https://hb.ru-msk.vkcs.cloud')\n\n   test_bucket_name = 'boto3-test-bucket-name'\n\n   #Удаление одного объекта\n   s3_client.delete_object(Bucket='boto3-bucket-name-test', Key='object_name.txt',)\n\n   #Удаление множества объектов\n   object_to_delete = [{'Key':'objectkey1.txt'}, {'Key':'objectkey2.txt'}]\n   s3_client.delete_objects(Bucket=test_bucket_name, Delete={'Objects': object_to_delete})\n\n   Значение переменной endpoint_url должно соответствовать региону аккаунта:\n\n   https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n   https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n   Команда delete_objects подробно описана в официальной документации к библиотеке boto3.\n\nУдаление частей загруженного объекта\n\nЕсли составная загрузка не завершена, объект не создается и не может использоваться, но хранение загруженных частей тарифицируется. Чтобы средства не списывались, удаляйте составные загрузки, которые не будут завершены.\n\nВы можете настроить автоматическое удаление незавершенных загрузок через жизненный цикл объектов или удалить загрузку вручную.\n\n\n\nЛичный кабинет\nAWS CLI\n\n\n\nЧтобы узнать, есть ли у вас незавершенные составные загрузки:\n\nПерейдите в личный кабинет VK Cloud.\nПерейдите в раздел Объектное хранилище → Бакеты.\nНажмите на имя нужного бакета и перейдите на вкладку Multipart.\n\n\n\n\nУстановите и настройте AWS CLI, если он еще не установлен. Установите выходной формат JSON или YAML, так как текстовые форматы не распознаются при выполнении команд составной загрузки или удаления.\nОткройте консоль и выполните команду:\n\n      aws s3api list-multipart-uploads --bucket  --endpoint-url\n\n   Здесь:\n\n    `` — имя бакета, для которого нужно удалить незавершенные загрузки.\n\n    `` — домен сервиса Cloud Storage, должен соответствовать региону аккаунта:\n\n      https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n      https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n      Пример команды просмотра незавершенных загрузок\n\n      Пример команды:\n\n            aws s3api list-multipart-uploads --bucket mybucket --endpoint-url https://hb.ru-msk.vkcs.cloud\n\n      Пример ответа:\n\n            {\n         \"Uploads\": [\n          {\n            \"UploadId\": \"example5kqtRsMpLxb1eZoHh8y9wmpjgfGA6mgDRRag\",\n            \"Key\": \"inupload.avi\",\n            \"Initiated\": \"2023-10-27T11:54:45.984000+00:00\",\n            \"StorageClass\": \"STANDARD\",\n            \"Owner\": {\n               \"DisplayName\": \"project\",\n               \"ID\": \"XXXXrs3jZaLwhimPAbVEiny\"\n            },\n            \"Initiator\": {\n               \"ID\": \"XXXXrs3jZaLwhimPAbVEiny\",\n               \"DisplayName\": \"project\"\n            }\n          }\n         ],\n         \"RequestCharged\": null\n      }\n\n\nЧтобы удалить незавершенные загрузки:\n\n\n\nЛичный кабинет\nAWS CLI\n\n\n\nЭто групповая операция: при необходимости можно удалить сразу несколько частей, выбрав их с помощью флажков.\n\nПерейдите в личный кабинет VK Cloud.\nПерейдите в раздел Объектное хранилище → Бакеты.\nНажмите на имя нужного бакета и перейдите на вкладку Multipart.\nВыполните одно из действий для нужной части:\n\n   Выберите с помощью флажка часть, затем нажмите кнопку Удалить над таблицей.\n   Выберите часть, которую нужно удалить, и нажмите на значок Удалить справа.\nПодтвердите удаление.\n\n\n\n\nУстановите и настройте AWS CLI, если он еще не установлен. Установите выходной формат JSON или YAML, так как текстовые форматы не распознаются при выполнении команд составной загрузки или удаления.\nОткройте консоль и выполните команду:\n\n      aws s3api abort-multipart-upload --bucket  --key  --upload-id  --endpoint-url\n\n   Здесь:\n\n    `` — имя бакета, для которого нужно удалить незавершенные загрузки.\n\n    ` — Название объекта на вкладке Multipart в личном кабинете или значение параметра Key` в ответе AWS CLI.\n\n    ` — ID объекта на вкладке Multipart в личном кабинете или значение параметра UploadId` в ответе AWS CLI.\n\n    `` — домен сервиса Cloud Storage, должен соответствовать региону аккаунта:\n\n      https://hb.ru-msk.vkcs.cloud — домен региона Москва;\n      https://hb.kz-ast.vkcs.cloud — домен региона Казахстан.\n\n   Пример выполнения команды:\n\n      aws s3api abort-multipart-upload --bucket mybucket --key inupload.avi --upload-id example3K1xj3g1KUb2pKeDAfeT2zP6K74XiyJtceMeXH --endpoint-url https://hb.ru-msk.vkcs.cloud\n\nВ результате все незавершенные загрузки будут отменены, а загруженные части — удалены.\n\n\n","url":"/en/storage/s3/service-management/objects/manage-object","description":"View, download, edit access, copy and delete objects from the bucket.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"aa58b445-5ea4-46a2-a9c7-57325bc1f672","title":"Multipart upload","data":"VK Cloud provides a mechanism for working with large files that might be too large to upload a file to a bucket one time using the GUI or CLI.\n\nMultiple (multipart) loading allows you to load one object as a set of parts. Each piece is a contiguous piece of object data. You can load these parts of the object independently and in any order. If the transfer of a part is unsuccessful, you can re-transmit that part without affecting other parts. After all the parts of the object are loaded, VK Cloud assembles these parts and creates the object. Generally, when an object reaches 100MB in size, it is a good idea to consider using multipart loading instead of loading the object in one operation.\n\nUsing a composite load has the following benefits:\n\nImproved performance - You can load parts in parallel to improve performance.\nFast recovery for any network problem - The smaller chunk size minimizes the impact of a failed boot restart due to a network error.\nPause and Resume Object Loading - You can download portions of an object over time. After a multipart download is initiated, there is no expiration date; you must explicitly complete or cancel a compound load.\nStart loading before the final size of the object is known - you can load the object as it is created.\n","url":"/en/storage/s3/service-management/objects/multipart","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"ea6eeb0b-3ef1-49dc-91f7-f1c7e5f820b6","title":"Signed URLs","data":"A pre-signed URL grants access to the object specified in the URL, provided that the creator of the pre-signed URL has permission to access that object.\n\nPre-signed URLs are useful when you want a user / client to be able to upload a specific object to a bucket, but don't need them to have VK Cloud credentials or security permissions.\n\nUsing pre-signed URLs, any user can perform operations in Object Storage, for example:\n\nDownload object\nUpload object\nCreate bucket\n\nWhen creating a pre-signed URL, you need to provide:\n\nYour security credentials (access keys)\nSpecify bucket name\nObject key\nHTTP method\nDate\nExpiration time\n\nPre-signed URLs are only valid for the specified period. It is possible to use the pre-signed URL multiple times, up to the expiration date and time.\n\n\n\nYou can programmatically generate a pre-signed URL using the REST API, AWS CLI, and AWS SDK for Java, .NET, Ruby, PHP, Node.js, and Python.\n\n\n\nThe AWS SDK uses version 4 signature by default to authenticate requests. When using AWS signed version 3 SDKs, you might need to request signature version 4 from the service.\n\nGeneral view of the signed URL\n\n https://hb.vkcs.cloud/  / ?\nX-Amz-Algorithm = AWS4-HMAC-SHA256\n& X-Amz-Expires =\n& X-Amz-SignedHeaders =\n& X-Amz-Signature =\n& X-Amz-Date =\n& X-Amz-Credential = % 2F % 2Fru-msk% 2Fs3% 2Faws4_request\n\nWhere\n\nX-Amz-Algorithm** - An indication of the version of the signature and the algorithm for its calculation. The value is AWS4-HMAC-SHA256.\nX-Amz-Expires** - Link expiration time in seconds. The maximum value is 604800 seconds (7 days).\nX-Amz-SignedHeaders* - Request headers to be signed. It is mandatory to sign the host header and any x-amz- \\ headers used in the request.\nX-Amz-Signature** - Request signature.\nX-Amz-Date** - Time in ISO8601 format, for example 20180719T000000Z. The date must be the same as the date in the X-Amz-Credential parameter.\nX-Amz-Credential** - Identifier for signing the format ` /  / ru-msk / s3 / aws4_request, where ` must match the date set in the X-Amz-Date header.\n\nComposing a signed URL\n\nTo get a pre-signed URL:\n\nCalculate signature: compose a string and calculate a signature using the string signature algorithm.\nCompose a signed URL for your request.\n\nNote\n\nIn order to compose a signed URL, you must own static access keys.\n\nExamples of getting a signed link\n\nUsing the AWS CLI, you can generate a pre-signed link with the following command:\n\n aws s3 presign s3: //  /  --endpoint-url https://hb.vkcs.cloud --expires-in\n","url":"/en/storage/s3/service-management/objects/signed-url","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"7c515fbb-ba95-4593-99df-3425effedb49","title":"Life cycle","data":"Description\n\nThe Object Storage service allows you to use an automated mechanism for cleaning objects at specified intervals.\n\nLifecycle configuration allows you to specify rules for managing the lifecycle of objects in a bucket. A configuration is a set of one or more rules. Each rule specifies the actions to be applied to the service for a group of objects after a specified period.\n\nObject life cycles can be configured only for each individual bucket. You cannot configure lifecycles for a bucket group, directory, or project.\n\nLifecycle rules in the VK Cloud panel are managed on the \"Lifecycle\" tab of the selected bucket.\n\nCreate a rule\n\nWhen creating a rule in the VK Cloud panel interface, fill in the fields and confirm the creation:\n\nRule Name - The display name of the rule. Only numbers, Latin letters, and special characters -, _ and ..\nObject key prefix - a key prefix that is used to filter objects that are subject to the rule. A filter can contain only one prefix. Examples of prefixes for a key image, image /, image / photo.\nDelete after a specified number of days - a timer after which the rule for deleting objects starts.\nActivate rule - a mechanism for activating a rule that allows you to enable or disable a rule without having to delete it.\n","url":"/en/storage/s3/service-management/objects/s3-lifecycle","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"0774f4fb-d138-4e6b-b3a9-8f3dd73eed5e","title":"Storage classes","data":"List of classes\n\nThere are several bucket classes that differ in both the purpose and the amount of payment for the objects placed in them:\n\nHotbox** - storage and fast distribution of a large number of files for media services, online media, sites with multi-user content and mobile applications\nIcebox** - cloud storage of rarely used data: backups, logs, media content, scientific, statistical data, as well as working archives\nBackup** - placement of backups of instances created both automatically and manually. A bucket of this class cannot be created or deleted on its own, but is managed by a backup service.\n\nChanging the bucket class\n\nAfter creating a bucket in the panel interface, it is possible to change its class: from Hotbox to Icebox and back. To do this, in the VK Cloud panel, select the bucket whose class is to be changed, then change the class on the \"Storage class\" tab and confirm the changes with the \"Save changes\" button.\n","url":"/en/storage/s3/service-management/change-storage-class","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"5a35fb4c-c152-451a-8b5f-b652acc1776c","title":"API keys","data":"To work with services through the API, you must create a user who will be granted access to manage operations in the Cloud Storage CLI.\n\nTo do this, on the \"Accounts\" tab of the required service, create an account by clicking the \"Add account\" button, specify any required name and save the received API keys.\n","url":"/en/storage/s3/service-management/access-management/apikeys","description":"Description of the process for generating access keys for the user.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"e99900d8-2e5c-487b-9c53-73d9e5e2c7ad","title":"CORS","data":"General information\n\nCORS (Cross-Origin Resource Sharing) is a resource sharing technology between different origins that allows you to determine how client web applications loaded in one domain interact with resources in another domain.\n\nIn other words, it is a mechanism that uses additional HTTP headers to allow the user agent to obtain permissions to access selected resources from a server on an origin (domain) other than what the site is currently using.\n\nCORS supports cross-domain requests and the transfer of data between the browser and web servers over a secure connection. Modern browsers use CORS in container APIs like XMLHttpRequest or Fetch to mitigate the risks inherent in requests from other sources.\n\nUsage scenarios\n\nThe use cases for CORS technology are listed below.\n\nScenario #1**. For example, personal resources host a website in a bucket called website. Users load the site endpoint URL. It now becomes necessary to use JavaScript on web pages stored in this bucket to send authenticated GET and PUT requests to the same bucket using the bucket service API endpoint. Normally, the browser will prevent JavaScript from executing to allow such requests, but using CORS technology, you can set up a bucket so that cross-origin requests are allowed from the website.\nScenario #2**. For example, you need to place a web font from a service bucket on your resources. Browsers are required to perform CORS checks (preflight) to load web fonts, so the bucket hosting the web font must be configured to allow requests from any origin.\n\nCreate a CORS rule\n\nThe \"Object storage\" service supports the technology of cross-domain requests for resources in a bucket. You can create a rule on the CORS tab of an open bucket.\n\nWhere:\n\nAllowedOrigins - a website from which cross-domain requests to the bucket are allowed. Can contain at most one \\* character.\nAllowedMethods - HTTP method allowed to be used for cross-domain request. You can use multiple methods in one rule.\nMaxAgeSeconds — time in seconds during which the browser caches the result of a request to an object using the options method.\nAllowedHeaders — allowed header in the request to the object. A single \\* character can be used in the header name to define a pattern. Object storage matches the headers passed in Access-Control-Request-Headers to the AllowedHeaders set and responds to options with a list of allowed ones.\nExposeHeaders - header allowed to be displayed in a JavaScript application in a browser. In a request to an object, the JavaScript client can only operate on the headers defined in the ExposeHeaders elements.\n\nIf necessary, you can add multiple parameter values ​​in the rules configurator.\n","url":"/en/storage/s3/service-management/access-management/cors","description":"Scenarios using and creation of the CORS Rules","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"bbc8dd2f-6a1d-4252-94f9-29b1f657b57a","title":"Access Control List","data":"general information\n\nAccess Control List VK Cloud (ACL or Access Control List) allow you to control access to buckets and objects. Each bucket and object has its own ACL, with which you can determine which accounts and groups are granted access, as well as the type of access. When receiving a request for a resource, the service checks the corresponding ACL to see if the requestor has access rights.\n\n\n\nBy default, the created bucket or object gets a Private ACL, where the owner has full rights, and the rest of the participants have access denied.\n\n\n\nWhen a container or object is created, the service creates a standard ACL that grants the resource owner full control over that resource and denies access to other projects. This is shown in the following bucket ACL example (the default object ACL has the same structure).\n\n .hb.vkcs.cloud / images / 01.jpg /\">\n\n \\\\\\* Owner-Canonical-User-ID \\\\\\*\n owner-display-name\n\n \\\\\\* Owner-Canonical-User-ID \\\\\\*\n display-name\n\n FULL_CONTROL\n\nThe `` element identifies the owner by the canonical user ID of the VK Cloud account.\nThe `` element identifies the recipient (VK Cloud account or predefined group) and grants permission.\n\nThe base ACL shown as an example defaults to one Grant element per owner. To grant permission, ` elements are added, each ` element must specify the grantor and permission options.\n\n\n\nACLs can contain up to 100 permissions.\n\nRecipient of rights\n\nThe recipient of rights can be an VK Cloud project or one of the global VK Cloud groups. Rights can be granted to an VK Cloud project using the project ID address (mcs_pid) or the canonical user ID. Moreover, if you specify the project identifier in the request for access rights, then the service determines the canonical user ID for the corresponding project and adds it to the ACL. As a result, the ACL will always contain the canonical user ID for the project, not the project ID.\n\nTo grant access rights, you must specify each recipient as a type = value pair, where type is one of the following:\n\nid - the canonical user ID of the VK Cloud account\nuri is a predefined group to which permission is granted\nemailAddress - VK Cloud project ID\n\nExample: project ID\n\nFor example, the x-amz-grant-read header grants VK Cloud accounts identified by mcs_pid permissions to read object data and its metadata:\n\n x-amz-grant-read: emailAddress = \"mcs1447309426\", emailAddress = \"mcs1380112926\"\n\n\n\nWhen granting other VK Cloud accounts access to their resources, please note that VK Cloud accounts can delegate their permissions to users under their own accounts. This is known as multi-account access.\n\nCanonical user ID\n\nThe canonical user ID is associated with the VK Cloud account. It's a long string like eab55955-ebdb-4f18-a94d-f3558ff150da.\n\nIt can be calculated using the command\n\n aws s3api list-buckets --query Owner.ID --output text --endpoint-url https://hb.vkcs.cloud\n\nYou can also find the canonical user ID of the VK Cloud account by reading the ACL of the bucket or object that the VK Cloud account has access to. When an individual VK Cloud account is granted permissions on a Grant request, a grant entry with the canonical user ID of the VK Cloud account is added to the ACL.\n\n\n\nIf you make your bucket public (not recommended), any unauthenticated user can upload objects to the bucket. These anonymous users do not have an VK Cloud account. When an anonymous user uploads an object to a bucket, Cloud Storage adds a special canonical user ID (65a011a29cdf8ec533ec3d1ccaae921c) as the owner of the object to the ACL.\n\nProject ID\n\nProject ID (mcs_pid) - a unique parameter that characterizes a project on the VK Cloud platform. You can get it in your personal account in the account information area.\n\nThe button located next to the project ID allows you to copy the parameter for convenience.\n\nPredefined groups\n\nCloud Storage has a set of predefined groups. When granting access to an account to a group, one of the URIs is specified instead of the canonical user ID. The following predefined groups are available:\n\nAuthenticated Users - a group of authorized users, represented by http://acs.amazonaws.com/groups/global/AuthenticatedUsers.\n\nAll VK Cloud accounts are represented in this group. Permission to access this group allows any VK Cloud account to access the resource. However, all requests must be signed (authenticated).\n\n\n\nWhen granting access to the Authenticated Users group of users, any authorized VK Cloud user from the Internet can access the resource.\n\n\n\nAll Users - The All Users group, represented by http://acs.amazonaws.com/groups/global/AllUsers.\n\nPermission to access this group allows anyone on the Internet to access the resource. Requests can be signed (authenticated) or unsigned (anonymous). Unsigned requests omit the Authentication header in the request.\n\n\n\nIt is highly recommended that you do not grant permission to the All Users group with WRITE, WRITE_ACP, or FULL_CONTROL authority. For example, WRITE permissions allow anyone to store objects in a bucket, which is paid for by the current owner. It also allows others to delete objects that might need to be kept.\n\nTypes of permits\n\nThe table lists the permission sets that Cloud Storage supports in the ACL. The ACL permission set is the same for object ACL and bucket ACL. These ACLs grant permissions for specific buckets or object operations. The table lists the permissions and describes what they mean in the context of objects and buckets.\n\nResolutionApplying to a bucketApplying to an objectREADHeadBucketGetBucketLifecycleGetBucketNotificationListObjectsListPartsListMultipartsAllows you to get the content of an object and its metadata:GetObjectHeadObjectGetObjectRangeWRITEAllows you to create, delete, overwrite any objects in the bucket:DeleteBucketNotificationPutBucketNotificationPutBucketLifecycleDeleteBucketLifecycleDeleteObjectDeletMultipleObjectsAbortMultipartInitMultipartUploadPartCompliteMultipartPutObjectPutObjectCopyNot applicableREAD_ACPAllows reading the bucket ACL:GetBucketAclGetBucketCorsAllows reading the ACL of an object:GetObjectAclWRITE_ACPAllows you to change the bucket ACL:CreatePrefixKeyDeletePrefixKeyListPrefixKeysPutBucketCorsDeleteBucketCorsPutBucketAclAllows changing the ACL of an objectPutObjectAclFULL_CONTROLCombines READ, WRITE, READ_ACP, WRITE_ACP permissions for bucketCombines READ, WRITE, READ_ACP, WRITE_ACP rights for an object\n\nMapping ACL Permissions and Access Policy Permissions\n\nThe ACL only allows a finite set of permissions compared to the number of permissions that can be set in the access policy. Each of these permissions allows one or more Cloud Storage operations to be performed.\n\nThe following table shows how each ACL permission maps to the corresponding access policy permissions. As you can see, the access policy allows more permissions than the ACL. ACL is used primarily to grant basic read and write permissions, similar to file system permissions.\n\nACL permissionAccess policy for bucketObject access policyREADs3: ListBucket, s3: ListBucketMultipartUploadss3: GetObjectWRITEs3: PutObject, s3: DeleteObjectNot applicableREAD_ACPs3: GetBucketAcls3: GetObjectAclWRITE_ACPs3: PutBucketAcls3: PutObjectAclFULL_CONTROLEquivalent to providing READ, WRITE,READ_ACP, and WRITE_ACP ACL permissionsEquivalent to granting READ, READ_ACP, and WRITE_ACP ACL permissions\n\nStatus keys\n\nWhen granting access policy permission, you can use conditional keys to restrict the ACL value for an object using a bucket policy. The following context keys correspond to ACLs. These context keys are intended to indicate the use of a specific ACL in a request:\n\ns3: x-amz-grant-read - Read access\ns3: x-amz-grant-write - Write rights\ns3: x-amz-grant-read-acp - Access to read ACL bucket\ns3: x-amz-grant-write-acp - Bucket ACL write permissions\ns3: x-amz-grant-full-control - Full control\ns3: x-amz-acl - Use a templated ACL\n\nACL example\n\n .hb.vkcs.cloud / images / 01.jpg /\">\n\n Owner-canonical-user-ID\n display-name\n\n Owner-canonical-user-ID\n display-name\n\n FULL_CONTROL\n\n user1-canonical-user-ID\n display-name\n\n WRITE\n\n user2-canonical-user-ID\n display-name\n\n READ\n\n http://acs.amazonaws.com/groups/global/AllUsers\n READ\n\nFixed ACL\n\nCloud Storage supports a set of predefined permissions known as standard ACLs. Each fixed ACL has a predefined set of recipients and permissions. The following table lists the standard ACLs and their associated predefined permissions.\n\nFixed ACLRefers toPermissions added to ACLprivateBucket and objectThe owner gets FULL_CONTROL. Nobody else has access rights (default).public-readBucket and objectThe owner gets FULL_CONTROL. The AllUsers group gets READ access.public-read-writeBucket and objectThe owner gets FULL_CONTROL. The AllUsers group gets READ and WRITE access.aws-exec-readBucket and objectThe owner gets FULL_CONTROL.authenticated-readBucket and objectThe owner gets FULL_CONTROL. AuthenticatedUsers group gets READ access.bucket-owner-readAn objectThe owner of the object gets FULL_CONTROL. The bucket owner gets READ access. If you specify this template ACL when creating a bucket, Cloud Storage will ignore it.bucket-owner-full-controlAn objectBoth the object owner and the bucket owner get FULL_CONTROL over the object. If you specify this fixed ACL when creating a bucket, will ignore it.\n\n\n\nOnly one of these fixed ACLs can be specified in a request.\n\n\n\nThe request specifies a fixed ACL using the x-amz-acl request header. When receives a request with a standard ACL in the request, it adds the predefined permissions to the ACL of the resource.\n","url":"/en/storage/s3/service-management/access-management/s3-acl","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"8f2f61c3-5ffd-496f-9f55-7006c314a2b8","title":"Loading large-volume images of OS","data":"VK Cloud has restrictions on the size of uploaded images of operating systems. If the limit is exceeded, a message appears like:\n\nAn error occurred (InvalidArgument) when calling the UploadPart operation: Part number must be an integer between 1 and 10000, inclusive\n\nNext, we consider downloading images larger than 500 GB through the Cloud Storage.\n\nWill be used:\n\na virtual machine of at least 500 GB in size;\na local machine of the Linux family with the utility installed gzip.\n\n1. Preparatory steps\n\nCheck the possibility of migration. The virtual machine must meet the following requirements:\n\n   The VM operating system has a 64-bit architecture.\n   VM uses BIOS emulation.\n   The current user has administrator rights.\n   At least one disk is connected to the VM.\n\n   To migrate a VM with UEFI emulation, use Hystax or transfer data to a new Hyper-V VM with BIOS emulation.\n\nCreate an account and bucket uc_bucket.\nMake sure that you have installed and configured AWS CLI. Specify the data for connecting to the bucket in it (Access key ID and Secret key). Open the configuration file ~/.aws/config and make changes to it:\n\n      [default]\n   region = ru-msk\n   output = json\n   s3 =\n       max_concurrent_requests = 20\n       max_queue_size = 10000\n       multipart_threshold = 1024MB\n       multipart_chunksize = 384MB\n       addressing_style = path\n\nCreate Ubuntu 22.04 VM to VK Cloud.\nInstall the gzip utility on the VM.\nCreate a disk of at least 600 GB in size and connect it to the VM.\n\n2. Upload the image to the object storage\n\nRun the command:\n\n      dd if=/dev/vdX bs=32M | gzip -c | aws s3 cp - s3://uc_bucket/image.raw.gz --endpoint-url http://hb.ru-msk.vkcs.cloud\n\nMake sure that the download has started using the command:\n\n      aws s3api list-multipart-uploads --bucket uc_bucket --endpoint-url http://hb.ru-msk.vkcs.cloud\n\n    Example of command output\n\n       {\n        \"Uploads\": [\n            {\n                \"UploadId\": \"3ceXH7brs7r8DohqQ9BsJzfjkkhMxQux67Z8MQXYGh9BvfrmffLufWMzwiLWPbU3XkWE3ibSefgQ1GU81ER66EEHfKMZM8xxqRsDkBaN63XXXX\",\n                \"Key\": \"image.raw.gz\",\n                \"Initiated\": \"2021-12-08T11:57:42.929000+00:00\",\n                \"StorageClass\": \"STANDARD\",\n                \"Owner\": {\n                    \"DisplayName\": \"mcs0000000000\",\n                    \"ID\": \"4ed36441-69f5-4ac7-XXXX-07013f9ac3c5\"\n                },\n                \"Initiator\": {\n                    \"ID\": \"P95mF7Kjo6aEfpiLA7XXXXX\",\n                    \"DisplayName\": \"mcs0000000000\"\n                }\n            }\n        ]\n    }\n\n\nCheck the partitions using the command:\n\n      aws s3api list-parts --bucket uc_bucket --endpoint-url http://hb.ru-msk.vkcs.cloud --key image.raw.gz  --upload-id 3ceXH7brs7r8DohqQ9BsJzfjkkhMxQux67Z8MQXYGh9BvfrmffLufWMzwiLWPbU3XkWE3ibSefgQ1GU81ER66EEHfKMZM8xxqRsDkBaN63XXXX\n\n    Example of command output\n\n       {\n        \"Parts\": [\n            {\n                \"PartNumber\": 1,\n                \"LastModified\": \"2021-12-08T11:57:49.613000+00:00\",\n                \"ETag\": \"\\\"6c8659343a53b1c4247e3769548e7181\\\"\",\n                \"Size\": 402653184\n            },\n            {\n                \"PartNumber\": 2,\n                \"LastModified\": \"2021-12-08T11:57:49.019000+00:00\",\n                \"ETag\": \"\\\"dc6138a7be543ec5b720e9a2a6273b76\\\"\",\n                \"Size\": 402653184\n            }\n        ],\n        \"Initiator\": {\n            \"ID\": \"P95mF7Kjo6aEfpiLA7XXXXX\",\n            \"DisplayName\": \"mcs0000000000\"\n        },\n        \"Owner\": {\n            \"DisplayName\": \"mcs0000000000\",\n            \"ID\": \"P95mF7Kjo6aEfpiLA7XXXXX\"\n        },\n        \"StorageClass\": \"STANDARD\"\n    }\n\n\nWait for the upload to the object storage. The output of the aws s3api list-multipart-uploads command should not contain data in the Uploads block.\n\n3. Upload the image to the VK Cloud disk\n\nConnect to VM via SSH.\nCheck for a connected disk using the command lsblk.\n\n    Example of command output\n\n      NAME  MAJ:MIN RM SIZE RO TYPE MOUNTPOINT\n   vda   252:0   0  10G  0  disk\n   -vda1 252:1   0  10G  0  part /\n   vdb   252:16  0  600G 0  disk\n\n\nMove the image to disk using the command:\n\n      wget https://uc_bucket.hb.bizmrg.com/image.raw.gz -O /dev/vdb/image.raw.gz\n\nUnpack the image using the command:\n\n      gunzip -c image.raw.gz | dd of=/dev/vdb bs=32M\n\n4. Create an image from disk\n\nUse the instructions.\n\n5. Check the health of the image\n\nCreate a VM by selecting the downloaded image as the operating system, according to instructions.\n\nDelete unused resources\n\nThe created resources are charged and consume computing resources. If you don't need them anymore:\n\nDelete загруженный образ из объектного хранилища.\nDelete or stop VM.\nDelete диск.\n","url":"/en/storage/s3/how-to-guides/load-large-image","description":"How to upload a large-volume VM image of operating systems to VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"3a85479b-d4d1-4d53-8cc3-326559bf4c2a","title":"Hosting static websites","data":"The Object Storage service allows you to use resources for hosting static sites based on HTML, CSS and JavaScript technologies. The store can also be used to forward requests.\n\n\n\nThe bucket used for hosting cannot be used to host scripts that require running on the web server side.\n\nDomain binding\n\nTo provide access to bucket objects using links leading to a unique site, you can bind your domain on the \"Bind Domain\" tab.\n\nPre-required:\n\nRegister a domain with any hosting provider\nCreate a subdomain that will be used for the created bucket\nLink subdomain in VK Cloud panel\n\nAfter binding, the bucket will be available via a link of the form:\n\n http: // . .\n\nWhen you create a bucket with a period (.) Character in the name, it will only be available when using the HTTP protocol. If you need to use the HTTPS protocol, you must install an SSL certificate.\n\nInstalling an SSL Certificate\n\nTo install a personal SSL certificate, you should contact technical support , specifying the project, bucket data and providing an SSL certificate for installation.\n\nAfter installing the certificate, the bucket will be available via a link of the form:\n\n https: // . .\n","url":"/en/storage/s3/how-to-guides/s3-domain","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"d5b6de10-50dd-4960-bf14-1fc2746655a3","title":"Tariffication","data":"About tariffication\n\nThe cost of Cloud storage service is given in price list. You can use calculator to calculate the total cost of the service usage.\n\nTariffication is based on the \"pay as you go\" principle: you pay only for the resources you have consumed up to second.\n\nSee Billing for how the tools related to paying for platform services work.\n\nWhat is charged\n\nTarrification in Cloud storage depends on data storage classes.\n\nYou pay in Hotbox and Icebox for:\n\nThe amount of data stored (per GB).\nOutgoing traffic (per GB).\n\nYou pay in Backup only for the amount of data stored (per GB).\n\nWhat is not charged\n\nIncoming traffic.\nAPI usage.\n","url":"/en/storage/s3/tariffication","description":"How the service is charged, cost calculation.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"778f3c6c-dec5-4978-9c11-0fe9e71b43fa","title":"Bucket","data":"Operations with Buckets:\n\nCreate - Create bucket\nListBuckets - Get a list of buckets\nHeadBucket - Check if a bucket exists and has access to it\nListObjects - Get a list of bucket objects\nDeleteBucket - Delete bucket\n\nAll operations with buckets are characterized by typical error messages, request headers and response headers. If the operation has special error messages, request or response headers, this will be indicated in the description of the operation.\n\nCreateBucket\n\nThe PUT operation creates a new bucket. To create a bucket, you need to register on the VK Cloud platform and obtain an access key to authorize requests. Bucket creation using anonymous requests is not possible. The initiator of the bucket creation request automatically becomes its owner.\n\nWhen using the operation to create a bucket, you can specify projects or groups that need to be granted certain rights to the bucket. There are two ways to grant rights using request headers:\n\nSpecifying the prepared ACL in the request using the x-amz-acl request header.\nExplicitly specifying access rights using the x-amz-grant-read, x-amz-grant-write, x-amz-grant-read-acp, x-amz-grant-write-acp, x-amz-grant- headers full-control.\n\nYou can use a ready-made ACL or specify the access rights explicitly, but you cannot use both methods at the same time.\n\nInquiry:\n\n PUT / HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-acl: public-read\nx-amz-content-sha256: c6f1fc479f5f690c443b73a258aacc06ddad09eca0b001e9640ff2cd56fe5710\nx-amz-date: 20200831T173143Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-acl; x-amz-content-sha256; x-amz-date, Signature = 6cab03bef74a80a0441ab7fd33c829a2cdb46bba07e82da518cdb78ac238fda5\n\n\n ru-msk\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nDate: Mon, 31 Aug 2020 17:31:43 GMT\nContent-Length: 0\nContent-Type: text / plain; charset = utf-8\nConnection: close\n\nListBuckets\n\nThe GET operation will return a list of buckets existing in the project.\n\nInquiry:\n\n GET / HTTP / 1.1\n\nHost: hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T183940Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = 245eb867ab4dba65c71\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nx-amz-request-id: tx000000000000002ba2427-0059651b6d-1268c-ru-mska\nDate: Mon, 31 Aug 2020 17:31:43 GMT\nContent-Length: 523\nContent-Type: text / plain\nConnection: close\n\n\n\n\n 6174283\n 6174283\n\n static-images\n 2020-08-31T18: 37: 48.157Z\n\n log-files\n 2020-08-31T18: 37: 48.157Z\n\nListObjects\n\nThe GET operation returns some or all (up to 1000) objects in the bucket. You can use query parameters as selection criteria to return a subset of objects in a bucket.\n\nTo successfully complete the operation, you must have the rights to read the bucket (READ).\n\nInquiry:\n\n GET / HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T172613Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = de1bf77684d915c74\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nx-amz-request-id: tx00000000000000029ac87-0059690330-8d1a-ru-mska\nDate: Mon, 31 Aug 2020 17:31:43 GMT\nContent-Length: 775\nContent-Type: application / xml\nConnection: close\n\n\n\n static-images\n\n 1000\n false\n\n example.txt\n 2020-08-31T18: 40: 46.777Z\n \"b3a92f49e7ae64acbf6b3e76f2040f5e\"\n 14\n STANDARD\n\n 6174283\n 6174283\n\n sammy.png\n 2020-08-31T17: 44: 03.597Z\n \"fb08934ef619f205f272b0adfd6c018c\"\n 35369\n STANDARD\n\n 6174283\n 6174283\n\nHeadBucket\n\nThe operation is used to determine whether a bucket exists and whether it has permission to access it. The operation returns a 200 OK message if the bucket exists and you have permission to access it. Otherwise, this operation may return messages such as \"404 Not Found\" or \"403 Forbidden\".\n\nInquiry:\n\n HEAD / HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20170714T185156Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = e3b4551b249278fc34ae\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nx-amz-id-2: JuKZqmXuiwFeDQxhD7M8KtsKobSzWA1QEjLbTMTagkKdBX2z7Il / jGhDeJ3j6s80\nx-amz-request-id: 32FE2CEB32F5EE25\nx-amz-bucket-region: ru-msk\nDate: Mon, 31 Aug 2020 21:34:56 GMT\nConnection: close\n\nDeleteBucket\n\nDELETE operations delete the bucket specified in the URI. Success of the operation will be indicated by receiving 204 (No content) as the response code.\n\nIf 409 (BucketNotEmpty) is received as a response code, it means there are objects in the bucket. You must remove all objects from the bucket (move or delete) before you can remove the bucket itself.\n\nInquiry:\n\n DELETE / HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20170710T181321Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = b0558a259d9dbbcdate, Signature = b0558a259d9794\n\nAnswer:\n\n HTTP / 1.1 204 No Content\n\nDate: Mon, 31 Aug 2020 18:13:21 GMT\nConnection: close\n","url":"/en/storage/s3/s3-rest-api/bucket-api","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"44c3a879-7fa3-4e76-9f59-145a2a30bdba","title":"Object","data":"Operations with Objects:\n\nGet - Download an object from a bucket\nUpload - Upload an object to a bucket\nCopy - Copy an object in the bucket\nHeadObject - Get information about an object\nDelete - Delete object\nDeleteMultipleObjects - Delete a group of objects from the list\n\nAll operations with buckets are characterized by typical error messages, request headers and response headers. If the operation has special error messages, request or response headers, this will be indicated in the description of the operation.\n\nGet\n\nGET operations retrieves an object from a bucket. To use GET, you must have READ access to the object. By granting READ access to an anonymous user, you can return an object without using an authorization header.\n\nThe bucket does not have a directory hierarchy like the standard file system. However, you can create a logical hierarchy using object key names that imply a directory structure. For example, you can name the object not sample.jpg, but photos / 2020 / August / 01.jpg.\n\nTo retrieve an object with this logical hierarchy, you must specify the fully qualified name of the object in the GET operation.\n\nInquiry:\n\n GET /example.txt HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T190539Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20170710 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = e912de558677884f08b83d3bd3d558677884f07d8bd3bd3d8687884f08d4d4d1d3d1d3d2c8d8c884f04d4d4d4d1c4d4d4d4d4d4d4d4d4d4d4f4d4f4d4f4d4f4d4f8244f07104f07104f07104f07104f07104f07104f0710\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nDate: Mon, 31 Aug 2020 19:05:39 GMT\nx-amz-request-id: tx00000000000000279f46e-005963d003-1268c-ru-mska\nContent-Type: text / plain\nContent-Length: 14\nAccept-Ranges: bytes\nLast-Modified: Mon, 31 Aug 2020 19:05:09 GMT\nEtag: \"b3a92f49e7ae64acbf6b3e76f2040f5e\"\nConnection: close\n\nExample text.\n\nUpload\n\nThe PUT operation adds an object to the bucket. To perform the operation, you must have WRITE rights to the bucket.\n\nCloud Storage adds partial objects if the object size exceeds 50MB. If a successful response is received, then the entire object has been added to the bucket.\n\nWhen performing concurrent PUT operations and when there are identical loadable objects, Cloud Storage overwrites everything except the last written object.\n\nTo prevent data corruption while traversing the network, it is recommended to use the Content-MD5 header, which checks the object against the provided MD5 value and returns an error if it does not match. Alternatively, you can compute MD5 by placing an object in a bucket and compare the returned ETag with the computed MD5.\n\nInquiry:\n\n PUT /example.txt HTTP / 1.1\n\nContent-Length: 14\nContent-Type: text / plain\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: 003f0e5fe338b17be8be93fec537764ce199ac50f4e50f2685a753c4cc781747\nx-amz-date: 20200831T194605Z\nx-amz-meta-s3cmd-attrs: uid: 1000 / gname: asb / uname: asb / gid: 1000 / mode: 33204 / mtime: 1499727909 / atime: 1499727909 / md5: fb08934ef619f205f272b0adfd6c099713 / c405time: 1499727909\nx-amz-storage-class: STANDARD\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = content-length; content-type; host; x-amz-content-sha256; x-amz-date; x-amz -meta-s3cmd-attrs; x-amz-storage-class, Signature = a9a9e16da23e0b37ae8362824de77d66bba2edd702ee5f291f6ecbb9ebac6013\n\n\nExample text.\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nDate: Mon, 31 Aug 2020 19:46:06 GMT\nx-amz-request-id: tx0000000000000027bd57c-005963d97e-1268c-ru-mska\nContent-Length: 0\nAccept-Ranges: bytes\nLast-Modified: Mon, 31 Aug 2020 19:05:09 GMT\nEtag: \"fb08934ef619f205f272b0adfd6c018c\"\nConnection: close\n\nCopy\n\nPUT operations can create a copy of an object that is already stored in the bucket. The copy PUT operation is identical to the sequential execution of GET and PUT. Adding the x-amz-copy-source request header causes the PUT operation to copy the source object to the target bucket.\n\nWhen copying an object, you can keep most of the metadata (default) or specify new metadata. To write new metadata when copying an object, you must include the x-amz-metadata-directive: REPLACE header in the request. However, the ACL is not saved and is configured as private for the user making the request.\n\nAll copy requests must be authenticated and cannot contain message text. In addition, you must have READ access to the source object and WRITE access to the target bucket.\n\nA copy request can return an error in two cases. This can happen when a copy request is received, or when objects are being copied. If an error occurs before the copy operation starts, a standard error will be received. If an error occurs during a copy operation, the error will be embedded in the 200 OK response. This means that a 200 OK response can contain both success and error messages.\n\nInquiry:\n\n PUT /copied-example.txt HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-copy-source: /static-images/example.txt\nx-amz-date: 20200831T202253Z\nx-amz-metadata-directive: COPY\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20170710 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-copy-source; x-amz-date; x-amz -metadata-directive; x-amz-storage-class, Signature = 0cb03470dd80bdd41a4b8fb06c1800b27a5059b61b0303fe589578835531c877\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nDate: Mon, 31 Aug 2020 20:22:54 GMT\nx-amz-request-id: tx0000000000000027d8430-005963e21d-1268c-ru-mska\nContent-Length: 183\nConnection: close\n\n\n 2020-08-31T20: 22: 54.167Z\n 7967bfe102f83fb5fc7e5a02bf05e8fc\n\nHeadObject\n\nThe HEAD operation retrieves metadata from an object without returning the object itself. This operation is used only if only the object metadata is needed. In order to use the HEAD operation, you must have read rights to the object (READ).\n\nThe parameters for requesting a HEAD operation are the same as for a GET operation for an object. The response is identical to the GET response, except for the absence of a response body.\n\nIf the requested object does not exist, then the error returned depends on whether the account has additional s3: ListBucket permission.\n\nIf there is s3: ListBucket permission on the bucket, then an error is returned - HTTP status code 404 (\"key missing\").\nIf s3: ListBucket permission is not present, then an error is returned — HTTP status code 403 (“access error”).\n\nInquiry:\n\n HEAD /example.txt HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T185156Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20170710 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = e3b4998b249278fc34\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nDate: Mon, 31 Aug 2020 18:51:58 GMT\nx-amz-request-id: tx0000000000000002ff1c9-00596912ce-6441-ru-mska\nContent-Type: text / plain\nContent-Length: 14\nAccept-Ranges: bytes\nLast-Modified: Mon, 31 Aug 2020 18:40:46 GMT\nEtag: \"b3a92f49e7ae64acbf6b3e76f2040f5e\"\nConnection: close\n\nDelete\n\nDELETE operations delete the object specified in the request.\n\nInquiry:\n\n DELETE /sammy.png HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T194408Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20170710 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = c2a46b21e2e885cd, Signature = c2a46b21b504e858cfd9fdb10b8b21b2e2e85c\n\nAnswer:\n\n HTTP / 1.1 204 No Content\n\nDate: Mon, 31 Aug 2020 19:44:09 GMT\nx-amz-request-id: tx0000000000000027bbc48-005963d908-1268c-ru-mska\nConnection: close\n\nDeleteMultipleObjects\n\nRemoves objects according to the list of keys passed in the request. The list for deletion can contain no more than 1000 keys. If the object does not exist, then the response will mark it as deleted.\n\nFor this request, the Content-MD5 and Content-Length headers are required, and the list of keys for deletion is sent in XML format.\n\nInquiry:\n\n DELETE /sammy.png HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T194408Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20170710 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = c2a46b21e2e885cd, Signature = c2a46b21b504e858cfd9fdb10b8b21b2e2e85c\n\n\n\n true\n\n Key\n\n...\n\n\nAnswer:\n\n HTTP / 1.1 204 No Content\n\nDate: Mon, 31 Aug 2020 19:44:09 GMT\nx-amz-request-id: tx0000000000000027bbc48-005963d908-1268c-ru-mska\n\n\n some / key.txt\n\n some / another / key.txt\n TextErrorCode\n Describing message\n\nConnection: close\n","url":"/en/storage/s3/s3-rest-api/object-api","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"ba217534-cf76-4910-a890-e87b51664e18","title":"ACL","data":"ACL (Access Control List) позволяет контролировать, какие операции разрешены каким пользователям. ACL может стоять как и на уровне всего бакета, так и на уровне конкретного объекта. Установить и прочесть ACL можно через приведенные методы ниже.\n\nОперации с ACL:\n\nGet Bucket ACL - Получить ACL для бакета\nPut Bucket ACL - Установить ACL у бакета\nGet Object ACL - Получить ACL для объекта\nPut Object ACL - Установить ACL у объекта\n\nОбщая XML структура конфигурации ACL:\n\n    eab55955-ebdb-4f18-a94d-f3558ff150da\n    example@example.com\n\n        eab55955-ebdb-4f18-a94d-f3558ff150da\n        VK Cloud_UserName\n\n      WRITE\n\n\nGet Bucket ACL\n\nПолучение ACL для бакета. Для получения необходимо иметь у бакета право \\READ_ACP\\.\n\nЕсли право \\READ_ACP\\ предоставлено анонимному пользователю, то можно получать ACL у бакета без использования заголовка авторизации.\n\nЗапрос:\n\nGET /?acl HTTP/1.1\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T174434Z\nAuthorization: AWS4-HMAC-SHA256 Credential=II5JDQBAN3JYM4DNEB6C/20200831/ru-msk/s3/aws4_request,SignedHeaders=host;x-amz-content-sha256;x-amz-date,Signature=71dfa4666fb740d40d05307a29321c65cc620cdb17e8a9cb83d4f0e1b1b9d236\n\nОтвет:\n\nHTTP/1.1 200 OK\nDate: Mon, 31 Aug 2020 17:44:35 GMT\nx-amz-request-id: tx000000000000002764fa6-005963bd03-1268c-ru-mska\nContent-Type: application/xml\nContent-Length: 848\nConnection: close\n\n      eab55955-ebdb-4f18-a94d-f3558ff150da\n      VK Cloud_UserName\n\n          http://acs.amazonaws.com/groups/global/AllUsers\n\n        READ\n\n          eab55955-ebdb-4f18-a94d-f3558ff150da\n          VK Cloud_Username\n\n        FULL_CONTROL\n\n\nPut Bucket ACL\n\nУстановка ACL для бакета. Для установки ACL необходимо иметь у бакета право \\WRITE_ACP\\.\n\nЕсть два способа установки ACL:\n\nВ теле запроса.\nВ заголовках запроса.\n\nНевозможно использовать для способа одновременно в одном запросе.\n\nЗапрос:\n\nPUT /?acl HTTP/1.1\nContent-Type: application/xml\nContent-Length: 675\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: 724483e3830b19d6960345c484fb7904b26e8f2fb34a6c002fa779353b68c8d8\nx-amz-date: 20200831T183709Z\nAuthorization: AWS4-HMAC-SHA256 Credential=II5JDQBAN3JYM4DNEB6C/20200831/ru-msk/s3/aws4_request,SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date,Signature=1cf3f7771a4086375e5b6597026db6d55d84fbc86e3c3a86ec420ea9123e3163\n\n    eab55955-ebdb-4f18-a94d-f3558ff150da\n\n        eab55955-ebdb-4f18-a94d-f3558ff150da\n\n      FULL_CONTROL\n\n        http://acs.amazonaws.com/groups/global/AllUsers\n\n      READ\n\n\nОтвет:\n\nHTTP/1.1 200 OK\nDate: Mon, 31 Aug 2020 18:37:10 GMT\nx-amz-request-id: tx00000000000000278ac49-005963c956-1268c-ru-mska\nContent-Type: application/xml\nContent-Length: 0\nConnection: close\n\nGet Object ACL\n\nПолучение ACL для объекта. Для получения необходимо иметь у объекта право \\READ_ACP\\.\n\nЗапрос:\n\nGET /sammy.png?acl HTTP/1.1\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T191224Z\nAuthorization: AWS4-HMAC-SHA256 Credential=II5JDQBAN3JYM4DNEB6C/20200831/ru-msk/s3/aws4_request,SignedHeaders=host;x-amz-content-sha256;x-amz-date,Signature=950e133849cd19d626291fd2937d927957cf3e97a36707d30d51a9b61ac08a8e\n\nОтвет:\n\nHTTP/1.1 200 OK\nDate: Mon, 31 Aug 2020 19:12:24 GMT\nx-amz-request-id: tx0000000000000027a42dc-005963d198-1268c-ru-mska\nContent-Type: application/xml\nContent-Length: 848\nConnection: close\n\n      eab55955-ebdb-4f18-a94d-f3558ff150da\n      VK Cloud_UserName\n\n          http://acs.amazonaws.com/groups/global/AllUsers\n\n        READ\n\n          eab55955-ebdb-4f18-a94d-f3558ff150da\n          VK Cloud_UserName\n\n        FULL_CONTROL\n\n\nPut Object ACL\n\nУстановка ACL для объекта. Для установки ACL необходимо иметь у объекта право WRITE_ACP.\n\nЕсть два способа установки ACL:\n\nВ теле запроса.\nВ заголовках запроса.\n\nНевозможно использовать для способа одновременно в одном запросе.\n\nЗапрос:\n\nPUT /sammy.png?acl HTTP/1.1\nСontent-Type: application/xml\nContent-Length: 443\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256:c0bd9ba784be78d4f38bbc1e3b0da2de2e7a8f4ee259b3b840369cf00a78dad2\nx-amz-date:20200831T192142Z\nAuthorization: AWS4-HMAC-SHA256 Credential=II5JDQBAN3JYM4DNEB6C/20200831/ru-msk/s3/aws4_request,SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date,Signature=dfeeb2386f76b29097adadb35ac15f7d5f244f18cc95f082b0ac6d14ced48b10\n\n    eab55955-ebdb-4f18-a94d-f3558ff150da\n\n        eab55955-ebdb-4f18-a94d-f3558ff150da\n\n      FULL_CONTROL\n\n\nОтвет:\n\nHTTP/1.1 200 OK\nDate: Mon, 31 Aug 2020 19:21:42 GMT\nx-amz-request-id: tx0000000000000027aafc9-005963d3c6-1268c-ru-mska\nContent-Type: application/xml\nContent-Length: 0\nConnection: close\n","url":"/en/storage/s3/s3-rest-api/acl-api","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"137af533-914e-4a3a-b99e-c3608e9f54fe","title":"Multipart","data":"Composite or multipart loading allows you to save objects in Cloud Storage in parts. This can be useful when loading or copying large objects. We recommend using multiple uploads for objects from 100 MB.\n\nComposite loading consists of the following steps:\n\nInitiate Multipart Upload - Initialize Upload\nUpload Part - Upload an object in parts\nComplete Multipart Upload - Completing the upload by combining previously uploaded parts\nAbort Multipart Upload - Abort upload\nList Parts - Returns a list of loaded parts\n\nInitiate Multipart Upload\n\nThe operation initializes a multipart download and returns the download ID. The load ID is used to combine all parts of a single multipart load. You must include this download ID in each of your subsequent download requests for a part.\n\nIf a lifecycle rule is configured to abort unfinished multipart downloads, then the download must complete within the number of days specified in the bucket lifecycle configuration. Otherwise, an interrupt operation becomes available for the unfinished multipart download, and the service aborts the multipart download.\n\nNote\n\nAfter the initialization of the multipart download and the actual download of one or more parts, you must complete or interrupt the multipart download in order to suspend the charge for storing the downloaded parts. Only after the completion or interruption of a multipart upload does Cloud Storage free up the space allocated to the parts in the storage and stop charging for the storage of these parts.\n\nInquiry:\n\n POST /multipart-file.tar.gz?uploads HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T174652Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = e356964d19e38cd5d44\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nContent-Length: 258\nContent-Type: application / xml\nDate: Mon, 31 Aug 2020 17:46:53 GMT\nx-amz-request-id: tx00000000000000ab66a13-005991e20d-66a8-ru-mska\nConnection: close\n\n\n\n static-images\n multipart-file.tar.gz\n 2 ~ iCw_lDY8VoBhoRrIJbPMrUqnE3Z-3Qh\n\nUpload Part\n\nThe operation loads a portion of a multipart load. To perform this operation, you must provide data from the part in the request. To load a part from an existing object, use the load part (copy) operation.\n\nA part can have any number from 1 to 10,000 inclusive. The part number uniquely identifies the part and its position in the created object. If a new part is loaded, with the assigned part number used for one of the existing parts, the existing part will be overwritten. Each part, except the last, must be at least 5 MB in size. The last part of the multipart download has no size limit.\n\nTo ensure that the data is not tampered with as it travels over the network, you should include the Content-MD5 header in the download request for the part. VK Cloud checks the data from the parts against the provided MD5 hash value and returns an error if they do not match.\n\nInquiry:\n\n PUT /multipart-file.tar.gz?partNumber=1&uploadId=2~iCw_lDY8VoBhoRrIJbPMrUqnE3Z-3Qh HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nContent-Length: 5242880\nx-amz-content-sha256: 6ab0931e613fd0cd39af0fa4733edb6ad942df0ad33e057713d44df7195b6ecf\nx-amz-date: 20200831T184459Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = 2e4adca4d4d4404274f2e4adca4d17d440430\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nContent-Length: 0\nContent-Type: application / xml\nDate: Mon, 31 Aug 2020 18:45:01 GMT\nEtag: \"d8d3ed3a4de016917a814a2cf5acad3c\"\nx-amz-request-id: tx00000000000000ab85dab-005991efac-66a8-ru-mska\nConnection: close\n\nComplete Multipart Upload\n\nThe operation completes the multi-part download by combining the previously loaded parts. Upon receiving this request, VK Cloud merges all loaded parts in ascending order by part number, creating a new object. In a request to complete a multipart download, you must provide a list of parts. For each part from the list, you must provide the part number and the ETag header value returned after that part is loaded.\n\nIt can take several minutes to process a request to complete a multipart upload. After starting to process the request, VK Cloud sends an HTTP response header containing a 200 OK response. Whitespace is periodically sent during request processing to prevent time-outs. Since an error can occur in a request after the initial 200 OK response has been sent, the body of the response should be checked to determine if the request was successful.\n\nInquiry:\n\n POST /multipart-file.tar.gz?uploadId=2~iCw_lDY8VoBhoRrIJbPMrUqnE3Z-3Qh HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nContent-Length: 5242880\nx-amz-content-sha256: 6ab0931e613fd0cd39af0fa4733edb6ad942df0ad33e057713d44df7195b6ecf\nx-amz-date: 20200831T184459Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = 2e4adca4d4d4404274f2e4adca4d17d440430\n\n\n\n 1\n \"d8d3ed3a4de016917a814a2cf5acad3c\"\n\n 2\n \"adf5feafc0fe4632008d5cb30beb1c49\"\n\n 3\n \"363f6bb50866541d78e5f6f626592263\"\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nContent-Length: 311\nContent-Type: application / xml\nDate: Mon, 31 Aug 2020 18:45:01 GMT\nx-amz-request-id: tx00000000000000ab962c8-005991f6fe-66a8-ru-mska\nConnection: close\n\n\n\n my-test-bucket1.hb.vkcs.cloud\n my-test-bucket1\n multipart-file.tar.gz\n f935869350d7cbfcdd219df3f377531b-3\n\nAbort Multipart Upload\n\nThe operation aborts the multipart download. After aborting a multipart download, it is not possible to download additional parts using the download ID of the interrupted multipart download. The space allocated for storing the previously loaded parts will be freed. Moreover, if any parts are loaded, then such an operation may be completed or interrupted. As a result, it may be necessary to repeatedly interrupt the multi-part download in order to completely free up the space occupied by all parts.\n\nInquiry:\n\n DELETE /multipart-file.tar.gz?uploadId=2~iCw_lDY8VoBhoRrIJbPMrUqnE3Z-3Qh HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nContent-Length: 5242880\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T202611Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = 9e13513baz0e3473\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nDate: Mon, 31 Aug 2020 18:45:01 GMT\nx-amz-request-id: tx00000000000000abbaefe-0059920764-66a8-ru-mska\nConnection: close\n\nList Parts\n\nThe operation returns a list of parts loaded for a specific multipart load. The operation must include the download ID, which was received after the request was sent to initialize the multipart download. Such a query will return no more than 1000 loaded parts. You can limit the number of parts returned by specifying the max-parts query parameter. If the multipart load contains more than 1000 parts, then the response returns a NextPartNumberMarker element and an IsTruncated field with the value true. Subsequent requests to view the list of parts can include the part-number-marker query string parameter by setting it to the NextPartNumberMarker field value from the previous response.\n\nInquiry:\n\n PUT /multipart-file.tar.gz?partNumber=1&uploadId=2~iCw_lDY8VoBhoRrIJbPMrUqnE3Z-3Qh HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nContent-Length: 5242880\nx-amz-content-sha256: 6ab0931e613fd0cd39af0fa4733edb6ad942df0ad33e057713d44df7195b6ecf\nx-amz-date: 20200831T184459Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = 2e4adca4d4d4404274f2e4adca4d17d440430\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nContent-Length: 0\nContent-Type: application / xml\nDate: Mon, 31 Aug 2020 18:45:01 GMT\nEtag: \"d8d3ed3a4de016917a814a2cf5acad3c\"\nx-amz-request-id: tx00000000000000ab85dab-005991efac-66a8-ru-mska\nConnection: close\n\n\n\n my-test-bucket1\n multipart-file.tar.gz\n 2 ~ iCw_lDY8VoBhoRrIJbPMrUqnE3Z-3Qh\n STANDARD\n 0\n 1\n 1000\n false\n\n eab55955-ebdb-4f18-a94d-f3558ff150da\n VK Cloud_UserName\n\n 2017-08-14T18: 45: 01.601Z\n 1\n \"d8d3ed3a4de016917a814a2cf5acad3c\"\n 5242880\n\n 2017-08-14T18: 45: 01.601Z\n 2\n \"adf5feafc0fe4632008d5cb30beb1c49\"\n 5242880\n","url":"/en/storage/s3/s3-rest-api/multipart-api","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"959b1130-542b-466f-ab13-8f8cdc55284c","title":"Lifecycle","data":"All available methods for working with object lifecycle:\n\nGet Bucket Lifecycle Configuration - View the lifecycle configuration in a bucket\nConfigure Bucket Lifecycle - Configure lifecycle configuration rules\nDelete Bucket Lifecycle - Deletes lifecycle configuration\n\nGeneral view of the configuration\n\n\n Rule Description\n {Enabled | Disabled}\n\n key_prefix\n\n Storage class identifier\n or  ->\n\n...\n\n or  ->\n\n...\n\n\n...\n\n...\n\nGet Bucket Lifecycle Configuration\n\nIn a lifecycle configuration for a bucket, you can specify a lifecycle rule by prefixing the object key name, one or more object tags, or a combination of both. The response contains a filter item that you can use to change filter options and select a subset of objects to which the rule should apply.\n\nThe GET operation returns information on the lifecycle configuration set for the bucket.\n\nInquiry:\n\n GET /? Lifecycle HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T001757Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = e92d830826dad4ad3\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nDate: Mon, 31 Aug 2020 17:44:35 GMT\nx-amz-request-id: tx000000000000000023935-005a613936-fcf92-ru-mska\nContent-Type: application / xml\nContent-Length: 456\nConnection: close\n\n\n\n Expire old logs\n logs /\n Enabled\n\n 90\n\n Remove uncompleted uploads\n Enabled\n\n 1\n\nConfigure Bucket Lifecycle\n\nIn the lifecycle configuration for a bucket, you can specify a lifecycle rule by prefixing the object key name.\n\nThe PUT operation creates a new lifecycle configuration for the bucket or replaces an existing lifecycle configuration.\n\nInquiry:\n\n PUT /? Lifecycle HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nContent-Length: 456\nContent-Type: application / xml\nx-amz-content-sha256: 34850007f92ec3331486b48fd7db15f48315fe73c4a9b135e6d9fd629276c1e7\nx-amz-date: 20200831T000345Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = content-md5; content-type; host; x-amz-content-sha256; x-amz-faca09db5a56; x-amz-fac6e-395a5a56; x-amz-fc-date739adature5;\n\n\n\n Expire old logs\n logs /\n Enabled\n\n 90\n\n Remove uncompleted uploads\n Enabled\n\n 1\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nDate: Mon, 31 Aug 2020 17:31:43 GMT\nx-amz-request-id: tx00000000000000010ad2b-005a6135e2-f647d-ru-mska\nContent-Length: 0\nContent-Type: application / xml\nConnection: close\n\nDelete bucket lifecycle\n\nThe DELETE operation removes the lifecycle configuration from the specified bucket. Removes all lifecycle configuration rules from the lifecycle subresource associated with the bucket, allowing expiration to be excluded from objects. As a result, the service will no longer automatically delete objects according to the rules contained in the remote lifecycle configuration.\n\nInquiry:\n\n DELETE /? Lifecycle HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T204101Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = 376fe4178b33fe3641793\n\nAnswer:\n\n HTTP / 1.1 204 No Content\n\nDate: Mon, 31 Aug 2020 18:13:21 GMT\nConnection: close\n","url":"/en/storage/s3/s3-rest-api/lifecycle-api","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"70a24c0b-3150-4e5b-989f-1670c7956d05","title":"CORS","data":"The Cloud Storage service allows you to manage the CORS bucket configuration. To load the CORS configuration, you need to generate an XML document.\n\nList of available methods for CORS bucket configurations:\n\nGet Bucket CORS - Get a list of CORS configurations for a bucket\nSet Bucket CORS - Set the CORS configuration\nDelete Bucket CORS - Delete CORS configuration\n\nGeneral view of XML CORS configuration\n\n\n URL\n HTTP_Method\n Header_Name\n...\n\n...\n\nGet Bucket CORS\n\nThe operation returns information on the CORS configuration set for the bucket.\n\nIn order to use this operation, you must have WRITE_ACP write rights. The bucket owner has this permission by default and can grant it to other users.\n\nInquiry:\n\n GET /? Cors HTTP / 1.1\n\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T185319Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = host; x-amz-content-sha256; x-amz-date, Signature = f7d7d387996d9d300\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\n\nDate: Mon, 31 Aug 2020 18:53:20 GMT\nx-amz-request-id: tx00000000000000279651f-005963cd20-1268c-ru-mska\nContent-Type: application / xml\nContent-Length: 390\nConnection: close\n\n\n\n\n PUT\n DELETE\n POST\n http://example.com\n \\*\n\n GET\n \\*\n\nSet Bucket CORS\n\nThe PUT operation sets the CORS configuration for the bucket. If the configuration already exists, it is overwritten.\n\nIn order to use this operation, you must have WRITE_ACP write rights.\n\nYou can set this configuration on a bucket so that it can serve Cross-origin requests. For example, you can grant access to a request from the source http://www.example.com to the my.example.bucket.com bucket using the XMLHttpRequest browser functionality.\n\nTo enable cross-origin resource sharing (CORS) on a bucket, you need to add a CORS subresource to the bucket. A CORS subresource is an XML document that configures rules that define HTTP sources and methods that can be used in your bucket. The maximum document size is 64 KB. For example, a CORS configuration on a bucket might have the following two rules set:\n\nThe first CORSRule rule allows cross-origin PUT, POST and DELETE requests from https://www.example.com. This rule also allows all headers in a preflight OPTIONS request using the Access-Control-Request-Headers header. Therefore, in response to any OPTIONS pre-flight request, the service returns any requested header.\nThe second rule allows cross-origin GET requests from all sources. The wildcard \"\\*\" indicates that any source can be used.\n\nInquiry:\n\n PUT /? Cors HTTP / 1.1\n\n\nHost: my-test-bucket1.hb.vkcs.cloud\nContent-Length: 374\nContent-Type: application / xml\nx-amz-content-sha256: 745320970930725bd18820ec990f7334960f0a47358be189e77504cc094be77e\nx-amz-date: 20200831T185043Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = content-md5; content-type; host; x-amz-content-sha256; x-amz-fs6beed9ad2cc-amz-amz-content-sha256; x-amz-86-a-d9ed9ecc07;\n\n\n\n\n http://example.com\n\n PUT\n POST\n DELETE\n\n \\*\n\n \\*\n GET\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\n\nDate: Mon, 31 Aug 2020 18:50:44 GMT\nx-amz-request-id: tx0000000000000027946fc-005963cc84-1268c-ru-mska\nContent-Type: application / xml\nContent-Length: 0\nConnection: close\n\nDelete Bucket CORS\n\nThe DELETE operation removes the CORS configuration information set for the bucket.\n\nIn order to use this operation, you must have WRITE_ACP write rights.\n\nInquiry:\n\n DELETE / static-images? Cors HTTP / 1.1\n\n\nHost: my-test-bucket1.hb.vkcs.cloud\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20200831T182537Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = content-md5; content-type; host; x-amz-content-sha256; x-amz-date, Signature = e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n\nAnswer:\n\n HTTP / 1.1 204 No Content\n\n\nDate: Mon, 31 Aug 2020 18:25:38 GMT\nx-amz-request-id: tx0000000000000002fae1f-0059690ca2-6441-ru-mska\nConnection: close\n","url":"/en/storage/s3/s3-rest-api/cors-api","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"f3d76867-a9b1-45ef-9fff-5161f061301c","title":"Prefix access keys","data":"general information\n\nPrefix access keys allow you to create users for whom access is restricted to a specific path. These users are bound to buckets for which their prefix works. There are the following restrictions when working with prefix keys:\n\nthe user can only be bound to 1 bucket (at creation)\nthe user will not have access to other buckets and the contents of his bucket, for which the path will not start with the prefix of this user\naccess keys can only be obtained upon creation\na user can only have 1 pair of keys\nfor ease of use, it is recommended to name users associated with the prefix by which they have access. For example username = user / folder1 / file1; prefix = folder1 / file1\n\nGenerating prefix keys\n\nSupported PAK methods\n\nCreatePrefixKey\nListPrefixKeys\nDeletePrefixKey\n\nDescription\n\nCreate user examplename with prefix keys to access the path exampleprefix. One such user can only have one pair of prefix keys\n\nRequest\n\nPUT /? Pak & username = examplename & prefix = exampleprefix HTTP / 1.1\nHost: bucketName.hb.vkcs.cloud\nDate: Wed, 14 Feb 2018 11:21:57 GMT\nAuthorization: authorization string\nConnection: close\n\nRequest parameters\n\nusername - the user who will own the keys\nprefix - the prefix of the path that will be available for these keys\n\nAnswer\n\nHTTP / 1.1 200 OK\n\nServer: nginx / 1.12.1\nDate: Wed, 14 Feb 2018 13:38:54 GMT\nContent-Type: application / xml\nContent-Length: 351\nConnection: close\nX-req-id: 2NrqrYuki\n\n\n\n     bucketName\n     exampleprefix\n     examplename\n     LVQaicnPLR7eVg5soGgWCJjGe3w3S8toaRxd329xv4w\n     6hGka6NefpEoNse4xJOEx3\n\nDescription of XML elements\n\nBucketName - the name of the bucket\nPrefix - path prefix that will be available for these keys\nUserName - the user who owns the keys\nSecretKey - a secret key, it is returned only upon creation, it is impossible to get the key later\nAccessKey - public key\n\nList of prefix users\n\nInquiries\n\nGET /? Pak & marker = prefix & max-keys = 2 & name-prefix = prefix HTTP / 1.1\nAuthorization: authorization string\nConnection: close\nDate: Wed, 14 Feb 2018 12:28:10 GMT\nHost: bucketName.hb.vkcs.cloud\n\nRequest parameters\n\nmax-keys - the maximum number of items in the optional listing\nname-prefix - username prefix optional\nmarker - the name or part of the username from which the listing will begin optional\n\nAnswer\n\nHTTP / 1.1 200 OK\n\nServer: nginx / 1.12.1\nDate: Wed, 14 Feb 2018 13:39:27 GMT\nContent-Type: application / xml\nContent-Length: 457\nConnection: close\nX-req-id: 35RBZWzAX\n\n\n\n     bucketName\n     false\n     prefix\n     2\n     prefix\n\n           prefixusers / prefix / for1\n           prefix / for1\n\n           prefixusers / prefix / for2\n           prefix / for2\n\n\nDescription of XML elements\n\nBucketName - the name of the bucket\nIsTruncated - true if only part of prefixed users are displayed\nMarker - the name or part of the username from which the listing will begin\nNamePrefix - username prefix\nMaxKeys - the maximum number of items in the listing\nContents - block containing the user\nUserName - username\nPrefix - prefix by which data is available for this user\n\nRemoving a prefix key from a user\n\nRequest\n\nDELETE /? Pak & prefix = prefix% 2Ffor1 & username = prefixusers% 2Fprefix% 2Ffor1 HTTP / 1.1\nAuthorization: authorization string\nConnection: close\nDate: Wed, 14 Feb 2018 13:05:31 GMT\nHost: bucketName.hb.vkcs.cloud\n\nRequest parameters\n\nusername - username\nprefix - the path where objects are available for this user optional\n\nAnswer\n\nHTTP / 1.1 200 OK\n\nServer: nginx / 1.12.1\nDate: Wed, 14 Feb 2018 13:39:27 GMT\nContent-Type: application / xml\nContent-Length: 207\nConnection: close\nX-req-id: 33yVonNmQ\n\n\n\n      prefixusers / prefix / for1\n      prefix / for1\n\nDescription of XML elements\n\nPrefix - path prefix that will be available for these keys\nUserName - the user who owns the keys\n","url":"/en/storage/s3/s3-rest-api/pak-api","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"f308615d-dbc7-41ba-beed-6b093d15b711","title":"Webhooks","data":"Webhooks for Cloud Storage is the ability to configure sending HTTP / S requests on events for a bucket using an API. For example, you can:\n\nconfigure processing and converting files after upload\nintegrate with any external systems\nconfigure logging for object storage\n\nThe list of events (Event) for which it is possible to configure the Webjook configuration:\n\ns3: ObjectCreated: \\* - PutObject, PutObjectCopy, CompleteMultipartUpload\ns3: ObjectCreated: Put - PutObject\ns3: ObjectCreated: Copy - PutObjectCopy\ns3: ObjectCreated: CompleteMultipartUpload - CompleteMultipartUpload\ns3: ObjectRemoved: \\* - DeleteObject\ns3: ObjectRemoved: Delete - DeleteObject\n\nThe following methods are available for working with WebHooks:\n\nPutBucketNotificationConfiguration\nGetBucketNotificationConfiguration\n\nGeneral XML configuration\n\n PUT /? Notification HTTP / 1.1\nHost: Bucket.hb.vkcs.cloud\n\n\n\n\n string\n string\n string\n...\n\n\n\n string\n string\n\n...\n\n\n\n...\n\nPut Bucket Notification Configuration\n\nThe PUT method allows you to enable notification about some event (PutObject, DeleteObject, etc.) in the bucket.\n\nAt the moment, 1 type of event is supported - SimpleTopicConfiguration - a request for a url provided by the user.\n\nExample: It is required to execute a request to url http://test.com with PutObject into bucket bucketA of objects whose names match the mask image / \\*. Png.\n\nInquiry:\n\n PUT /? Notification HTTP / 1.1\n\nHost: bucketA.hb.vkcs.cloud\nContent-Length: 456\nContent-Type: application / xml\nx-amz-content-sha256: 34850007f92ec3331486b48fd7db15f48315fe73c4a9b135e6d9fd629276c1e7\nx-amz-date: 20200831T000345Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = content-md5; content-type; host; x-amz-content-sha256; x-amz-faca09db5a56; x-amz-fac6e-395a5a56; x-amz-fc-date739adature5;\n\n\n\n\n 1\n s3: ObjectCreated: Put\n http://test.com\n\n Prefix\n image /\n\n Suffix\n .png\n\nAnswer:\n\n HTTP / 1.1 200 OK\n\nDate: Mon, 31 Aug 2020 17:31:43 GMT\nx-amz-request-id: tx00000000000000010ad2b-005a6135e2-f647d-ru-mska\nContent-Length: 0\nContent-Type: application / xml\nConnection: close\n\nGetBucketNotificationConfiguration\n\nReturns the current rule configuration (SimpleNotificationConfiguration) of the bucket. If no rules have been set for this bucket, an empty NotificationConfiguration element will be returned.\n\nInquiry:\n\n GET /? Notification HTTP / 1.1\n\nHost: my-test-bucket1.hb.vkcs.cloud\nContent-Length: 456\nContent-Type: application / xml\nx-amz-content-sha256: 34850007f92ec3331486b48fd7db15f48315fe73c4a9b135e6d9fd629276c1e7\nx-amz-date: 20200831T000345Z\nAuthorization: AWS4-HMAC-SHA256 Credential = II5JDQBAN3JYM4DNEB6C / 20200831 / ru-msk / s3 / aws4_request, SignedHeaders = content-md5; content-type; host; x-amz-content-sha256; x-amz-faca09db5a56; x-amz-fac6e-395a5a56; x-amz-fc-date739adature5;\n\nAnswer:\n\n HTTP / 1.1 200\n\nDate: Mon, 31 Aug 2020 17:31:43 GMT\nx-amz-request-id: tx00000000000000010ad2b-005a6135e2-f647d-ru-mska\nContent-Length: 0\nContent-Type: application / xml\nConnection: close\n\n\n\n\n 1\n s3: ObjectCreated: Put\n http://test321.com\n\n Prefix\n image /\n\n Suffix\n .png\n\n 2\n s3: ObjectRemoved: Delete\n http://test123.com\n\n Prefix\n image /\n\n Suffix\n .png\n\nWebhook execution example\n\nFor an example of the established rules, when loading objects into bucketA with the names image / \\*. Png, the following request will come:\n\n POST  HTTP / 1.1\nx-amz-sns-messages-type: SubscriptionConfirmation\n\n{\"Records\":\n[\n{\n\"s3\": {\n\"object\": {\n\"eTag\": \"aed563ecafb4bcc5654c597a421547b2\",\n\"sequencer\": 1577453615,\n\"key\": \"some-file-to-bucket\",\n\"size\": 100\n},\n\"configurationId\": \"1\",\n\"bucket\": {\n\"name\": \"bucketA\",\n\"ownerIdentity\": {\n\"principalId\": \"mcs2883541269\"}\n},\n\"s3SchemaVersion\": \"1.0\"\n},\n\"eventVersion\": \"1.0\",\n\"requestParameters\": {\n\"sourceIPAddress\": \"185.6.245.156\"\n},\n\"userIdentity\": {\n\"principalId\": \"2407013e-cbc1-415f-9102-16fb9bd6946b\"\n},\n\"eventName\": \"s3: ObjectCreated: Put\",\n\"awsRegion\": \"ru-msk\",\n\"eventSource\": \"aws: s3\",\n\"responseElements\": {\n\"x-amz-request-id\": \"VGJR5rtJ\"\n}\n}\n]\n}\n","url":"/en/storage/s3/s3-rest-api/webhook-api","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Storage","link":"/en/storage/s3","isActive":false,"evenSimpler":true}]},{"uuid":"965a5380-d93a-4c29-9fd7-da32ce103243","title":"Creating backups","data":"Automatic and manual modes of creating backups of virtual machines are available on the VK Cloud platform.\n\nUse automatic mode to create backups regularly.\n\nUse manual mode to create backups before operations involving the risk of data loss.\n\n\n\nA backup is created for all VM disks.\n\nAutomatic backup\n\n\n\nPersonal account\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud Computing → Backup.\nGo to the Automatic tab.\nClick the Add.\nEnter the Name of plan for backup.\nChoose retention policy full backups using the switch: Enable GFS or Maximum number of full backups.\nConfigure the storage of full backups.\n\n   For GFS strategy:\n      Specify how many weeks Keep weekly full backups.\n      If necessary, enable the option Keep monthly full backups and specify the number of months.\n      If necessary, enable the option Keep yearly full backups and specify the number of years.\n\n   For option Maximum number of full backups set the quantity limit full backups. When the limit is reached, the oldest backups will be deleted automatically.\n\nSpecify which backups to create: full only or full and incremental.\n\n   If you only want to create full backups, select one or more days in the Backup schedule field and enter the time.\n   If you need to create both full and incremental backups, enable the option Enable incremental backups, choose Full backup create date and enter the time. Incremental backups will be created on the remaining days.\n\n\n      Option Enable incremental backups allows you to speed up the creation of backups, reduce their volume and reduce storage costs, but the recovery time of a VM from such a copy will be longer.\n\n\nSelect the required virtual machines in the field Apply to the following instances.\nClick the Save plan.\n\n\n\n\nThe created backup plan will appear in the list. Read more about working with plans and backups in the article Managing backups.\n\nManual backup\n\n\n\nPersonal account\n\n\n\nGo to personal account VK Cloud.\nCreate a backup copy of the VM in one of the ways.\n\n   In the section Backup:\n\n      Go to Cloud Computing → Backup.\n      Go to the Manual tab.\n      Click Add.\n      In the window that opens, fill in the fields:\n         if necessary, enter a Comments;\n         in the Instance field, select the appropriate VM.\n      Click Create backup.\n\n   Via the VM context menu:\n\n      Go to Cloud Computing → Virtual machines.\n      Click   for the VM for which you want to create a backup, and select Create backup.\n      If necessary, enter Comments.\n      Click Create backup.\n\n   On the virtual machine page:\n\n      Go to Cloud Computing → Virtual machines.\n      In the list of virtual machines, find the VM for which you want to create a backup.\n      Click on the name of this VM.\n      On the VM page, go to the General Information tab.\n      Above the table with VM parameters, click Create backup.\n      If necessary, enter a Comments.\n      Click Create backup.\n\n\n\n\nThe created backups, grouped by virtual machines, are available in the Backup section on the Manual tab. To view the list of backups of a VM created manually, click on the number of recovery points in the row of the VM you need.\n\nFor more information about working with backups, see the article Managing backups.\n","url":"/en/storage/backups/vm-backup/vm-backup-create","description":"Backup of virtual machines in automatic and manual mode.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Backup","link":"/en/storage/backups","isActive":false,"evenSimpler":true}]},{"uuid":"7173c73f-f340-4e7b-ba4c-bf1bd48f7da5","title":"Managing backups","data":"Restoring a VM from a backup\n\n\n\nPersonal account\n\n\n\nGo to personal account VK Cloud.\nGo to the Backup section in one of the ways.\n\n   Through the services menu: Cloud Computing → Backup.\n\n   Through the context menu of the virtual machine:\n\n      Go to Cloud Computing → Virtual machines.\n      Click   for the required VM.\n      Select Restore from backup.\n\nOpen the required backup list.\n\n   Copies created automatically:\n\n      Go to the Automatic tab.\n      In the list of plans, find the required plan.\n      Click on the name of the plan or click   for the plan and select View backups.\n\n   Copies created manually:\n\n      Go to the Manual tab.\n      In the list of virtual machines, find the VM you need.\n      Click on the number of recovery points in the row of the required VM.\n\nSelect the backup to restore.\n\n   In the list of backups created automatically:\n\n      Click   for the required copy.\n      Select Restore instance.\n      In the window that opens, select the backup.\n      Click Restore instance from backup.\n\n   In the list of manually created backups:\n\n      Click   for the required copy.\n      Select Restore from backup.\n\nOn the page Restoring instance from backup choose Restoring type using the button To a new instance or To the existing instance.\n\n   To restore to a new VM, specify the necessary parameters. The remaining parameters will be restored from the backup.\n\n\n      Virtual machines created on a private network will not work when restored to an external ext-net network.\n\n\n   To restore to the original VM, do not specify additional parameters. The disks of the target VM will be restored without changing their properties. The VM will be rebooted.\n\nClick Restore instance.\n\n\n\n\n\n\nVM recovery speed from backup is affected by:\n\nBackup type** — Recovery from an incremental copy takes longer, since data from the full copy is restored first, and then changes from later incremental copies are added sequentially.\nDisk type** — If the original VM had an SSD disk type, recovery in a VM with a HDD disk type will be slower.\nRecovery type** — Recovery to the original VM takes less time.\n\nRestoring a VM disk from a backup\n\n\n\nPersonal account\n\n\n\nGo to personal account VK Cloud.\nGo to the Backup section in one of the ways.\n\n   Through the services menu: Cloud Computing → Backup.\n\n   Through the context menu of the virtual machine:\n\n      Go to Cloud Computing → Virtual machines.\n      Click   for the required VM.\n      Select Restore from backup.\n\nOpen the required backup list.\n\n   Copies created automatically:\n\n      Go to the Automatic tab.\n      In the list of plans, find the required plan.\n      Click on the name of the plan or click   for the plan and select View backups.\n\n   Copies created manually:\n\n      Go to the Manual tab.\n      In the list of virtual machines, find the VM you need.\n      Click on the number of recovery points in the row of the required VM.\n\nSelect the backup to restore.\n\n   In the list of backups created automatically:\n\n      Click   for the required copy.\n      Select Restore instance.\n      In the window that opens, select the backup.\n      Click Restore Volume.\n\n   In the list of manually created backups:\n\n      Click   for the required copy.\n      Select Restore Volume.\n\nTo the form Select a backup to restore the volume select the disk for which you plan to restore.\nClick Restore a volume form backup.\nIn the window that appears:\n\n   Instance: select the VM from the backups for which the disk is being restored.\n   Recovery Volume: specify the disk of the selected VM to restore.\n   Disk Name: if necessary, specify the name of the new disk.\n   Availability zone: select the data center where the VM will be launched.\n   Disk Type: select one of the values — HDD, SSD or High-IOPS SSD. For more information, see Cloud Servers service overview.\n   Attach a volume to a VM: enable the option if you want to connect the disk to an existing VM from the list Virtual machine.\n\n\n\n\nAs a result of recovery, a new disk will be created and connected to the VM. The old disk will not be removed.\n\n\n\nIf you no longer need the disk, remove it manually to save resources.\n\nDeleting backups\n\n\n\nPersonal account\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud computing → Backup.\nOpen the required backup list.\n\n   Copies created automatically:\n      Go to the Automatic tab.\n      In the list of plans, find the required plan.\n      Click on the name of the plan or Click   for the plan and select View backups.\n\n   Copies created manually:\n      Go to the Manual tab.\n      In the list of virtual machines, find the VM whose backups you want to delete.\n      Click on the number of recovery points in the row of this VM.\n\nDelete outdated backups in one of the ways.\n\n   Using group operations — for multiple copies:\n\n      In the list of backups, select the copies using the checkboxes.\n      Above the list of plans, click Delete.\n      Confirm the action.\n\n   Through the context menu — for one copy:\n\n      Click   for the copy you want to delete.\n      Select Delete copy and confirm the action.\n\n\n\n\n\n\nDelete unnecessary backups in a timely manner to save storage space.\n\nChanging the backup plan\n\n\n\nPersonal account\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud computing → Backup.\nGo to the Automatic tab.\nIn the list of backup plans, find the plan you need.\nUse one of the methods to go to the backup plan change page.\n\n   Through the context menu:\n      Click   for the plan.\n      Select Edit.\n\n   Using the button:\n      Select a plan using the checkbox.\n      Above the list of plans, click the Edit button.\n\nChange the required parameters.\n\n   Name of plan for backup.\n   Retention policy full backups using the switch: Enable GFS or Maximum number of full backups.\n      For GFS strategy: retention periods of full backups.\n      For option Maximum number of full backups: the quantity limit full backups. When the limit is reached, the oldest backups will be deleted automatically.\n\n   Which backups to create: full only or full and incremental.\n\n      If you only want to create full backups, select one or more days in the Backup schedule field and enter the time.\n      If you need to create both full and incremental backups, enable the option Enable incremental backups, choose Full backup create date and enter the time. Incremental backups will be created on the remaining days.\n\n\n         Option Enable incremental backups allows you to speed up the creation of backups, reduce their volume and reduce storage costs, but the recovery time of a VM from such a copy will be longer.\n\n\nClick Save plan.\n\nActivating, stopping, and deleting a backup plan\n\n\n\nWhen you delete a backup plan, all backups created under that plan will be deleted.\n\n\n\n\n\nPersonal account\n\n\n\nGo to personal account VK Cloud.\nGo to Cloud computing → Backup.\nGo to the Automatic tab.\nChange the state of the backup plan in one of the ways.\n\n   Using group operations — for multiple plans:\n\n      In the list of backup plans, select the required plans using the checkboxes.\n      Above the list of plans, click the button with the required action.\n      Confirm the action.\n\n   Through the context menu — for one plan:\n\n      Click   for the required plan.\n      Select and confirm the action.\n\n\n\n","url":"/en/storage/backups/vm-backup/vm-backup-manage","description":"Managing backup plans and backups.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Backup","link":"/en/storage/backups","isActive":false,"evenSimpler":true}]},{"uuid":"d7d6f2dd-1eb7-40cf-b495-1fa5b20b6362","title":"Backups FAQ","data":"\n\nHow do I make a VM backup?\n\nYou can create a VM backup manually or using a backup plan, for more information, see Managing backups.\n\n\n\n\n\nHow many backups can be stored?\n\nThe maximum number of stored automatic backups is 200. When the storage limit is reached, old backups will be automatically deleted.\n\n\n\n\n\nWill the backups be lost?\n\nBackups are stored in secure object storage, which is automatically replicated to preserve data integrity.\n\nBackups are not deleted in case of hardware or service failures.\n\n\n\n\n\nHow long does it take to create a backup?\n\nThe time to create a backup depends on the amount of data on the disk of the VM that is being backed up.\n\n\n\n\n\nHow do I delete a backup?\n\nYou can delete a backup in the section Cloud Computing → Backup, for more information in the article Managing backups.\n\n\n\n\n\nWhere is the next backup copy recorded when the limit on their number is exceeded?\n\nIf the specified number of backups is exceeded, a new one will be written to the place of the oldest copy.\n\n\n\n\n\nUnable to perform backup\n\nFor example, the menu item is missing or the VM is not in the list. The problem may occur if there is no QEMU guest agent in the system (qemu guest-agent) — install it.\n\nAfter installation, you need to register additional metadata on the VM – for this contact technical support. In some cases, you may need to restart the VM.\n\n\n","url":"/en/storage/backups/vm-backup/faq-vm-backup","description":"Answers to questions about the time of creation of backups, their total number, distribution by availability zones","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Backup","link":"/en/storage/backups","isActive":false,"evenSimpler":true}]},{"uuid":"5ede036c-195a-43fb-9205-fa6a8b9912a4","title":"Creating, deleting and configuring a backup plan","data":"When creating a database, a backup plan is created in the VK Cloud panel, which can be managed in the Backup section control panels.\n\nThe current status of the plan can be seen by hovering over the colored status indicator.\n\nSetting up a plan\n\n\n\nPersonal account\nAPI\n\n\n\nGo to VK Cloud personal account.\nGo to Databases → Backup.\nOn the Automatic tab, click   for the required backup and select Edit.\nOn the page that opens, set the necessary settings:\n\n    Name of plan.\n    Backup schedule: select a period from the list.\n    Enable GFS: select the option if you want to configure backup by GFS strategy.\n    Maximum number of full backups: specify the maximum number of backups stored. Old backups will be deleted.\n    Databases: select the DB instances from the list for which you want to create a backup.\n\nClick the Save plan button.\n\n\n\nIf the parameter Maximum number of full backups is changed, is deleted after 1 hour after saving the changes.\n\n\n\n\n\n\nUse the API PUT method /{project_id}/plans/{plan_id}.\n\nStopping the plan\n\n\n\nPersonal account\nAPI\n\n\n\nGo to VK Cloud personal account.\nGo to Databases → Backup.\nOn the Automatic tab, click   for the required backup and select Stop Backup.\nIn the window that appears, confirm the operation.\n\n\n\n\nUse the API PUT method /{project_id}/triggers/{trigger_id}.\n\n\n\n\n\n\nChanging the backup schedule activates the plan.\n\nStopping backup\n\n\n\nPersonal account\nAPI\n\n\n\nGo to VK Cloud personal account.\nGo to Databases → Backup.\nSelect a backup from the general list.\nClick   for the required backup and select Stop.\nIn the window that appears, confirm the operation.\n\nThe backup will be stopped, and the files recorded before the stop will be deleted.\n\nDeleting a plan\n\n\n\nWhen you delete a backup plan, all restore points will be deleted.\n\n\n\n\n\nPersonal account\nAPI\n\n\n\nGo to VK Cloud personal account.\nGo to Databases → Backup.\nOn the Automatic tab, click   for the required backup and select Delete.\nIn the window that appears, confirm the operation.\n\n\n\n\nUse the API PUT method /{project_id}/plans/{plan_id}.\n\n\n\n","url":"/en/storage/backups/db-backup/db-backup-create","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Backup","link":"/en/storage/backups","isActive":false,"evenSimpler":true}]},{"uuid":"d133f96e-8893-4fd2-bb57-e445a1a37b01","title":"Restoring from backup","data":"\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to Databases → Backup.\nGo to the backup plan tab.\nSelect a backup from the general list.\nClick   for the backup and select Restore the instance.\nIn the window that opens, select the backup and click Restore an instance from backup.\nConfigure the parameters of the DB instance being created.\nClick the Next step button.\n\nThe process of creating a new DB instance will start. The creation status can be seen on the Database Instances page.\n\n\n\n","url":"/en/storage/backups/db-backup/db-recover-backup","description":"Restoring from backup","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Backup","link":"/en/storage/backups","isActive":false,"evenSimpler":true}]},{"uuid":"7c4ec246-f5b2-4fc6-be26-91280dbb4621","title":"Creating and deleting backups","data":"In addition to automatic backups, you can create database backups manually in your VK Cloud personal account.\n\nCreating a backup\n\nGo to VK Cloud personal account.\nCreate a backup using one of the following methods:\n\n\n\nVia DB instance\nThrough the backup section\n\n\n\nGo to Databases → Database Instances.\nClick   for the DB instance and select Create backup.\nIn the window that opens, specify a comment if necessary and click Create a backup.\n\n\n\n\nGo to Databases → Backup.\nGo to the Manual tab.\nClick the Add button.\nIn the window that opens, select the DB instance and specify a comment if necessary.\nPress the button Create a backup.\n\n\n\n\n\n\nBackups are made to the object storage.\n\nViewing a list of backups\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to Databases → Backup.\nGo to the Manual tab.\nClick on the link in the column Restore points.\n\nA list of recovery points for the selected backup will be displayed.\n\nDeleting backups\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to Databases → Backup.\nGo to the Manual tab.\nClick   for the required backup and select the Delete option.\nIn the window that appears, confirm the operation.\n\n\n\n\n\n\nAutomatically created backups are deleted according to the plan settings.\n\n\n","url":"/en/storage/backups/db-backup/manage-db-backups","description":"In addition to automatic backups, you can create database backups manually in the control panel. Creating and managing VK Cloud manual backups.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Backup","link":"/en/storage/backups","isActive":false,"evenSimpler":true}]},{"uuid":"3cb0781c-03d6-48ab-b5b5-d09985d3b8c9","title":"Point-in-time recovery (PITR)","data":"\n\nThe PITR function is only available for databases running PostgreSQL.\n\nCreating a schedule\n\nWhen creating a PITR schedule, DBMS logs will be copied.\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to Databases → Backup.\nGo to the Point-in-time recovery tab.\nClick the Add.\nIn the window that opens, specify:\n\n   Schedule Name: the name for the schedule being created.\n   Start time: specify the start time of the backup in the GMT+03:00 time zone.\n   Store, number of copies: specify the number of backups stored.\n   Backup interval: select the appropriate backup interval.\n   Database: select a deployed instance with a PostgreSQL DBMS.\n\nClick the Save schedule.\n\nEditing an existing schedule\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to Databases → Backup.\nGo to the Point-in-time recovery tab.\nClick   for the required schedule and select Edit schedule.\nMake the necessary changes and click the Save schedule.\n\nViewing schedule backups\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to Databases → Backup.\nGo to the Point-in-time recovery tab.\nClick on the name of the required schedule.\n\nA list of backups for the selected schedule will be displayed.\n\nRestore from backup\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to Databases → Backup.\nGo to the Point-in-time recovery tab.\nClick on the name of the required schedule.\nClick   for the required backup and select Restore from backup.\nOn “Create instance” step specify the necessary parameters and click the Next step.\n(Optional) Specify the date and time of the required backup in the field of the same name. If you leave the field empty, the last backup created will be automatically selected.\nClick the Create database.\n\n\n\n","url":"/en/storage/backups/db-backup/point-in-time-recovery-pitr","description":"Point-in-time recovery (PITR) provides continuous backup of PostgreSQL table data.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Backup","link":"/en/storage/backups","isActive":false,"evenSimpler":true}]},{"uuid":"6f457a9a-ac52-407d-98e9-1a6e6f9c7311","title":"Forward Incremental Backups","data":"The Forward Incremental Backup allows you to set the maximum number of full backups for storage. If the maximum number of backups is exceeded, VK Cloud will automatically delete the oldest of them.\n\n\n\nBackups can be created successfully or with an error. Backups created with an error are not charged and cannot be used for recovery.\n\n\n\nBackups are deleted according to the following algorithm:\n\nThe number of successfully created copies is checked. If it exceeds what is allowed in the plan, the oldest of these copies are deleted. The number of deleted copies will be equal to the difference between the total number of successfully created backups and the set maximum number.\nThe total number of backups is checked. If it exceeds what is allowed in the plan, all backups created with an error, created before the oldest successful one, are deleted.\nIf there are no successfully created backups, the oldest backups created with an error are deleted. The number of deleted copies will be equal to the difference between the number of backups created with an error and the maximum number set.\n\nThe deletion process starts after the backup is created and an hour after the backup plan settings are changed.\n\n\n    Example of the algorithm operation\n\nThe parameter Max. number of full backups is set to: 2.\n\nAccording to the backup plan, backups are created:\n\nCopy 1**, created with an error.\nThe number of successfully created copies: 0, does not exceed the maximum.\nThe total number of backups: 1, does not exceed the maximum.\nCopy 2**, created successfully.\nThe number of successfully created copies: 1, does not exceed the maximum.\nTotal number of backups: 2, does not exceed the maximum.\nCopy 3**, created successfully.\nThe number of successfully created copies: 2, does not exceed the maximum.\nTotal number of backups: 3, exceeds the maximum. There are copies created with an error earlier than the oldest successfully created copy. The Copy 1 will be deleted.\nCopy 4**, created with an error.\nThe number of successfully created copies: 2, does not exceed the maximum.\nTotal number of backups: 3, exceeds the maximum. There are no copies created with an error earlier than the oldest successfully created copy. Backups will not be deleted.\nCopy 5**, created with an error.\nThe number of successfully created copies: 2, does not exceed the maximum.\nTotal number of backups: 4, exceeds the maximum. There are no copies created with an error earlier than the oldest successfully created copy. Backups will not be deleted.\nCopy 6**, created successfully.\nNumber of successfully created copies: 3, exceeds the maximum. The Copy 2 will be deleted.\nTotal number of backups: 5, exceeds the maximum. There are no copies created with an error earlier than the oldest successfully created copy. Backups created with an error will not be deleted.\n\nBackups will remain:\n\nCopy 3**, created successfully.\nCopy 4**, created with an error.\nCopy 5**, created with an error.\nCopy 6**, created successfully.\n\n\n\n\n\nWhen deleting full backups, the incremental ones will also be deleted.\n\n\n","url":"/en/storage/backups/retention-policy/forward-incremental","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Backup","link":"/en/storage/backups","isActive":false,"evenSimpler":true}]},{"uuid":"22e28394-c473-4865-ba9b-5a92eaff3bfd","title":"Backup by GFS strategy","data":"Description\n\nThe industry uses GFS (\"grandfather-father-son\", Grandfather-Father-Son) backup storage strategy.\nIn the grandfather-father-son algorithm, backup consists of three consecutive steps:\n\n\"Grandfather\" is a full backup for the beginning of the year.\n\"Father\" — full backup at the beginning of the month.\n\"Son\" — full backup and increments once a week.\n\nWorking principle\n\nAfter each successful backup, the process of deleting outdated backups is started. The GFS backup strategy does not create weekly, monthly or annual backups separately. This process is dynamic and depends on the current GFS settings. The process of deleting backups first checks that there is the required number of weekly backups. Next, it checks the presence/absence of monthly and annual backups. After that, it makes a decision to delete outdated/redundant backups, if any.\n\nChanging the GFS settings is applied at the next successful backup. If you specify to store fewer full backups than are stored now, the outdated backups will be deleted.\n\nGFS settings store:\n\nWeekly backups — full and incremental backups for the specified number of weeks.\nMonthly backups — backups created at the beginning of the month remain. Only full backups for the specified number of calendar months remain. Incremental backups are deleted.\nAnnual backups — backups created at the beginning of the year remain. Only annual backups for the specified number of years (including the current calendar year) remain. Incremental backups are deleted.\n\nUsage Scenarios\n\nA typical scenario for using GFS is to follow the requirements of corporate policies or legislation.\n\nThe most commonly used and recommended best practices are the following settings.\n\nKeep weekly backups: 4 weeks.\nStore monthly backups: 12 months.\nStore annual backups: 5 years.\n\nFAQ\n\nWhen does the \"deletion\" happen?\n\nAfter the first successful backup.\n\nCan there be one backup at the same time Monthly and Weekly (or Annual and Monthly)?\n\nNo. In the current implementation of GFS, backups have unique tags \"weekly\", \"monthly\" or \"annual\".\n\nIf it's 2022 now, how to save the annual backup for 2021, how much is it per annum?\n\nTwo. The current year also counts.\n\nWhy is Weekly backup a mandatory field?\n\nArchitectural requirement. This is necessary for the correct labeling of backups when applying the retention policy.\n","url":"/en/storage/backups/retention-policy/gfs-backup","description":"Strategy for long-term storage of GFS backups","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Backup","link":"/en/storage/backups","isActive":false,"evenSimpler":true}]},{"uuid":"208f930b-95fe-4793-9a28-7c8440895ce9","title":"API usage examples","data":"The following are some examples of using the VK Cloud Karboii API.\n\n\n\nTo follow the examples given:\n\nMake sure that two-factor authentication and API access are enabled.\nGet an access token X-Subject-Token.\nFind out the endpoint for the Karboii service.\n\n\n\nThe requests will use:\n\nendpoint:\n\n    https://mcs.mail.ru/infra/karboii/v1\n\ntoken:\n\n    gBkZhRqVNHSuSAJBI6duyXXXX\n  virtual machine:\n\n    +-----------------------------+-----------------------------------------------------------+\n  | Field                       | Value                                                     |\n  +-----------------------------+-----------------------------------------------------------+\n  | id                          | 8f1ba150-XXXX-4ae4-9693-d18844b30d19                      |\n  | name                        | Example_CentOS_STD1-1                             |\n  | project_id                  | b5b7ffd4ef0547e5b222f44555dfXXXX                          |\n  | status                      | ACTIVE                                                    |\n  +-----------------------------+-----------------------------------------------------------+\n\nGet a list of created backup plans\n\nRequest example:\n\ncurl -X GET \\\n-H \"Accept: application/json\" \\\n-H \"X-Auth-Token: gBkZhRqVNHSuSAJBI6duyXXXX\" \\\nhttps://mcs.mail.ru/infra/karboii/v1/${OS_PROJECT_ID}/plans\n\n\n   Response example\n\n{\n    \"plans\": [{\n            \"full_day\": null,\n            \"id\": \"811bf3ff-24fa-405a-XXXX-be9172d3b13f\",\n            \"name\": \"Backup_plan_19.05.2023\",\n            \"project_id\": \"b5b7ffd4ef0547e5b222f44555dfXXXX\",\n            \"provider_id\": \"37997f75-0637-XXXX-bf7e-49ff2ff11fa5\",\n            \"resources\": [{\n                    \"id\": \"6ed263c6-3066-XXXX-a8e1-140278a4XXXX\",\n                    \"name\": \"example-instance\",\n                    \"type\": \"OS::Nova::Server\"\n                }\n            ],\n            \"retention_type\": \"max_backups\",\n            \"status\": \"running\"\n        }\n    ]\n}\n\nCreate a new backup plan for the VM\n\n1. Get the prodiver_id\n\nRequest example:\n\ncurl -X GET \\\n    -H \"Accept: application/json\" \\\n    -H \"X-Auth-Token: gBkZhRqVNHSuSAJBI6duyXXXX\" \\\n    https://mcs.mail.ru/infra/karboii/v1/${OS_PROJECT_ID}/providers\n\n\n   Response example\n\n{\n    \"providers\": [{\n            \"id\": \"37997f75-0637-XXXX-bf7e-49ff2ff11fa5\",\n            \"name\": \"OS::Nova\"\n        }, {\n            \"id\": \"7ab9410c-edda-XXXX-a51a-1bb806666cb3\",\n            \"name\": \"OS::Trove\"\n        }\n    ]\n}\n\n2. Create a backup plan\n\nWill be used prodiver_id = 37997f75-0637-XXXX-bf7e-49ff2ff11fa5, because a plan is being created for a virtual machine (not a DB instance).\n\nRequest example:\n\ncurl -X POST \\\n    -H \"Accept: application/json\" \\\n    -H \"Content-type: application/json\" \\\n    -H \"X-Auth-Token: gBkZhRqVNHSuSAJBI6duyXXXX\" \\\n    -d '{\n      \"plan\": {\n        \"name\": \"example-plan\",\n        \"resources\": [\n          {\n            \"id\": \"8f1ba150-XXXX-4ae4-9693-d18844b30d19\",\n            \"type\": \"OS::Nova::Server\",\n            \"name\": \"Example_CentOS_STD1-1\"\n          }\n        ],\n        \"provider_id\": \"37997f75-0637-XXXX-bf7e-49ff2ff11fa5\",\n        \"full_day\": null,\n        \"retention_type\": \"max_backups\"\n      }\n    }' \\\n        https://mcs.mail.ru/infra/karboii/v1/${OS_PROJECT_ID}/plans\n\n\n   Response example\n\n{\n    \"plan\": {\n        \"full_day\": null,\n        \"id\": \"17f09168-62c3-419b-XXXX-8889aac8fb0f\",\n        \"name\": \"example-plan\",\n        \"project_id\": \"b5b7ffd4ef0547e5b222f44555dfXXXX\",\n        \"provider_id\": \"37997f75-0637-XXXX-bf7e-49ff2ff11fa5\",\n        \"resources\": [{\n                \"id\": \"8f1ba150-XXXX-4ae4-9693-d18844b30d19\",\n                \"name\": \"Example_CentOS_STD1-1\",\n                \"type\": \"OS::Nova::Server\"\n            }\n        ],\n        \"retention_type\": \"max_backups\",\n        \"status\": \"running\"\n    }\n}\n\n3. Create a trigger\n\nRequest example:\n\ncurl -s -X POST \\\n    -H \"Accept: application/json\" \\\n    -H \"Content-type: application/json\" \\\n    -H \"X-Auth-Token: gBkZhRqVNHSuSAJBI6duyXXXX\" \\\n    -d '{\n      \"trigger_info\": {\n        \"name\": \"example-plan\",\n        \"plan_id\": \"17f09168-62c3-419b-XXXX-8889aac8fb0f\",\n        \"properties\": {\n          \"pattern\": \"3 3 * * 2\",\n          \"max_backups\": 20,\n          \"next_time\": 0\n        }\n      }\n    }' \\\n    https://mcs.mail.ru/infra/karboii/v1/${OS_PROJECT_ID}/triggers\n\n\n   Response example\n\n{\n    \"trigger_info\": {\n        \"id\": \"6d398d13-XXXX-41d5-bf9b-5ce6f0b1fb50\",\n        \"name\": \"example-plan\",\n        \"plan_id\": \"17f09168-62c3-419b-XXXX-8889aac8fb0f\",\n        \"project_id\": \"b5b7ffd4ef0547e5b222f44555dfXXXX\",\n        \"properties\": {\n            \"max_backups\": 10,\n            \"next_time\": 1684810980,\n            \"pattern\": \"3 3 * * 2\"\n        }\n    }\n}\n\n\n","url":"/en/storage/backups/api-examples","description":"Examples of Karboii API requests for creating a backup plan.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Storage","link":"/en/storage","isActive":false,"evenSimpler":true},{"title":"Cloud Backup","link":"/en/storage/backups","isActive":false,"evenSimpler":true}]},{"uuid":"0a55d18e-5867-4570-990e-827062e43dd1","title":"Quick start","data":"Quick start will help you get started with the service and become familiar with its features. PostgreSQL will be used as example.\n\nAfter going through all the steps of the quick start, you will:\n\nCreate a single host PostgreSQL DB instance.\nInstall the TimescaleDB extension to the instance.\nLearn how to connect to the instance for both viewing logs and executing SQL queries.\nCreate test data and quieries for TimescaleDB to make sure that the extension operates correctly.\n\n   An automatically generated test dataset will be used in this quick start. It comprises the information from the IoT sensors: temperature and CPU utilization.\n   Read more about the dataset in Timescale documentation.\n\n   The procedure for generating such datasets is discussed in details in Timescale blog.\n\nFamiliarize yourself with the monitoing data collected during the operation of the DB instance.\n\n\n\nThe DB instance is charged and consumes computational resources. After completing the quick start delete both the instance and the floating IP address assigned to it if you no longer need them.\n\nPreparatory steps\n\nMake sure that the psql utility is installed. To do this, view its version:\n\n      psql --version\n\n   If the utility is installed, its version will be displayed:\n\n      psql (PostgreSQL) 14.7\n\n   Otherwise, there will be shown a message stating that the psql command is not found.\n\nIf the psql utility is not installed, then install it:\n\n   Linux (APT)\n   Linux (RPM)\n   macOS (Homebrew)\n   Windows\n\n\n   Connect the PostgreSQL repository:\n\n            sudo apt install curl ca-certificates gnupg\n      curl https://www.postgresql.org/media/keys/ACCC4CF8.asc | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/apt.postgresql.org.gpg > /dev/null\n      sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\n      sudo apt update\n\n   Install the psql utility:\n\n            sudo apt install -y postgresql-client\n\n\n   Connect the PostgreSQL repository:\n\n      Fill in the questionnaire on RedHat Linux PostgreSQL download page:\n\n         Select version: select the most current version.\n         Select platform: select the operating system distribution.\n         Select architecture: select the architecture. If you don't know whch architecture to choose, select x86_64.\n\n         The installation script text will appear.\n\n      Copy the command under the Install the repository RPM comment in the script.\n\n         Its appearance depends on the selected platform:\n\n         yum\n         dnf\n\n\n                  sudo yum install -y https://download.postgresql.org/pub/repos/yum/...\n\n\n                  sudo dnf install -y https://download.postgresql.org/pub/repos/yum/...\n\n\n      Execute the copied command.\n\n      If the executed command looked like sudo dnf..., then disable the built-in PostgreSQL module:\n\n                  sudo dnf -qy module disable postgresql\n\n   Install the psql utility:\n\n      yum\n      dnf\n\n\n            sudo yum install -y postgresql\n\n\n            sudo dnf install -y postgresql\n\n\n   Install the psql utility:\n\n            brew install libpq\n\n   Create the symbolic links to the libpq executable files:\n\n            brew link --force libpq\n\n      This step is necessary if you want to run the psql utility without specifying the full path to its executable file.\n\n\n   Download the EDB installer for the most current PostgreSQL version.\n\n   Do the installation:\n\n      When installing:\n\n      On the Installation Directory step write down the path PostgreSQL will be installed to.\n      On the Select Components remove the selection from all components, except Command Line Tools.\n\n   Add the path to the PostgreSQL command line utilities executable files to the Path environment variable:\n\n      Open Start → This PC → Properties → Advanced system settings → Environment variables.\n      In the System variables list change the Path variable by adding the path to the PostgreSQL command line utilities executable files to its existing value.\n\n         Example of the path when installing PostgreSQL with default settings:\n\n                  C:\\Program Files\\PostgreSQL\\15\\bin\\\n\n\n1. Create PostgreSQL DB instance\n\nGo to VK Cloud personal account.\nSelect project, where the instance will be placed.\nGo to Databases → Database instances.\nIf there are no instances in the selected project, click the Create database button.\n\n   Otherwise, click the Add button.\n\nOn the “Configuration” step: select:\n\n   The PostgreSQL database type and the most current version.\n   The Single configuration.\n\nClick the Next step button.\n\nOn the “Create instance” step set:\n\n   Database instance name: for example, vk-cloud-dbaas-quickstart.\n   Type of virtual machine: STD3-2-8.\n   Availability zone: Moscow (GZ1).\n   Disk Type: SSD.\n   Disk size, GB: 10.\n   Enable volume autoscaling: make sure that this option is disabled.\n   Network: Create new network.\n   Subnet address: 10.0.1.0/24.\n   Assign external IP: make sure that this option is enabled.\n   Настройки Firewall: select ssh from the drop-down list.\n\n     The resulting list of the secrity groups should look like: default, ssh.\n\n   Create replica: make sure that this option is disabled.\n   SSH access key: Create a new key.\n\n\n     With this option enabled, a private SSH key will be downloaded to your computer. Save it.\n\n     This key is required to connect to the instance via SSH, for example, for viewing the logs.\n\n\n   Backup: Disabled.\n   Enable monitoring: make sure that this option is enabled.\n\nClick the Next step button.\n\nOn the “Initialization” step set:\n\n   Creation type: New database.\n   Name of database for creation: tsdb1.\n   Username: tsuser1.\n   User password: provide a password or generate it.\n\n\n     It is recommended either to use strong and lenghty password, that comprises several types of characters, or to generate password.\n\n     The strong password is crucial, because this DB instance will be exposed to the internet.\n\n\nClick the Create database button.\n\n   Wait until the creation of the DB instance is completed, this process can take a long time.\n\n2. Get the external IP address of the DB instance\n\nGo to VK Cloud personal account.\nSelect the project where the DB instance is located.\nGo to Databases → Database instances.\nClick on the name of the DB instance. A page with information will open.\nNavigate to the Information tab.\n\n   The necessary address will be listed in the External IP address parameter.\n\n3. (Optional) View DB instance logs\n\nConnect to DB instace via SSH. Use private SSH key, obtained during the instance creation:\n\n   Linux/macOS\n   Windows\n\n\n      chmod 0600\n   ssh -i  admin@\n\n\n      ssh -i  admin@\n\n\nView DB instance logs:\n\n      journalctl -u postgresql\n\n   Example output fragment\n\n      -- Logs begin at Fri 2023-05-19 10:28:34 UTC, end at Mon 2023-05-22 06:08:42 UTC. --\n   May 19 10:28:41 vk-cloud-dbaas-quickstart.novalocal systemd[1]: Starting PostgreSQL 14 database server...\n   May 19 10:28:41 vk-cloud-dbaas-quickstart.novalocal postmaster[1096]: 2023-05-19 10:28:41.800 UTC [1096] LOG:  redirecting log output to logging collector process\n   May 19 10:28:41 vk-cloud-dbaas-quickstart.novalocal postmaster[1096]: 2023-05-19 10:28:41.800 UTC [1096] HINT:  Future log output will appear in directory \"log\".\n   May 19 10:28:41 vk-cloud-dbaas-quickstart.novalocal systemd[1]: Started PostgreSQL 14 database server.\n   May 19 10:29:18 vk-cloud-dbaas-quickstart.novalocal systemd[1]: Stopping PostgreSQL 14 database server...\n   May 19 10:29:18 vk-cloud-dbaas-quickstart.novalocal systemd[1]: postgresql.service: Succeeded.\n   May 19 10:29:18 vk-cloud-dbaas-quickstart.novalocal systemd[1]: Stopped PostgreSQL 14 database server.\n   May 19 10:29:33 vk-cloud-dbaas-quickstart.novalocal systemd[1]: Starting PostgreSQL 14 database server...\n   May 19 10:29:33 vk-cloud-dbaas-quickstart.novalocal postmaster[1978]: 2023-05-19 10:29:33.720 GMT [1978] LOG:  starting PostgreSQL 14.7 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-16), 64-bit\n   May 19 10:29:33 vk-cloud-dbaas-quickstart.novalocal postmaster[1978]: 2023-05-19 10:29:33.720 GMT [1978] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n   May 19 10:29:33 vk-cloud-dbaas-quickstart.novalocal postmaster[1978]: 2023-05-19 10:29:33.720 GMT [1978] LOG:  listening on IPv6 address \"::\", port 5432\n   May 19 10:29:33 vk-cloud-dbaas-quickstart.novalocal postmaster[1978]: 2023-05-19 10:29:33.725 GMT [1978] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n   May 19 10:29:33 vk-cloud-dbaas-quickstart.novalocal postmaster[1980]: 2023-05-19 10:29:33.735 GMT [1980] LOG:  database system was shut down at 2023-05-19 10:29:18 GMT\n   May 19 10:29:33 vk-cloud-dbaas-quickstart.novalocal postmaster[1978]: 2023-05-19 10:29:33.828 GMT [1978] LOG:  database system is ready to accept connections\n\n   ...\n\n\n   It is possible to conclude from this output that PostgreSQL is up and running and ready to accept incoming connections.\n\n4. Install the TimescaleDB extension\n\nGo to VK Cloud personal account.\nSelect the project where the DB instance is located.\nGo to Databases → Database instances.\nClick on the name of the DB instance. A page with information will open.\nNavigate to the Extensions tab.\nClick the Add button.\nSelect Open-source database extension for storing time-series data (timescaledb) from the Available extensions drop-down list.\nClick the Add button.\n\n    Wait until the installation of the extension is completed: its status should change from Creating to Active.\n\n5. Connect to the database\n\nConnect to the tsdb1 database via the psql utility:\n\nExecute the command:\n\n      psql -h  -d tsdb1 -U tsuser1\n\nEnter the tsuser1 user's password, specified during the instance creation.\n\nIf the connection is successful, the following prompt will be displayed:\n\ntsdb1=>\n\n\n\nAll further steps must be performed from the psql command line.\n\n6. Create necessary tables\n\nActivate the TimescaleDB extension:\n\n      CREATE EXTENSION timescaledb;\n\n   Wait for the tsdb1=> prompt to appear.\n\nCreate the sensors table:\n\n      CREATE TABLE sensors(\n     id SERIAL PRIMARY KEY,\n     type VARCHAR(50),\n     location VARCHAR(50)\n   );\n\nCreate the sensor_data table:\n\n      CREATE TABLE sensor_data (\n     time TIMESTAMPTZ NOT NULL,\n     sensor_id INTEGER,\n     temperature DOUBLE PRECISION,\n     cpu DOUBLE PRECISION,\n     FOREIGN KEY (sensor_id) REFERENCES sensors (id)\n   );\n\nMake sure that the tables were created successfully:\n\n      SELECT tablename\n   FROM pg_catalog.pg_tables\n   WHERE tablename LIKE 'sensor%';\n\n   Query result\n\n        tablename\n   -------------\n    sensor_data\n    sensors\n   (2 rows)\n\n\nConvert the PostgreSQL sensor_data table into the TimescaleDB hypertable:\n\n      SELECT create_hypertable('sensor_data', 'time');\n\nMake sure that the sensor_data hypertable was created successfully:\n\n      SELECT hypertable_name\n   FROM timescaledb_information.hypertables;\n\n   Query result\n\n       hypertable_name\n   -----------------\n    sensor_data\n   (1 row)\n\n\n7. Fill in the tables with the data\n\n\n\nThe sensor_data table will be filled in with the randomly generated dataset. The data in your table will differ from the shown examples.\n\n\n\nFill in the sensors table with data:\n\n      INSERT INTO sensors (type, location) VALUES\n   ('a','floor'),\n   ('a', 'ceiling'),\n   ('b','floor'),\n   ('b', 'ceiling');\n\nMake sure that the data was successfully inserted into the table:\n\n      SELECT * FROM sensors;\n\n   Query result\n\n       id | type | location\n   ----+------+----------\n     1 | a    | floor\n     2 | a    | ceiling\n     3 | b    | floor\n     4 | b    | ceiling\n   (4 rows)\n\n\nFill in the sensor_data table with randomly generated data:\n\n      INSERT INTO sensor_data (time, sensor_id, cpu, temperature)\n   SELECT\n     time,\n     sensor_id,\n     random() AS cpu,\n     random()*100 AS temperature\n   FROM generate_series(now() - interval '24 hour', now(), interval '5 minute') AS g1(time), generate_series(1,4,1) AS g2(sensor_id);\n\nMake sure that the data was successfully inserted into the table by selecting a few rows from the table:\n\n      SELECT * FROM sensor_data ORDER BY time LIMIT 8;\n\n   Example query result\n\n                   time              | sensor_id |    temperature     |         cpu\n   -------------------------------+-----------+--------------------+---------------------\n    2023-05-21 07:43:52.133888+00 |         4 | 34.633736865959364 |  0.5569185687389719\n    2023-05-21 07:43:52.133888+00 |         3 | 56.905440887294034 | 0.07377927779113236\n    2023-05-21 07:43:52.133888+00 |         1 |  56.63560698774681 |  0.5716904026292013\n    2023-05-21 07:43:52.133888+00 |         2 | 36.502832944119135 |  0.6536441978766803\n    2023-05-21 07:48:52.133888+00 |         4 |   76.2173939498279 |  0.6182606187228714\n    2023-05-21 07:48:52.133888+00 |         3 |  71.45127267625107 | 0.27642300219178395\n    2023-05-21 07:48:52.133888+00 |         1 | 49.732367773230024 |   0.770096500403703\n    2023-05-21 07:48:52.133888+00 |         2 |  31.10197931398453 |  0.8426620901373241\n   (8 rows)\n\n\n8. Execute test queries\n\n\n\nEarlier the sensor_data table was filled in with the randomly generated dataset. Your query results will differ from the shown examples.\n\n\n\nOutput the average temperature and CPU utilization values over thirty-minute intervals:\n\n      SELECT\n     time_bucket('30 minutes', time) AS period,\n     AVG(temperature) AS avg_temp,\n     AVG(cpu) AS avg_cpu\n   FROM sensor_data\n   GROUP BY period\n   ORDER BY period;\n\n   Example query result fragment\n\n               period         |      avg_temp      |       avg_cpu\n   ------------------------+--------------------+---------------------\n    2023-05-21 07:30:00+00 |  53.61763620602069 |  0.5413570794268665\n    2023-05-21 08:00:00+00 | 45.105700825199456 |  0.5967307199039785\n    2023-05-21 08:30:00+00 | 49.534870531748844 |  0.5244548233136944\n    2023-05-21 09:00:00+00 |  48.11141443258068 | 0.40274852600539646\n    2023-05-21 09:30:00+00 | 52.020211415250266 | 0.41570437093237916\n    2023-05-21 10:00:00+00 |  56.50992475543965 |  0.5744052407007274\n    2023-05-21 10:30:00+00 | 46.664477309117196 |  0.6187918344821526\n    2023-05-21 11:00:00+00 |  47.68186282450759 |  0.5020627643634109\n\n   ...\n\n\nOutput the average temperature and CPU utilization values over thirty-minute intervals, along with the last temperature value recorded during the interval:\n\n      SELECT\n     time_bucket('30 minutes', time) AS period,\n     AVG(temperature) AS avg_temp,\n     last(temperature, time) AS last_temp,\n     AVG(cpu) AS avg_cpu\n   FROM sensor_data\n   GROUP BY period\n   ORDER BY period;\n\n   Example query result fragment\n\n               period         |      avg_temp      |     last_temp      |       avg_cpu\n   ------------------------+--------------------+--------------------+---------------------\n    2023-05-21 07:30:00+00 |  53.61763620602069 | 24.940269958716854 |  0.5413570794268665\n    2023-05-21 08:00:00+00 | 45.105700825199456 | 14.089769297588717 |  0.5967307199039785\n    2023-05-21 08:30:00+00 | 49.534870531748844 | 15.675718258720295 |  0.5244548233136944\n    2023-05-21 09:00:00+00 |  48.11141443258068 | 54.914047493504725 | 0.40274852600539646\n    2023-05-21 09:30:00+00 | 52.020211415250266 |  90.50501566182483 | 0.41570437093237916\n    2023-05-21 10:00:00+00 |  56.50992475543965 |  17.26290517165019 |  0.5744052407007274\n    2023-05-21 10:30:00+00 | 46.664477309117196 |  58.94930860972387 |  0.6187918344821526\n    2023-05-21 11:00:00+00 |  47.68186282450759 | 62.973228176266005 |  0.5020627643634109\n\n   ...\n\n\nOutput of query results similar to the above ones indicates that PostgreSQL and TimescaleDB extension are working correctly.\n\n9. (Optional) Familiarize yourself with the DB instance monitoring data\n\nThe monitoring feature was enabled during the instance creation. Familiarize yourself with gathered monitoring data:\n\nGo to VK Cloud personal account.\nSelect the project where the DB instance is located.\nGo to Databases → Database instances.\nClick on the name of the DB instance. A page with information will open.\nNavigate to the Monitoring tab.\nSelect the required time interval and look at the collected data.\n\nDelete unused resources\n\nThe DB instance is charged and consumes computational resources. If you no longer need it, then:\n\nDelete DB instance.\nIf necessary, delete floating IP address, that was assigned to the DB instance. Floating IP addresses, that exist in the project, are charged.\n","url":"/en/dbs/dbaas/quick-start","description":"Getting started with the service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"09846f16-d910-4ec9-b5ba-bafd3c7ec201","title":"About the service","data":"The Cloud Databases service allows you to quickly deploy the DBMS of the types that you need in various configurations in VK Cloud.\n\nManaging cloud DBMSs does not require deep knowledge of administration. A DBMS instance is created in your personal account in a couple of clicks. VK Cloud takes care of all the work of deploying the instance and its technical support. Depending on the configuration and selected options, the following will be automatically configured:\n\ndata replication between hosts;\nload balancing;\nsharding;\nbackup;\nscaling disks as data volumes grow.\n\nIn the Moscow region, for some types of DBMS, you can create geo-distributed configurations with hosts in different availability zones (GZ1, MS1 and ME1) to further increase fault tolerance.\n\nAfter deploying the DBMS instance, all that remains is to create the necessary databases on it, fill them with content, connect your applications to them, configure access and monitoring.\n\nFor some types of DBMS, extensions are available. You can install them through your personal account.\n","url":"/en/dbs/dbaas/concepts/about","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"82509a15-93de-4093-8538-efcda2109b73","title":"Service architecture","data":"In a simplified form, the Cloud Databases architecture (database as a service) is shown in the picture below:\n\n\n\nA virtual machine or cluster with pre-installed database management tools is deployed in the VK Cloud control panel. When creating a virtual machine or cluster, you can choose whether to create a replica (for the Master-Slave configuration), or the number of nodes (for the \"Cluster\" configuration).\n\nFurther, client applications interact with the database via the network interface (IP address, connector, API) - as with a regular local database, adjusted for the speed of the network connection.\n","url":"/en/dbs/dbaas/concepts/architecture","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"b198a646-1db8-41a9-a455-5ee981c4121d","title":"Working with the network","data":"Использование балансировщика нагрузки\n\nДля каждого кластера PG/MySQL создается TCP-балансировщик, который имеет 3 порта. Они указывают на:\n\nмастер;\nсинхронную реплику;\nасинхронную реплику.\n\nНастройка файервола\n\nВы можете настроить группы безопасности (файервола) как при создании инстанса БД, так и после его развертывания через раздел Виртуальные сети → Настройки firewall.\n","url":"/en/dbs/dbaas/concepts/network","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"b1380eea-6206-4d65-8d88-22ed797aa48b","title":"Clickhouse","data":"ClickHouse — это столбцовая аналитическая СУБД с открытым исходным кодом, разработанная компанией Яндекс. Главное преимущество этой СУБД — быстрая обработка OLAP-запросов на структурированных больших данных. ClickHouse эффективно использует ресурсы процессора и позволяет обрабатывать данные, которые хранятся на жестких дисках, без полной загрузки этих данных в кеш памяти.\n\nClickHouse использует собственный диалект языка SQL, с рядом расширений и ограничений по сравнению со стандартным SQL. Среди основных расширений — возможность работы с внешними хранилищами key-value (словарями), агрегатные функции для приближенных вычислений. Среди основных ограничений — отсутствие транзакций и поддержки точечных UPDATE/DELETE (доступен только пакетный UPDATE/DELETE), ограниченная поддержка синтаксиса JOIN.\n\nClickHouse рекомендуется использовать, когда:\n\nнужно генерировать аналитические отчеты по большим данным в режиме реального времени — например, рассчитывать аналитику поведения клиентов приложения;\nв базу постоянно записывается большой объем входящих данных, при этом нужна стабильно высокая скорость обработки запросов на чтение;\nважна линейная масштабируемость.\n\nСписок версий ClickHouse, которые поддерживаются в VK Cloud, доступен при создании инстанса этой базы данных.\n\nЧто дальше?\n\nИзучите официальную документацию ClickHouse.\nПознакомьтесь с основными операциями, которые доступны для инстансов этой СУБД в VK Cloud.\n","url":"/en/dbs/dbaas/concepts/types/clickhouse","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"06d2c38c-da96-41fb-b471-61ccb2c98ed0","title":"Postgres Pro","data":"Postgres Pro — это объектно-реляционная СУБД, которая разрабатывается компанией Postgres Professional на базе открытой СУБД PostgreSQL. Postgres Pro входит в Реестр российского программного обеспечения.\n\nPostgres Pro отличается от PostgreSQL улучшенной производительностью при нагрузках, оптимизацией потребления памяти и большей отказоустойчивостью. Выпуски этой СУБД следуют за выпусками PostgreSQL, но на базе одной версии PostgreSQL может быть выпущено несколько версий Postgres Pro.\n\nВ VK Cloud доступны:\n\nPostgres Pro Standard;\nPostgres Pro Enterprise;\nPostgres Pro Enterprise 1C — специальный выпуск Postgres Pro Enterprise для работы с продуктами 1С.\n\nПодробнее о функциональных возможностях различных версий Postgres Pro Standard и Enterprise читайте в официальной документации. Поддерживаемые дополнительные модули описаны в Приложении F.\n\nСписок поддерживаемых версий доступен при создании инстанса Postgres Pro Standard, Postgres Pro Enterprise или Postgres Pro Enterprise 1C.\n\nЧто дальше?\n\nСоздайте инстанс базы данных.\nПодключитесь к базе данных.\nОзнакомьтесь с доступными расширениями баз данных.\n","url":"/en/dbs/dbaas/concepts/types/postgrespro","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"a1e20628-5910-4fbe-82ec-26b45dd34f88","title":"Postgres","data":"PostgreSQL — это объектно-реляционная СУБД с открытым исходным кодом. Она может хранить большие объемы данных, поддерживает множество типов данных и предоставляет возможность создания собственных типов.\n\nОсновные преимущества PostgreSQL:\n\nсохранение целостности данных (близкое к стандарту ANSI-SQL:2008);\nвозможность создания сложных выборок (представления, правила, подзапросы);\nуправление параллельным доступом посредством многоверсионности (MVCC).\n\nПодробнее в официальной документации PostgreSQL.\n\nСписок версий PostgreSQL, которые поддерживаются в VK Cloud, доступен при создании инстанса этой базы данных.\n\nЧто дальше?\n\nСоздайте инстанс базы данных.\nПодключитесь к базе данных.\nОзнакомьтесь с доступными расширениями баз данных.\n","url":"/en/dbs/dbaas/concepts/types/postgresql","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"16f11128-6ec9-44cf-a5fb-f352e2d2908d","title":"Redis","data":"\nRedis (Remote Dictionary Server) is a highly efficient NoSQL—type open source DBMS that stores data in RAM in the form of key/value pairs. The main advantage of Redis is that it has no competitors in performance, even among its analogues.\n\nRedis is optimal for tasks where speed is a priority: for caching data, storing user sessions, for implementing chats, queues, message brokers, real-time analytics, and others.\n\nFor more information, see official Redis documentation.\n\nThe list of Redis versions that are supported in VK Cloud is available when creating an instance of this database.\n\nWhat's next\n\nCreate DB instance.\nConnect to the DB instance.\nCheck out available database extensions.\n","url":"/en/dbs/dbaas/concepts/types/redis","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"1326c92a-f43d-4624-9d89-39831d63ab68","title":"MongoDB","data":"MongoDB is a document-oriented NoSQL-type open source DBMS written in C++. This DBMS stores data in the form of collections of documents that have a format similar to JSON. MongoDB is effective when:\n\nThe data does not have a clear structure, or their structure may change.\nYou need to process a large stream of requests in real time.\n\nMongoDB can be used, for example, to store product catalogs, geo-analytics data, social media materials and various media platforms. You can also store files in MongoDB — the GridFS specification is implemented for this.\n\nFor more information, see official MongoDB documentation.\n\nThe list of MongoDB versions that are supported in VK Cloud is available when creating an instance of this database.\n\nWhat's next\n\nCreate DB instance.\nConnect to the DB instance.\nCheck out available database extensions.\n","url":"/en/dbs/dbaas/concepts/types/mongodb","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"f659953f-b8c9-4a76-bf05-1237724d84e4","title":"MySQL","data":"MySQL — реляционная СУБД с открытым исходным кодом, которая разрабатывается компанией Oracle при поддержке Open Source сообщества.\n\nЭта одна из самых распространенных СУБД для небольших решений — она входит в состав стеков LAMP, WAMP, Denwer и других. MySQL универсальна: доступны версии для всех основных операционных систем, предоставляются API и коннекторы для большинства часто используемых языков программирования. Для этой СУБД существует множество дополнительных расширений (плагинов).\n\nMySQL будет оптимальным решением, если:\n\nнужна полнофункциональная реляционная СУБД, распространяемая бесплатно;\nот базы данных требуется хорошая горизонтальная масштабируемость и способность выдерживать много одновременных запросов на чтение;\nважно, чтобы решение было простым в развертывании и настройке.\n\nСписок версий MySQL, которые поддерживаются в VK Cloud, доступен при создании инстанса этой базы данных.\n\nЧто дальше?\n\nИзучите официальную документацию MySQL.\nУзнайте, в чем различия между версиями MySQL 5.7 и 8.0.\nПознакомьтесь с основными операциями, которые доступны для инстансов этой СУБД в VK Cloud.\n","url":"/en/dbs/dbaas/concepts/types/mysql","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"4520d8b0-b0bd-4d5e-8765-670ea47938d3","title":"DB instance configurations","data":"There are three types of DBMS instance configurations available on the VK Cloud platform. The configuration type determines how many DB instances will be created and their architecture.\n\nFor any type of configuration, a replica can be created — an additional instance for replication. The replica disk type and its size may differ from the master and is set separately.\n\nThe replica can be converted to a master. In this case, replication will stop, and the instance will turn into an independent master instance, available for reading and writing. The new master instance will contain the same data as the replica before conversion to the master.\n\nBy default, replicas and the master are placed in the same data center. To transfer one of the instances to another data center, contact technical support.\n\nSingle\n\nOne virtual machine with a DBMS server of the selected type installed. An instance in the Single configuration can be stopped, started, and restarted.\n\nMaster-Replica\n\nTwo virtual machines with installed DBMS servers of the selected type. Instances support synchronous replication in the master-replica (active-passive) mode and scale separately.\n\nAn instance in the Master-Replica configuration can be stopped, started, and restarted.\n\nCluster\n\n\n\nThe principle of operation of the PostgreSQL cluster is described.\n\n\n\nA group of virtual machines with installed DBMS servers of the selected type that support synchronous and asynchronous data replication, with a load balancer. If the master instance is unavailable, automatic switching will work: one of the replicas will be converted to the master, and another replica will be created instead.\n\nWhen increasing the disk size or scaling vertically, the changes are applied to all instances of the cluster.\n\n\n\nTo ensure high availability of the PostgreSQL cluster, the Patroni service is used.\n\n\n\nAn instance in the Cluster configuration cannot be started, restarted, or stopped.\n\nAvailable configurations for DBMS types\n\n| DBMS type | Single | Master-Replica | Cluster |\n| --------------------------| -------- | ---- | ------ |\n| PostgresPro Enterprise    | &#10003; | &#10003; | &#10003; |\n| PostgresPro Enterprise 1C | &#10003; | &#10003; | &#10003; |\n| PostgresPro Standard      | &#10003; | In development| In development |\n| MySQL                     | &#10003; | &#10003; | &#10003; |\n| Tarantool                 | &#10003; | —  | &#10003; |\n| PostgreSQL                | &#10003; | &#10003;  | &#10003; |\n| ClickHouse                | &#10003; | — | &#10003; |\n| Redis                     | &#10003; | — | &#10003; |\n| MongoDB                   | &#10003; | —  | &#10003; |\n| OpenSearch                | — | — | &#10003; |\n","url":"/en/dbs/dbaas/concepts/work-configs","description":"Available configurations of deployed database instances in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"0c7d5499-cd89-4002-b92e-47b16d171dae","title":"Available extensions and plugins","data":"VK Cloud supports the installation of extensions on already deployed DB instances. Extensions — a software package that allows you to expand the basic functionality of the DBMS, for example, add the collection of metrics.\n\nAvailable extensions:\n\n[cols=\"4,^1,^1,^1,^1,^1\", options=\"header\"]\n|===\n| Extension\n| Postgres\n| MySQL\n| ClickHouse\n| Redis\n| MongoDB\n\n| Prometheus for the server\n| &#10003;\n| &#10003;\n| &#10003;\n| &#10003;\n| —\n\n| Prometheus for server metrics (node_exporter)\n| &#10003;\n| &#10003;\n| &#10003;\n| &#10003;\n| &#10003;\n\n| Zabbix Agent (zabbix)\n| &#10003;\n| &#10003;\n| &#10003;\n| &#10003;\n| &#10003;\n\n| Postgres optimization service (holistic)\n| &#10003;\n| —\n| —\n| —\n| —\n\n| Popular Hints for the PostgreSQL glider (pg_hint_plan)\n| &#10003;\n| —\n| —\n| —\n| —\n\n| Set of extensions (postgres_extensions)\n| &#10003;\n| —\n| —\n| —\n| —\n\n| A tool for SQL traffic analysis and charting (pgbadger)\n| &#10003;\n| —\n| —\n| —\n| —\n\n| Extension for storing time series data (timescaledb)\n| &#10003;\n| —\n| —\n| —\n| —\n\n| Extension for creating and managing sets of table partitions (pg_partman)\n| &#10003;\n| —\n| —\n| —\n| —\n\n| An extension that collects statistics on system metrics (pg_stat_kcache) (pg_stat_kcache)\n| &#10003;\n| —\n| —\n| —\n| —\n\n| An extension that supports the jsonb data query language (jsquery)\n| &#10003;\n| —\n| —\n| —\n| —\n\n| Support for geographical objects in Postgres (postgis)\n| &#10003;\n| —\n| —\n| —\n| —\n\n| Tracking SQL statement execution statistics (pg_stat_statements)\n| &#10003;\n| —\n| —\n| —\n| —\n\n|===\n\nFor some extensions, mandatory parameters must be added — without them, the installation will fail with an error. Learn more about extensions and their parameters.\n\nThe number of extensions installed per DB instance is limited and depends on the type of DBMS.\n\nLearn more about working with extensions in VK Cloud in the article Managing extensions.\n\n\n\nThe extension is installed on all instance databases.\n\n\n","url":"/en/dbs/dbaas/concepts/extensions","description":"Extensions for database instances in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"0429ed53-28a0-4f82-baa9-6170e547a992","title":"Connecting to a database","data":"You can connect to the database of instances deployed in VK Cloud using the SDK.\n\nPostgreSQL\n\n\n\nPHP\nPython\nRuby\nJDBC\nNode.js\nODBC\n\n\n\nMake sure that PHP and composer are installed.\nInstall php-pgsql library.\nConnect to the database using the function pg_connect():\n\n      $conn = pg_connect(\"host= dbname= user= password=\");\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   `` — DB user name;\n   `` — DB user password.\n\nRead more about connecting to PostgreSQL in PHP in documentation.\n\n\n\n\nMake sure Python is installed.\nInstall psycopg module.\nConnect to the database using the function connect():\n\n      import psycopg2\n\n   conn = psycopg2.connect(\"host= dbname= user= password=\")\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nMake sure Ruby is installed.\nInstall pg library.\nConnect to the database using the function PGconn.connect():\n\n      require \"pg\"\n\n   conn = PGconn.connect(:host => '', :dbname => '', :user='', :password => '')\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nMake sure that Oracle Java or OpenJDK are installed.\nInstall JDBC driver for PostgreSQL.\nConnect to the database using the function DriverManager.getConnection():\n\n      import java.sql.*;\n\n   Connection conn = DriverManager.getConnection(\"jdbc:postgresql:///\", \"\", \"\");\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nMake sure that the Node.js installed.\nInstall node-postgres modules.\nConnect to the database using the function pg.Client():\n\n      var pg = require('pg');\n   var conn = new pg.Client(\"postgres://:@/\");\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nInstall ODBC driver.\nConnect to the database:\n\n      Driver={PostgreSQL UNICODE}; Server=\"\"; Port=5432; Database=\"\"; Uid=\"\"; Pwd=\"\";\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   `` — DB user name;\n   `` — DB user password.\n\nMySQL\n\n\n\nPHP\nPython\nRuby\nJDBC\nNode.js\nODBC\n\n\n\nMake sure that PHP and composer are installed.\nInstall php-mysql library.\nConnect to the database using the function mysqli_real_connect():\n\n      $conn = mysqli_init();\n\n   mysqli_real_connect($conn, \"\", \"\", \"\", \"\", );\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   ` — connection port, standard — 3306`;\n   `` — DB user name;\n   `` — DB user password.\n\nRead more about connecting to MySQL in PHP in documentation.\n\n\n\n\nMake sure Python is installed.\nInstall mysql.connector module.\nConnect to the database using the function mysql.connector.connect():\n\n      import mysql.connector\n\n   conn = mysql.connector.connect(user=\"\", password=\"\", host=\"\", port=, database=\"\")\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   ` — connection port, standard — 3306`;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nMake sure Ruby is installed.\nInstall mysql2 module.\nCreate a connection using the class Mysql2::Client:\n\n      require \"mysql2\"\n\n   conn = Mysql2::Client.new(username: \"\", password: \"\", database: \"\", host: \"\", port: )\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   ` — connection port, standard — 3306`;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nInstall Oracle Java or OpenJDK.\nInstall JDBC driver for MySQL.\nConnect to the database using the function DriverManager.getConnection():\n\n      import java.sql.*;\n\n   DriverManager.getConnection(\"jdbc:mysql://:/\", , );\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   ` — connection port, standard — 3306`;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nInstall Node.js.\nInstall mysql module.\nCreate a database connection using the method mysql.createConnection:\n\n      var mysql = require('mysql')\n   var conn = mysql.createConnection({\n       host: \"\",\n       user: \"\",\n       password: \"\",\n       database: \"\",\n       port:\n   });\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   ` — connection port, standard — 3306`;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nInstall ODBC driver.\nConnect to the database:\n\n      DRIVER={MySQL ODBC 5.3 UNICODE Driver}; Server=\"\"; Port=; Database=\"\"; Uid=\"\"; Pwd=\"\";\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   ` — connection port, standard — 3306`;\n   `` — DB user name;\n   `` — DB user password.\n\nTarantool\n\n\n\nYou can connect to the Tarantool database as a guest user.\n\n\n\n\n\nPHP\nPython\nJava\nShell\nGo\n\n\n\nMake sure that PHP and composer are installed.\nInstall tarantool/client library.\nConnect to the database using the method Client::fromDsn:\n\n      require_once DIR . '/vendor/autoload.php';\n\n   use Tarantool\\Client\\Client;\n   $client = Client::fromDsn('tcp://:PASSWORD>@');\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — DB user name;\n   `` — DB user password.\n\nRead more about connecting to Tarantool in PHP in documentation.\n\n\n\n\nInstall Python.\nInstall tarantool module.\nConnect to the database using the function tarantool.connect():\n\n      import tarantool\n\n   connection = tarantool.connect(\"\", , user=, password=)\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   ` — connection port, standard — 3301`;\n   `` — DB user name;\n   `` — DB user password.\n\nRead more about connecting to Tarantool in Python in documentation.\n\n\n\n\nInstall Oracle Java or OpenJDK.\nInstall Java connector cartridge-java.\nConnect to the database using the function TarantoolClientFactory.createClient():\n\n      TarantoolClient> client = TarantoolClientFactory.createClient()\n       .withAddress(\"\")\n       .withCredentials(container.getUsername(\"\"), container.getPassword(\"\"))\n       .build();\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nInstall Tarantool.\nConnect to the database:\n\n      tarantoolctl connect :@\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nMake sure that Go is installed.\nInstall go-tarantool library.\nConnect to the database using the function tarantool.Connect():\n\n      import (\n       \"github.com/tarantool/go-tarantool\"\n   )\n\n   opts := tarantool.Opts{\n       User: \"\",\n       Pass: \"\",\n   }\n   connection, err := tarantool.Connect(\"\", opts)\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — DB user name;\n   `` — DB user password.\n\nRead more about connecting to Tarantool in Go in documentation.\n\nClickHouse\n\n\n\nPHP\nPython\nRuby\nJDBC\nNode.js\nODBC\n\n\n\nMake sure that PHP and composer are installed.\nInstall php-curl.\nInstall smi2/phpclickhouse module.\nConnect to the database:\n\n      require_once DIR . '/vendor/autoload.php';\n\n   $config = [\n       'host' => '',\n       'port' => '',\n       'username' => '',\n       'password' => ''\n   ];\n   $db = new ClickHouseDB\\Client($config);\n   $db->database('');\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   ` — connection port, standard — 8123`;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nMake sure Python is installed.\nInstall clickhouse-driver driver.\nConnect to the database using the class Connection:\n\n      from clickhouse_driver.connection import Connection\n\n   conn = Connection('', port=8123, database='', user='', password='')\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nMake sure Ruby is installed.\nInstall ClickHouse driver.\nConnect to the database using the class Connection:\n\n      require \"clickhouse\"\n\n   conn = Clickhouse::Connection.new(:url => '', :database => '', :username => '', :password => '')\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nMake sure that Oracle Java or OpenJDK are installed.\nInstall JDBC driver for ClickHouse.\nCreate a connection using the class ClickHouseDataSource:\n\n      import ru.yandex.clickhouse.ClickHouseDataSource;\n   import ru.yandex.clickhouse.settings.ClickHouseProperties;\n\n   ClickHouseProperties properties = new ClickHouseProperties().withCredentials(, );\n   ClickHouseDataSource ds = new ClickHouseDataSource(String.format(\"jdbc:clickhouse://%s:%s/%s\", “”, , ), properties);\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   ` — connection port, standard — 8123`;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nInstall Node.js.\nInstall ClickHouse JS module.\nCreate a DB connection object:\n\n      const { ClickHouse } = require('clickhouse');\n\n   const clickhouse = new ClickHouse({\n    url: '',\n    port: ,\n    database: '',\n    basicAuth: {\n           username: '',\n           password: '',\n       }\n   });\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   ` — connection port, standard — 8123`;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nInstall ODBC driver for ClickHouse.\nConnect to the database:\n\n      Driver={PATH_OF_CLICKHOUSE_ODBC_SO}; Server=\"\"; Port=; Database=\"\"; Uid=\"\"; Pwd=\"\";\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   ` — connection port, standard — 8123`;\n   `` — DB user name;\n   `` — DB user password.\n\nMongoDB\n\n\n\nPHP\nPython\nJDBC\nNode.js\n\n\n\nMake sure that PHP and composer are installed.\nInstall PHP library for MongoDB.\nConnect to the database:\n\n      $manager = new MongoDB\\Driver\\Manager(\"mongodb:///\", array(\"username\" => , \"password\" => ));\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nMake sure Python is installed.\nInstall PyMongo driver.\nConnect to the database using the function MongoClient:\n\n      from pymongo import MongoClient\n\n   client = MongoClient(\"mongodb://:@\")\n\n   db = client[]\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nMake sure that Oracle Java or OpenJDK are installed.\nInstall JDBC driver for MongoDB.\nCreate a connection:\n\n      MongoCredential credential = MongoCredential.createCredential(, , .toCharArray());\n\n   MongoClient mongoClient = new MongoClient(new ServerAddress(“”), Arrays.asList(credential));\n\n   MongoDatabase db = mongoClient.getDatabase();\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nInstall Node.js.\nInstall driver for MongoDB.\nCreate a DB connection object:\n\n      var MongoClient = require('mongodb').MongoClient;\n\n   MongoClient.connect(\"mongodb://:@/\", function(err, db) {\n       if(!err) {\n           console.log(\"You are connected!\");\n       };\n       db.close();\n   });\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   `` — DB user name;\n   `` — DB user password.\n\nRedis\n\n\n\nPHP\nPython\nRuby\nJava\nNode.js\n\n\n\nMake sure that PHP and composer are installed.\nInstall predis.\nConnect to the database:\n\n      require_once DIR . '/vendor/autoload.php';\n\n   Predis\\Autoloader::register();\n\n   $client = new Predis\\Client('tcp://');\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   `` — database name;\n   `` — DB user name;\n   `` — DB user password.\n\n\n\n\nMake sure Python is installed.\nInstall redis-py client.\nConnect to the database using the function redis.Redis:\n\n   import redis\n\n   conn = redis.Redis(host='', port=, db=0)\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   ` — connection port, standard — 6379`.\n\nRead more about connecting to Redis in Python in documentation.\n\n\n\n\nMake sure Ruby is installed.\nInstall redis-rb driver.\nConnect to the database using the function Redis.new:\n\n      require \"redis\"\n\n   conn = Redis.new(host: \"\", port: , db: 0)\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   ` — connection port, standard — 6379`.\n\n\n\n\nMake sure that Oracle Java or OpenJDK are installed.\nInstall Jedis client.\nCreate a connection using the function Jedis:\n\n      import redis.clients.jedis.Jedis;\n\n   Jedis jedis = new Jedis(\"\");\n\n   Here `` — external IP address of the DB instance.\n\n\n\n\nMake sure that the Node.js installed.\nInstall node-redis client.\nConnect to the database using the function redis.createClient():\n\n      var redis = require(\"redis\"),\n\n   client = redis.createClient(, \"\");\n\n   Here:\n\n   `` — external IP address of the DB instance;\n   ` — connection port, standard — 6379`.\n\nLearn more about connecting to Redis in Node.js read in documentation.\n\n\n\n","url":"/en/dbs/dbaas/connect","description":"Connecting to databases via SSH or using an SDK.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"91ae80be-f228-4571-b1ab-76ce6ee97894","title":"Single and Master-Replica configurations","data":"\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect the project where you want to create a DB instance.\nGo to Databases → Database instances.\nClick the Create database or Add button.\nOn the “Configuration” step:\n\n   Select the required database type.\n   Select the required version.\n   Select the configuration Single or Master-Replica. The configuration is available for some DBMS types.\n   Click the Next step button.\n\nOn the “Create instance” step:\n\n   Set general instance parameters:\n\n      Database instance name: it can contain only Latin letters, numbers, and characters ., -, _.\n\n        Instance hostnames will consist of the specified name and a suffix. The suffix will be different for different DBMS.\n\n\n        In the Master-Replica configuration, the suffix -1 will be added to the instance name, which does not affect the host names. For example, if the instance name is my-instance, then the final instance name will take the form my-instance-1, and the hosts will get names like my-instance-..., but not my-instance-1-....\n\n\n      Category of virtual machine: select the flavor of the VM. For more information, see Cloud Servers service overview.\n\n      Type of virtual machine: configuration template for the DB instance.\n\n        Templates with high-performance CPUs are available on request to the support service. To use these templates, select the option Show only high performance CPUs.\n\n      Availability zone: availability zone for the DB instance.\n\n      Disk Type: disk type for the DB instance.\n\n      Disk size, GB: disk size (in gigabytes).\n\n        The larger the disk size, the higher its performance in some disk operations.\n\n      Enable volume autoscaling: select this option so that the disk size increases automatically when the disk is filled with data. When selecting this option, also specify The maximum volume size, GB.\n\n      Network: the network where the DB instance will be hosted. If the required network is not in the list, create it.\n\n      Assign an external IP: select this option to assign a floating IP address to the DB instance.\n\n        Such a DB instance will be accessible from the Internet.\n\n\n        The use of a floating IP address is charged.\n\n\n      Firewall settings: a list of security groups for the DB instance.\n\n        Add the ssh security group to the list to be able to connect to DB instance via SSH.\n\n      Create replica: this option affects the instance configuration and determines whether a replica will be added.\n\n        The option is available for the following DBMS types:\n\n        MySQL,\n        PostgreSQL,\n        PostgresPro Enterprise,\n        PostgresPro Enterprise 1C.\n\n        The option works like this:\n\n        If this option is not selected, a DB instance will be created in the Single configuration: with a single host with the Master role.\n        If this option is selected, a DB instance will be created in the Master-Replica configuration: with one host as Master and another host as Replica.\n\n\n        The name of each replica consists of the instance name and a suffix with the host sequence number (for example, my-instance-3). Instance hosts numbering starts with 1.\n\n\n      SSH access key: select an existing key or create a new one.\n\n        The key is used to connect to instance hosts via SSH.\n\n   Set up a backup. If necessary, the backup parameters can be set or changed after the cluster is created.\n\n      Backup is not available for Tarantool.\n\n      Disabled\n      Point-in-time recovery\n      Full\n\n\n      Select this option to not use backup for the DB instance.\n\n\n      This option is only available for PostgreSQL, Postgres Pro Enterprise and PostgresPro Enterprise 1C. This option allows you to perform Point-In-Time-Recovery backups (PITR).\n\n      If the option is selected, set the backup options:\n\n      Start time: the start time of the backup in the format HH:MM. The time zone indicated in the hint corresponds to the time zone of your device. The time format is 24 hours.\n\n      Store, number of copies: the number of copies to be stored.\n\n      Backup interval: the frequency of creating backups.\n\n\n      Set the backup options:\n\n      Set the Backup period.\n      If necessary, enable backup by GFS strategy and configure storage settings.\n      If GFS backup is not enabled, set the maximum number of full backups.\n\n\n   Select the option Enable monitoring if you need to monitor the DB instance using the tools of the VK Cloud.\n\n      This option is not available for Redis and MongoDB.\n\n   Click the Next step button.\n\nOn the “Initialization” step:\n\n   Specify the database initialization parameters. The available parameters depend on the Creation type selected:\n\n      New database\n      Restore from copy\n\n\n      A new empty database will be created.\n\n      There are no initialization parameters for Redis.\n\n      For Tarantool: set a username and password.\n\n      For other types of DBMS:\n\n      Specify the name of the database.\n      Specify a username and password.\n\n\n      This option is inactive if:\n\n      there are no backups corresponding to the selected DBMS type and version.\n      backup is not supported (for Tarantool).\n\n      The databases will be restored from the backup.\n\n      For PostreSQL, PostgresPro Enterprise and PostgresPro Enterprise 1C select Backup type:\n\n      Point-in-time recovery:\n\n        From the drop-down list Backup select the backup from which the recovery should be performed.\n        If necessary, select Date and time. The databases will be restored to the state at the specified time.\n\n          If you do not select a date and time, the recovery will be performed at the last available time.\n\n      Full: from the drop-down list Backup select the backup from which you want to restore.\n\n      For others DBMS from the drop-down list Backup select the backup from which you want to restore.\n\n\n   Click the Create database button.\n\n      Wait for the operation to complete. Creating a DB instance can take a long time.\n\n\n\n","url":"/en/dbs/dbaas/service-management/create/create-single-replica","description":"Creating a DB instance in Single and Master-Replica configurations to VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"33138913-cb67-4741-9f73-061cd6fe1faa","title":"Cluster configuration","data":"OpenSearch\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect the project where you want to create a cluster.\nGo to Databases → Database instances.\nClick the Create database or Add button.\nOn the “Configuration” step:\n\n   Select the OpenSearch database type.\n   Select the required OpenSearch version, for example, 2.\n   Click the Next step button.\n\nOn the “Parameters” step:\n\n   Set the general cluster parameters:\n\n      Database cluster name: it can contain only Latin letters, numbers, and characters ., _.\n\n        Cluster node names will consist of the specified name and a suffix. The suffix will be different for different DBMS.\n\n      Network: the network where the cluster will be hosted. If the required network is not in the list, create it.\n      SSH access key: select an existing key or create a new one.\n\n        The key is used to connect to instance hosts via SSH.\n\n   Set up a backup. If necessary, the backup parameters can be set or changed after the cluster is created.\n\n      Disabled\n      Full\n\n\n      Select this option to not use backup for the cluster.\n\n\n      Set the backup options:\n\n      Set the Backup period.\n      If necessary, enable backup by GFS strategy and configure storage settings.\n      If GFS backup is not enabled, set the maximum number of full backups.\n\n\n   Click the Next step button.\n\nOn the “Nodes” step:\n\n   In the Dashboards block, select the option Enable Dashboards if you need access to the OpenSearch Dashboards web interface.\n\n      Then set the parameters:\n\n      Category of virtual machine: select the flavor of the VM. For more information, see Cloud Servers service overview.\n      Type of virtual machine: configuration template for the cluster.\n\n        Templates with high-performance CPUs are available on request to the support service. To use these templates, select the option Show only high performance CPUs.\n\n      Availability zone: availability zone for the cluster.\n\n      Disk Type: disk type for the cluster.\n\n      Disk size, GB: disk size (in gigabytes).\n\n        The larger the disk size, the higher its performance in some disk operations.\n\n      Assign external IP: select this option to access the web interface from the Internet.\n\n        A node with the Dashboards role will be assigned a floating IP address.\n\n\n        The use of a floating IP address is charged.\n\n\n   In the Master Node block, select the option Use dedicated master nodes if you need to assign the master role to a dedicated cluster node.\n\n\n      If this option is not selected, the master role will be assigned to nodes with the data role. Learn more about the cluster structure and roles.\n\n\n      Then set the parameters:\n\n      Category of virtual machine: select the flavor of the VM. For more information, see Cloud Servers service overview.\n\n      Type of virtual machine: configuration template for the cluster.\n\n        Templates with high-performance CPUs are available on request to the support service. To use these templates, select the option Show only high performance CPUs.\n\n      Availability zone: availability zone for the cluster.\n\n      Disk Type: disk type for the cluster.\n\n      Disk size, GB: disk size (in gigabytes).\n\n        The larger the disk size, the higher its performance in some disk operations.\n\n   In the Data Node block, configure nodes with the data role.\n\n      Set the parameters:\n\n      Category of virtual machine: select the flavor of the VM. For more information, see Cloud Servers service overview.\n\n      Type of virtual machine: configuration template for the cluster.\n\n        Templates with high-performance CPUs are available on request to the support service. To use these templates, select the option Show only high performance CPUs.\n\n      Availability zone: availability zone for the cluster.\n\n      Disk Type: disk type for the cluster.\n\n      Disk size, GB: disk size (in gigabytes).\n\n        The larger the disk size, the higher its performance in some disk operations.\n\n      Number of nodes.\n\n   Click the Next step button.\n\nOn the “Initialization” step:\n\n   Specify the database initialization parameters. The available parameters depend on the Creation type selected:\n\n      New database\n      Restore from copy\n\n\n      A new empty database will be created.\n\n      Specify:\n\n      The name of the database.\n      A username and password.\n\n\n      This option is inactive if there are no backups corresponding to the selected DBMS type and version.\n\n      The databases will be restored from the backup.\n\n\n      Before restoring, make sure that:\n\n      The number of shards in the new DB instance is the same as in the instance backup.\n      The size of the disks in the new DB instance is not less than in the backup copy.\n      The names of the shards of the new DB instance are the same as in the backup.\n\n\n      From the drop-down list Backup select the backup from which you want to restore.\n\n\n   Click the Create database button.\n\n      Wait for the operation to complete. Creating a cluster can take a long time.\n\nMySQL, PostgreSQL, PostgresPro Enterprise, PostgresPro Enterprise 1C\n\n\n\nWhen creating a cluster of the listed DBMS types, a service load balancer will be created for it.\n\nUsing a load balancer charged.\n\n\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect the project where you want to create a cluster.\nGo to Databases → Database instances.\nClick the Create database or Add button.\nOn the “Configuration” step:\n\n   Select one of the database types: MySQL, PostgreSQL, PostgresPro Enterprise or PostgresPro Enterprise 1C.\n   Select the required version.\n   Select the configuration Cluster.\n   Click the Next step button.\n\nOn the “Create instance” step:\n\n   Set the general cluster parameters:\n\n      Database cluster name: it can contain only Latin letters, numbers, and characters ., -, _.\n\n        Cluster node names will consist of the specified name and a suffix. The suffix will be different for different DBMS.\n\n      Category of virtual machine: select the flavor of the VM. For more information, see Cloud Servers service overview.\n\n      Type of virtual machine: configuration template for the cluster.\n\n        Templates with high-performance CPUs are available on request to the support service. To use these templates, select the option Show only high performance CPUs.\n\n      Availability zone: availability zone for the cluster.\n\n      Disk Type: disk type for the cluster.\n\n      Disk size, GB: disk size (in gigabytes).\n\n        The larger the disk size, the higher its performance in some disk operations.\n\n      Enable volume autoscaling: select this option so that the disk size increases automatically when the disk is filled with data. When selecting this option, also specify The maximum volume size, GB.\n\n      Network: the network where the cluster will be hosted. If the required network is not in the list, create it.\n\n      Assign an external IP: select this option to assign a floating IP address to the cluster.\n\n        Such a cluster will be accessible from the Internet.\n\n\n        The use of a floating IP address is charged.\n\n\n      Firewall settings: a list of security groups for the cluster.\n\n        Add the ssh security group to the list to be able to connect to cluster hosts via SSH.\n\n      Number of nodes: the number of hosts in the cluster.\n\n      SSH access key: select an existing key or create a new one.\n\n        The key is used to connect to instance hosts via SSH.\n\n   Set up a backup. If necessary, the backup parameters can be set or changed after the cluster is created.\n\n      Disabled\n      Point-in-time recovery\n      Full\n\n\n      Select this option to not use backup for the cluster.\n\n\n      This option allows you to perform Point-In-Time-Recovery (PITR) backups. This option is not available for MySQL.\n\n      If the option is selected, set the backup options:\n\n      Start time: the start time of the backup in the format HH:MM. The time zone indicated in the hint corresponds to the time zone of your device. The time format is 24 hours.\n\n      Store, number of copies: the number of copies to be stored.\n\n      Backup interval: the frequency of creating backups.\n\n\n      Set the backup options:\n\n      Set the Backup period.\n      If necessary, enable backup by GFS strategy and configure storage settings.\n      If GFS backup is not enabled, set the maximum number of full backups.\n\n\n   Select the option Enable monitoring if you need to monitor the cluster using the tools of the VK Cloud.\n\n      This option is not available for MySQL.\n\n   Click the Next step button.\n\nOn the “Initialization” step:\n\n   Specify the database initialization parameters. The available parameters depend on the Creation type selected:\n\n      New database\n      Restore from copy\n\n\n      A new empty database will be created.\n\n      Specify:\n\n      The name of the database.\n      A username and password.\n\n\n      This option is inactive if there are no backups corresponding to the selected DBMS type and version.\n\n      The databases will be restored from the backup.\n\n      For MySQL: from the drop-down list Backup select the backup from which you want to restore.\n\n      For PostreSQL, PostgresPro Enterprise and PostgresPro Enterprise 1C select Backup type:\n\n      Point-in-time recovery:\n\n        From the drop-down list Backup select the backup from which the recovery should be performed.\n        If necessary, select Date and time. The databases will be restored to the state at the specified time.\n\n          If you do not select a date and time, the recovery will be performed at the last available time.\n\n      Full: from the drop-down list Backup select the backup from which you want to restore.\n\n\n   Click the Create database button.\n\n      Wait for the operation to complete. Creating a cluster can take a long time.\n\nTarantool\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect the project where you want to create a cluster.\nGo to Databases → Database instances.\nClick the Create database or Add button.\nOn the “Configuration” step:\n\n   Select the Tarantool database type.\n   Select the required version.\n   Select the configuration Cluster.\n   Click the Next step button.\n\nOn the “Create instance” step:\n\n   Set the general cluster parameters:\n\n      Database cluster name: it can contain only Latin letters, numbers, and characters ., -, _.\n\n        Cluster node names will consist of the specified name and a suffix. The suffix will be different for different DBMS.\n\n      Category of virtual machine: select the flavor of the VM. For more information, see Cloud Servers service overview.\n\n      Type of virtual machine: configuration template for the cluster.\n\n        Templates with high-performance CPUs are available on request to the support service. To use these templates, select the option Show only high performance CPUs.\n\n      Availability zone: availability zone for the cluster.\n\n      Disk Type: disk type for the cluster.\n\n      Disk size, GB: disk size (in gigabytes).\n\n        The larger the disk size, the higher its performance in some disk operations.\n\n      Network: the network where the cluster will be hosted. If the required network is not in the list, create it.\n\n      Assign external IP: select this option to assign a floating IP address to cluster hosts.\n\n        Such a cluster will be accessible from the Internet.\n\n\n        The use of a floating IP address is charged.\n\n\n      Firewall settings: a list of security groups for the cluster.\n\n        Add the ssh security group to the list to be able to connect to cluster hosts via SSH.\n\n      SSH access key: select an existing key or create a new one.\n\n        The key is used to connect to instance hosts via SSH.\n\n      Select the option Enable monitoring if you need to monitor the cluster using the VK Cloud tools.\n\n   Click the Next step button.\n\nOn the “Initialization” step:\n\n   Specify the database initialization parameters. The available parameters depend on the Creation type selected.\n\n      Only the New database type is available for Tarantool.\n\n      New database\n\n\n      A new empty database will be created.\n\n      Enter a username and password.\n\n\n   Click the Create database button.\n\n      Wait for the operation to complete. Creating a cluster can take a long time.\n\nOther types of DBMS\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect the project where you want to create a cluster.\nGo to Databases → Database instances.\nClick the Create database or Add button.\nOn the “Configuration” step:\n\n   Select the required database type.\n   Select the required version.\n   Select the configuration Cluster.\n   Click the Next step button.\n\nOn the “Parameters” step:\n\n   Set the general cluster parameters:\n\n      Database cluster name:\n\n        ClickHouse: it can contain only Latin letters and numbers;\n        MongoDB and Redis: it can contain only Latin letters, numbers, and characters -, _;\n\n        Cluster node names will consist of the specified name and a suffix. The suffix will be different for different DBMS.\n\n      Network: the network where the cluster will be hosted. If the required network is not in the list, create it.\n\n      Assign external IP: select this option to assign a floating IP address:\n\n        for the cluster (MongoDB);\n        for cluster hosts (ClickHouse).\n\n        Such a cluster will be accessible from the Internet.\n\n        This option is not available for Redis.\n\n\n        The use of a floating IP address is charged.\n\n\n      SSH access key: select an existing key or create a new one.\n\n        The key is used to connect to instance hosts via SSH.\n\n   Set up a backup. If necessary, the backup parameters can be set or changed after the cluster is created.\n\n      Disabled\n      Full\n\n\n      Select this option to not use backup for the cluster.\n\n\n      Set the backup options:\n\n      Set the Backup period.\n      If necessary, enable backup by GFS strategy and configure storage settings.\n      If GFS backup is not enabled, set the maximum number of full backups.\n\n\n   Select the option Enable monitoring if you need to monitor the cluster using the VK Cloud tools.\n\n      This option is not available for MongoDB.\n\n   Click the Next step button.\n\nIn the “Shards” step, set the shards settings.\n\n   For ClickHouse, the number of shards can be changed by adding and removing shards (there must be at least one shard in the cluster). For Redis and MongoDB, the number of shards is fixed.\n\n   For each shard, set:\n\n      Name of shard.\n      Number of replicas in shard.\n\n        For Redis, the number of replicas in sharjah is fixed.\n\n      Category of virtual machine: select the flavor of the VM. For more information, see Cloud Servers service overview.\n\n      Type of virtual machine: configuration template for the shard hosts.\n\n        Templates with high-performance CPUs are available on request to the support service. To use these templates, select the option Show only high performance CPUs.\n\n      Availability zone: availability zone for the shard hosts.\n\n      Disk Type: disk type for the shard hosts.\n\n      Disk size, GB: disk size (in gigabytes).\n\n        The larger the disk size, the higher its performance in some disk operations.\n\n      Enable volume autoscaling: select this option so that the disk size increases automatically when the disk is filled with data. When selecting this option, also specify The maximum volume size, GB.\n\n   Click the Next step button.\n\nOn the “Initialization” step:\n\n   Specify the database initialization parameters. The available parameters depend on the Creation type selected:\n\n      New database\n      Restore from copy\n\n\n      A new empty database will be created.\n\n      There are no initialization parameters for Redis.\n\n      For others DBMS specify:\n\n      The name of the database.\n      A username and password.\n\n\n      This option is inactive if there are no backups corresponding to the selected DBMS type and version.\n\n      The databases will be restored from the backup.\n\n\n      Before restoring, make sure that:\n\n      The number of shards in the new DB instance is the same as in the instance backup.\n      The size of the disks in the new DB instance is not less than in the backup copy.\n      The names of the shards of the new DB instance are the same as in the backup.\n\n\n      From the drop-down list Backup select the backup from which you want to restore.\n\n\n   Click the Create database button.\n\n      Wait for the operation to complete. Creating a cluster can take a long time.\n\n\n\n","url":"/en/dbs/dbaas/service-management/create/create-cluster","description":"Creating a DB instance in the Cluster configuration to VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"33baa411-3d3d-4fae-9be2-a21cfeab82fa","title":"Changing DB Instance parameters","data":"Сервис Cloud Databases дает возможность задавать параметры инстансов БД, позволяя влиять на их работу. Для каждого типа СУБД есть свой набор параметров.\n\nЧтобы добавить, изменить или удалить параметры:\n\n\n\nЛичный кабинет\n\n\n\nПерейдите в личный кабинет VK Cloud.\nВыберите проект, где находится нужный инстанс БД.\nПерейдите в раздел Базы данных → Инстансы баз данных.\nУбедитесь, что нужный инстанс баз данных запущен.\nНажмите на имя инстанса.\nПерейдите на вкладку Параметры баз данных или Параметры кластера.\nВыполните требуемые действия:\n\n   Чтобы добавить параметр:\n\n     Нажмите Добавить параметр.\n     Выберите нужный параметр из выпадающего списка и задайте его значение в поле рядом с названием параметра.\n     При необходимости добавьте другие нужные параметры.\n     Нажмите кнопку Сохранить.\n\n   Чтобы изменить параметр:\n\n     Задайте новое значение в поле рядом с названием параметра.\n     Нажмите кнопку Сохранить.\n\n   Чтобы удалить параметр:\n\n     Нажмите на значок Корзина в строке параметра.\n     Нажмите кнопку Сохранить.\n\n\n\n","url":"/en/dbs/dbaas/service-management/db-config","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"fcce7858-28b6-49fd-8df5-d933a0517591","title":"Getting database logs","data":"Важно\n\nЛоги баз данных доступны только для БД, созданных после 1 декабря 2020 года.\n\nВ настоящее время реализована возможность получения логов для следующих баз данных:\n\nMySQL\nPostgreSQL/PostgresPro\nClickHouse\nRedis\nMongoBD\n\nОтметим, что для реализации получения логов необходимо подключиться к БД по SSH.\n\nНиже представлено детальное описание получения логов из каждого типа БД.\n\nMySQL\n\nДля получения логов необходимо подключиться по SSH и ввести следующую команду:\n\n\\[admin@mysql-8658 ~\\]journalctl -u mysqld\n\nДалее, ответом будут выведены логи:\n\n\\-- Logs begin at Fri 2020-12-25 08:35:15 UTC, end at Mon 2021-01-11 18:41:34 UTC. --\nDec 25 08:35:21 mysql-v5dot6-2020-11-30.novalocal systemd\\[1\\]: Starting MySQL Community Server...\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 0 \\[Warning\\] TIMESTAMP with implicit D\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 0 \\[Note\\] Ignoring --secure-file-priv\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 0 \\[Note\\] /usr/sbin/mysqld (mysqld 5.6\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: Using atomics to\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: The InnoDB memory\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: Mutexes and rw\\_lo\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: Memory barrier is\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: Compressed tables\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: Using Linux nativ\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: Using CPU crc32 i\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: Initializing buff\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: Completed initial\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: The first specifi\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: Setting file ./ib\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: Database physical\nDec 25 08:35:25 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:25 1276 \\[Note\\] InnoDB: Setting log file\nDec 25 08:35:26 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:26 1276 \\[Note\\] InnoDB: Setting log file\nDec 25 08:35:26 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:26 1276 \\[Note\\] InnoDB: Renaming log file\nDec 25 08:35:26 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:26 1276 \\[Warning\\] InnoDB: New log files\nDec 25 08:35:26 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:26 1276 \\[Note\\] InnoDB: Doublewrite buffe\nDec 25 08:35:26 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:26 1276 \\[Note\\] InnoDB: Doublewrite buffe\nDec 25 08:35:26 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:26 1276 \\[Note\\] InnoDB: 128 rollback segm\nDec 25 08:35:26 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:26 1276 \\[Warning\\] InnoDB: Creating forei\nDec 25 08:35:26 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:26 1276 \\[Note\\] InnoDB: Foreign key const\nDec 25 08:35:26 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:26 1276 \\[Note\\] InnoDB: Creating tablespa\nDec 25 08:35:26 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:26 1276 \\[Note\\] InnoDB: Tablespace and da\nDec 25 08:35:26 mysql-8658.novalocal mysql-systemd-start\\[867\\]: 2020-12-25 08:35:26 1276 \\[Note\\] InnoDB: Waiting for purge\nlines 1-29\n\nPostgreSQL/PostgresPro\n\nДля получения логов необходимо подключиться по SSH и ввести следующую команду:\n\n\\[admin@postgresql-9641 ~\\]journalctl -u postgresql\n\nДалее, ответом будут выведены логи:\n\n\\-- Logs begin at Fri 2020-12-25 07:48:19 UTC, end at Mon 2021-01-11 18:46:16 UTC. --\nDec 25 07:48:25 postgresql-12-2020-12-11.novalocal systemd\\[1\\]: Starting PostgreSQL 12 database server...\nDec 25 07:48:25 postgresql-12-2020-12-11.novalocal postmaster\\[883\\]: 2020-12-25 07:48:25.309 UTC \\[883\\] LOG:  starting Pos\nDec 25 07:48:25 postgresql-12-2020-12-11.novalocal postmaster\\[883\\]: 2020-12-25 07:48:25.310 UTC \\[883\\] LOG:  listening on\nDec 25 07:48:25 postgresql-12-2020-12-11.novalocal postmaster\\[883\\]: 2020-12-25 07:48:25.310 UTC \\[883\\] LOG:  listening on\nDec 25 07:48:25 postgresql-12-2020-12-11.novalocal postmaster\\[883\\]: 2020-12-25 07:48:25.315 UTC \\[883\\] LOG:  listening on\nDec 25 07:48:25 postgresql-12-2020-12-11.novalocal postmaster\\[883\\]: 2020-12-25 07:48:25.364 UTC \\[883\\] LOG:  listening on\nDec 25 07:48:25 postgresql-12-2020-12-11.novalocal postmaster\\[883\\]: 2020-12-25 07:48:25.414 UTC \\[883\\] LOG:  redirecting\nDec 25 07:48:25 postgresql-12-2020-12-11.novalocal postmaster\\[883\\]: 2020-12-25 07:48:25.414 UTC \\[883\\] HINT:  Future log\nDec 25 07:48:25 postgresql-12-2020-12-11.novalocal systemd\\[1\\]: Started PostgreSQL 12 database server.\nDec 25 07:48:32 postgresql-9641.novalocal systemd\\[1\\]: Stopping PostgreSQL 12 database server...\nDec 25 07:48:32 postgresql-9641.novalocal systemd\\[1\\]: Stopped PostgreSQL 12 database server.\nDec 25 07:48:40 postgresql-9641.novalocal systemd\\[1\\]: Starting PostgreSQL 12 database server...\nDec 25 07:48:40 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 07:48:40.885 GMT \\[1660\\] LOG:  starting PostgreSQL\nDec 25 07:48:40 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 07:48:40.885 GMT \\[1660\\] LOG:  listening on IPv4 a\nDec 25 07:48:40 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 07:48:40.885 GMT \\[1660\\] LOG:  listening on IPv6 a\nDec 25 07:48:40 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 07:48:40.905 GMT \\[1660\\] LOG:  listening on Unix s\nDec 25 07:48:40 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 07:48:40.962 GMT \\[1662\\] LOG:  database system was\nDec 25 07:48:40 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 07:48:40.998 GMT \\[1660\\] LOG:  database system is\nDec 25 07:48:41 postgresql-9641.novalocal systemd\\[1\\]: Started PostgreSQL 12 database server.\nDec 25 07:48:41 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 07:48:41.023 GMT \\[1671\\] FATAL:  role \"os\\_admin\" d\nDec 25 07:48:41 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 07:48:41.039 GMT \\[1673\\] FATAL:  role \"os\\_admin\" d\nDec 25 07:48:41 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 07:48:41.082 GMT \\[1677\\] FATAL:  role \"os\\_admin\" d\nDec 25 13:20:10 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 13:20:10.575 GMT \\[16971\\] FATAL:  unsupported fron\nDec 25 13:20:10 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 13:20:10.833 GMT \\[16972\\] FATAL:  unsupported fron\nDec 25 13:20:11 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 13:20:11.086 GMT \\[16973\\] FATAL:  no PostgreSQL us\nDec 25 16:16:02 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 16:16:02.956 GMT \\[19980\\] FATAL:  unsupported fron\nDec 25 16:16:03 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 16:16:03.333 GMT \\[19981\\] FATAL:  unsupported fron\nDec 25 16:16:03 postgresql-9641.novalocal postmaster\\[1660\\]: 2020-12-25 16:16:03.711 GMT \\[19982\\] FATAL:  no PostgreSQL us\nlines 1-29\n\nClickHouse\n\nДля получения логов необходимо подключиться по SSH и ввести следующую команду:\n\n\\[admin@clickhouse5992 ~\\]less /var/log/clickhouse-server/clickhouse-server.log\n\nДалее, ответом будут выведены логи:\n\n2020.12.25 20:27:13.406404 \\[ 1025 \\] {}  SentryWriter: Sending crash reports is disabled\n2020.12.25 20:27:13.433539 \\[ 1025 \\] {}  Pipe: Pipe capacity is 1.00 MiB\n2020.12.25 20:27:13.583904 \\[ 1025 \\] {}  : Starting ClickHouse 20.8.4.11 with revision 54438, no build id, PID 1025\n2020.12.25 20:27:13.584058 \\[ 1025 \\] {}  Application: starting up\n2020.12.25 20:27:13.823630 \\[ 1025 \\] {}  Application: Will mlockall to prevent executable memory from being paged out. It may take a few seconds.\n2020.12.25 20:27:14.690702 \\[ 1025 \\] {}  Application: The memory map of clickhouse executable has been mlock'ed\n2020.12.25 20:27:14.691026 \\[ 1025 \\] {}  Application: rlimit on number of file descriptors is 500000\n2020.12.25 20:27:14.691049 \\[ 1025 \\] {}  Application: Initializing DateLUT.\n2020.12.25 20:27:14.691056 \\[ 1025 \\] {}  Application: Initialized DateLUT with time zone 'UTC'.\n2020.12.25 20:27:14.691101 \\[ 1025 \\] {}  Application: Setting up /var/lib/clickhouse/tmp/ to store temporary data in it\n2020.12.25 20:27:14.770070 \\[ 1025 \\] {}  Application: Configuration parameter 'interserver\\_http\\_host' doesn't exist or exists and empty. Will use 'clickhouse5992.novalocal' as replica host.\n2020.12.25 20:27:14.771874 \\[ 1025 \\] {}  ConfigReloader: Loading config '/etc/clickhouse-server/users.xml'\n2020.12.25 20:27:14.773718 \\[ 1025 \\] {}  ConfigReloader: Loaded config '/etc/clickhouse-server/users.xml', performing update on configuration\n2020.12.25 20:27:14.774576 \\[ 1025 \\] {}  ConfigReloader: Loaded config '/etc/clickhouse-server/users.xml', performed update on configuration\n2020.12.25 20:27:14.775142 \\[ 1025 \\] {}  Access(local directory): File /var/lib/clickhouse/access/users.list doesn't exist\n2020.12.25 20:27:14.775180 \\[ 1025 \\] {}  Access(local directory): Recovering lists in directory /var/lib/clickhouse/access/\n2020.12.25 20:27:14.775619 \\[ 1025 \\] {}  Application: Uncompressed cache size was lowered to 1.85 GiB because the system has low amount of memory\n2020.12.25 20:27:14.775701 \\[ 1025 \\] {}  Application: Mark cache size was lowered to 1.85 GiB because the system has low amount of memory\n2020.12.25 20:27:14.775850 \\[ 1025 \\] {}  Application: Setting max\\_server\\_memory\\_usage was set to 3.33 GiB (3/var/log/clickhouse-server/clickhouse-server.log\n\nRedis\n\nДля получения логов необходимо подключиться по SSH и ввести следующую команду:\n\n \\[admin@redis-4275 ~\\]journalctl -u redis\n\nДалее, ответом будут выведены логи:\n\n\\-- Logs begin at Mon 2021-01-11 15:45:47 UTC, end at Mon 2021-01-11 19:06:17 UTC. --\nJan 11 15:45:47 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:47.013 \\* MASTER  REPLICA sync start\nJan 11 15:45:47 redis-4275.novalocal redis\\[1440\\]: MASTER  REPLICA sync started\nJan 11 15:45:47 redis-4275.novalocal redis\\[1440\\]: Non blocking connect for SYNC fired the event.\nJan 11 15:45:47 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:47.034 \\* Non blocking connect for SYNC\nJan 11 15:45:47 redis-4275.novalocal redis\\[1440\\]: Error reply to PING from master: '-Reading from master: Resource tempo\nJan 11 15:45:47 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:47.050 # Error reply to PING from mast\nJan 11 15:45:48 redis-4275.novalocal redis\\[1440\\]: Connecting to MASTER 194.40.243.61:8886\nJan 11 15:45:48 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:48.016 \\* Connecting to MASTER 194.40.2\nJan 11 15:45:48 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:48.016 \\* MASTER  REPLICA sync start\nJan 11 15:45:48 redis-4275.novalocal redis\\[1440\\]: MASTER  REPLICA sync started\nJan 11 15:45:48 redis-4275.novalocal redis\\[1440\\]: Non blocking connect for SYNC fired the event.\nJan 11 15:45:48 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:48.035 \\* Non blocking connect for SYNC\nJan 11 15:45:48 redis-4275.novalocal redis\\[1440\\]: Master replied to PING, replication can continue...\nJan 11 15:45:48 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:48.053 \\* Master replied to PING, repli\nJan 11 15:45:48 redis-4275.novalocal redis\\[1440\\]: Partial resynchronization not possible (no cached master)\nJan 11 15:45:48 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:48.089 \\* Partial resynchronization not\nJan 11 15:45:48 redis-4275.novalocal redis\\[1440\\]: Full resync from master: ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ:1\nJan 11 15:45:48 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:48.108 \\* Full resync from master: ZZZZ\nJan 11 15:45:48 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:48.108 \\* MASTER  REPLICA sync: rece\nJan 11 15:45:48 redis-4275.novalocal redis\\[1440\\]: MASTER  REPLICA sync: receiving 55648 bytes from master\nJan 11 15:45:48 redis-4275.novalocal redis\\[1440\\]: MASTER  REPLICA sync: Flushing old data\nJan 11 15:45:48 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:48.181 \\* MASTER  REPLICA sync: Flus\nJan 11 15:45:48 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:48.182 \\* MASTER  REPLICA sync: Load\nJan 11 15:45:48 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:48.182 # Wrong signature trying to loa\nJan 11 15:45:48 redis-4275.novalocal redis-server\\[1440\\]: 1440:S 11 Jan 2021 15:45:48.182 # Failed trying to load the MAS\nJan 11 15:45:48 redis-4275.novalocal redis\\[1440\\]: MASTER  REPLICA sync: Loading DB in memory\nJan 11 15:45:48 redis-4275.novalocal redis\\[1440\\]: Wrong signature trying to load DB from file\nJan 11 15:45:48 redis-4275.novalocal redis\\[1440\\]: Failed trying to load the MASTER synchronization DB from disk\nlines 1-29\n\nMongoBD\n\nДля получения логов необходимо подключиться по SSH и ввести следующую команду:\n\n\\[admin@mongodb-5604 ~\\]$ journalctl -u mongod\n\nДалее, ответом будут выведены логи:\n\n\\-- Logs begin at Thu 2020-12-31 03:20:03 UTC, end at Mon 2021-01-11 19:09:03 UTC. --\nDec 31 03:20:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:20:16.602+0000 I NETWORK  \\[listener\\] connection\nDec 31 03:20:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:20:16.602+0000 I NETWORK  \\[conn49341\\] received\nDec 31 03:20:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:20:16.605+0000 I NETWORK  \\[listener\\] connection\nDec 31 03:20:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:20:16.605+0000 I NETWORK  \\[conn49342\\] received\nDec 31 03:20:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:20:16.608+0000 I NETWORK  \\[conn49341\\] end conne\nDec 31 03:20:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:20:16.608+0000 I NETWORK  \\[conn49342\\] end conne\nDec 31 03:21:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:21:16.604+0000 I NETWORK  \\[listener\\] connection\nDec 31 03:21:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:21:16.605+0000 I NETWORK  \\[conn49343\\] received\nDec 31 03:21:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:21:16.606+0000 I NETWORK  \\[listener\\] connection\nDec 31 03:21:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:21:16.607+0000 I NETWORK  \\[conn49344\\] received\nDec 31 03:21:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:21:16.609+0000 I NETWORK  \\[conn49343\\] end conne\nDec 31 03:21:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:21:16.610+0000 I NETWORK  \\[conn49344\\] end conne\nDec 31 03:22:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:22:16.607+0000 I NETWORK  \\[listener\\] connection\nDec 31 03:22:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:22:16.608+0000 I NETWORK  \\[conn49345\\] received\nDec 31 03:22:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:22:16.609+0000 I NETWORK  \\[listener\\] connection\nDec 31 03:22:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:22:16.609+0000 I NETWORK  \\[conn49346\\] received\nDec 31 03:22:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:22:16.611+0000 I NETWORK  \\[conn49345\\] end conne\nDec 31 03:22:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:22:16.612+0000 I NETWORK  \\[conn49346\\] end conne\nDec 31 03:23:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:23:16.612+0000 I NETWORK  \\[listener\\] connection\nDec 31 03:23:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:23:16.613+0000 I NETWORK  \\[conn49347\\] received\nDec 31 03:23:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:23:16.613+0000 I NETWORK  \\[listener\\] connection\nDec 31 03:23:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:23:16.614+0000 I NETWORK  \\[conn49348\\] received\nDec 31 03:23:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:23:16.615+0000 I NETWORK  \\[conn49347\\] end conne\nDec 31 03:23:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:23:16.616+0000 I NETWORK  \\[conn49348\\] end conne\nDec 31 03:24:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:24:16.611+0000 I NETWORK  \\[listener\\] connection\nDec 31 03:24:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:24:16.611+0000 I NETWORK  \\[conn49349\\] received\nDec 31 03:24:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:24:16.612+0000 I NETWORK  \\[listener\\] connection\nDec 31 03:24:16 mongodb-5604.novalocal mongod.27017\\[1718\\]: 2020-12-31T03:24:16.613+0000 I NETWORK  \\[conn49350\\] received\nlines 1-29\n\n--\n","url":"/en/dbs/dbaas/service-management/dbaas-logs","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"393fd594-cb20-466a-9379-5f5aeb395fae","title":"Deleting a DB instance","data":"To delete an instance, it is enough to select \"Delete instance\" in the context menu and confirm the deletion with the appropriate button.\n","url":"/en/dbs/dbaas/service-management/delete","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"a0a3d85d-1d1b-41b7-90bb-59019b6910a7","title":"Users management","data":"Управление пользователями\n\nУправление пользователями БД и правами его доступа производится на вкладке “Пользователи”:\n\n\n\nПри добавлении нового пользователя необходимо указать его имя, а в поле “Применить для следующих баз данных” необходимо выбрать БД, которые должны быть доступны этому пользователю:\n\n\n\nИзменить список БД, доступных пользователю, можно через редактирование пользователя.\n\nВажно\n\nСуществуют ограничения на использование имён пользователей (см. таблицу ниже).\n\nИмена и префиксы служебных пользователей по типам СУБД, использование которых запрещено:\n\nТип СУБДИмя пользователяВсе имена пользователя с префиксомPosgreSQLos_adminpostgresrootreplicatormcs_MySQLos_adminrootclusterrepusermcs_Clickhouseos_admindefaultrootreplicatormcs_mongoDBos_adminrootmcs_\n","url":"/en/dbs/dbaas/service-management/users","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"967cc6f8-302b-4da9-bdaa-9f0835ef264d","title":"Updating the PostgreSQL version","data":"To get access to new PostgreSQL features, you need to upgrade your DBMS versions. Patch management allows you to upgrade Postgres and Postgres Pro from a lower version to a higher version.\n\nTo upgrade PostgreSQL, use the pg_upgrade utility.\n\n\n\nA backup will be made before the update. Plan ahead for the upgrade because PostgreSQL will be unavailable at the time of the upgrade.\n\n\n\n\n\nSingle instance\nMater + replicas\nCluster\n\n\n\nMake an API request:\n\ncurl --location --request PATCH '/v1.0/project_id/instances/instance_id' \\\n--header 'X-Auth-Token:  ' \\\n--header 'Content-Type: application/json' \\\n-d '{\n  \"instance\":{\n      \"datastore_version\":\n  }\n}'\n\n\n\n\nTo update the replicas, a new version of PostgreSQL will be installed on the servers and the data on the replication tools will be synchronized with the master.\n\nMake an API request:\n\ncurl --location --request PATCH '/v1.0/project_id/instances/instance_id' \\\n--header 'X-Auth-Token:  ' \\\n--header 'Content-Type: application/json' \\\n-d '{\n  \"instance\":{\n      \"datastore_version\":\n  }\n}'\n\n\n\n\nTo update the replicas, a new version of the DBMS will be installed and Patroni will synchronize the data on them.\n\nMake an API request:\ncurl --location --request POST '/v1.0/project_id/clusters/cluster_id' \\\n--header 'X-Auth-Token: ' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n  \"upgrade\": [\n      {\n          \"datastore_version\":\n      }\n  ]\n}'\n\n\n\n","url":"/en/dbs/dbaas/service-management/postgre-update","description":"Updating the PostgreSQL version on instances in Single, Master-Replica, Cluster configurations.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"cfaff7ad-9982-4994-b907-f8b99a81f48b","title":"Replication","data":"The Cloud Databases service allows you to add replicas to existing databases created in the master-replica configuration.\n\nIn personal account VK Cloud go to Databases → Database Instances.\nClick   for the database that was created in the master-replica configuration.\nSelect Create Replica.\nOn the page that opens, fill in the fields for creating a new database.\nPress the button Create a replica.\n\nThe DB replica will appear in the general list of DB instances.\n","url":"/en/dbs/dbaas/service-management/replication","description":"Managing database scaling by creating replicas allows for even load balancing.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"fed2a654-c8d8-42bb-84cc-480cb7cba587","title":"Managing extensions","data":"You can install extensions for DB instances and change extension parameters. Depending on the type of DB instance, different extensions will be available.\n\nViewing installed extensions\n\n\n\nOpenSearch\nOther DBMS\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the required DB instance is located.\nGo to Databases → Database instances.\nClick on the instance name and go to the Plugins tab.\n\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the required DB instance is located.\nGo to Databases → Database instances.\nClick on the instance name and go to the Extensions tab.\n\nInstalling the extension\n\n\n\nOpenSearch\nOther DBMS\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the required DB instance is located.\nGo to Databases → Database instances.\nMake sure that the required DB instance is running.\nClick on the instance name and go to the Plugins tab.\nClick the Add button.\nSelect one or more extensions to install from the list.\nClick the Install selected plugins.\n\n   The installation of the extension will begin. After the process is completed, the extension will appear in the general list.\n\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the required DB instance is located.\nGo to Databases → Database instances.\nMake sure that the required DB instance is running.\nClick on the instance name and go to the Extensions tab.\nClick the Add button.\nSelect the extension to install from the list.\nSet the required extension parameters.\nIf necessary, edit the optional extension parameters:\n\n   Click the link Add parameter.\n   Select the required parameter from the drop-down list and specify its value. If necessary, add more parameters.\n\n\n      To delete a parameter, click on the icon Trash.\n\n\nClick the Add button.\n\n   The installation of the extension will begin. After the process is completed, the extension will appear in the general list.\n\nChanging extension parameters\n\n\n\nChanging extension parameters is not available for OpenSearch.\n\n\n\nTo change the extension settings:\n\nGo to VK Cloud personal account.\nSelect the project where the required DB instance is located.\nGo to Databases → Database instances.\nMake sure that the required DB instance is running.\nClick on the instance name and go to the Extensions tab.\nClick on the icon Pencil in the extension line whose parameters you want to change.\nChange the extension settings.\nClick the Save button.\n","url":"/en/dbs/dbaas/service-management/managing-extensions","description":"How to manage DB instance extensions in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"48955f3f-64e0-4156-8b2d-c66e270d6d39","title":"Migration from local databases","data":"Для миграции данных из локальной базы в облачную нужно выполнить три шага:\n\nВыбрать тип коннектора к базе\nПодключиться к инстансу\nЗагрузить свои данные\n\nВыбор типа коннектора\n\nТип коннектора выбирается в зависимости от того, какие локальные приложения установлены и используются. Ниже приведён пример коннектора для языка Python:\n\nfrom pymongo import MongoClient\n\nclient = MongoClient(\"mongodb://:@172.00.0.0\")\n\ndb = client.\n\nПодключение к инстансу\n\nДля просмотра способов подключения к инстансу можно кликнуть по названию инстанса в списке виртуальных машин, или навести курсор на значок информации (i).\n\nКарточка инстанса отображает всю необходимую информацию.\n\nВ разделе «Параметры подключения» приведены примеры кода из популярных языков.\n\nПри использовании примеров следует заменить макросы `,,` на актуальные значения:\n\n`` - название базы данных, указанное при создании.\n`` - имя пользователя (указывается при создании).\n`` - пароль пользователя (указывается при создании).\n\nДополнительную информацию можно найти в документации по используемому коннектору (коннектор - сущность, через которую выполняется подключение к БД).\n\nЗагрузка данных\n\nНиже приведён пример кода загрузки в базу SQL с помощью языка PHP:\n\n\" . mysqli\\_error($conn);}mysqli\\_close($conn);?>\n\nВажно\n\nНапрямую дамп локальной базы нельзя развернуть в VK Cloud. Только перелить данные с помощью коннекторов по инструкции выше.\n","url":"/en/dbs/dbaas/how-to-guides/db-migration","description":"Migration of a local database to a deployed instance of the VK Cloud DBMS.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"241ee89c-2407-4b63-a6f5-8d968f599d28","title":"Using the PostGIS extension in PostgreSQL and PostgresPro","data":"How to build a spatial query?\n\nwhich will contain your GIS, you must create a table with a column of type geometria, which will include your GIS data. Connect to your database using PSQL and execute SQL:\n\nCREATE A gtest TABLE ( ID int4, NAME varchar(20) ); SELECT AddGeometryColumn(\", 'gtest','geom', -1,'LINESTRING',2);\n\nIf the geometry failed to add a column, then you probably haven't loaded functions and objects from PostGIS into your database. See the installation instructions.\n\nNext, you can insert the geometry into the table using the SQL command insert. The GIS object will be formatted according to the format of the well-known text of the OpenGIS Consortium:\n\nINSERT INTO gtest (ID, NAME, GEOM) VALUES (1, 'First Geometry', GeomFromText('LINESTRING(2 3,4 5,6 5,7 8)', -1) );\n\nDetailed information about other GIS objects can be found in the object directory. Viewing your GIS data in a table:\n\nSELECT ID, name, AsText(geom) AS geom FROM gtest;\n\nThe result should look something like this:\n\n id | name           | geom\n+----------------+------------------------------\n 1  | First geometry | line(2 3,4 5,6 5,7 8) (1 row)\n\nHow to insert a GIS object into a database?\n\nLike you build other database queries using the SQL query environment to get values, functions, and logic tests.\n\nWhen building spatial queries, you should take into account:\n\nis there a spatial index that can be used;\nwhether it is necessary to perform complex calculations on a large number of geometries.\n\nMost often you will need an \"intersection operator\" (&&), which checks whether the boundaries of objects intersect. The benefit of the && operator is that it can use a spatial index if it exists. This will speed up the execution of the request.\n\nIn addition, you can use spatial functions such as Distance(), ST_Intersects(), ST_Contains() and ST_Within() and others to narrow the search results. Most spatial queries include an index test and a spatial function test. The index test is useful because it limits the number of value checks to those that can fall into the required set. Next, spatial functions are used for the final verification of the condition.\n\nSELECT id, the_geom FROM thetable WHERE the_geom && 'POLYGON((0 0, 0 10, 10 10, 10 0, 0 0))' AND _ST_Contains(the_geom,'POLYGON((0 0, 0 10, 10 10, 10 0, 0 0))');\n","url":"/en/dbs/dbaas/how-to-guides/using-postgis","description":"Getting started with the PostGIS extension in PostgreSQL and PostgresPro.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"6cfa6eb4-dade-43d0-b0c3-45f06a770e65","title":"MongoDB replication and update on Ubuntu","data":"In this article, we will look at how to replicate and upgrade MongoDB on Ubuntu 18.04.\n\n\n\nYou can learn about the hardware configuration, as well as how to install and configure MongoDB in the article Installing and configuring MongoDB on Ubuntu.\n\n\n\n\n\nIn the description below, we use the following server names and IP addresses:\n\nserver 1 - mongo1.testdomain.com (10.0.0.2);\nserver 2 - mongo2.testdomain.com (10.0.0.3);\nserver 3 - mongo1.testdomain.com (10.0.0.4).\n\nSet up ReplicaSet MongoDB\n\nReplicaset is multiple servers that contain the same set of data, provide failover protection, and provide high data availability. Replicaset MongoDB has one primary node for writing and reading, and one or more secondary nodes that are synchronized with the primary node and provide the ability to read data to reduce load. In the event of a failure in the operation of the primary node, one of the secondary nodes is automatically assigned as the primary node. To speed up the selection of a new primary node, an arbiter node is designed that does not contain data. Also, the advantage of Replicaset is the ability to update the cluster without having to stop its operation.\n\nMake sure port 27017 is open on all three servers.\nMake sure the name of each host (mongo1.testdomain.com, mongo2.testdomain.com, mongoar.testdomain.com) is known on each server. If the names are not registered in DNS, on each server, specify them in the /etc/hosts file. In our case:\n\n10.0.0.2 mongo1 mongo1.testdomain.com\n10.0.0.3 mongo2 mongo2.testdomain.com\n10.0.0.4 mongoar mongoar.testdomain.com\n\nAdd the following to the /etc/mongod.conf configuration file in the replication section:\n\nreplication:\n    replSetName: \"rs0\"\nsecurity:\n    authorization: enabled\n    clusterAuthMode: x509\n    clusterIpSourceWhitelist:\n    10.0.0.2\n    10.0.0.3\n    10.0.0.4\n127.0.0.1\n\nRestart mongod:\n\nroot@mongo1:~# systemctl restart mongod.service\n\nSign in to the mongo console and run the command:\n\n\\> rs.initiate()\n{\n    \"info2\" : \"no configuration specified. Using a default configuration for the set\",\n    \"me\" : \"mongo1:27017\",\n    \"ok\" : 1\n}\nrs0:SECONDARY>\nrs0:PRIMARY>\n\nReplicaset is initialized, node is in primary state.\n\nIn the configuration file, in the host variable, instead of the value mongo1:27017, specify the CommonName value of the certificate:\n\nrs0:PRIMARY> cfg.members[0].host = \"mongo1.testdomain.com:27017\"\nmongo1.testdomain.com:27017\nrs0:PRIMARY> rs.reconfig(cfg)\n{\n    \"ok\" : 1,\n    \"operationTime\" : Timestamp(1577365311, 1),\n    \"$clusterTime\" : {\n    \"clusterTime\" : Timestamp(1577365311, 1),\n    \"signature\" : {\n \"hash\" : BinData(0,\"4b6xDb/6y5tuq9jDjEDjbscYmGk=\"),\n    \"keyId\" : NumberLong(\"6774685910094053378\")\n        }\n    }\n}\nrs0:PRIMARY>\n\nSign in to the mongo2 server, import the MongoDB repository key, add the MongoDB repository and install MongoDB (how to do this, see the article Installing and configuring MongoDB on Ubuntu).\nIf you edited the /etc/hosts file on the mongo1 server, do the same on the current server.\nFrom the mongo1 server, copy the configuration file /etc/mongod.conf to the current server, change the key name in the file PEMKeyFile: from /etc/ssl/mongo1.pem to PEMKeyFile: /etc/ssl/mongo2 .pem.\nCopy the /etc/ssl/mongoCA.pem key to the current server.\nIssue the server certificate, just like on the mongo1 server, using mongo2.testdomain.com as the CommonName:\n\nroot@mongo2:~# openssl genrsa -out /tmp/mongo2.key 4096\nroot@mongo2:~# openssl req -new -key /tmp/mongo2.key -out /tmp/mongo2.csr\n...\nCommon Name (e.g. server FQDN or YOUR name) []:mongo2.testdomain.com\n...\nroot@mongo2:~# openssl x509 -req -in /tmp/mongo2.csr -CA /etc/ssl/mongoCA.pem -CAcreateserial -out /tmp/mongo2.crt -days 10000\nroot@mongo2:~# cat /tmp/mongo2.key /tmp/mongo2.crt > /etc/ssl/mongo2.pem\nroot@mongo2:~# rm /tmp/mongo2.key /tmp/mongo2.crt /tmp/mongo2.csr\n\nIf the var/lib/mongodb directory is not empty, delete all files from it.\nRestart the mongod server and add it to the list of applications to load automatically.\nLogin to the mongo1 server and sign in to the mongo console.\nAdd the mongo2 server to replicaset:\n\nroot@mongo1:~# mongo --ssl --sslPEMKeyFile /etc/ssl/client.pem --sslCAFile /etc/ssl/mongoCA.pem --host mongo1.testdomain.com -u admin\nMongoDB shell version v4.0.14\nEnter password:\nconnecting to: mongodb://mongo1.testdomain.com:27017/?gssapiServiceName=mongodb\nImplicit session: session { \"id\" : UUID(\"7a4087c2-470c-48ce-8c8d-af10fb4aa9a7\") }\nMongoDB server version: 4.0.14rs0:PRIMARY> rs.add({ host: \"mongo2.testdomain.com:27017\" })\n{\n    \"ok\" : 1,\n    \"operationTime\" : Timestamp(1577364817, 1),\n    \"$clusterTime\" : {\n        \"clusterTime\" : Timestamp(1577364817, 1),\n        \"signature\" : {\n            \"hash\" : BinData(0,\"t5nfdP1DMgNaAcUsvYXy2luwNvg=\"),\n            \"keyId\" : NumberLong(\"6774685910094053378\")\n        }\n    }\n}\nrs0:PRIMARY>\n\n\nCheck the status of replicaset:\n\nrs0:PRIMARY> rs.status()\n{\n    \"set\" : \"rs0\",\n    \"date\" : ISODate(\"2019-12-26T13:06:09.455Z\"),\n    \"myState\" : 1,\n    \"term\" : NumberLong(5),\n    \"syncTo\" : \"\",\n    \"syncSourceHost\" : \"\",\n    \"syncSourceId\" : -1,\n    \"heartbeatIntervalMillis\" : NumberLong(2000),\n    \"optimum\" : {\n        \"lastCommittedOpTime\" : {\n            \"ts\" : Timestamp(1577365562, 1),\n            \"t\" : NumberLong(5)\n        },\n        \"readConcernMajorityOpTime\" : {\n            \"ts\" : Timestamp(1577365562, 1),\n            \"t\" : NumberLong(5)\n        },\n        \"appliedOpTime\" : {\n            \"ts\" : Timestamp(1577365562, 1),\n            \"t\" : NumberLong(5)\n        },\n        \"durableOpTime\" : {\n            \"ts\" : Timestamp(1577365562, 1),\n            \"t\" : NumberLong(5)\n        }\n    },\n    \"lastStableCheckpointTimestamp\" : Timestamp(1577365562, 1),\n    \"electionCandidateMetrics\" : {\n        \"lastElectionReason\" : \"electionTimeout\",\n        \"lastElectionDate\" : ISODate(\"2019-12-26T12:30:20.224Z\"),\n        \"electionTerm\" : NumberLong(5),\n        \"lastCommittedOpTimeAtElection\" : {\n            \"ts\" : Timestamp(0, 0),\n            \"t\" : NumberLong(-1)\n        },\n        \"lastSeenOpTimeAtElection\" : {\n            \"ts\" : Timestamp(1577363413, 1),\n            \"t\" : NumberLong(4)\n    },\n    \"numVotesNeeded\" : 1,\n    \"priorityAtElection\" : 1,\n    \"electionTimeoutMillis\" : NumberLong(10000),\n    \"newTermStartDate\" : ISODate(\"2019-12-26T12:30:22.224Z\"),\n    \"wMajorityWriteAvailabilityDate\" : ISODate(\"2019-12-26T12:30:22.358Z\")\n    },\n    \"member\" : [\n        {\n            \"_id\" : 0,\n            \"name\" : \"mongo1.testdomain.com:27017\",\n            \"health\" : 1,\n            \"state\" : 1,\n            \"stateStr\" : \"PRIMARY\",\n            \"uptime\" : 2154,\n            \"optime\" : {\n                \"ts\" : Timestamp(1577365562, 1),\n                \"t\" : NumberLong(5)\n        },\n        \"optimeDate\" : ISODate(\"2019-12-26T13:06:02Z\"),\n        \"syncTo\" : \"\",\n        \"syncSourceHost\" : \"\",\n        \"syncSourceId\" : -1,\n        \"infoMessage\" : \"\",\n        \"electionTime\" : Timestamp(1577363420, 1),\n        \"electionDate\" : ISODate(\"2019-12-26T12:30:20Z\"),\n        \"configVersion\" : 70500,\n        \"self\" : true\n        \"lastHeartbeatMessage\" : \"\"\n    },\n    {\n        \"_id\" : 1,\n        \"name\" : \"mongo2.testdomain.com:27017\",\n        \"health\" : 1,\n        \"state\" : 2,\n        \"stateStr\" : \"SECONDARY\",\n        \"uptime\" : 26,\n        \"optime\" : {\n            \"ts\" : Timestamp(1577365562, 1),\n            \"t\" : NumberLong(5)\n    },\n    \"optimeDurable\" : {\n        \"ts\" : Timestamp(1577365562, 1),\n\"t\" : NumberLong(5)\n},\n            \"optimeDate\" : ISODate(\"2019-12-26T13:06:02Z\"),\n            \"optimeDurableDate\" : ISODate(\"2019-12-26T13:06:02Z\"),\n            \"lastHeartbeat\" : ISODate(\"2019-12-26T13:06:08.795Z\"),\n            \"lastHeartbeatRecv\" : ISODate(\"2019-12-26T13:06:09.139Z\"),\n            \"pingMs\" : NumberLong(1),\n            \"lastHeartbeatMessage\" : \"\",\n            \"syncingTo\" : \"mongo1.testdomain.com:27017\",\n            \"syncSourceHost\" : \"mongo1.testdomain.com:27017\",\n            \"syncSourceId\" : 0,\n            \"infoMessage\" : \"\",\n            \"configVersion\" : 70500\n        }\n    ],\n    \"ok\" : 1,\n    \"operationTime\" : Timestamp(1577365562, 1),\n    \"$clusterTime\" : {\n        \"clusterTime\" : Timestamp(1577365562, 1),\n        \"signature\" : {\n            \"hash\" : BinData(0,\"9Y9LWPudizuTGUP0+M2wMihAFO8=\"),\n            \"keyId\" : NumberLong(\"6774685910094053378\")\n        }\n    }\n}\nrs0:PRIMARY>\n\nAs you can see, the second node with _id=1 is accepted into Repliset in the secondary state.\n\nWhen a node is added to replicaset, synchronization occurs with the primary node. Until the synchronization is completed, the second node will be in the STARTUP state. Writing data to the main node slows down the synchronization, however, this is a possible scenario (for example, when writing to the database, the database of about 350 GB was synchronized for about 14 hours).Once the synchronization is complete, on the third server (mongoar) do the same as on the second (mongo2), except for the step of adding the server to repliset:\n\nrs0:PRIMARY> rs.addArb(\"mongoar.testdomain.com:27017\")\n{\n\"ok\" : 1,\n\"operationTime\" : Timestamp(1577366355, 1),\n\"$clusterTime\" : {\n\"clusterTime\" : Timestamp(1577366355, 1),\n\"signature\" : {\n\"hash\" : BinData(0,\"pA6zLoq8JmYCFM2HZVAhr4pvK1A=\"),\n\"keyId\" : NumberLong(\"6774685910094053378\")\n}\n}\n}\nrs0:PRIMARY>\n\nCheck the status of Repliset (nothing is omitted from the output for brevity):\n\nrs0:PRIMARY> rs.status()\n{\n\"set\" : \"rs0\",\n\"date\" : ISODate(\"2019-12-26T13:21:20.710Z\"),\n\"myState\" : 1,\n\"members\" :\n{\n\"_id\" : 0,\n\"name\" : \"mongo1.testdomain.com:27017\",\n\"health\" : 1,\n\"state\" : 1,\n\"stateStr\" : \"PRIMARY\",\n\"uptime\" : 3065,\n},\n{\n\"_id\" : 1,\n\"name\" : \"mongo2.testdomain.com:27017\",\n\"health\" : 1,\n\"state\" : 2,\n\"stateStr\" : \"SECONDARY\",\n\"uptime\" : 937,\n},\n{\n\"_id\" : 2,\n\"name\" : \"mongoar.testdomain.com:27017\",\n\"health\" : 1,\n\"state\" : 7,\n\"stateStr\" : \"ARBITER\",\n\"uptime\" : 125,\n}\n}\n\nAs a result, we got a primary node, a secondary node and an arbiter node. Repliset is synchronized and is in a healthy state.\n\nUpdate Replicaset MongoDB\n\nLet's consider an update using the transition from version 4.0 to version 4.2 as an example. To understand what changes might break the current data schema, please refer to [document.\n\nChange the feature compatibility version parameter to limit the version of MongoDB that can be used with the current dataset. To upgrade from version 4.0 to version 4.2, set it to 4.0:\n\nrs0:PRIMARY> db.adminCommand( { setFeatureCompatibilityVersion: \"4.0\" } )\n{\n    \"ok\" : 1,\n    \"operationTime\" : Timestamp(1577429084, 1),\n    \"$clusterTime\" : {\n        \"clusterTime\" : Timestamp(1577429084, 1),\n        \"signature\" : {\n            \"hash\" : BinData(0,\"mL6lecQQeLB8QkjNqrJIGA+ICiY=\"),\n            \"keyId\" : NumberLong(\"6774685910094053378\")\n        }\n    }\n}\n\n\nCheck the result:\n\nrs0:PRIMARY> db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )\n{\n        \"featureCompatibilityVersion\" : {\n            \"version\" : \"4.0\"\n    },\n    \"ok\" : 1,\n    \"operationTime\" : Timestamp(1577429094, 1),\n    \"$clusterTime\" : {\n        \"clusterTime\" : Timestamp(1577429094, 1),\n        \"signature\" : {\n            \"hash\" : BinData(0,\"Mo5HB7K+IVwAivtJ4ZCC4GMKOAk=\"),\n            \"keyId\" : NumberLong(\"6774685910094053378\")\n    }\n}\n\n\nConnect the repository with the new version of MongoDB on all three servers (example for the first server):\n\nroot@mongo1:~# wget -qO - https://www.mongodb.org/static/pgp/server-4.2.asc | sudo apt key add -\nOK\nroot@mongo1:~# echo \"deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.2.list\ndeb [arch=amd64] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2multiverse\nroot@mongo1:~# apt-get update\n\n\n\nBased on documentation, into the configuration file /etc/mongod.conf add the option enableMajorityReadConcern: false and replace net:ssl with net:tls (arbiter example):\n\nnet:\n  port: 27017\n  bindIP: 0.0.0.0\n  tls:\n      mode: requireTLS\n      certificateKeyFile: /etc/ssl/mongoar.pem\n      CAFile: /etc/ssl/mongoCA.pem\n\n\nreplication:\nreplSetName: \"rs0\"\nenableMajorityReadConcern: false\n\nUpdate the arbiter node:\n\nroot@mongoar:~# systemctl stop mongod.service\nroot@mongoar:~# apt-get install -y mongodb-org mongodb-org-mongos mongodb-org-server mongodb-org-shell mongodb-org-tools\nroot@mongoar:~# systemctl start mongod.service\n\nCheck the status of rs.status(). Make sure the arbiter is running and see which of the two remaining nodes is secondary (command output shortened):\n\nrs0:PRIMARY> rs.status()\n{\n    \"set\" : \"rs0\",\n    \"date\" : ISODate(\"2019-12-27T06:54:13.228Z\"),\n    \"myState\" : 1,\n    \"term\" : NumberLong(6),\n\n    \"member\" : [\n        {\n            \"_id\" : 0,\n            \"name\" : \"mongo1.testdomain.com:27017\",\n            \"health\" : 1,\n            \"state\" : 1,\n            \"stateStr\" : \"PRIMARY\",\n        },\n        {\n            \"_id\" : 1,\n            \"name\" : \"mongo2.testdomain.com:27017\",\n            \"health\" : 1,\n            \"state\" : 2,\n            \"stateStr\" : \"SECONDARY\",\n        },\n        {\n            \"_id\" : 2,\n            \"name\" : \"mongoar.testdomain.com:27017\",\n            \"health\" : 1,\n            \"state\" : 7,\n            \"stateStr\" : \"ARBITER\",\n}\n\nCurrently mongo2 server is secondary, update MongoDB on it:\n\nroot@mongo2:~# systemctl stop mongod.service\nroot@mongo2:~# apt-get install -y mongodb-org\nThe following packages will be upgraded:\nmongodb-org\nPreparing to unpack .../mongodb-org_4.2.2_amd64.deb ...\nUnpacking mongodb-org (4.2.2) over (4.0.14) ...\nSetting up mongodb-org (4.2.2) ...\nroot@mongo2:~# systemctl start mongod.service\n\nCheck rs.status() to make sure the node is running:\n\nrs0:PRIMARY> rs.status()\n{\n    \"set\" : \"rs0\",\n    \"date\" : ISODate(\"2019-12-27T07:08:22.025Z\"),\n    \"myState\" : 1,\n\"members\" : [\n    {\n        \"_id\" : 0,\n        \"name\" : \"mongo1.testdomain.com:27017\",\n        \"health\" : 1,\n        \"state\" : 1,\n        \"stateStr\" : \"PRIMARY\",\n    },\n    {\n        \"_id\" : 1,\n        \"name\" : \"mongo2.testdomain.com:27017\",\n        \"health\" : 1,\n        \"state\" : 2,\n        \"stateStr\" : \"SECONDARY\",\n    },\n    {\n        \"_id\" : 2,\n        \"name\" : \"mongoar.testdomain.com:27017\",\n        \"health\" : 1,\n        \"state\" : 7,\n        \"stateStr\" : \"ARBITER\",\n    }\n}\n\nForce change the primary node to the one you have already updated (in our case, mongo2):\n\nrs0:PRIMARY> rs.stepDown()\n2019-12-27T12:25:10.922+0000 I NETWORK [js] DBClientConnection failed to receive message from mongo1.testdomain.com:27017 - SocketException: stream truncated\n2019-12-27T12:25:10.923+0000 E QUERY [js] Error: error doing query: failed: network error while attempting to run command 'replSetStepDown' on host 'mongo1.testdomain.com:27017' :\nDB.prototype.runCommand@src/mongo/shell/db.js:168:1\nDB.prototype.adminCommand@src/mongo/shell/db.js:186:16\nrs.stepDown@src/mongo/shell/utils.js:1489:12\n@(shell):1:1\n2019-12-27T12:25:10.924+0000 I NETWORK [js] trying to reconnect to mongo1.testdomain.com:27017 failed\n2019-12-27T12:25:10.941+0000 I NETWORK [js] reconnect mongo1.testdomain.com:27017 ok\nrs0:SECONDARY>\n\n\nNow the mongo1 node has become secondary, update it in the same way as the previous one.\nCheck the status of replicaset after the update (output shortened):\n\nrs0:SECONDARY> rs.status()\n{\n    \"set\" : \"rs0\",\n    \"date\" : ISODate(\"2019-12-27T12:30:03.888Z\"),\n    \"myState\" : 2,\n    \"term\" : NumberLong(6),\n    \"syncingTo\" : \"mongo2.testdomain.com:27017\",\n    \"syncSourceHost\" : \"mongo2.testdomain.com:27017\",\n    \"syncSourceId\" : 1,\n    \"heartbeatIntervalMillis\" : NumberLong(2000),\n    \"majorityVoteCount\" : 2,\n    \"writeMajorityCount\" : 2,\n    \"member\" : [\n        {\n            \"_id\" : 0,\n            \"name\" : \"mongo1.testdomain.com:27017\",\n            \"ip\" : \"89.208.223.162\",\n            \"health\" : 1,\n            \"state\" : 2,\n            \"stateStr\" : \"SECONDARY\",\n\n        },\n        {\n            \"_id\" : 1,\n            \"name\" : \"mongo2.testdomain.com:27017\",\n            \"ip\" : \"95.163.212.165\",\n            \"health\" : 1,\n            \"state\" : 1,\n            \"stateStr\" : \"PRIMARY\",\n        },\n        {\n            \"_id\" : 2,\n            \"name\" : \"mongoar.testdomain.com:27017\",\n            \"ip\" : \"95.163.215.118\",\n            \"health\" : 1,\n            \"state\" : 7,\n        \"stateStr\" : \"ARBITER\",\n        }\n    ],\n    \"ok\" : 1,\n    \"$clusterTime\" : {\n        \"clusterTime\" : Timestamp(1577449801, 1),\n        \"signature\" : {\n            \"hash\" : BinData(0,\"cC03fOHq3hLJX5oorqgTGlDuick=\"),\n            \"keyId\" : NumberLong(\"6775081949028417537\")\n        }\n    },\n    \"operationTime\" : Timestamp(1577449801, 1)\n}\nrs0:SECONDARY>\n\nAll completed successfully. If there is a need to preserve the order of the nodes that was before the update, go to the console of the current primary node (mongo2) and run the command:\n\nrs.stepDown()\n\nThe node will become secondary, the primary node will become mongo1, as it was before the upgrade.\n\nWe performed the upgrade without taking the cluster out of service, that is, seamlessly.\n\nIf upgrading a single MongoDB server, update the feature compatibility version parameter, link the repository with the new mongodb version, fix the server configuration file, and upgrade the mongodb server to the new version as described above. In this case, a server shutdown will be required.\n","url":"/en/dbs/dbaas/how-to-guides/case-mongo-update","description":"Creating replicas of the MongoDB DBMS, updating the version.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"3b71b3ea-8349-46c7-8df5-06eae82a4fcc","title":"Monitoring Redis with Prometheus, Node exporter and Grafana","data":"Prometheus is a server designed to collect and store metrics received from processes (exporters). The accumulated data can be viewed both through the Prometheus web interface and through separate visualization tools, for example, Grafana.\n\nThis instruction will help:\n\ndeploy Prometheus 2.45.1 server in CentOS 8.4 operating system in VK Cloud;\ninstall the Node exporter extension in the Redis 5 Single configuration DBMS;\ncustomize the visualization of data from Redis in Prometheus and Grafana 10.\n\nPrometheus server, DBMS and Grafana will be deployed on separate VMs.\n\nPreparatory steps\n\nCreate a virtual network, for example, monitoring-net.\nCreate VM for Prometheus server:\n\n   name: Centos_8_5_Prometheus;\n   operating system: CentOS 8.4;\n   network: monitoring-net;\n   public IP address: assign it, as an example is given 87.239.239.239;\n   security groups: default, all.\n\n   The internal IP address of the created instance: 10.0.3.7.\n\nCreate a DB instance:\n\n   name: Redis-5;\n   DBMS: Redis 5;\n   configuration type: Single;\n   network: monitoring-net;\n\n   The internal IP address of the created instance: 10.0.3.13.\n\nDeploy Grafana 10 is on the monitoring-net network.\n\n2. Install and configure Prometheus\n\nInstall the Node exporter extension for the Redis-5 DB instance. When installing, specify the parameter listen_port = 9100.\nConnect to the Centos_8_5_Prometheus VM.\nDownload Prometheus and unzip the downloaded archive:\n\n      sudo su\n   export VERSION=\"2.45.1\"\n   wget https://github.com/prometheus/prometheus/releases/download/v$VERSION/prometheus-$VERSION.linux-amd64.tar.gz -O - | tar -xzv -C /tmp\n\nCopy the contents of the repository prometheus-2.45.1.linux-amd64:\n\n      mkdir /etc/prometheus\n   mkdir /var/lib/prometheus\n   cp /tmp/prometheus-$VERSION.linux-amd64/prometheus /usr/local/bin\n   cp /tmp/prometheus-$VERSION.linux-amd64/promtool /usr/local/bin\n   cp -r /tmp/prometheus-$VERSION.linux-amd64/consoles /etc/prometheus\n   cp -r /tmp/prometheus-$VERSION.linux-amd64/console_libraries /etc/prometheus\n   cp /tmp/prometheus-$VERSION.linux-amd64/prometheus.yml /etc/prometheus/\n\n(Optional) Delete files from the temporary directory:\n\n      rm -rf /tmp/prometheus-$VERSION.linux-amd64\n\nCreate a group and a user prometheus, assign him rights to the associated repositories:\n\n      groupadd --system prometheus\n   useradd --system -g prometheus -s /bin/false prometheus\n   chown -R prometheus:prometheus /var/lib/prometheus /etc/prometheus\n   chown prometheus:prometheus /usr/local/bin/prometheus /usr/local/bin/promtool\n\nIn the file /etc/prometheus/prometheus.yml add content to the scrape_configs block:\n\n      job_name: \"node\"\n     scrape_interval: 10s\n     static_configs:\n       targets: [\"10.0.3.13:9100\"]\n\nCreate a Prometheus startup script /etc/systemd/system/prometheus.service with the contents:\n\n      [Unit]\n   Description=Prometheus\n   Wants=network-online.target\n   After=network-online.target\n\n   [Service]\n   User=prometheus\n   Group=prometheus\n   ExecStart=/usr/local/bin/prometheus \\\n       --config.file /etc/prometheus/prometheus.yml \\\n       --storage.tsdb.path /var/lib/prometheus \\\n       --web.console.templates=/etc/prometheus/consoles \\\n       --web.console.libraries=/etc/prometheus/console_libraries\n   ExecReload=/bin/kill -HUP $MAINPID\n   [Install]\n   WantedBy=default.target\n\nRun Prometheus:\n\n      systemctl daemon-reload\n   systemctl start prometheus.service\n   systemctl enable prometheus.service\n\nMake sure that Prometheus is working correctly:\n\n      systemctl status prometheus.service\n\n   The output of the command must contain the active status.\n\n    An example of the expected output\n\n        prometheus.service - Prometheus\n     Loaded: loaded (/etc/systemd/system/prometheus.service; enabled; vendor preset: disabled)\n     Active: active (running) since Mon 2023-11-20 16:11:25 UTC; 25min ago\n    Main PID: 1065 (prometheus)\n     Tasks: 6 (limit: 5921)\n     Memory: 51.3M\n     CGroup: /system.slice/prometheus.service\n             └─1065 /usr/local/bin/prometheus --config.file /etc/prometheus/prometheus.yml --storage.tsdb.path /var/lib/prometheus --web.console.templates=/etc/prometheus/consoles\n\n    Nov 20 16:11:25 centos-8-5-prometheus.novalocal prometheus[1065]: ts=2023-11-20T16:11:25.319Z caller=tls_config.go:274 level=info component=web msg=\"Listening on\" address=[::]:9090\n    Nov 20 16:11:25 centos-8-5-prometheus.novalocal prometheus[1065]: ts=2023-11-20T16:11:25.319Z caller=tls_config.go:277 level=info component=web msg=\"TLS is disabled.\" http2=false address=[::]:9090\n    Nov 20 16:11:25 centos-8-5-prometheus.novalocal prometheus[1065]: ts=2023-11-20T16:11:25.319Z caller=head.go:755 level=info component=tsdb msg=\"WAL segment loaded\" segment=0 maxSegment=0\n    Nov 20 16:11:25 centos-8-5-prometheus.novalocal prometheus[1065]: ts=2023-11-20T16:11:25.319Z caller=head.go:792 level=info component=tsdb msg=\"WAL replay completed\" checkpoint_replay_duration=44.387µs wal_replay_duration=1.206992ms wbl_replay_duration=160ns total_replay_duration=1.363332ms\n    Nov 20 16:11:25 centos-8-5-prometheus.novalocal prometheus[1065]: ts=2023-11-20T16:11:25.320Z caller=main.go:1040 level=info fs_type=EXT4_SUPER_MAGIC\n    Nov 20 16:11:25 centos-8-5-prometheus.novalocal prometheus[1065]: ts=2023-11-20T16:11:25.320Z caller=main.go:1043 level=info msg=\"TSDB started\"\n    Nov 20 16:11:25 centos-8-5-prometheus.novalocal prometheus[1065]: ts=2023-11-20T16:11:25.320Z caller=main.go:1224 level=info msg=\"Loading configuration file\" filename=/etc/prometheus/prometheus.yml\n    Nov 20 16:11:25 centos-8-5-prometheus.novalocal prometheus[1065]: ts=2023-11-20T16:11:25.327Z caller=main.go:1261 level=info msg=\"Completed loading of configuration file\" filename=/etc/prometheus/prometheus.yml totalDuration=6.828877ms db_storage=1.31µs remote_storage=1.172µs web_handler=292ns query_engine=475ns scrape=6.420765ms scrape_sd=40.431µs notify=24.848µs notify_sd=9.021µs rules=1.268µs tracing=5.417µs\n    Nov 20 16:11:25 centos-8-5-prometheus.novalocal prometheus[1065]: ts=2023-11-20T16:11:25.327Z caller=main.go:1004 level=info msg=\"Server is ready to receive web requests.\"\n    Nov 20 16:11:25 centos-8-5-prometheus.novalocal prometheus[1065]: ts=2023-11-20T16:11:25.327Z caller=manager.go:995 level=info component=\"rule manager\" msg=\"Starting rule manager...\"\n\n\nWait a few minutes for the data to accumulate.\nGo to the address http://87.239.239.239:9090.\n\n   The Prometheus server web interface opens.\n\n(Optional) Review the available data:\n\n   Create a search query to view the data in tabular form.\n   Open the template graph sets to http://87.239.239.239:9090/consoles/index.html.example.\n\n(Optional) Unbind floating address from VM Centos_8_5_Prometheus.\n\n3. Set up data visualization in Grafana\n\nGo to the Grafana web interface.\nAdd new data source: to Prometheus server URL specify http://10.0.3.7:9090.\nInstall sets of graphs for visualizing the received data, for example, by importing a ready-made version — Node Exporter Full.\n\n4. (Optional) Check the monitoring data after the test load\n\nCreate a test load on the Redis-5 VM in a way that is convenient for you.\n\n    Example with the sysbench utility\n\n      sysbench cpu  --cpu-max-prime=2000000 --time=60 run\n   sysbench fileio --file-test-mode=rndrw --time=60 prepare\n   sysbench fileio --file-test-mode=rndrw --time=60 run\n   sysbench threads --time=60 run\n   sysbench mutex --time=60 run\n\n\n   Installation of additional software on virtual machines of the Cloud Databases service is possible only through a request to technical support.\n\n\nMake sure to change the indicators of the Grafana charts.\n\nDelete unused resources\n\nThe created resources are charged and quotas are spent. If you don't need them anymore:\n\nDelete deployed virtual machines.\nDelete a virtual machine with Grafana.\nRemove the floating IP address assigned to the Centos_8_5_Prometheus VM.\nDelete the port to which the virtual IP address is assigned.\nDelete a monitoring-net network.\n","url":"/en/dbs/dbaas/how-to-guides/case-node-exporter","description":"Configuring monitoring of the Redis DBMS in VK Cloud using the Prometheus server, Node exporter extension and subsequent visualization in Grafana.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"f096c6fd-8dcb-4841-b248-f04ed11fa1a8","title":"Using replicated and distributed ClickHouse tables","data":"Репликация — одна из техник масштабирования баз данных. Состоит техника в том, что данные с одного сервера базы данных постоянно копируются (реплицируются) на один или несколько других. Появляется возможность использовать не один сервер для обработки запросов, а несколько. Таким образом, появляется возможность распределить нагрузку с одного сервера на несколько.\n\nВ сервисе VK Cloud есть возможность создать реплику на этапе создания инстанса БД — для этого активируйте опцию Создать реплику на шаге Создание инстанса.\n\nОписание\n\nВ СУБД Clickhouse репликация данных настраивается на уровне таблиц, в отличие от традиционных, где это обычно уровень БД или даже инстанса. То есть для того, чтобы данные реплицировались на другие хосты в кластере, необходимо правильным образом создать таблицу. Архитектура Clickhouse подразумевает, что таблицы с одинаковым именем реплицируются между двумя репликами (то есть имеют одинаковые данные), а между шардами никак не связаны, то есть данные “шардируются”. При этом, к сожалению, это никак не контролируется Clickhouse, поэтому вам надо самостоятельно  все создать/настроить правильно.\n\nСоздание реплицируемых таблиц\n\nИтак, у нас в наличии кластер Clickhouse, например с 2 шардами - в каждом 2 реплики. Имя кластера Clickhouse в VK Cloud всегда cluster. Выполним запрос:\n\nCREATE TABLE table\\_name ON CLUSTER cluster\n(\n    EventDate DateTime,\n    CounterID UInt32,\n    UserID UInt32\n) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/table\\_name', '{replica}')\nPARTITION BY toYYYYMM(EventDate)\nORDER BY (CounterID, EventDate, intHash32(UserID))\nSAMPLE BY intHash32(UserID);\n\nЕго можно выполнить один раз на одном сервере, так как есть опция ON CLUSTER. В этом случае, Clickhouse обращается к конфигурационному файлу, где указаны все ноды кластера и выполняет запрос на каждом из них. Без этой опции можно просто выполнить такой запрос на каждой ноде.\n\nДавайте разберем особенности этой команды. Здесь используется движок таблицы ReplicatedMergeTree. Этот движок и другие из семейства Replicated рассчитаны на то, чтобы публиковать изменения в таблице в ZooKeeper. Соответственно, аргументами и являются параметры для сохранения этой информации в Zookeeper. Первая часть - это путь, вторая название ключа, в который публикуется информация. В фигурных скобках указаны макросы, макросы определяются в конфиге у каждого сервера. В кластере VK Cloud они уже будут сконфигурированы соответственно составу кластера, так что можно указывать прямо так, как в примере.\n\nВ итоге серверы реплики одного шарда будут иметь одинаковый путь в ZK, и будут применять изменения с других серверов и публиковать свои (двусторонняя репликация). Получается что можно обратиться к одному из серверов и получить актуальные данные, но как получить данные по всем шардам? Для удобства в этом случае можно использовать тип таблицы Distributed.\n\nСоздание Distributed-таблиц\n\nCREATE TABLE distributed ON CLUSTER cluster\n(\n    EventDate DateTime,\n    CounterID UInt32,\n    UserID UInt32\n) ENGINE = Distributed(cluster, default, table\\_name, UserID);\n\nТакая таблица будет указывать на таблицу на всех серверах в кластере. То есть при селектах данные будут собираться со всех нод. Создав такую таблицу на всех серверах, можно обращаться за всеми данными к каждому из них, получается своего рода балансировка нагрузки (однако, не стоит на это сильно рассчитывать, так как Clickhouse не предназначен для одновременного выполнения большого количества тяжелых запросов).\n\nЕсть два способа записывать данные на кластер:\n\nВо-первых, вы можете самостоятельно определять, на какие серверы какие данные записывать, и выполнять запись непосредственно на каждый шард. То есть делать INSERT в те таблицы, на которые «смотрит» распределенная таблица. Это наиболее гибкое решение, поскольку вы можете использовать любую схему шардирования, которая может быть нетривиальной из-за требований предметной области.\n\nТакже это является наиболее оптимальным решением, так как данные могут записываться на разные шарды полностью независимо.\n\nВо-вторых, вы можете делать INSERT в Distributed таблицу. В этом случае, таблица будет сама распределять вставляемые данные по серверам. Для того, чтобы писать в Distributed таблицу, у нее должен быть задан ключ шардирования (последний параметр). Также, если шард всего-лишь один, то запись работает и без указания ключа шардирования (так как в этом случае он не имеет смысла).\n","url":"/en/dbs/dbaas/how-to-guides/clickhouse-replication","description":"Configuring data replication for Clickhouse at the table level.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"5df5a2ca-427b-48cc-8409-ce5c61490496","title":"Monitoring PostgreSQL","data":"You can monitor the status of individual PostgreSQL hosts using the monitoring tool built into the VK Cloud panel.\n\n\n\nThis tool is only available if the PostgreSQL cluster was created with the \"Enable monitoring\" option enabled and config template \"Single\" or \"Master-Replica\".\n\nHow to use monitoring\n\nTo view monitoring data:\n\nOpen the VK Cloud panel.\nGo to Databases→Database Instances.\nFind the required instance in the list.\nClick on the instance host for which you want to view monitoring data.\nGo to the \"Monitoring\" card.\n\nSeveral counters and graphs will be displayed:\n\nCounters reflect the current value of the metrics.\nGraphs reflect the dynamics of changes in metric values ​​within a certain time period.\n\n  On the ruler at the top, you can select the time period of interest for which you want to display data on the charts.\n  The default period is 12 hours.\n\nAvailable monitoring metrics\n\nCounters\n\nCPU metrics:\n\n  Current IOWait\n\n    Percentage of total host processor resources spent waiting for I/O operations to complete.\n\n  Current CPU\n\n    Percentage of time the host processor is running in user mode. Or, in other words, it powers the database instance and its associated infrastructure.\n\nRAM Used**\n\n  The percentage of memory used (of the total memory on the host).\n\nFree connections**\n\n  The percentage of available connections of the total.\n\nReplication lag**\n\n  Replica lag time (in seconds) from the replication source.\n\n\n  This metric is only shown when viewing replica host monitoring data.\n\n\nGraphs\n\nDatabase load:\n\nFetch Data (Select)**\n\n  Database read intensity: the number of table rows fetched during the execution of queries against the database of table rows (per second).\n\nReturned Data**\n\n  Database response rate: The number of query result rows returned (per second).\n\nUpdate Data**\n\n  Database update rate: number of rows modified by UPDATE requests (per second).\n\nInsert Data**\n\n  Database insert rate: number of rows inserted by INSERT queries (per second).\n\nDeleted Data**\n\n  Database deletion rate: number of rows deleted by DELETE requests (per second).\n\nLoad on the disk subsystem:\n\nDisk Read Time**\n\n  Time (in seconds) spent on disk read operations.\n\nDisk Write Time**\n\n  Time (in seconds) spent on disk write operations.\n\nDisk used**\n\n  The percentage of used disk space (of the total on the host).\n  Bar graphs are shown for some sections, such as those related to PostgreSQL and the WAL transaction log.\n\nUsing monitoring information\n\nHigh CPU and memory utilization rates, disk subsystem load, and heavy or uneven database load may indicate high load on hosts or suboptimal indexes and queries.\n\n  In this case, it is recommended to use the built-in PostgreSQL performance diagnostic tools on behalf of the _DB administrator, for example:\n\n  Run queries against the pg_stat_activity system table to collect statistics on running queries.\n\n     It is recommended to pay attention to queries that take the longest to complete.\n\n  Use the EXPLAIN command to find bottlenecks in such slow queries.\n\n     It is recommended to pay attention to queries that do not use indexes (large number of rows in Seq Scan). You can then create or refresh required indices.\n\nA low percentage of free connections (Free connections) may indicate both a large number of simultaneously connected clients, and that there are long transactions that use an open connection for too long.\n\n  In this case, it is recommended:\n\n  Increase option value max_connections.\n  Optimize queries so that there are no long transactions.\n\nFor troubleshooting replica lagging issues, see the Patroni and PostgreSQL.\n","url":"/en/dbs/dbaas/monitoring/postgresql","description":"Monitoring the health of PostgreSQL instances with detailed metrics for individual hosts (both masters and replicas).","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"f00a4006-6500-4ad7-951c-9e8e0f225139","title":"Tariffication","data":"Billing is carried out according to the “pay as you go” principle: the fee is charged only for the resources consumed with an accuracy of up to a second.\n\nThe configuration and cost of the services are given in the price list. To calculate the total cost of the service, you can use the calculator. For information on how the tools related to the payment of platform services work, see the section Billing.\n\nCharged\n\nCPU (vCPU) — for each core. 1 vCPU corresponds to 1 physical core of the virtualization server.\nRAM — for each 1 GB of RAM.\nLicenses (for example, for PostgresPro) — for each day of license use.\nDisks — for each 1 GB of disk space, the price depends on the type of disk (SSD or High-IOPS SSD).\nDisk snapshot (backup) — for every 1 GB of snapshot size.\nIP addresses and load balancers, for more information, see Virtual networks.\nSystem disks.\nDepending on the DBMS type: disk for the transaction log.\n\nNot charged\n\nAutomatic creation of backups.\nIncoming and outgoing traffic.\nMonitoring.\n","url":"/en/dbs/dbaas/tariffication","description":"Rates for receiving VK Cloud services.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"fda46cf0-2c4d-408f-875a-5b8578ffff4e","title":"Node Exporter","data":"Описание\n\nРасширения увеличивают функциональность баз данных и могут быть установлены в любое время.\n\nPrometheus - центральный сервер, предназначенный для сбора и хранения данных. Данные постоянно изменяются во времени (например, уровень заполненности диска, трафик через сетевой интерфейс, время отклика сайта). Элементы данных называются метриками. Сервер Prometheus с заданной периодичностью считывает метрики и помещает полученные данные в Time Series DB. Time Series DB - это разновидность баз данных, предназначенная для хранения временных рядов (значений с привязкой ко времени). Кроме того, Prometheus предоставляет интерфейс для выполнения запросов и визуализации полученных данных. Язык запросов Prometheus называется PromQL. Prometheus работает по модели Pull, то есть он сам опрашивает endpoints с целью получения данных.\n\nExporters - процессы, обеспечивающие сбор и их передачу серверу Prometheus. Существует много разных exporters, например:\n\nNode_exporter - сбор системных метрик (процессор, память, и т.д.).\nMysqld_exporter - сбор метрик работы сервера MySQL.\n\nPrometheus для серверных метрик (node_exporter)\n\nСуществует разная поддержка для сборщиков в каждой операционной системе.В таблицах ниже перечислены некоторые сборщики и поддерживаемые системы.\n\n| Name       | Description                                                              | OS                                         |\n| ---------- | ------------------------------------------------------------------------ | ------------------------------------------ |\n| cpu        | Собирает статистику использования процессора                             | Darwin, Dragonfly, FreeBSD, Linux, Solaris |\n| cpufreq    | Собирает статистику частоты процессора                                   | Linux, Solaris                             |\n| diskstats  | Собирает статистику дискового ввода-вывода (I/O).                         | Darwin, Linux, OpenBSD                     |\n| filesystem | Собирает статистику файловой системы, например, занятого места на дисках. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD |\n| meminfo    | Собирает статистику использования оперативной памяти.                    | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD |\n| vmstat     | Собирает статистику процессов из /proc/vmstat.                           | Linux                                      |\n\nПолный список всех сборщиков вы можете увидеть на официальном ресурсе по этой ссылке.\n\nУстановка расширения\n\nУстановить расширения довольно просто. Для этого на карточке виртуальной машины на закладке \"Расширения\" нажмите кнопку Добавить:\n\n\n","url":"/en/dbs/dbaas/extensions/node-exporter","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"608ecece-7cd6-4e58-a4f0-d489863224d6","title":"Zabbix agent","data":"Zabbix is software for monitoring applications, servers, and network devices.\n\nTo monitor database cluster instances, you can use the Zabbix monitoring system. This section does not include information about deploying the Zabbix monitoring server. It contains information about additionally installed components that extend the capabilities of the server and monitoring agents. It is assumed that you will install and configure the client part of the Zabbix agent monitoring system on DB cluster instances yourself.\n\nNumerous pre-installed templates developed for Zabbix allow you to get information about the status of many services. The solutions are compatible with Zabbix versions 3.4 and higher.\n\nUser parameters\n\nserver (required parameter) — the IP address or name of the Zabbix monitoring server.\nlisten_port (default: 10050) — Zabbix agent port for passive checks.\nserver_port (default: 10051) — Zabbix server port for active checks.\nb64_pkcs12_container (default: none) — a file in PKS#12 format without a password converted to base4 containing the certificate of the certification authority, the private key, and the Zabbix agent certificate.\npsk (default: none) — secret key (Pre Shared Key).\nzabbix_agent_version (default: 3.4) — Zabbix agent version (possible values 3.4 or 5.0). The parameter is available for PostgreSQL and PostgreSQL Pro.\nhostname — the name of the host (case sensitive) to display on the Zabbix server. Valid characters: Latin lowercase, uppercase, numbers, spaces, and special characters: ., _, -. By default, the name of the VM instance of the DBMS. Do not use DBMS for clusters.\n\nPostgreSQL monitoring\n\nserver (required parameter) — the IP address or name of the Zabbix monitoring server.\nlisten_port (default: 10050) — Zabbix agent port for passive checks.\nserver_port (default: 10051) — Zabbix server port for active checks.\nb64_pkcs12_container (default: none) — a file in PKS#12 format without a password converted to base4 containing the certificate of the certification authority, the private key, and the Zabbix agent certificate.\npsk (default: none) — secret key (Pre Shared Key).\nzabbix_agent_version (default: 3.4) — Zabbix agent version. Possible values are 3.4 and 5.0. For PostgreSQL and PostgreSQL Pro, 5.0v2 is also available — for installing Zabbix Agent 2.\nhostname — the name of the host (case sensitive) to display on the Zabbix server. Valid characters: Latin lowercase, uppercase, numbers, spaces, and special characters: ., _, -. By default, the name of the VM instance of the DBMS. Do not use DBMS for clusters.\n\nPostgreSQL monitoring\n\nWhen installing agent version 3.4 on the Zabbix server, the template pg_monz should be installed. To do this, you need to unpack the archive and import the template from the directory /pg_monz-2.2/pg_monz/template/ on the Zabbix server. At the same time, the template does not need to be configured.\n\nFor Agent version 5.0, the Zabbix template of the PostgreSQL Agent server should be used. At the same time, no template configuration is required.\n\nFor Agent version 5.0v2, the Zabbix template of the PostgreSQL Agent 2 server should be used.\n\nIn the template settings, you should set:\n\nPG.URI — tcp://127.0.0.1:5432;\nPG.DB — postgres.\n\nMySQL Monitoring\n\nInstalling the Zabbix Agent extension for ClickHouse allows you to collect the metrics listed below:\n\n| Metric| Description|\n|-------------------------|---------------------------------------------------------------------------------|\n| mysql status | Version, ID, status, continuous operation time.                     |\n| connections status | Communication errors and interrupted connections.                                           |\n| traffic | Received/sent, bytes per second.                                            |\n| temporary objects usage | Using temporary files, tables, and tables on disks.                      |\n| keys usage | Number of records, reads, usage of blocks, and MyISAM cache.                 |\n| operations count | Operations per second for begin, commit, delete, insert, rollback, select, and update. |\n| queries | Number of requests per second and slow requests.                             |\n| table locks | The number of immediate and expected table locks.                           |\n| threads count | The number of running, created connected and cached threads.           |\n\nWhen selecting Zabbix agent version 3.4, the components of the template mysbix are installed.\n\nTo start using import this template.\n\n\n\nIn some versions of the Zabbix server, a template with the same name may already be installed. We recommend changing the template name in the XML file to a unique one before importing to avoid collisions.\n\nWhen choosing the agent version v.5.0, use the template Template DB MySQL by Zabbix agent preinstalled on Zabbix server version 5.0 and higher.\n\nGalera Monitoring\n\nTo monitor Galera, the template components zabbix-galera-template are installed together with Zabbix agent version 3.4, which allows you to collect the metrics listed below:\n\n| Metric| Description |\n|----------------------------|----------------------------------------------------------------------------------------------------------------------------------|\n| cluster information | cluster id, number of members |\n| cluster member status | ready, cluster connection status, EVS protocol status, group communication ID, last transaction number |\n| cluster member performance | control flow events and request queue status |\n| replication counters | quantitative indicators of replicated data and keys |\n\nTo start using, import this template.\n\nMongoDB Monitoring\n\nInstalling the Zabbix agent extension for ClickHouse allows you to collect the metrics listed below:\n\n|Metric|Description|\n|----------------------|---------------------------------------------------------------------------------------------------------------------------|\n| db status | Ready, connection status.                                                                                        |\n| operations count | Commands, inserts, deletes, requests per second.                                                                            |\n| storage engine cache | Using the storage cache.                                                                                             |\n| network usage | Network activity indicators.                                                                                            |\n| memory usage | Memory usage.                                                                                                     |\n| per db metrics | Average size and number of objects, number of collections, amount of data, number and size of indexes, storage size. |\n\nWhen selecting Zabbix agent version 3.4, the components of the template zabbix-mongodb are installed.\n\nTo start using import this template.\n\nWhen selecting Zabbix agent version 5.0, a second generation agent will be installed that supports the template Template DB MongoDB node by Zabbix Agent 2 preinstalled on Zabbix server version 5.0 and higher. Set the macro value “{$MONGODB.CONNSTRING}” to “mcs_mongodb\" for authorization on the monitored host.\n\nClickhouse Monitoring\n\nInstalling the Zabbix Agent extension for ClickHouse allows you to collect the metrics listed below:\n\n|Metric|Description|\n|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|\n| db status | Ready, connection status.                                                                                                              |\n| operations count | Inserts\\rows per second, requests per second.                                                                                                    |\n| query | The number of current queries, the maximum execution time of current queries, the number of processed queries and inserts, and delays during inserts. |\n| merge | Merges of uncompressed bytes and merges of strings per second.                                                                                                |\n| replication | Memory usage for replication, lag, number of replication tasks in the queue.                                                                |\n| memory usage | Memory usage for background merges, mutations, deliveries.                                                                                    |\n\nWhen selecting the zabbix agent version 3.4, the template components clickhouse-zabbix-template are installed.\n\nTo start using import this template.\n\nIf your Zabbix server is version 5.0 or higher, use the built-in template Template DB ClickHouse by HTTP.\n\nWhen installing the extension, specify the parameters:\n\nsource_ip_addresses — to specify allowed addresses, to connect to clickhouse-server via HTTP in a format similar to the permission settings cllickhouse users (comma delimiter).\nzabbix_clickhouse_password — to use the value in the macro “{$CLICKHOUSE.PASSWORD}”\nmcs_user — for the macro {$CLICKHOUSE.USER}.\n\nRedis Monitoring\n\nInstalling the Zabbix agent extension for Redis allows you to collect the metrics listed below:\n\n|Metrics|Description|\n|--------------------|---------------------------------------------------------------------------------------------------------------------------------|\n| command statistics | Statistics by teams: number, delay.                                                                                   |\n| clients | connection statistics: the number of connections per second, blocked, maximum buffer, and maximum client output. |\n| performance | Memory and PROCESSOR usage.                                                                                              |\n| keys | Key counters, and statistics related to the database.                                                                         |\n| replication | Replication status.                                                                                                           |\n| slowlog | Information about slow requests.                                                                                                |\n\nTo monitor the Redis Zabbix agent version 3.4, the components of the template zabbix-redis-template are installed.\n\nTo start using import this template.\n\nWhen selecting Zabbix agent version 5.0, a second-generation agent will be installed that supports the template Template DB Redis preinstalled on zabbix-server v5.0 and higher.\n\nExample of a scenario for connecting instances to a monitoring server\n\nEnsure network connectivity of the instance and the monitoring server by ports for active and passive checks (more here).\nDecide on the encryption method of the transmitted data (without encryption, TLS, or PSK). When installing the Zabbix agent, you can choose both methods of connection protection, in this case, during operation, the encryption method can be changed by changing the connection configuration only on the server (more here).\nInstall the Zabbix agent on an instance or all cluster instances with the required user parameters.\nConnect the instances to the Zabbix server in the Configuration -> Hosts -> Create Hosts section (more details here).\n\n\n\nUse the short instance name (hostname -s) for the value of the Hostname, and PSK identity fields (if PSK is used).\n\nAssign the monitoring template(s) for the instance or for the group that the instance belongs to (more details here).\nAfter 10 minutes, make sure that the data from the agent arrives on the server in the Monitoring -> Latest Data section by filtering the content by instance name. You can view historical information on metrics in the same place — in the last column.\n\n\n","url":"/en/dbs/dbaas/extensions/zabbix","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"ff7ebc03-df4c-4ace-b84f-21d60d0a0ffd","title":"PostgreSQL extensions","data":"Extensions specific to PostgreSQL and PostgresPro are described. Read about extensions for monitoring DB instances in node_exporter, postgres_exporter and zabbix.\n\n\n\nAvailability of the extensions depends on selected version of PostgreSQL and PostgresPro, as well as the PostgresPro edition.\n\nHolistic.dev (holistic)\n\n\n\nExtension description\nExtension parameters\n\n\n\nHolistic.dev is a static analyzer and tool for extracting information about database organization. Thanks to the collected data, the tool automatically monitors the integrity of relations between database objects and detects possible issues. Administrators use information from Holistic.dev reports to optimize the database. Holistic.dev only collects information about DML queries and database schema, database configuration and query execution plans are not analyzed.\n\nRead more in official FAQ of the extension.\n\n\n\n\ndatabases: the list of databases in which the extension should operate. Required parameter.\napi_key: the Holistic.dev API key. Required parameter.\nproject_name: the Holistic.dev project name. Required parameter.\n\nJsQuery\n\n\n\nExtension description\nExtension parameters\n\n\n\nThe extension provides the JsQuery query language that extends jsonb data type processing capabilities in PostgreSQL.\n\n\n\n\ndatabase: the list of databases in which the extension should operate.\n\n\n\nWhen the extension is installed, it is not possible to remove existing entries from the database parameter, only to add the new ones.\n\npgBadger\n\n\n\nExtension description\nExtension parameters\n\n\n\nThe pgBadger extension allows analyzing PostgreSQL logs and building reports. These reports provide statistics on SQL queries and Autovacuum, as well other data. Read more about features in the extension documentation.\n\n\n\n\ns3_bucket: name of an object storage bucket where the reports will be stored. Required parameter.\n\n  If the bucket with this name does not exist, then create it.\n\n\n  The bucket should have the default ACL setting set to public-read-write.\n\n\n  The reports for a particular DB instance will be stored in a directory with a name that matches the instance identifier. To get the identifier, explore the information about the DB instance.\n\nperiod: the interval between report generation (in hours).\n\n  The value must be greater than zero. The default value is 24.\n\nlog_min_duration_statement: the minimum SQL query duration (in milliseconds). All queries that match or exceed this threshold will be logged by PostgreSQL. Then, pgBadger will analyze the logged SQL queries.\n\n  If a negative value is set, then no one SQL query will be logged. A pgBadger report will contain no information about queries.\n\n  The default value is 0 (all SQL queries will be logged).\n\nlog_rotation: the number of days to store reports. The older reports will be deleted.\n\n  The default value is 0 (do not delete anything).\n\npg_hint_plan\n\n\n\nExtension description\nExtension parameters\n\n\n\nWhen executing an SQL query, the PostgreSQL planner tries to select the best execution plan. However, the chosen plan is not always optimal, as the planner does not take into account some data properties, such as correlations between columns. The pg_hint_plan allows adjusting query execution plans by giving hints to the PostgreSQL planner. Hints can be given as comments of a special form to SQL queries,  or as records in the special hint_plan.hints table.\n\n\n\n\nenable_hint_table: specifies whether hints from the hint_plan.hints table are allowed to be used.\n\n  The default value: on (allowed).\n\nparse_messages: specifies the log level from which messages, related to the hint parsing, will be logged. For example, if the info level is selected, then messages with the levels info, notice, warning and error will be logged.\n\n  Allowed values:\n\n  error\n  warning\n  notice\n  info\n  log\n  debug\n\n  The default value: info.\n\ndebug_print: controls the debug output and its verbosity level.\n\n  Allowed values:\n\n  off: debug output is disabled.\n  on: debug output is enabled.\n  detailed: more verbose debug output is enabled.\n  verbose: the most verbose debug output is enabled.\n\n  The default value: off.\n\nmessage_level: specifies the log level from which debug messages will be logged. For example, if the info level is selected, then messages with the levels info, notice, warning and error will be logged.\n\n  Allowed values:\n\n  error\n  warning\n  notice\n  info\n  log\n  debug\n\n  The default value: info.\n\npg_partman\n\n\n\nExtension description\nExtension parameters\n\n\n\nThe pg_partman extension is advanced table partitioning manager. Two partitioning methods are supported: partitioning based on pg_partman triggers, and native partitioning based on the PostgreSQL's built-in functionality. Read more in the extension documentation.\n\nThe partman schema is created alongside the extension. The extension then is installed in the schema.\n\nTo do native partitioning, a partman user will be created and configured in PostgreSQL. It is possible to set a password for this user. To do partitioning based on pg_partman triggers, the postgres user should be used.\n\n\n\n\ndatabase: the list of databases in which the extension should operate.\n\n\n  When the extension is installed, it is not possible to remove existing entries from the database parameter, only to add the new ones.\n\n\ninterval: interval (in seconds) between running the run_maintenance() function by the background worker (BGW).\n\n  The default value: 3600 (1 hour).\n\nanalyze: determines whether to run parent table analysis after creation of a child table.\n\n  Allowed values:\n\n  default: use the default behavior. It differs for different PostgreSQL versions:\n\n    for PostgreSQL 10: run parent table analysis.\n    for PostgreSQL 11 and higher: do not run parent table analysis.\n\n  on: run parent table analysis.\n  off: do not run parent table analysis.\n\n  The default value: default.\n\npg_stat_kcache\n\n\n\nExtension description\nExtension parameters\n\n\n\nThe pg_stat_kcache extension collects statistics about real reads and writes done by the filesystem layer.\n\n\n\nThe extension requires the pg_stat_statements extension to be installed. If it is not already installed, then it will be installed automatically.\n\n\n\n\n\n\ndatabase: the list of databases in which the extension should operate.\n\npg_stat_statements\n\n\n\nExtension description\nExtension parameters\n\n\n\nThe extension pg_stat_statements allows tracking planning and execution statistics of all SQL statements.\n\n\n\n\ndatabase: the list of databases in which the extension should operate.\n\nPostGIS\n\n\n\nExtension description\nExtension parameters\n\n\n\nThe PostGIS extension adds capabilities of working with geographic data into PostgreSQL. It implements several geospatial data types, as well supports storing, indexing and processing geographic data.\n\nFor more information abount using the extension, see Using the PostGIS extension in PostgreSQL and PostgresPro.\n\n\n\n\ndatabase: the list of databases in which the extension should operate.\n\nextension_list: list of additional extensions to be installed alongside PostGIS.\n\n  The default value: no extension selected.\n\n  Additional extensions available to install\n\n  | Name    |  Description |\n  | ----------- | --------- |\n  | address_standardizer_data_us   | Allows doing address normalization for USA |\n  | address_standardizer           | Allows doing address normalization |\n  | postgis_tiger_geocoder               | Allows doing geocoding in TIGER format |\n  | postgis_topology                                  | Implements types and functions that are used to manage topological objects |\n  | pgrouting                                                          | Adds geospatial routing functionality |\n\n\npostgres_extensions\n\n\n\nExtension description\nExtension parameters\n\n\n\nA set of popular PostgreSQL extensions that require no additional configuration. You can select one or more extensions to install. They are listed on the Extension parameters tab.\n\n\n\n\ndatabases: the list of databases in which the extension should operate.\n\nextension_list: the list of extensions to install.\n\n  The default value: all extensions are selected.\n\n  Installable extensions\n\n  | Name    |  Description |\n  | ----------- | --------- |\n  | amcheck                         | Provides functions that allow verifying the logical consistency of the structure of relations |\n  | autoinc      | Provides functions to automatically inrement fileds |\n  | bloom                             | Provides an index access method based on Bloom filters |\n  | btree_gin                     | Provides sample GIN operator classes which implement B-tree equivalent behavior for enums and many other data types |\n  | btree_gist                   | Provides GiST operator classes which implement B-tree equivalent behavior for enums and many other data types |\n  | citext                           | Implements the citext data type for storing case-insensitive strings |\n  | cube                               | Implements the cube data type for representing miltidimensional cubes |\n  | dblink          | Allows executing a SQL query in a remote PostgreSQL database from the current session |\n  | dict_int                       | Provides sample of additional dictionary template for full-text search. The dictionary allows indexing integers, both signed and unsigned. Thanks to this, the list of unique words does not grow and the search speed increases |\n  | dict_xsyn                     | Provides sample of additional dictionary template for full-text search. The dictionary replaces words with groups of their synonims, allowing to search word using its synonims |\n  | earthdistance             | Allows calculating a distance between two points on the serface of the Earth. The extension provides two approaches: cube-based and point-based.When doing the calculations, the Earth is assumed to have a perfect sphere shape. If that is too imprecise for a given application, then use the PostGIS extension.Dependencies: the cube extension. If it is not already installed, then it will be installed automatically |\n  | fuzzystrmatch             | Provides fuctions to determine similarities and distance between strings |\n  | hstore                           | Implements the hstore data type for storing key/value pairs as a single value |\n  | intarray                       | Provides functions and operators for working with arrays of integers that do no contain NULL values |\n  | isn                                 | Implements data types for the following international product numbering standards:EAN13UPCISBN (books)ISMN (music)ISSN (serial numbers) |\n  | lo                                   | Implements the lo data type and the lo_manage trigger for managing binary large objects (BLOBs). Allows correct handling of such objects when using JDBC and ODBC drivers |\n  | ltree                             | Implements the ltree data type for representing labels of data. The labels are stored and hierarchical tree-like structure. Also, advanced search capabilites are provided for the structure |\n  | moddatetime  | Implements a trigger that stores the current time in the field of the timestamp data type. For example, it may be useful for tracking the last modification time of a table row |\n  | pg_buffercache            | Provides means for monitoring the state of shared buffer cache in a real time |\n  | pg_trgm                          | Provides trigram-based means for determining similarities between strings and fast searching for similar strings |\n  | pgcrypto                       | Provides cryptographic functions for PostgreSQL: hashing, encryption, random number generation |\n  | pgrowlocks                   | Provides the function to view information about row locks in a given table |\n  | pgstattuple                 | Provides functions to view tuple-level statistics |\n  | postgres_fdw               | Provides the foreign-data wrapper (FDW) to access the data that are stored in external PostgreSQL server |\n  | seg                                 | Implements the seg data type for representing line segments and floating point intervals |\n  | tablefunc                     | Provides multiple functions that return a set of rows (tables) |\n  | uuid-ossp                     | Provides functions to generate universally unique identifiers (UUIDs) using the standard algorithms |\n  | xml2                               | Provides functions to work with XML. It is possible to execute XPath queries and do XSLT transformations |\n\n\nTimescaleDB\n\n\n\nExtension description\nExtension parameters\n\n\n\nThe TimescaleDB extension adds advanced time-series data manipulating capabilities to PostgreSQL. It provides full support for standard PostgreSQL SQL syntax when working with time-series data that is stored in hypertables. Automatic partitioning of hypertables with time-series data by time and space is supported.\n\n\n\n\ndatabase: the list of databases in which the extension should operate.\n\n\n\nWhen the extension is installed, it is not possible to remove existing entries from the database parameter, only to add the new ones.\n\n\n\n\n\n","url":"/en/dbs/dbaas/extensions/postgresql","description":"The list of available PostgreSQL extensions and their parameters.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"17f6b92f-f031-4850-8d56-76e684867966","title":"OpenSearch plugins","data":"VK Cloud allows the installation of OpenSearch plugins on already deployed database instances:\n\nanalysis-icu\n\n    The plugin makes available Lucene ICU, an advanced Unicode support module. The module uses the ICU libraries and allows you to analyze Asian languages, convert Unicode case, supports collation and transliteration.\n\nanalysis-kuromoji\n\n    The plugin makes available  Lucene Kuromoji, a Japanese language analysis module.\n\nanalysis-nori\n\n    The plugin makes available  Lucene Nori, a Korean language analysis module.\n\nanalysis-stempel\n\n    InclThe plugin makes available udes Lucene Stempel, a Polish language analysis module.\n\nanalysis-ukrainian\n\n    The plugin makes available Lucene UkrainianMorfologikAnalyzer, a module for analyzing the Ukrainian language. The module uses the Morfologik project library.\n\nanalysis-phonetic\n\n    The plugin adds phonetic filters for token analysis using Soundex, Metaphone and other algorithms.\n\nanalysis-smartcn\n\n    The plugin connects Lucene Smart Chinese, a module for analyzing texts in Chinese or mixed in Chinese and English. The extension breaks text into sentences and then each sentence into words for better word segmentation in Simplified Chinese.\n\ndiscovery-azure-classic\n\n    The plugin makes available to discover source host addresses for duplicate address detection using Azure Classic API.\n\ndiscovery-ec2\n\n    The plugin allows you to search for cluster nodes on AWS EC2 that meet the master's requirements. The plugin uses the AWS API.\n\ndiscovery-gce\n\n    The plugin makes available to discover source host addresses for duplicate address detection using GCE API.\n\ningest-attachment\n\n    The plugin allows to extract attached files. The plugin uses the Apache Tika library.\n\nmapper-annotated-text\n\n    The plugin allows to index text that contains markup for annotations.\n\nmapper-murmur3\n\n    The plugin allows to calculate a hash of field values by index time and store field values in the index.\n\nmapper-size\n\n    The plugin allows to index the uncompressed size of _source documents and record it in the _size metadata in bytes.\n\nrepository-azure\n\n    The plugin allows to store data with Azure.\n\nrepository-gcs\n\n    The plugin allows to store data with Google Cloud Storage.\n\nrepository-hdfs\n\n    The plugin allows to store data with Hadoop Distributed File System (HDFS).\n\nrepository-s3\n\n    The plugin allows to store data with S3.\n\nstore-smb\n\n    The plugin makes available to integrate SMB servers with OpenSearch search functions.\n\ntransport-nio\n\n    The plugin implements a transport layer using the NIO approach. NIO Transport provides asynchronous, non-blocking I/O processing, which improves system performance and scalability.\n\nRead more about working with extensions and plugins in VK Cloud in Managing extensions.\n","url":"/en/dbs/dbaas/extensions/opensearch-plugins","description":"Available OpenSearch plugins.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"a9e6ebc0-8753-4013-8893-02804358b808","title":"FAQ","data":"\n\n\nHow to create a database?\n\nYou can create a database using step-by-step instructions or quick start.\n\n\n\n\n\nWhich DBMS are available in the cloud?\n\nFull list of supported DBMS and their configurations.\n\n\n\n\n\nHow do I select the required DB instance configuration?\n\nYou can select the required configuration type when creating a DB instance. After creating an instance, the configuration type cannot be changed, but you can always add a replica. For more information about configuration types, see DB instance configurations.\n\n\n\n\n\nCan I change the DB instance configuration file myself?\n\nNo, the configuration file is not editable by the user. To change the configuration, contact technical support. But you can change individual parameters for some DBMS according to instructions.\n\n\n\n\n\nWhat does the cost of the service depend on?\n\nThe cost of the service depends on the selected configuration. Payment is calculated only for the resources used, for more information, see the article Tariffication.\n\n\n\n\n\nHow do I add a database user?\n\nUse the instructions User management.\n\n\n\n\n\nIs backup enabled by default?\n\nBackup is not enabled by default. You can enable it when creating an instance or configure it in the Backup section.\n\n\n\n\n\nIs backup enabled for my DB?\n\nTo check if backup is enabled:\n\nGo to VK Cloud personal account.\nSelect the project where the required DB instance is located.\nGo to Databases → Backup.\nGo to the Automatic or Point-in-time recovery tab.\n\n   If there is a green indicator next to the name of the required plan, the backup works according to plan. If the indicator is red, the backup is suspended.\n\nBackup can be enabled when creating a DB instance or configured separately by creating a backup plan.\n\n\n\n\n\nIs it possible to restore a database of a different version from a backup?\n\nNo, you can restore the database only to the version for which the backup was created.\n\nTo restore a database from a backup, use the instructions.\n\n\n\n\n\nHow is the database disk autoscaling?\n\nIf autoscaling is enabled, when the free space threshold is reached, the disk expands by 10 GB.\n\n\n\n\n\nHow does database scaling work?\n\nYou can change the type of VM hosting the databases or increase the disk size. When changing the VM type, the changes take effect after the VM is restarted.\n\nYou can also enable autoscaling of the database disk size. Then, as the amount of data increases, the disk size will increase automatically.\n\nFor instructions on scaling a DB instance, see DB instance management.\n\n\n\n\n\nHow are the master, synchronous and asynchronous replicas distributed across the data center?\n\nThe master and replicas are located in the same data center, but upon request, technical support engineers can distribute them to different data centers.\n\n\n\n\n\nIs replication asynchronous or synchronous in the Master-Replica database configuration?\n\nAsynchronous replication works for Master-Replica configurations. Synchronous and asynchronous replication is used for the cluster.\n\n\n\n\n\nHow do I create a database replica?\n\nUse the instructions in the article Replication.\n\n\n\n\n\nHow do I install monitoring extensions for the database?\n\nInstalling extensions is described in detail in the section Managing extensions.\n\n\n\n\n\nHow is auto-switching between database nodes configured?\n\nWhen using a database cluster, if the wizard is unavailable, switching to other nodes will be configured automatically. When using configuration Master-Replica switching will need to be done manually:\n\nMySQL;\nPostgreSQL, Postgres Pro.\n\n\n\n\n\nWhat is the maximum amount of data that can be stored in a DBMS?\n\nThe volume is limited by the capabilities of the disk — 2 TB for High-IOPS SSD, 5 TB for SSD (for one shard), or RAM (Redis, Tarantool).\n\n\n\n\n\nIs it possible to create a user with read-only rights to the database through a personal account?\n\nNo, this is only possible through a direct SQL query.\n\n\n\n\n\nHow is user access to PostgreSQL organized?\n\nUsers access to databases is organized according to the role model of PostgreSQL itself. The user who has maximum access to one database will have access to other databases according to the public scheme.\n\n\n\n\n\nIs it possible to use tags in Terraform?\n\nVK Cloud does not support tags in Terraform.\n\n\n\n\n\nIs it possible to move the database to another project?\n\nTransferring PaaS service objects between projects is not supported. The virtual machine on which the database was deployed can only be transferred to another project as a regular virtual machine. It is not possible to migrate such a virtual machine as a database instance or create a database instance with a disk transferred from another project.\n\n\n","url":"/en/dbs/dbaas/faq","description":"Answers to frequently asked questions about the Cloud Databases service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Cloud Databases","link":"/en/dbs/dbaas","isActive":false,"evenSimpler":true}]},{"uuid":"f74c1da7-47d8-4301-b952-9e8ad387a781","title":"Create","data":"To create a database, go to the section of your personal account \"Analytical databases\", select the license version \"Arenadata DB\" and click \"Create database\".\n\nIn the next step, select the required cluster type, ArenadataDB instance version, and instance configuration.\n\nThe Cluster configuration is a cluster with synchronous data replication. It is used when there are increased requirements for the reliability and fault tolerance of the system.\n\nIn the following steps, select the required configuration options:\n\n| Parameter name | Description |\n|------------------------------------|-------------------- ------------------------|\n| Cluster name | Leave the default name or enter your name in Latin characters.|\n| Amount of uncompressed clean data | The maximum size of the database. |\n| Node configuration | CPU / RAM / HDD (SSD) master configuration.A replica/slave will be created automatically with the same configuration. |\n| Use reservation | Creation of redundant redundant nodes for fault tolerance. |\n| Connect monitoring node ||\n| Disk type | For best performance, we recommend choosing SSD or Hi-IOPS SSD.|\n| Network | Leave as default or choose your own private network.|\n| Assign external IP | Assign an external IP to the cluster to be able to connect to the cluster from external networks. |\n| Accessibility zone | specify the availability zone (we recommend DP1 or MS1).|\n\nIn the next step, set the database initialization parameters:\n\n| Parameter name | Description |\n|------------------------------------|-------------------- ------------------------|\n| Database name to create | You can leave the default or enter your own. |\n| Username | You must specify a username for remote access. |\n| User password | You must set a user password for remote access. |\n\nThe instance will be created within a few minutes. After that, information about the instance and connection methods will appear.\n","url":"/en/dbs/adb/quick-start/create-adb","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Arenadata DB as a Service","link":"/en/dbs/adb","isActive":false,"evenSimpler":true}]},{"uuid":"3eefe365-4d48-4ba4-ba5e-14a26bddfc85","title":"Deleting","data":"To delete an Arenadata DB cluster, select the required cluster on the list of analytical database instances and click the \"Delete\" button. The same operation can be performed in the context menu of the instance:\n\n\n","url":"/en/dbs/adb/quick-start/delete-adb","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Arenadata DB as a Service","link":"/en/dbs/adb","isActive":false,"evenSimpler":true}]},{"uuid":"5497a821-e2e0-4b3b-b04a-67d8aa48f038","title":"Launching, connecting and loading data","data":"Connecting to an instance\n\nTo connect to an ADB instance, use Instructions for connecting to a database instance.\n\nLoading data\n\nAfter the connector for connecting to the database is selected and the connection is established, you can start loading data.\n\nThe main tool for loading data into Greenplum and uploading data from Greenplum is the gpfdist utility. This is an http server that Greenplum talks to all its nodes at once, achieving high speeds for both downloading and uploading data.\n\nOfficial gpfdist documentation available at the official resource.\n\nWhat is gpfdist used for? Greenplum has the concept of external table (external tables), the description of which looks almost the same as for ordinary tables, but at the end a condition like LOCATION ('gpfdist://hostname:8080/database_name/table_name.csv') is added, which indicates exactly where the file is stored, the structure of which is described in the external table. External table can be either readable or writable.\n\nIf the external table is created for reading, then, firstly, instead of the file name, you can use a mask (that is, you can read or download many files at once), and secondly, you can read from archived files (.gz, .zip or . bz2), which significantly increases the speed of reading and loading files, since reading data from disks is reduced many times compared to reading unarchived files.\n\nExternal table can work with multiple instances of gpfdist running at once, even on different nodes. This further increases the performance of the system - both for loading and unloading data.\n","url":"/en/dbs/adb/quick-start/launch-adb","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Arenadata DB as a Service","link":"/en/dbs/adb","isActive":false,"evenSimpler":true}]},{"uuid":"d79c3b19-e66a-44be-8fd8-f2f5955d1420","title":"About the service","data":"Description\n\nArenadata DB (ADB) is a distributed DBMS that uses the MPP (massively parallel processing) concept and is based on an open source DBMS - Greenplum.\n\nAnalytical massively parallel DBMS are designed for storing and processing large amounts of data - from units to hundreds of terabytes of data. Such DBMSs are most often used for predictive analytics, regular reporting, customer churn analysis, and building corporate data warehouses.\n\nArenadata DB is a fast clustering solution with which you can deploy distributed databases. It allows you to store and process large amounts of structured and semi-structured data and build models based on them, for example, for BI analytics. In contrast to the use of similar analytical databases On Premises, Arenadata DB as a service allows you to speed up the construction of complex analytical queries up to 5 times by being able to quickly scale to hundreds of nodes in the VK Cloud cloud infrastructure.\n\nThe ADB architecture is a classic cluster: several shard servers, one master server and one standby server, interconnected by fast networks. Each server shard contains several PostgreSQL shards (instances) containing data. If one or more segments fail, they are marked as bad and their mirrored segments are started instead, the data for which is replicated using the Wright Ahead Log (WAL) technology used in PostgreSQL - all table and index changes are written to the file only after logging them).\n\nKey benefits of Arenadata DB\n\nAll support and expertise for implementation is available in Russia and in Russian.\nA package of utilities for offline installation (without Internet access) has been developed.\nThe distribution is based on the Open-source Greenplum DBMS kernel.\nCompletely Russian software.\nSupport is available both remotely and on-site. There is a set of available package services for planning, installing, auditing the system.\nThere is a possibility of further development and customization of the product for the specific needs of the customer.\nImplementation is available both on \"bare metal\" and in the cloud.\n","url":"/en/dbs/adb/concepts/about","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Arenadata DB as a Service","link":"/en/dbs/adb","isActive":false,"evenSimpler":true}]},{"uuid":"9c723882-5ac4-46cb-b6b9-fd36d99ff493","title":"Connections from internal networks","data":"Connecting to an instance\n\nTo connect to an ADB instance, use the database instance connection instructions.\n\nAdditional information can be found in the documentation for the connector used (a connector is an entity that connects to a database).\n","url":"/en/dbs/adb/connect/int-connect-adb","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Arenadata DB as a Service","link":"/en/dbs/adb","isActive":false,"evenSimpler":true}]},{"uuid":"f195b39d-32b6-4198-af62-3150e5afacfa","title":"External connections","data":"Connecting to an instance\n\n\n\nAttention\n\nConnecting externally requires an ADB instance with external IP enabled. You can learn more about enabling an external IP address on the “Creating a Cluster” page.\n\n\n\nTo connect to an ADB instance, use the database instance connection instructions.\n\nWhen using the examples, replace `` with the external IP address of the instance.\n\nAdditional information can be found in the documentation for the connector used (a connector is an entity that connects to a database).\n","url":"/en/dbs/adb/connect/ext-connect-adb","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Databases","link":"/en/dbs","isActive":false,"evenSimpler":true},{"title":"Arenadata DB as a Service","link":"/en/dbs/adb","isActive":false,"evenSimpler":true}]},{"uuid":"b601c190-7aaf-4419-a976-ecd52f7c7caf","title":"Quick start","data":"Quickstart will help you get started with the service and become familiar with its features.\n\nAfter going through all the steps of the quickstart, you will:\n\nCreate a small Kubernetes cluster.\nLearn how to connect to it.\nBecome familiar with Kubernetes and addons for it:\n   Connect management and monitoring tools.\n   Load the Docker images into the Docker registry.\n   Deploy simple applications based on the downloaded images, with the ability to use Cloud Storage.\n   Provide access to the deployed applications using the Ingress controller.\n   Make sure that these applications actually work.\n\n\n\nA running Kubernetes cluster consumes computing resources.\n\nAfter completing a quickstart, stop or delete the cluster if you no longer need it.\n\n1. Preparatory steps\n\n1.1. Create a cluster\n\nGo to personal account VK Cloud.\nSelect project, where the cluster will be placed.\nGo to Containers → Kubernetes clusters.\nIf there are no clusters in the selected project yet, click Create cluster.\n\n   Otherwise, click Add.\n\nIn the “Configuration” step:\n\n   Select the cluster configuration Dev environment with the newest version of Kubernetes.\n\n\n      Note the version of Kubernetes selected. This is important for further installation of kubectl.\n\n\n   Click the Next Step button.\n\nIn the “Create cluster” step, set:\n\n   Cluster name: for example, vk-cloud-k8s-quickstart.\n   Virtual machine type — Master: STD3-2-8.\n   Availability zone: Moscow (MS1).\n\n\n      Configuration files for creating and configuring resources in the cluster are designed to use this zone.\n\n      If you choose another zone, adjust configuration files.\n\n\n   Network: Create new network.\n   Assign external IP: make sure this option is selected.\n   Leave the other settings unchanged.\n   Press the Next step button.\n\nIn the “Node group” step, set:\n\n   Node type: STD3-4-8.\n   Availability zone: Moscow (MS1).\n\n\n      Configuration files for creating and configuring resources in the cluster are designed to use this zone.\n\n      If you choose another zone, adjust configuration files.\n\n\n   Leave the other settings unchanged.\n   Click the Create cluster button.\n\nWait for the cluster to complete, this process may take a while.\n\n1.2. Install add-ons in the cluster\n\n\n\nWhen installing the Docker Registry and Ingress NGINX add-ons, standard load balancers will be created for them.\n\nUsage of this load balancer is charged.\n\n\n\nInstall the docker-registry add-on.\n\n   Write down the data for accessing the Docker registry.\n\nInstall the kube-prometheus-stack add-on.\n\n   Write down the password to access the Grafana web interface.\n\nInstall the ingress-nginx add-on with default parameters.\n\n   Write down the floating IP address for the load balancer.\n\nFurther, the following values will be used in the commands and configuration files for the example. Replace them with the ones that are relevant to you.\n\n| Parameter                                                   | Value                      |\n| ----------------------------------------------------------- | -------------------------- |\n| IP address of the load balancerfor the Ingress controller | 192.0.2.2              |\n| URL of the Docker registry endpoint                         | 192.0.2.22:5000          |\n| Login of the Docker registry user                           | registry                 |\n| Password of the Docker registry user                        | registry-password-123456 |\n| The password of the user admin for Grafana                | grafana-password-123456  |\n\n1.3. Configure the environment to work with the cluster from\n\nSet up the host from which you will work with the cluster.\nThis can be a real computer or a virtual machine.\n\nInstall the following tools on the host:\n\nBrowser.\nThe kubectl utility.\n\n\n  Download a version of kubectl that matches the version of the cluster, or differs by one minor version in any direction.\n\n  For example, for cluster version 1.23.6, kubectl versions 1.22, 1.23 and 1.24 are suitable.\n\n  See Connecting to a cluster with kubectl for details.\n\n\nThe [kauthproxy] utility (https://github.com/int128/kauthproxy/releases). See Connecting to the cluster with Kubernetes Dashboard for details.\nThe client-keystone-auth utility. See Connecting to the cluster with kubectl for details.\nThe curl utility.\nDocker Engine:\n  For Windows and macOS: Docker Desktop.\n  For Linux, Docker Desktop is also recommended, but you can install and use Docker from the command line.\n\n1.4. Connect to the cluster\n\nAdd the Administrator Kubernetes role in personal account for the user on whose behalf the connection to the cluster will be performed:\n\n   Go to VK Cloud personal account.\n   Select the project where the previously created cluster is located.\n   Go to Manage access.\n   Click   for the required user and select Edit.\n   Select the Kubernetes Administrator role from the drop-down list.\n   Save your changes.\n\nActivate API access for this user.\n\nGet kubeconfig for the cluster in VK Cloud personal account:\n\n   Go to Containers → Kubernetes Clusters.\n   Find the required cluster in the list, then select Get Kubeconfig to access the cluster in its menu.\n\nMove kubeconfig to the ~/.kube directory, so you don't have to specify additional arguments when using kubectl.\n\n   The commands below assume that kubeconfig has been downloaded into the ~/Downloads directory under the name mycluster_kubeconfig.yaml.\n\n   Linux/macOS\n   Windows\n\n\n      mkdir ~/.kube && \\\n   mv /Downloads/mycluster_kubeconfig.yaml /.kube/config\n\n\n      mkdir ~/.kube; `\n   mv /Downloads/mycluster_kubeconfig.yaml /.kube/config\n\n\nCheck that kubectl can connect to the cluster:\n\n   Run the command:\n\n            kubectl cluster-info\n\n   Enter the user password from your VK Cloud account.\n\n   If the cluster works properly and kubectl is configured to work with it, similar information will be displayed:\n\n      Kubernetes control plane is running at...\n   CoreDNS is running at...\n\n   To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\n2. Get access to cluster monitoring tools\n\nAn add-on with monitoring tools was installed in the cluster based on Prometheus and Grafana have been enabled. Also Kubernetes Dashboard is available for all Cloud Containers clusters, which allows you to not only manage the cluster, but also monitor it.\n\n\n\nPrometheus + Grafana\nKubernetes Dashboard\n\n\n\nIn a separate terminal session, run the command:\n\n      kubectl -n prometheus-monitoring port-forward service/kube-prometheus-stack-grafana 8001:80\n\n\n   Do not close this session, or you will lose access to the Grafana web interface.\n   If port 8001 is already in use by another application, adjust the command by specifying a free port.\n\n\nOpen the Grafana web interface:\n\n   In your browser, go to the URL http://127.0.0.1:8001/.\n   Authorize with the login/password pair admin/grafana-password-123456.\n   If a password change is requested, change it.\n\nSelect Dashboards → Browse from the side menu of any pre-configured dashboard to get information about the cluster resources.\n\n\n\n\nIn a separate terminal session, run the command:\n\n      kauthproxy -n kubernetes-dashboard https://kubernetes-dashboard.svc\n\n\n   Do not close this session, or you will lose access to the Kubernetes Dashboard web interface.\n\n\nEnter the user password from your VK Cloud account, if a password is requested.\n\nThe browser will be opened and you will be redirected to the Kubernetes Dashboard web interface. Next, select any available category to view information about the cluster resources.\n\n3. Upload the necessary images to the Docker registry\n\nThe Docker Registry add-on was installed in the cluster which will store the Docker images.\n\n\n\nTo best demonstrate the capabilities of the cluster, a special Docker image with the NGINX web server will be built next.\nThe image is based on the plaintext demo image from NGINX.\n\n\n\nTo put your own images in the Docker cluster registry:\n\nAdd the Docker registry to the list of trusted registries:\n\n   Add the following parameter to the Docker daemon.json configuration file with the URL of the Docker registry endpoint:\n\n            {\n        ...\n\n        \"insecure-registries\": [\n          \"192.0.2.22:5000\"\n        ],\n\n        ...\n      }\n\n      The location of this file for different Docker Engine installations is given in official Docker documentation.\n\n   Restart the Docker Engine.\n\n      Linux\n      Windows\n      macOS\n\n\n      Do one of the following:\n\n      Run one of the commands to perform restart:\n\n                sudo systemd restart docker\n\n                sudo service docker restart\n\n      Restart the Docker Engine from the Docker Desktop GUI (if installed).\n\n\n      Restart the Docker Engine from the Docker Desktop GUI.\n\n\n      Restart the Docker Engine from the Docker Desktop GUI.\n\n\nBuild a Docker image:\n\n   Create a directory for the files and navigate to it:\n\n      Linux/macOS\n      Windows\n\n\n            mkdir /image-build && cd /image-build\n\n\n            mkdir /image-build; cd /image-build\n\n\n   Place the following files in this directory:\n\n      Dockerfile\n\n            FROM nginx:mainline-alpine\n\n      RUN chmod -R a+w /var/cache/nginx/ \\\n              && touch /var/run/nginx.pid \\\n              && chmod a+w /var/run/nginx.pid \\\n              && rm /etc/nginx/conf.d/*\n\n      COPY nginx-config.conf /etc/nginx/conf.d/\n      USER nginx\n\n      nginx-config.conf\n\n            server {\n          listen 8080;\n\n          location / {\n\n              set $k8s_pv \"not present\";\n\n              if (-d /etc/nginx/k8s_demo_pv/) {\n                set $k8s_pv \"present\";\n              }\n\n              default_type text/plain;\n              expires -1;\n              return 200 'Server address: $server_addr:$server_port\\nServer name: $hostname\\nDate: $time_local\\nURI: $request_uri\\nRequest ID: $request_id\\nRemote address (NGINX Ingress Controller): $remote_addr\\nX-Forwarded-For (Request source): $http_x_forwarded_for\\n\\nK8S Persistent Volume status: $k8s_pv\\n';\n          }\n      }\n\n\n   Run the build process:\n\n            docker build . -t 192.0.2.22:5000/nginx-k8s-demo:latest\n\n   Wait until the image build is complete.\n\nPlace the built image in the Docker registry:\n\n   Sign in to the registry:\n\n            docker login 192.0.2.22:5000 --username registry --password registry-password-123456\n\n   Push the image to the registry:\n\n            docker push 192.0.2.22:5000/nginx-k8s-demo:latest\n\n   Check that the image is in the registry:\n\n            curl -k -X GET -u registry:registry-password-123456 https://192.0.2.22:5000/v2/_catalog\n\n      Output should give you the similar information:\n\n            {\"repositories\":[\"nginx-k8s-demo\"]}\n\n   Create a Kubernetes secret so you can access the uploaded image from Kubernetes:\n\n            kubectl create secret docker-registry k8s-registry-creds --docker-server=192.0.2.22:5000 --docker-username=registry --docker-password=registry-password-123456\n\n4. Deploy demo applications\n\nBased on the nginx-k8s-demo image loaded in the Docker registry, two applications will be deployed: tea and coffee.\nFor each of the applications the following will be created:\n\nPersistent Volume Claim, so that data volumes can be mounted inside the application.\nDeployment, in which will be set:\n  Number of replicas.\n  Volume to mount in pod.\nService to provide access to the application. The Ingress controller will forward incoming requests to this Service.\n\nTo deploy the applications:\n\nCreate a directory for the files and navigate to it:\n\n   Linux/macOS\n   Windows\n\n\n      mkdir /k8s-deployments && cd /k8s-deployments\n\n\n      mkdir /k8s-deployments; cd /k8s-deployments\n\n\nPlace the following files in this directory:\n\n   deploy-coffee.yaml\n\n      kind: PersistentVolumeClaim\n   apiVersion: v1\n   metadata:\n     name: k8s-demo-pvc-coffee\n   spec:\n     storageClassName: \"csi-ceph-hdd-ms1\"\n     accessModes:\n       ReadWriteOnce\n     resources:\n       requests:\n         storage: 1Gi\n\n   ---\n   apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: coffee\n   spec:\n     replicas: 3\n     selector:\n       matchLabels:\n         app: coffee\n     template:\n       metadata:\n         labels:\n           app: coffee\n       spec:\n         volumes:\n           name: k8s-pv-coffee\n             persistentVolumeClaim:\n               claimName: k8s-demo-pvc-coffee\n         imagePullSecrets:\n           name: k8s-registry-creds\n         containers:\n           name: coffee\n             image: 192.0.2.22:5000/nginx-k8s-demo:latest\n             imagePullPolicy: Always\n             ports:\n               containerPort: 8080\n             volumeMounts:\n               name: k8s-pv-coffee\n                 mountPath: /etc/nginx/k8s_demo_pv\n\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: coffee-svc\n   spec:\n     ports:\n       port: 80\n         targetPort: 8080\n         protocol: TCP\n         name: http\n     selector:\n       app: coffee\n\n   deploy-tea.yaml\n\n      kind: PersistentVolumeClaim\n   apiVersion: v1\n   metadata:\n     name: k8s-demo-pvc-tea\n   spec:\n     storageClassName: \"csi-ceph-hdd-ms1\"\n     accessModes:\n       ReadWriteOnce\n     resources:\n       requests:\n         storage: 1Gi\n\n   ---\n   apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: tea\n   spec:\n     replicas: 2\n     selector:\n       matchLabels:\n         app: tea\n     template:\n       metadata:\n         labels:\n           app: tea\n       spec:\n         volumes:\n           name: k8s-pv-tea\n             persistentVolumeClaim:\n               claimName: k8s-demo-pvc-tea\n         imagePullSecrets:\n           name: k8s-registry-creds\n         containers:\n           name: tea\n             image: 192.0.2.22:5000/nginx-k8s-demo:latest\n             imagePullPolicy: Always\n             ports:\n               containerPort: 8080\n             volumeMounts:\n               name: k8s-pv-tea\n                 mountPath: /etc/nginx/k8s_demo_pv\n\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: tea-svc\n   spec:\n     ports:\n       port: 80\n         targetPort: 8080\n         protocol: TCP\n         name: http\n     selector:\n       app: tea\n\n\n   Note that the configuration files deploy-coffee.yaml and deploy-tea.yaml for Persistent Volume Claim specify the storage class corresponding to the availability zone of the node (MS1) on which you plan to deploy applications.\n\n   Attempting to place an application on a node in one availability zone to which a volume from another availability zone is mounted will fail.\n\n\nDeploy the applications:\n\n      kubectl apply -f deploy-coffee.yaml -f deploy-tea.yaml\n\nCheck if the deployment is correct for:\n\n   Persistent volumes\n   Workloads\n   Services\n\n\n   Use one of the ways:\n\n   kubectl: run the command.\n\n          kubectl get pv\n\n   Grafana: open the Kubernetes → Compute Resources → Persistent Volumes dashboard.\n\n   Kubernetes Dashboard: open the Cluster → Persistent Volumes dashboard.\n\n   You will see information that 1GB persistent volumes, that have been created with Persistent Volume Claim for deployments tea and coffee, are present.\n\n\n   Use one of the ways:\n\n   kubectl: run the command.\n\n          kubectl get deployment\n\n   Grafana: open the Kubernetes → Compute Resources → Namespace (Workloads) dashboard.\n\n   Kubernetes Dashboard: open the Workloads → Deployments dashboard.\n\n   You will see that there is a deployment coffee of three pods, and a deployment tea of two pods.\n\n\n   Use one of the ways:\n\n   kubectl: run the command.\n\n          kubectl get svc\n\n   Kubernetes Dashboard: open the Service → Services dashboard.\n\n   You will see that there are two services coffee-svc and tea-svc of type ClusterIP.\n\n\n5. Configure Ingress for demo applications\n\nThe Ingress Controller add-on was installed in the cluster NGINX was enabled to route incoming user requests to the applications deployed in the cluster.\n\nFor Ingress controller to route requests to the corresponding Service resources, through which the tea and coffee demo applications were published, do the following:\n\nPlace the following file in the ~/k8s-deployments directory:\n\n   deploy-ingress.yaml\n\n      apiVersion: networking.k8s.io/v1\n   kind: Ingress\n   metadata:\n     name: cafe-ingress\n   spec:\n     ingressClassName: nginx\n     rules:\n       host: cafe.example.com\n         http:\n           paths:\n             path: /tea\n               pathType: Prefix\n               backend:\n                 service:\n                   name: tea-svc\n                   port:\n                     number: 80\n             path: /coffee\n               pathType: Prefix\n               backend:\n                 service:\n                   name: coffee-svc\n                   port:\n                     number: 80\n\n\nDeploy the Ingress resource:\n\n      kubectl apply -f deploy-ingress.yaml\n\nCheck if the deployment is correct by running the following kubectl command:\n\n      kubectl get ingress\n\n   You will see information that there is a working Ingress resource.\n\n6. Check that all the created resources in the cluster are working\n\nTo verify that the example is working, run curl requests to the IP address 192.0.2.2 of the load balancer. The Ingress controller associated with the load balancer will then deliver these requests to the appropriate applications.\n\nRequests for:\n\n\n\nThe tea application\nThe coffee application\n\n\n\ncurl --resolve cafe.example.com:80:192.0.2.2 http://cafe.example.com/tea\n\nOutput should give you the similar information:\n\nServer address: 10.100.109.3:8080\nServer name: tea-8697dc7b86-s5vgn\nDate: 24/Aug/2022:09:27:34 +0000\nURI: /tea\nRequest ID: ed83bd555afd25c103bfa05ee12cbfff\nRemote address (NGINX Ingress Controller):\nX-Forwarded-For (Request source):\n\nK8S Persistent Volume status: present\n\n\n\n\ncurl --resolve cafe.example.com:80:192.0.2.2 http://cafe.example.com/coffee\n\nIP address\n\nServer address: 10.100.109.0:8080\nServer name: coffee-5f48899848-4q97z\nDate: 24/Aug/2022:09:35:57 +0000\nURI: /coffee\nRequest ID: 35e93a2538be8843c1c1fcd65b5aac4c\nRemote address (NGINX Ingress Controller):\nX-Forwarded-For (Request source):\n\nK8S Persistent Volume status: present\n\n\n\n\nThis result demonstrates that:\n\nYou can run applications using Docker images from the Docker cluster registry.\nYou can mount storage to pods using Persistent Volume Claim.\nThe Ingress controller provided with the cluster is configured correctly because it shows the real IP address of the request source.\n\nDelete unused resources\n\nA running cluster consumes computing resources. If you no longer need it:\n\nstop it to use it later;\ndelete it permanently.\n\nWhat's next?\n\nGet to know the usage scenarios of the cluster.\nGet to know the concepts of the container service.\nGet to know the detailed instructions on how to connect to the cluster.\n","url":"/en/kubernetes/k8s/quick-start","description":"Getting started with the Kubernetes cluster and getting acquainted with its main features.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"a03032dc-1ebe-4e75-a08b-37fd7d8daad7","title":"About service","data":"What the service can do\n\nThe container service allows you to create Kubernetes clusters and run various services and applications in them. All the usual Kubernetes tools are available, e.g:\n\nStoring and handling serverless functions in containers: OpenFaaS, OpenWhisk, Kubeless.\nService Mesh: Istio, Consul, Linkerd.\nMonitoring, analytics, logging: Prometheus, Fluentd, Jaeger, OpenTracing.\nCI/CD: GitLab, CircleCI, Travis CI.\nIaC (infrastructure as code): Terraform, Helm.\nBig Data and data science: Spark.\n\n  For big data analysts, the following features can be useful:\n\n  Autoscaling clusters that can handle large computational loads.\n  Creating event-triggered data handlers.\n  Integration of Kubernetes clusters and data with other VK Cloud machine learning platform services.\n\n\n\nVK Cloud's Kubernetes distribution has been certified Certified Kubernetes - Hosted by CNCF (Cloud Native Computing Foundation). This means that the distribution has been tested for reliability and standards compliance, it meets all functional requirements of the community and is compatible with the standard Kubernetes API. VK Cloud is the only cloud provider in Russia that has achieved this certification.\n\nService features\n\nManagement of cluster and groups of nodes using VK Cloud personal account and VK Cloud own Terraform provider.\n\nManage Cloud Containers objects and resources after connecting to the cluster using kubectl or Kubernetes Dashboard.\n\nAutomatic and manual cluster scaling.\n\n  When automatic scaling is enabled, applications instantly get additional computing power when the load peaks. When the load drops, the amount of resources available to the application is reduced.\n\n  This approach saves up to 60% of computing resources.\n\nCreating distributed installations within VK Cloud regions: to provide high availability, different nodes of the same cluster can be located in different availability zones (different data centers). It is also recommended to place application replicas on these nodes so that replicas are also in different availability zones.\n\nIntegration with the storage subsystem and networking subsystem of the VK Cloud platform.\n\nOne-click rolling update of clusters with no downtime. This applies to both minor and major versions of Kubernetes.\n\n\n  Cluster updates are available starting with cluster version 1.17 and higher.\n\n\nEnsuring security at all stages of cluster operation:\n\n  During network communication in the cluster, all connections are encrypted and certificates are used.\n  You can apply Calico network policies.\n  You can apply Gatekeeper constraint policies.\n  Integration of Kubernetes security role model with VK Cloud platform roles is available. Read more in Access management.\n\nBackup Cloud Containers clusters with Velero.\n\nMigration of other Kubernetes clusters to Cloud Containers using Velero.\n\nA set of preconfigured add-ons which can be selected when creating a cluster using Terraform or install into an existing cluster, saving time on their manual deployment.\nA set of preconfigured settings, which help improve cluster stability and security.\n\nMonitoring the state of the cluster with Prometheus. You can view the monitoring data in several ways.\n\nWhat's next\n\nGet to know the service architecture.\nGet to know the network structure in the cluster.\nGet to know the storage device in the cluster.\n","url":"/en/kubernetes/k8s/concepts/about","description":"VK cloud containers (Kubernetes) service overview.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"28a5cb50-e090-4cd1-9d80-9bc978ee8780","title":"Service architecture","data":"The VK Cloud platform is based on OpenStack. The Cloud Containers service architecture, based on OpenStack, gives users a wide range of capabilities, fault tolerance, scalability and integration with other services of the platform.\n\nCluster topologies\n\nA Kubernetes Cloud Containers cluster consists of two types of nodes, master nodes and worker nodes:\n\nMaster nodes store cluster-wide state information and manage workload distribution across worker nodes.\n\nWorker nodes perform the workload (workload). They can be organized into groups of worker nodes. Place groups in different availability zones to improve fault tolerance.\n\nCluster high availablity depends on the number of master nodes. Possible configurations:\n\nCluster with one master node.\n\n  Such a cluster is not high available: even if there are several worker nodes and they are organized in groups, the cluster will become inoperative if the single master node is lost.\n\nA cluster with an odd number of master nodes (at least three).\n\n  Such a cluster is high available: if several master nodes are lost, it will remain operational as long as there is at least one working master node. The further level of fault tolerance depends on the number and configuration of groups of worker nodes.\n\nRegardless of the cluster topology chosen, the master nodes use distributed key-value storage etcd to store information about the state of the cluster:\n\nA cluster with one master node has one instance of etcd.\nClusters with multiple master nodes have multiple instances of etcd running in cluster mode for high availability.\nEach etcd instance has a dedicated high-performance SSD disk (High-IOPS). This allows for the fastest possible interaction with the cluster API endpoint with minimal latency.\n\nFor high availability at the worker node level, it is recommended to create several groups of worker nodes in different availability zones and place application replicas on these nodes so that the replicas are also in different availability zones.\n\nCluster environment\n\nThe following operating systems are used on the master and worker nodes:\n\nCentOS (up to Kubernetes 1.20).\nAlmaLinux (starting with Kubernetes 1.21). AlmaLinux is a fork of CentOS.\n\nThe cluster runs containers via Kubernetes Container Runtime Interface (CRI) with CRI-O (starting with Kubernetes version 1.20).\n\n\n\nBefore Kubernetes 1.19, Docker\\\\Dockershim was used to run containers and is now deprecated.\n\n\n\nSee Available Kubernetes versions and version support policy for details.\n\nIntegration with the Kubernetes API\n\nAll interaction with the cluster is through the Kubernetes API.\n\nThe Cloud Containers cluster API endpoint is placed behind dedicated load balancer, so the cluster API can be accessed via the same IP address regardless of the number of master nodes.\n\nIntegration with VK Cloud platform\n\nIntegration with the VK Cloud platform is achieved through standard Kubernetes interfaces:\n\nContainer Storage Interface (CSI): integration with storage services.\n\n  Allows to use Cloud storage in clusters as persistent volumes.\n  Persistent Volume Claim (PVC) is available.\n\n  Integration is achieved using OpenStack Cinder API. See Storage in cluster for details.\n\nContainer Network Interface (CNI): integration with network services.\n\n  Every Cloud Containers cluster has a Calico plugin that supports this interface. This plugin provides:\n\n  network connectivity between containers, pods, and cluster nodes;\n  application and enforcement of Kubernetes network policies.\n\n  Calico integrates with VK Cloud platform using OpenStack Neutron API. See Network in cluster for details.\n\nBuilt-in support for the Open Policy Agent\n\nPods can access various information and components of a Kubernetes cluster. With certain settings, pods themselves and the Kubernetes cluster in which they run can be vulnerable to attacks.\n\nSince Kubernetes version 1.21, Open Policy Agent Gatekeeper is built into the cluster to increase the cluster's resistance to attacks. It allows you to apply constraints, which help to increase the security of deployed workload.\n\nThese constraints are created based on constraint templates. Cloud Containers clusters already contain preconfigured templates and constraints. You can create your own templates and constraints.\n\nFor clusters below version 1.21 it is recommended to install Gatekeeper manually or upgrade the cluster to the current version.\n\nCluster scaling options\n\nThe Cloud Containers cluster has built-in scaling capabilities for master nodes and worker nodes.\n\nAutomatic scaling is also supported: if it is configured, the number of worker nodes in the cluster is automatically adjusted depending on the needs of the workload.\n","url":"/en/kubernetes/k8s/concepts/architecture","description":"Service architecture details: Cloud Containers cluster topologies and environment, integration with Kubernetes API and VK Cloud platform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"dc21e4b3-faff-4d3a-b639-2a6b9847898a","title":"Add-ons","data":"Various add-ons (additional services) are available for Cloud Containers clusters. They can be selected in any combination and installed either when creating a cluster using Terraform, or later in already an existing cluster. The installation process is automated and requires minimal user intervention.\n\n{heading(Features of installing add-ons)[id=features_of_installing_addons]}\n\nAdd-ons are installed on the worker nodes of the cluster and consume their computing resources.\n\n  The following are the system requirements of add-ons based on the standard values requests and limits for Kubernetes resources in the add-on setup code. When using non-standard values, the system requirements of add-ons will change.\n\n\n  Some add-ons can be installed on all cluster nodes (including master nodes). For more details, see Configuring and installing add-ons.\n\n\nAdd-ons can be installed on a dedicated group of worker nodes or on Kubernetes worker nodes selected by the scheduler. Using the first approach allows you to exclude the influence of add-ons on the operation of production services deployed in the cluster.\n\n  The computing resources of a dedicated group of worker nodes should be sufficient for all add-ons, even if each add-on consumes the maximum resources specified in the system requirements. It is recommended to set up automatic scaling for such a group of nodes.\n\nThere are three options for installing add-ons:\n\n  Standard installation on Kubernetes worker nodes selected by the scheduler with a change in the add-on configuration code.\n  Installation on dedicated worker nodes with a change in the add-on configuration code.\n  Quick installation on Kubernetes worker nodes selected by the scheduler without changing the add-on setup code (with default settings).\n\n  Not all add-ons support all three installation options.\n\n  The installation process is described in the section Configuring and installing add-ons.\n\n{heading(Available add-ons)[id=available_addons]}\n\n\n\nThe availability of specific add-ons depends on the region in which the cluster is planned to be placed.\n\nCapsule\n\n\n\nDescription\nSystem requirements\n\n\n\nKubernetes clusters allow you to organize the logical division of Kubernetes resources at the level of individual namespaces. However, this may not be enough to achieve resource separation and isolation in complex scenarios. For example, let's say you want to provide isolated sets of resources to multiple development teams so that they are not accessible to each other. A typical solution to this problem is to create several separate clusters for each team. With this approach, as the number of teams increases, the number of clusters also increases, which complicates the administration of these clusters.\n\nCapsule allows you to organize isolated sets of resources within one cluster using tentants. An individual tenant represents namespaces assigned to a group of users combined with restrictions on the creation and consumption of Kubernetes resources. The Capsule policy engine not only monitors compliance with resource usage policies within a tenant, but also ensures the isolation of one tenant from another. Thus, it becomes possible to organize the work of several teams within one multi-tenant cluster without the need to administer additional clusters.\n\n\n\n\nCPU**: 200m.\nRAM**: 128Mi.\n\ncert-manager\n\n\n\nDescription\nSystem requirements\n\n\n\ncert-manager helps to manage certificates in Kubernetes clusters:\n\nIssue certificates, including self-signed certificates. To do this, cert-manager sends requests to sources acting as certificate authority (CA).\n\n  Examples of the sources:\n\n  Cybersecurity solutions providers such as Venafi.\n  Certificate providers, such as Let’s Encrypt.\n  Storage for secrets, such as HashiCorp Vault.\n  Local containers containing the public part of a certificate and private key.\n\nAutomatically reissue expiring certificates.\n\nA certificate issued with cert-manager will be available to other Kubernetes resources. For example, it can be used by Ingress.\n\n\n\n\nThe requirements of the individual add-on components:\n\ncert-manager:\n\n  CPU: 10m.\n  RAM: 32Mi.\n\ncert-manager-cainjector:\n\n  CPU: 10m.\n  RAM: 32Mi.\n\ncert-manager-webhook:\n\n  CPU: 10m.\n  RAM: 32Mi.\n\nDocker Registry\n\n\n\nWhen installing the add-on, a standard load balancer will be created.\n\nUsage of this load balancer is charged.\n\n\n\n\n\nDescription\nSystem requirements\n\n\n\nDocker Registry is designed to host and store Docker images. It works in a high availability (HA) configuration. Registry images can be used when deploying services in a cluster.\n\nSee Connecting to the Docker registry for details.\n\n\n\n\nCPU**: 100m.\nRAM**: 128Mi–512 Mi.\nS3 object storage capacity**: depends on the size and number of images that are planned to be placed in the registry.\nStandard load balancer**: one piece.\nFloating IP**: one piece.\n\nFluent Bit\n\n\n\nDescription\nSystem requirements\n\n\n\nFluent Bit allows you to flexibly configure log collection in Cloud Containers clusters and analyze them in the Cloud Logging service, for example, using Elasticsearch or Loki plugins.\n\nThe sources of the logs are kubelet services and pods located on cluster nodes.\n\n\n\n\nThe add-on does not have its own system requirements. The add-on subs use limit settings by default.\n\nFluent Bit for Cloud Logging (logaas-integration)\n\n\n\nDescription\nSystem requirements\n\n\n\nFluent Bit in combination with special filters, written in Lua, allows you to organize the delivery of logs from the Cloud Containers cluster to the Cloud Logging service for further analysis of these logs.\n\nThe sources of the logs are kubelet services and pods located on cluster nodes. For more information about how the add-on works, see the section about installing it.\n\n\n\n\nThe add-on does not have its own system requirements. The add-on subs use limit settings by default.\n\nIngress Controller (NGINX)\n\n\n\nWhen installing the add-on, a standard load balancer will be created.\n\nUsage of this load balancer is charged.\n\n\n\n\n\nDescription\nSystem requirements\n\n\n\nIngress controller based on NGINX works as a reverse proxy and allows to organize single entry point for services in cluster which work via HTTP or HTTPS.\n\nIf you have a controller, it is sufficient to create Ingress resource to make such services available from outside the Cloud Containers cluster.\n\nThe pre-installed Ingress controller integrates tightly with the VK Cloud platform. For more information, see Network in cluster.\n\n\n\n\nCPU**: 210m–610m.\nRAM**: 238Mi–660Mi.\nStandard load balancer**: one piece.\nFloating IP**: one piece. (when installed with default settings).\n\n{heading(Istio)[id=istio]}\n\n\n\nDescription\nSystem requirements\n\n\n\nIstio is a framework that implements the service mesh concept, which allocates a separate layer for interaction between application services. Using Istio provides traffic management for services without changing the code of the services (sidecar containers are used). Istio benefits:\n\nExpanded secure traffic transfer capabilities:\n\n  Traffic policies can be configured.\n  TLS can be used to communicate between services.\n\nExpanded traffic monitoring capabilities.\nComplex routing and balancing of traffic between services can be done.\n\n\n\n\nCPU**: 500m.\nRAM**: 2Gi.\n\n{heading(Jaeger)[id=jaeger]}\n\n\n\nDescription\nSystem requirements\n\n\n\nIn distributed systems based on microservices, requests are constantly exchanged. The Jaeger platform created for distributed query tracing. Jaeger tracks the flow of requests through microservices and allows you to:\n\ncollect information about the interrelationships of the system components in terms of the flow of requests;\ndetect query problems or bottlenecks in the system architecture related to processing the request stream.\n\nSuch a tool is necessary because query-related factors can significantly affect the behavior and performance of these systems as a whole. It is not enough to provide monitoring only for individual microservices.\n\nJaeger performs query tracing based on the data it receives from microservices. Therefore, it is necessary to integrate into microservices tool stack OpenTelemetry to send data about requests. You can get acquainted with the integration of OpenTelemetry into a microservice application using the example of Hot R.O.D.\n\n\n\n\nAdd-on requirements:\n\nThe number of worker nodes must be at least the selected number of Elasticsearch replicas.\n\n  Elasticsearch is used as a backend for storage. Each Elasticsearch replica will be hosted on a separate worker node to ensure fault tolerance.\n\n  For more information about choosing the number of replicas, see section about installing Jaeger.\n\nWorker nodes must use the configuration of computing resources:\n\n  STD2-4-4 or better (for a test environment);\n  STD2-6-6 or better (for a production environment).\n\nRequirements of individual add-on components:\n\nElasticsearch:\n\n  CPU: 100m–1000m.\n  RAM: 512M.\n\nAgent:\n\n  CPU: 250m–500m.\n  RAM: 128M–512M.\n\nCollector:\n\n  CPU: 500m–1000m.\n  RAM: 512M–1024M.\n\nQuery:\n\n  CPU: 250m–500m.\n  RAM: 128M–512M.\n\n\n\nTo ensure stable operation of Jaeger, it is recommended to install it on a dedicated group of worker nodes that meets the above requirements.\n\nKiali\n\n\n\nDescription\nSystem requirements\n\n\n\nKiali is a web interface for working with Istio. It allows to manage, monitor and visualize a service mesh.\n\n\n\n\nCPU**: 10m—500m.\nRAM**: 64Mi—1Gi.\n\n{heading(Kube Prometheus Stack)[id=kube_prometheus_stack]}\n\n\n\nDescription\nSystem requirements\n\n\n\nThe system for monitoring the status of the cluster and the services deployed in it is implemented on the basis of Prometheus and visualization tool Grafana.\n\nSee Cluster Monitoring for details.\n\n\n\n\nCPU**: 850m–2500m.\nRAM**: 968Mi–3804Mi.\nHDD**: 2GB.\nSSD**: 10GB.\n\n\n\n","url":"/en/kubernetes/k8s/concepts/addons-and-settings/addons","description":"Services that can be automatically installed in the cluster to extend its functionality.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"323f1045-1e05-44a1-b139-8b386a34ee47","title":"Cluster settings","data":"The Cloud Containers clusters already have certain settings applied, listed below.\n\nkube-proxy operation mode\n\nThe Cloud Containers network proxy runs on each node, providing access to IP addresses of services and other Cloud Containers resources.\n\nThis proxy can work in several modes, they are listed in the --proxy-mode setting description. In Cloud Containers clusters, the proxy works in iptables mode. This mode of operation affects:\n\nthe behavior of load balancers;\non the behavior and settings of the local caching DNS server.\n\nLimits settings for pods\n\nWhen working with pods it is recommended to specify in their configuration files requests and limits parameters for containers included in this pod.\n\nIf these parameters are not specified, Cloud Containers clusters automatically apply the values for the corresponding containers:\n\nrequests: 100m CPU and 64Mb allocated memory.\nlimits: 500m CPU and 512Mb allocated memory.\n\nThis prevents a container running incorrectly from exhausting all of the resources of a single worker node or even the entire cluster.\n\nPre-configured Gatekeeper templates and constraints\n\n\n\nDisabling or changing these templates and constraints can reduce the security of the Kubernetes Cloud Containers cluster. Any problems with the cluster that arise directly or indirectly due to disabling the settings below must be resolved by the customer themselves.\n\n\n\nTemplates are available for clusters starting with Cloud Containers version 1.21. For older versions, manually install Gatekeeper and the above templates and restrictions, or upgrade the cluster. For more information about Gatekeeper, see Architecture.\n\n\n\nhost-namespacesconstraint\nhost-filesystemconstraint\n\n\n\nDescription:.\n\nThis constraint prohibits running pods with the hostPID: true option.\n\nA pod launched with this option will have the following capabilities:\n\nView all processes running on the host.\nForce termination of any process on the host by a kill command sent from the pod.\nRead environment variables for each pod on the host by accessing the /proc/[PID]/environ file for each process.\n\nSuch capabilities are very broad and are themselves considered vulnerabilities, as they can expose sensitive environment variables and manipulate processes, as well as facilitate exploitation of other vulnerabilities.\n\nExample of how the constraint acts:\n\n\nThe pod_namespace.yaml manifest that does not satisfy the constraint\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-disallowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  hostPID: true\n  hostIPC: true\n  containers:\n    name: nginx\n      image: nginx\n\n\n\nIf you try to apply such a manifest with kubectl apply -f pod_namespace.yaml, you will get a similar message saying that the constraint for the pod has been violated:\n\nError from server ([...] Sharing the host namespace is not allowed: nginx-host-namespace-disallowed): error when creating \"pod_namespace.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [...] Sharing the host namespace is not allowed: nginx-host-namespace-disallowed\n\n\n\n\nDescription:\n\nThis restriction prevents pod from mounting the subdirectories of the host on which the pod is running. This protects the cluster data that resides on this host.\n\nExample of how the constraint acts:\n\n\nThe pod_filesystem.yaml manifest that does not satisfy the constraint\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-filesystem\n  labels:\n    app: nginx-host-filesystem-disallowed\nspec:\n  containers:\n    name: nginx\n      image: nginx\n      volumeMounts:\n        mountPath: /cache\n          name: cache-volume\n          readOnly: true\n  volumes:\n    name: cache-volume\n      hostPath:\n        path: /tmp # directory on host\n\n\n\nIf you try to apply such a manifest with kubectl apply -f pod_filesystem.yaml, you will get a similar message saying that the constraint for the pod has been violated:\n\nError from server ([...] HostPath volume {\"hostPath\": {\"path\": \"/tmp\", \"type\": \"\"}, \"name\": \"cache-volume\"} is not allowed, pod: nginx-host-filesystem. Allowed path: [{\"pathPrefix\": \"/tmp\", \"readOnly\": true}]): error when creating \"pod_filesystem.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [...] HostPath volume {\"hostPath\": {\"path\": \"/tmp\", \"type\": \"\"}, \"name\": \"cache-volume\"} is not allowed, pod: nginx-host-filesystem. Allowed path: [{\"pathPrefix\": \"/tmp\", \"readOnly\": true}]\n\n\n\n\nThe pod that violated the constraint will not be created.\n","url":"/en/kubernetes/k8s/concepts/addons-and-settings/settings","description":"Pre-configured cluster settings.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"25f0d71c-4e77-4597-83ae-9fb455a189f0","title":"Network in cluster","data":"Intra-cluster DNS server\n\nThe CoreDNS intra-cluster server is used as a replacement for kube-dns. This server provides discovery of services in the cluster and allows them to be addressed by their DNS names.\n\nCoreDNS also exports metrics to Prometheus, which allows you to monitor its performance through cluster monitoring tools.\n\nWorking with Container Network Interface (CNI)\n\nCloud Containers clusters use Calico to organize the intra-cluster network. Calico communicates with the VK Cloud platform using the OpenStack Neutron API. The platform itself uses a software-defined network Neutron.\n\nWhen using OpenStack Neutron in large cloud networks (about tens of thousands of network ports per network), there are problems with convergence speed if at least a few network members fail. Synchronization of topology changes in a network of this size takes a long time, which negatively affects its performance.\n\nTherefore, VK Cloud has developed its own software-defined network, Sprut. It is fully compatible with Neutron API but works on different principles, which allows to use it together with Calico in large cloud networks without experiencing the described issues. Read more about Sprut in How cloud networks work and how they differ from On-premise and in YouTube video.\n\n\n\nSprut is in the beta testing phase. Contact technical support to get access to the new SDN service.\n\nIntegration with load balancers\n\nCloud Containers cluster integrates with the load balancers of the VK Cloud platform. This applies to both regular Kubernetes load balancers (LoadBalancer) and Ingress controllers (IngressController): both will have a dedicated VK Cloud TCP balancer attached to them both when created. This also applies to the Ingress controller, which is installed as an add-on.\n\nIf necessary, you can use the HTTP load balancer. See example for Ingress controller for details.\n\nThe VK Cloud platform load balancer is based on OpenStack Octavia, which has HAProxy at its core and supports:\n\nproxying and balancing HTTP and TCP connections (the latter including proxy-protocol support);\nproxying and balancing HTTP/2 connections in addition to HTTP/1.1;\ntermination of SSL connections at the balancer.\n\nFor high availability, two instances of the balancer are deployed, one in active mode and one in standby mode. State synchronization and traffic switching between these balancers takes place using the VRRP protocol. The cluster treats such a high-available configuration as a single load balancer.\n\nIngress controller and real user IP address detection\n\nSometimes when using the Ingress controller, the pod in the cluster needs to see the real IP address of the user who sent the original request to the balancer, not the IP address of the balancer itself. This can be important for some kinds of network applications.\n\nTo allow a pod that is placed behind the Ingress controller to see the user's real IP address, use one of the options:\n\nIngress controller with proxy protocol support.\n\n  If you plan to handle HTTPS traffic, configure SSL connection termination on this Ingress controller because the TCP balancer that will be created for the controller cannot terminate SSL connections itself.\n\n  The NGINX-based Ingress Controller provided by VK Cloud supports the proxy protocol and is already configured to work with it.\n\nSeparate HTTP\\HTTPS balancer with additional settings:\n\n  If you plan to handle HTTPS traffic, configure SSL connection termination on this balancer.\n  Activate the ExternalTrafficPolicy: Local policy on the Ingress controller.\n\n  In this case only HTTP traffic will flow to the Ingress controller, which will show all headers, including the sender's IP address (if any).\n\nDefault network objects\n\nWhen a Kubernetes cluster is created, several network objects are created for it.\n\nLoad balancer for Kubernetes API\n\nA dedicated TCP load balancer is created for each cluster, which handles incoming requests to the Kubernetes API for all master nodes. This load balancer is used to connect to and manage the cluster.\n\nFirewall rules\n\nThree groups of rules are created for each cluster:\n\n-base: allow communication between master nodes and groups of worker nodes.\n-master: allow communication with the master nodes.\n-minion: provide communication between groups of worker nodes.\n\n\n\nChanging these rule groups may render the cluster inoperable.\n\nSee also\n\nContainer service overview.\nContainer service architecture.\nStorage in a cluster.\n","url":"/en/kubernetes/k8s/concepts/network","description":"Each Cloud Containers cluster is tightly integrated with VK Cloud network services. The section shows how this integration works.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"00d84a35-fd8d-49cc-87bc-20e6347bfb84","title":"Storage in cluster","data":"Data in a Kubernetes Cloud Containers cluster can be stored in several ways: directly in a container or on volumes. There are problems with storing data in a container:\n\nIf the container crashes or stops, data is lost.\nContainer data is inaccessible to other containers, even if all containers are in the same pod.\n\nTo solve these problems, Cloud Containers volumes are used. Volumes have different lifecycles depending on the usage scenario:\n\nEphemeral volumes (EVs) have the same lifecycle as a pod. When a pod using such a volume ceases to exist, the volume is also deleted. Ephemeral volumes can only be used by a single pod, so volumes are declared directly in the pod's manifest.\n\nPersistent volumes (PVs) have their own lifecycle, independent of the pod's lifecycle. Due to the separation of lifecycles, such volumes can be reused later with other pods. Pods and other workloads use Persistent Volume Claim (PVC) to handle persistent volumes.\n\nCloud Containers clusters are tightly integrated with the VK Cloud platform to handle PVs:\n\nThe cluster supports storage provided by the VK Cloud platform. Block storage support is implemented using Cinder CSI.\nPre-configured storage classes that implement different persistent volume reclaim policies are available for block storage in the cluster.\n\nSupported VK Cloud storage types\n\nBlock storages:\n\n  Based on Ceph. To ensure fault tolerance and data integrity, the storage consists of three replicas located in different server racks. The storage can use HDD or SSD disks.\n\n  Based on high-performance NVMe SSD disks (High-IOPS SSD). Such storage is connected via iSCSI. Hardware RAID-10 is used to provide fault tolerance and data integrity at the storage level.\n\nFile Storage connected via NFS and CIFS.\n\nWorking with Container Storage Interface (CSI)\n\nKubernetes clusters use OpenStack Cinder to integrate with block storage in VK Cloud.\n\nThe storage types available in a Kubernetes cluster via Cinder CSI correlate with VK Cloud block storage as follows:\n\nCeph HDD corresponds to ceph-hdd in Cinder.\nCeph SSD corresponds to ceph-ssd in Cinder.\nHigh-IOPS SSD corresponds to high-iops in Cinder.\n\nUsing Cinder CSI allows you to:\n\nStatically and dynamically provision PV that is based on block storage.\n\nAutomatically remount persistent volumes:\n  When the pod using the volume or the worker node hosting the pod fails (assuming the pod is restored to that node or another node).\n  When you migrate a pod using a volume from one worker node to another.\n\nManage the storage that is used by PV:\n  When a volume is dynamically provisioned, the disk corresponding to that volume in VK Cloud will be automatically created.\n  If the Delete reclaim policy is set for a volume, the associated volume and the corresponding disk in VK Cloud will be deleted after the PVC is deleted.\n\nAvailable reclaim policies for persistent volumes\n\nA reclaim policy can be set for a PV. The policy will be triggered when the PVC associated with that volume is deleted:\n\nKeep the volume (Retain). The PV and its associated storage will not be deleted.\n\n  This policy applies to both block and file storage. Use it for PVs with sensitive data to protect the data if the PVC is accidentally deleted. If necessary, you can manually clean up and delete PVs with this policy and their associated storage.\n\nDelete the volume (Delete). The PV and its associated storage will be deleted.\n\n  This policy is only applicable to block storage.\n\n\n  Use this policy and the storage classes that implement it with caution: deleting a PVC will cause the PV and the disk corresponding to that volume to be deleted.\n\n\nPre-configured storage classes\n\nWhen using dynamic provisioning of a persistent volume, a storage class should be specified. The default storage class is not configured in VK Cloud Kubernetes clusters. It is possible either to set the default class manually, or explicitly specify the required class when creating a PVC.\n\nThere are preconfigured storage classes that use Cinder CSI for block storage.\n\nThe classes provide different types of storage in multiple regions and availability zones.\nEach storage class has a distinct reclaim policy confugured for it.\n\n\n\nMoscow region\n\n\n\n| Storage classname           | Cinder CSIstorage type  | Availabilityzone | ReclaimPolicy |\n| ------------------------------- | --------------------------- | ------------------- | ----------------- |\n| csi-ceph-hdd-gz1                | ceph-hdd                  | GZ1                 | Delete            |\n| csi-ceph-hdd-gz1-retain         | ceph-hdd                  | GZ1                 | Retain            |\n| csi-ceph-hdd-ms1                | ceph-hdd                  | MS1                 | Delete            |\n| csi-ceph-hdd-ms1-retain         | ceph-hdd                  | MS1                 | Retain            |\n| csi-ceph-hdd-me1                | ceph-hdd                  | ME1                 | Delete            |\n| csi-ceph-hdd-me1-retain         | ceph-hdd                  | ME1                 | Retain            |\n| csi-ceph-ssd-gz1                | ceph-ssd                  | GZ1                 | Delete            |\n| csi-ceph-ssd-gz1-retain         | ceph-ssd                  | GZ1                 | Retain            |\n| csi-ceph-ssd-ms1                | ceph-ssd                  | MS1                 | Delete            |\n| csi-ceph-ssd-ms1-retain         | ceph-ssd                  | MS1                 | Retain            |\n| csi-ceph-ssd-me1                | ceph-ssd                  | ME1                 | Delete            |\n| csi-ceph-ssd-me1-retain         | ceph-ssd                  | ME1                 | Retain            |\n| csi-high-iops-gz1               | high-iops                 | GZ1                 | Delete            |\n| csi-high-iops-gz1-retain        | high-iops                 | GZ1                 | Retain            |\n| csi-high-iops-ms1               | high-iops                 | MS1                 | Delete            |\n| csi-high-iops-ms1-retain        | high-iops                 | MS1                 | Retain            |\n| csi-high-iops-me1               | high-iops                 | ME1                 | Delete            |\n| csi-high-iops-me1-retain        | high-iops                 | ME1                 | Retain            |\n\n\n\n\nAll storage classes listed:\n\nAllow volume expansion (allowVolumeExpansion: true).\nUse immediate volume provisioning and binding (volumeBindingMode: Immediate).\n\nSee also\n\nContainer service overview.\nContainer service architecture.\nNetwork in a cluster.\n","url":"/en/kubernetes/k8s/concepts/storage","description":"Each Cloud Containers cluster is tightly integrated with storage services and allows you to create persistent volumes based on them. Here's how that integration works.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"8ee899cb-8b9f-45bb-96a9-51125bffa3f0","title":"Available computing resources","data":"Configuration templates\n\nWhen creating Kubernetes clusters, you can select different virtual machine configuration templates that define the computing resources available to the cluster nodes. Some of the templates are available immediately, and some are available upon request to technical support.\n\nRecommendations for choosing CPU and RAM for clusters:\n\nFor small test clusters: 1 vCPU, 2 or 4 GB RAM.\n\nFor most clusters: 2 to 6 vCPUs, 2 to 24 GB RAM.\n\nFor high-performance clusters: 8 to 16 vCPUs, 8 to 64 GB RAM.\n\nThe following templates becomes available upon request to technical support:\n\n*Templates with high-performance CPUs: from 16 vCPUs, from 64 GB RAM. These CPUs are more powerful.\n\nCustomized configurations templates for special needs.\n\n\n\nCost high-performance and customized of configurations is calculated individually.\n\nCalculating the right cluster configuration\n\nFor a Kubernetes cluster to work stably:\n\nEstimate the amount of resources required for the planned workload.\nCreate a cluster with the following parameters:\n\n   Number of vCPUs: 5-10% more than the estimated number for the planned workload.\n   Amount of RAM: 20% more than the estimated amount for your planned load.\n   Storage capacity: 10% more than the planned load.\n   Storage type: Ceph SSD or High-IOPS SSD.\n   Number of master nodes: odd number for high avalability configuration (at least three).\n   Number of worker nodes: 10% more than the calculated number for the planned load.\n\nCalculation of available CPU and RAM computing resources\n\nEach node in a Kubernetes cluster runs a kubelet agent. Kubelets, system processes, and the operating system itself require a certain amount of resources, meaning not all node resources are available to run containers.\n\nIn most cases, CPU resources are reserved as follows:\n\n6% of the first core\n1% of the second core (up to 2 cores)\n0.5% of the third and fourth cores (up to 4 cores)\n0.25% of all cores above four\n\nMemory reservation is usually arranged like this:\n\n25% of the first 4 GB of memory\n20% of the next 4 GB (up to 8 GB)\n10% of the next 8 GB (up to 16 GB)\n6% of the next 112 GB (up to 128 GB)\n2% of any memory above 128 GB\n\nIn addition to reserved resources, system pods are created on the nodes, which run constantly and also consume memory:\n\n| Pod name | CPU Requests | CPU Limits | RAM Requests | RAM Limits |\n| --- | --- | --- | --- | --- |\n| calico-node | 100m | 0 | 512 Mi | 0 |\n| csi-cinder-nodeplugin | 75 m | 300 m | 96 Mi | 384 Mi |\n| haproxy | 20 m | 800 m | 80 Mi | 500 Mi |\n\nThe eviction threshold is typically 100 MB. This is the amount of free memory on a node, below which the kubelet begins to evict pods due to lack of memory.\n\nLarger nodes can use resources more efficiently due to lower percentage reservations compared to smaller nodes. However, scaling the cluster will require time to launch additional nodes. As new nodes are added, the time required to deploy and cache container images will increase, which is critical to maintaining fault tolerance.\n\nThe number of pods launched on a node affects the performance of the cluster. You can run a maximum of 110 pods, but for best node performance, plan for 30–40 pods per node.\n\nWhen choosing the size of nodes and their number in a cluster, choose a distribution that maximizes resource utilization and maintains the required level of fault tolerance and scalability.\n\n\n\nIf you change the VM type for a group of nodes, the reserved resources are not changed, as they are set when the node is created.\n\nAvailable computing resources on the node\n\nAvailable CPU resources on the node:\n\n|Node CPU |\tCPU Available |\n| --- | --- |\n| 1 | 0,94 |\n| 2 | 1,93 |\n| 4 |\t3,92 |\n| 6 | 5,915 |\n| 8 |\t7,91 |\n| 12 | 11,9 |\n| 16 | 15,89 |\n\nДоступные ресурсы RAM на узле:\n\n| Node RAM (GB) | Node RAM Capacity (Ki) |\tRAM Allocatable (Ki)\t| RAM Allocatable (Mi) |\n| --- | --- | --- | --- |\n| 2 | 2 005 132 | 1 641 612 | 1 603 |\n| 4 | 4 000 132 | 2 849 156 | 2 782 |\n| 6 | 6 056 116 | 4 905 140 | 4 790 |\n| 8 |\t8 122 536 | 6 132 699 | 5 989 |\n| 12 |\t12 238 576 |\t10 248 739 | 10 009 |\n| 16 | 16 365 320 | 13 536 622 |\t13 219 |\n| 18 | 18 406 708 |\t15 452 181 | 15 090 |\n| 24 | 24 599 856 | 21 267 842 | 20 769 |\n| 32 | 32 856 944 | 29 021 613 | 28 341 |\n| 36 | 36 935 672 | 32 848 683 | 32 079 |\n| 48 | 49 321 092 | 44 479 128 | 43 437 |\n| 64 | 65 836 152 |\t59 987 556 | 58 582 |\n","url":"/en/kubernetes/k8s/concepts/flavors","description":"When creating Cloud Containers cluster nodes, multiple virtual machine configurations with different compute resources can be used. This section lists available configuration templates and recommendations for selecting a cluster template.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"19cc448f-598e-4092-b0f3-0f9222305133","title":"Scaling cluster nodes","data":"A Cloud Containers cluster can be scaled by changing the number of its master nodes and worker nodes. This allows you to adapt the cluster to the changing needs of workloads (workloads).\n\nTypes of scaling\n\nTwo types of scaling are supported:\n\nVertical scaling: The virtual machine templates that are used by the master and worker nodes of the cluster are being changed. In this way, you can manage the computing resources of the cluster without affecting the number of nodes.\n\n  Scaling of worker nodes is performed within the framework of the worker node group.\n\nHorizontal scaling: The number of worker nodes within a separate node group is changed. In this way, you can manage the computing resources of the cluster without affecting the virtual machine templates that are used by the nodes.\n\nBoth types of scaling are performed manually. Autoscaling also works.\n\n{heading(Autoscaling)[id=autoscaling]}\n\nVertical autoscaling of master nodes works for all clusters, you cannot disable it.\n\nVertical autoscaling agent evaluates the master node load by CPU and RAM, monitoring the following threshold values:\n\nCPU load exceeds 80% for 30 seconds\nCPU load exceeds 60% for 5 minutes\nRAM load exceeds 65% for 60 seconds\n\nIf at least one of the thresholds is exceeded, a request will be sent to the Cloud Containers service to change the master node VM flavor. In this case, the CPU and RAM values ​​will be doubled. For example, the flavor STD2-2-4 will be changed to STD2-4-8.\n\nYou can change the master node VM flavor to a flavor with smaller CPU and RAM only in manual mode.\n\nYou can configure horizontal autoscaling for a group of nodes. The number of worker nodes in the group will be automatically adjusted depending on the needs of the workload. This mechanism allows you to save up to 60% on computing power.\n\nCurrent restrictions\n\nVertical scaling capabilities are limited by current quotas and available virtual machine templates.\nHorizontal scaling capabilities are limited by the current quotas and the limit on the number of worker nodes in a separate node group: from 1 to 100 nodes.\nIt is not possible to perform horizontal scaling manually if automatic scaling is configured. To perform manual scaling, turn off automatic scaling.\n\nEnsuring accessibility with vertical scaling\n\nWhen vertically scaling any cluster nodes, the virtual machines that are used by the nodes are sequentially restarted. This is necessary to apply the new virtual machine template. Therefore, the scaling process affects the availability of both the cluster API and the workloads hosted in the cluster:\n\nIf the cluster is not fault-tolerant and contains a single master node, then the Kubernetes API will be unavailable until scaling is completed.\nIf a node group contains only one worker node, then the workload hosted on it will be unavailable until scaling is completed.\nIf a node group contains multiple worker nodes, the workload hosted on them will not be available until scaling is complete if replication for the load is not configured or configured incorrectly.\n\n  For example, if you place all replicas on one worker node, then when it is restarted, the workload will become unavailable, even if there are other worker nodes in the node group.\n\n  Configure replication so that some of the workload replicas are available when worker nodes are restarted.\n\nIf there are not enough computing resources for the workload after scaling the node group, then this load may not work correctly or become unavailable.\n\n  Make sure that when scaling down computing resources, the total amount of resources in the node group is sufficient.\n","url":"/en/kubernetes/k8s/concepts/scale","description":"How the cluster can adapt to the demands of the workload.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"4c3df781-0296-4176-a968-28af79bf14f3","title":"Cluster version update","data":"For Kubernetes clusters with version 1.17.8 and higher, a Kubernetes version update operation to one of supported versions is available in VK Cloud. You cannot downgrade versions.\n\nClusters of versions 1.16 and lower can only be updated by transferring a backup of data to a new cluster of the required version, for example, using Velero.\n\nDuring a cluster update the following entities also are updated:\n\nInstalled add-ons.\nPart of the cluster components:\n\n  CoreDNS.\n\n    When CoreDNS is updating, the current Corefile is overwritten with the new one with default settings.\n\n    If the cluster uses a modified Corefile, then back it up prior to updating the cluster.\n\n  Gatekeeper.\n  Shell Operator.\n  Kubernetes Dashboard.\n\n  If a component, that is to be updated with the cluster, is deleted, then it will be restored during the next cluster update.\n\nUpdating is done as follows:\n\nMaster nodes are updated. The updating is performed according to the principle of rolling update (step-by-step):\n\n   The first master node is updated: it is removed from the cluster, updated, checked for successful update, and returned back to the cluster.\n   As soon as the updated master node returned to the cluster, the next node is updated. Consequently, one node at a time, all cluster master nodes are updated.\n\nWorker nodes in groups are updated.\n\n   The procedure also follows the rolling update principle (step-by-step), but in one step updates not a single node but the maximum possible number of nodes. This number is calculated based on the node group setting Percentage of unavailable nodes when updating the cluster. The calculations use rounding to integers upwards.\n\n   How to choose the value of the setting:\n\n   Suppose:\n   The cluster has a group of nine worker nodes that run business-critical workloads. These services and applications must be available throughout the upgrade process.\n   The workload requires at least six nodes, i.e., three nodes can be allowed to update at the same time.\n\n   To select the setting value:\n\n   Calculate what percentage of nodes are allowed to update. To do this, divide the maximum number of nodes that are allowed to update by the total number of nodes, and multiply by 100:\n\n      (3 / 9) x 100 = 33,33333...%\n\n   Round down the result. This will be the optimal setting:\n\n      9 x 33% = 2,97\n\n      The resulting fractional result will be rounded to a whole number upwards. Total you can update three nodes out of nine at the same time, which satisfies the requirements.\n\n   Set the setting value:\n\n      either strictly equal to the calculated percentage so that the maximum allowed number of nodes is updated;\n      or less than the calculated percentage so that fewer nodes are updated.\n\n      In the example above:\n      A setting of 33% will update three nodes at a time;\n      Setting it to 20% will update two nodes at a time;\n      A setting of 10% will update one node at a time.\n","url":"/en/kubernetes/k8s/concepts/update","description":"How the Kubernetes Cloud Containers cluster update procedure is arranged.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"2b68d53f-6843-41ab-98be-f9e078644aca","title":"Kubernetes version support policy","data":"New versions of Kubernetes aaS from VK Cloud are usually added 1 month after the official release.\n\nThe following versions of Kubernetes aaS from VK Cloud are currently available:\n\n1.27.6\n1.26.5\n1.25.10\n1.24.9\n\nWhen creating a new cluster, choose the latest available version of Kubernetes. If possible, update the cluster to the latest available version. The update procedure is described in relevant concept section.\n\nIf the cluster uses an outdated version of Kubernetes, then:\n\nits correct operation is not guaranteed;\ntechnical support will not be able to help you resolve issues if they occur.\n\nKubernetes version support\n\nKubernetes versions are supported for 14 months from the release date in Kubernetes aaS from VK Cloud.\n\nThirty days before Kubernetes version support is discontinued, users will receive an email and notification about this in their personal account notification center.\n\n|Version of Kubernetes|Official release date|VK Kubernetes aaS release date|VK Kubernetes aaS end of support date|\n|------|------|------|-------|\n| 1.27.x  | 13 September 2023 | 11.12.2023 | 11.02.2025 |\n| 1.26.x  | 17 May 2023     | 14.08.2023 | 17.10.2024 |\n| 1.25.x  | 17 May 2023     | 14.06.2023 | 14.08.2024 |\n| 1.24.x  | 8 December 2022 | 6.03.2023  | 6.05.2024  |\n| 1.23.x  | 13 April 2022   | 15.08.2022 | 15.10.2023 |\n| 1.22.x  | 19 January 2022 | 18.02.2022 | 18.04.2023 |\n| 1.21.4  | 8 April 2021    | 12.10.2021 | 12.12.2022 |\n| 1.20.4  | 8 December 2020 | 01.03.2021 | 01.05.2022 |\n| 1.19.4  | 26 August 2020  | 23.12.2020 | 23.02.2022 |\n| 1.18.12 | 23 March 2020   | 23.12.2020 | 23.02.2022 |\n| 1.17.8  | 9 December 2019 | 09.08.2020 | 09.12.2021 |\n\nThe version history can be viewed at Kubernetes version history.\n\nService feature matrix for Kubernetes\n\nNew features are added to all versions of Kubernetes, unless a feature and version are incompatible.\n\n| Feature                                  | 1.17.x–1.20.x | 1.21.4–1.23.х | 1.24.x–1.27.х |\n| ---------------------------------------- | ------------- | ------------- | ------ |\n| Node group scaling settings              | +             | +             | +      |\n| Invalidate the key pair                  | +             | +             | -      |\n| Change the size of the Prometheus disk   | +             | +             | +      |\n| Change the type of Master virtual machine| +             | +             | +      |\n| Update cluster version                   | +             | +             | +      |\n| Label & Taints                           | +             | +             | +      |\n| Cluster nodes on AlmaLinux               | -             | +             | +      |\n| Integration with VK Cloud IAM            | -             | -             | +      |\n","url":"/en/kubernetes/k8s/concepts/versions/version-support","description":"The VK Cloud platform provides several versions of Kubernetes that can be used in cloud clusters. Here you will find detailed information about the policy for supporting Kubernetes versions in clusters.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"006ac7ba-9931-46bf-8b98-6951357dbf5f","title":"Kubernetes version history","data":"Kubernetes 1.27.6\n\nChanges in Kubernetes aaS service:\n\nRemoval of storage.k8s.io/v1beta1 from CSIStorageCapacity.\nStopped support for deprecated seccomp annotations.\nRemoval of several feature gates for volume expansion.\n\nRead more about these and other changes in official Kubernetes documentation.\n\nImportant changes in Kubernetes:\n\nSeccompDefault moved to the stable status.\nMutable scheduling directives for Jobs transferred to the status of general availability.\nDownwardAPIHugePages moved to the stable status.\nAdded the ability to access cluster node logs via the Kubernetes API.\n\nVulnerability fixes:\n\nCVE-2023-2728: bypassing enforce mountable secrets policy imposed by the ServiceAccount admission plugin.\n\nRead more about these and other changes in official Kubernetes documentation.\n\nKubernetes 1.26.5\n\nChanges in Kubernetes aaS service:\n\nCalico updated to version 3.26.1.\nHelm updated to version 3.12.2.\nGatekeeper updated to version 3.12.0.\n\nImportant changes in Kubernetes:\n\nOutdated APIs in beta status removed and not supported.\nThe GlusterFS driver is completely removed in release 1.26. If you still have disks with such a driver, perform the migration.\nThe CLI flag pod-eviction-timeout has been moved to the deprecated status and will be removed in v1.27.\nThe CLI flag --master-service-namespace in Kube-apiserver has been moved to deprecated status and will be removed in v1.27.\n\nVulnerability fixes:\n\nCVE-2023-27561, CVE-2023-25809, CVE-2023-28642: changing the runc version from v1.1.4 to v1.1.5, fixed the error of deleting cgroup when using runc version higher than 1.1.6.\n\nRead more about these and other changes in official Kubernetes documentation.\n\nKubernetes 1.25.1\n\nImportant changes in Kubernetes:\n\nPodSecurityPolicy has been removed. After the withdrawal from support in Kubernetes version 1.21, users had the opportunity to switch to Pod Security Admission. If you are using PodSecurityPolicy, then follow the migration instructions.\nThe GlusterFS and Portworx disk plugins have been removed from support. Flocker, Quobyte, and StorageOS removed from Kubernetes.\nvSphere support below 7.0u2 is discontinued.\nStarting from version 1.25 Kubelet will not create iptables chains in NAT tables: KUBE-MARK-DROP, KUBE-MARK-MASQ, KUBE-POSTROUTING.\n\nRead more about these and other changes in official Kubernetes documentation.\n\nKubernetes 1.24.9\n\nChanges in Kubernetes aaS service: Calico updated to version 3.25.0.\n\nImportant changes in Kubernetes:\n\nDockershim deleted from kubelet.\nBeta APIs are off by default.\nKubernetes 1.24 offers beta support for publishing its APIs in the OpenAPI v3 format.\nSecrets for service accounts are not created by default.\n\nVulnerability fixes:\n\nCVE-2022-3294: node address isn't always verified when proxying.\nCVE-2022-3162: unauthorized read of custom resources.\n\nRead more about these and other changes in official Kubernetes documentation.\n\nKubernetes 1.23.13\n\nImportant changes in Kubernetes: Kubernetes is now built on Golang 1.17.13.\n\nVulnerability fixes:\n\nCVE-2022-3172: the aggregated kube-apiserver API server can cause client redirects.\nCVE-2021-25749: the runAsNonRoot logic is skipped for Windows containers.\n\nRead more about these and other changes in official Kubernetes documentation.\n\nKubernetes 1.23.6\n\nChanges in Kubernetes aaS service: Added integration with VK Cloud IAM.\n\nImportant changes in Kubernetes:\n\nThe FlexVolume functionality has been moved to deprecated status.\nFunctionality specific flags for klog has been moved to deprecated status.\nThe IPv4/IPv6 dual-stack networking functionality has been moved to general availability status.\nHorizontalPodAutoscaler v2 functionality has been moved to general availability status.\nGeneric Ephemeral Volume functionality has been moved to general availability status.\nSkip Volume Ownership change functionality has been moved to general availability status.\nThe functionality allowing CSI drivers to opt-in to volume ownership and permission changes has been moved to general availability status.\nThe PodSecurity functionality has been moved to beta status.\nKubelet now supports CRI v1 API.\nLogging structuring functionality has been moved to beta status.\nServerSideFieldValidation function returns warning if Kubernetes object in request contains unknown or duplicated fields.\nExpression language checking for CRD has been moved to alpha status.\nThe OpenAPI v3 functionality has been moved to alpha status.\n\nRead more about these and other changes in official Kubernetes documentation.\n\nKubernetes 1.22.6\n\nChanges in Kubernetes aaS service:\n\nThe AlmaLinux operating system version 9 is used on the cluster nodes.\nBy default, a limit on computational resources consumed (limit ranges) is set for namespaces.\n\nImportant changes in Kubernetes:\n\nThe PodSecurityPolicy component is replaced by alpha version of PodSecurity.\nThe Memory Manager component has been moved to beta status.\nNew API Server Tracing functionality added, now in alpha status.\nNew Generic data populators functionality added for Persistent Volumes, currently alpha.\nA new version of the configuration format for kubeadm has been added: v1beta3.\nKubernetes control plane now always uses CronJobs controller version 2.\nAll Kubernetes control plane components on nodes (including kubelet, kube-proxy and container runtime) can now be run as non-root users. This functionality is in alpha stage.\n\nRead more about these and other changes in official Kubernetes documentation.\n\nKubernetes 1.21.4\n\nChanges in Kubernetes aaS service: The AlmaLinux version 8 operating system is used on the cluster nodes.\n\nImportant changes in Kubernetes:\n\nFunctionality of CronJob moved to stable status.\nAdded support for immutable Secret and ConfigMap.\nAdded support for IPv4/IPv6 Dual-Stack.\nAdded support for Graceful Node Shutdown.\nAdded support for PersistentVolume Health Monitor.\nSimplified support for the Kubernetes build system.\nPodSecurityPolicy functionality moved to deprecated status.\nThe topologyKeys parameter for Service has been moved to deprecated status.\n\nRead more about these and other changes in official Kubernetes documentation.\n\nKubernetes 1.20.4\n\nChanges in Kubernetes aaS service:\n\nThe runtime environment of the cluster has been replaced by CRI-O.\nChanged log storage format.\n\nImportant changes in Kubernetes:\n\nThe Dockershim runtime component has been moved to deprecated status.\nThe Volume Snapshot Operations functionality has been moved to stable status.\nThe API Priority and Fairness (APF) functionality is now enabled by default.\nThe kubectl alpha debug command has been moved to beta status and becomes kubectl debug.\n","url":"/en/kubernetes/k8s/concepts/versions/version-changelog","description":"The VK Cloud platform provides several versions of Kubernetes for use in cloud clusters. Here you will find a detailed history of the Kubernetes versions that are supported or have been supported in the past. For each version, a brief description of changes in the VK Cloud service and in Kubernetes itself are presented.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"ebe13293-8fbf-4467-bafe-1389f856c610","title":"Versions of Kubernetes components","data":"The supported versions of components and add-ons depend on the version of the Cloud Containers cluster.\n\nWhen updating a cluster, part of its components are also updated, as well the installed add-ons. If a component, that is to be updated with the cluster, is deleted, then it will be restored during the next cluster update.\n\nComponents\n\nThese components are present in all Cloud Containers clusters.\n\n[cols=\"3,1,1,1,1\", options=\"header\"]\n|===\n.^|Компонент\n.^|Kubernetes1.21.4–1.22.x\n.^|Kubernetes1.24.x–1.25.x\n.^|Kubernetes1.26.x\n.^|Kubernetes1.27.x\n\n|CoreDNS\n|1.2.4\n|1.10.1\n|1.10.1\n|1.10.1\n\n|Calico\n|3.20.2\n|3.25.0\n|3.26.1\n|3.26.3\n\n|Gatekeeper\n|3.7.0\n|3.11.0\n|3.12.0\n|3.14.0\n\n|Helm\n|3.11.3\n|3.11.3\n|3.12.2\n|3.12.2\n\n|Kubernetes Dashboard\n|2.3.1\n|2.7.0\n|2.7.0\n|2.7.0\n\n|Metrics Server\n|0.5.0\n|0.6.1\n|0.6.3\n|0.6.4\n\n|Shell-operator\n|1.0.9\n|1.0.9\n|1.0.9\n|1.0.9\n|===\n\nAdd-ons\n\nThe necessary add-ons can be installed in the cluster individually.\n\n[cols=\"3,1,1,1,1\", options=\"header\"]\n|===\n.^|Аддон\n.^|Kubernetes1.21.4–1.23.x\n.^|Kubernetes1.23.x–1.24.x\n.^|Kubernetes1.25.x–1.26.x\n.^|Kubernetes1.27.x\n\n|Capsule\n|—\n|—\n|0.4.6\n|0.4.6\n\n|cert-manager\n|—\n|1.12.2\n|1.12.2\n|1.12.2\n\n|Docker Registry\n|2.2.2\n|2.2.2\n|2.2.2\n|2.2.2\n\n|Fluent Bit для Cloud Logging(logaas-integration)\n|—\n|0.34.2\n|0.34.2\n|0.34.2\n\n|Ingress NGINX\n|4.1.4\n|4.1.4\n|4.1.4\n|4.7.1\n\n|Istio\n|—\n|1.16.4\n|1.16.4\n|1.19.3\n\n|Jaeger\n|—\n|0.71.4\n|0.71.4\n|0.71.4\n\n|Kiali\n|—\n|1.59.1\n|1.59.1\n|1.75.0\n\n|Kube Prometheus Stack\n|36.2.0\n|36.2.0\n|36.2.0\n|54.2.2\n|===\n","url":"/en/kubernetes/k8s/concepts/versions/components","description":"Which versions of components and add-ons are supported in VK Cloud clusters.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"5e2f7148-41a2-45fe-830f-b5a7acaa20ae","title":"Access management","data":"Cloud Containers clusters version 1.23 and higher are tightly integrated with the VK Cloud platform:\n\nSingle Sign-On (SSO) technology is used.\n\n  The user authenticates in the Kubernetes cluster with the same credentials as when logging into the VK Cloud personal account.\n\n  SSO functionality cannot be disabled.\n\nThe user's roles in the personal account affect:\n\n  Available operations with clusters in the personal account.\n  Available actions in the cluster.\n\n    A user with a specific personal account role is assigned an appropriate Kubernetes role. The Kubernetes role defines which cluster objects are available to the user and what actions are allowed to be performed on these objects.\n\nThe Kubernetes superadministrator and administrator manage cluster access by assigning roles to users in a personal account.\n\n  There is no need to configure user rights separately for personal accounts and for Kubernetes clusters. For example, disabling a user account or revoking a role in a personal account results in revoking the access rights to Kubernetes clusters.\n\n\n\nTo get the same capabilities for older clusters, upgrade to version 1.23 or higher.\n\nHow does the authentication process work\n\nWhen using kubectl, the keystone-auth utility is responsible for authentication.\n\nThe cluster configuration file is used for the operation of kubectl (kubeconfig). This file contains all the user's details, except for the password (it is not specified for security reasons). Therefore, when using kubectl, the keystone-auth utility will require you to enter the user's password interactively in order to authenticate:\n\nPlease enter password:\n\nAfter successful authentication, a token with a short lifetime is issued, which gives temporary access to the cluster. When the token expires, keystone-auth will require you to enter the password again in order to update the token. This will happen both when working with kubectl and with other tools that work with such tokens — for example, kauthproxy uses them for authentication in the web interfaces of cluster components and add-ons.\n\nThis authentication process is inconvenient when working with automated tools that need access to the cluster. To provide access to the cluster for such tools, create a kubeconfig file for the service account. This kubeconfig contains the details of the service account and the corresponding token with an infinite lifetime, which allows you to authenticate without entering a password.\n\nRelationship between the roles of personal account and Kubernetes\n\n\n\nKubernetes Auditor\nKubernetes Operator\nProject OwnerProject AdministratorSuper AdministratorKubernetes Administrator\n\n\n\nKubernetes role: view.\n\nThe role provides read access to most objects in the namespace.\n\nThe role does not provide:\n\nAbility to view or change roles and role bindings.\nAccess to secrets.\n\n  A user with access to secrets can access the credentials of any service account in the namespace. This will allow access to the API on behalf of any service account in the namespace. For the read-only role, this will be considered as privilege escalation.\n\n\n\n\nKubernetes role: edit.\n\nRole provides:\n\nAll privileges available to the view role.\nRead and write access to most objects in the namespace.\nAccess to secrets, which allows you to run pods on behalf of any service account in the namespace. The role can be used, for example, to access the API.\n\nThe role does not provide:\n\nAbility to view or change roles and role bindings.\nWrite access to Cloud Containers cluster endpoints version 1.22 and higher.\n\n\n\n\nKubernetes role: admin.\n\nIt is recommended to assign the admin role within the namespace using role bindings.\n\nRole provides:\n\nAll privileges available to the edit role.\nRead and write access to the most objects in the namespace, including the ability to create other roles and role binding.\n\nThe role does not provide:\n\nWrite access to the resource quota or to the namespace itself.\nWrite access to Cloud Containers cluster endpoints version 1.22 and higher.\n\n\n\n\nTo view a list of available resources for a role, connect to the cluster and run the command:\n\nkubectl describe clusterrole\n","url":"/en/kubernetes/k8s/concepts/access-management","description":"How to manage access to the Kubernetes Cloud Containers cluster, using the user roles in the VK Cloud account.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"ed3fa2bc-5a3e-4760-8492-2c33d13532b3","title":"Connecting to cluster using kubectl","data":"The kubectl utility allows you to perform the full range of Kubernetes cluster management operations from the command line. See official Kubernetes documentation for details.\n\nThe way to connect to the cluster depends on its IP address:\n\nIf an external IP address is assigned to the cluster, then you can connect to it from any host with Internet access.\nIf the cluster is assigned only an internal IP address, then you can connect to it only from a host in VK Cloud — a virtual machine that is located in the same subnet as the cluster.\n\nPreparatory steps\n\nOn the host from which you plan to connect to the cluster, install kubectl if the utility is not already installed.\n\n\n   Make sure that the minor version of kubectl differs by no more than one from the minor version of the cluster you are connecting to. For example, kubectl version 1.23 works correctly with clusters of versions 1.22, 1.23 and 1.24.\n\n   See official Kubernetes documentation for more details.\n\n   Linux (curl)\n   Linux (apt)\n   Linux (yum)\n   macOS (curl)\n   macOS (Homebrew)\n   Windows\n\n\n   Download the correct version of kubectl.\n\n      An example command to download the kubectl utility which is compatible with cluster version 1.23.6:\n\n            curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.23.6/bin/linux/amd64/kubectl\n\n   Make the kubectl binary file executable:\n\n            sudo chmod +x ./kubectl\n\n   Place this file in a directory which is contained in the PATH environment variable, e.g. in /usr/local/bin:\n\n            sudo mv ./kubectl /usr/local/bin/kubectl\n\n   Check the kubectl version by running the command:\n\n            kubectl version --short\n\n\n   Add the Kubernetes repository:\n\n            sudo apt-get update && sudo apt-get install -y apt-transport-https\n      curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n      echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list\n      sudo apt-get update\n\n   Install the correct version of kubectl.\n\n      An example command to install the kubectl utility which is compatible with cluster version 1.23.6:\n\n            sudo apt-get install -y kubectl=1.23.6-00\n\n\n      You can get a list of all available versions by using the command sudo apt-cache policy kubectl.\n\n\n   Check the kubectl version by running the command:\n\n            kubectl version --short\n\n\n   Add the Kubernetes repository:\n\n            cat  /etc/yum.repos.d/kubernetes.repo\n      [kubernetes]\n      name=Kubernetes\n      baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64\n      enabled=1\n      gpgcheck=1\n      repo_gpgcheck=1\n      gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\n      EOF\n\n   Install the correct version of kubectl.\n\n      An example command to install the kubectl utility which is compatible with cluster version 1.23.6:\n\n            yum install -y kubectl-1.23.6-0\n\n\n      You can get a list of all available versions by using the command yum --showduplicates list kubectl.\n\n\n   Check the kubectl version by running the command:\n\n            kubectl version --short\n\n\n   Download the correct version of kubectl.\n\n      An example command to download the kubectl utility which is compatible with cluster version 1.23.6:\n\n            curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.23.6/bin/darwin/amd64/kubectl\n\n   Make the kubectl binary file executable:\n\n            sudo chmod +x ./kubectl\n\n   Place this file in a directory which is contained in the PATH environment variable, e.g. in /usr/local/bin:\n\n            sudo mv ./kubectl /usr/local/bin/kubectl\n\n   Check the kubectl version by running the command:\n\n            kubectl version --short\n\n\n   Run one of the installation commands:\n\n            brew install kubectl\n\n      Or:\n\n            brew install kubernetes-cli\n\n   Check the kubectl version by running the command:\n\n            kubectl version --short\n\n\n   Download the correct version of kubectl.\n\n      An example command to download the kubectl utility which is compatible with cluster version 1.23.6:\n\n            curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.23.6/bin/windows/amd64/kubectl.exe\n\n   In the PATH environment variable, specify the directory where the kubectl.exe file was downloaded:\n\n      Go to Start -> This Computer -> Properties -> Advanced System Settings -> Environment Variables -> System Variables.\n      Change the value of the Path variable by appending the path to the directory with the file kubectl.exe to it.\n\n\n      Note\n\n      Docker Desktop for Windows adds its own version of kubectl to the PATH environment variable. If Docker Desktop is installed:\n\n      either specify the path to the downloaded file before the entry added by the Docker Desktop installer;\n      or remove the kubectl shipped with Docker Desktop.\n\n\n   Check the kubectl version by running the command:\n\n            kubectl version --short\n\n\nIf you plan to connect to a Kubernetes cluster version 1.23 or higher, prepare everything you need to connect using Single Sign-On (SSO).\n\n   On the host from which you plan to connect to the cluster, install keystone-auth if the utility is not already installed:\n\n      Windows (PowerShell)\n      Linux (bash)/macOS (zsh)\n\n\n      Run the command:\n\n                  iex (New-Object System.Net.WebClient).DownloadString( `\n           'https://hub.mcs.mail.ru/repository/client-keystone-auth/latest/windows/client-install.ps1' `\n         )\n\n         This will start installing the keystone-auth utility.\n\n      Confirm adding the directory with the utility to the PATH environment variable by entering Y in response to the prompt:\n\n                  Add client-keystone-auth installation dir to your PATH? [Y/n]\n\n\n      Run the command:\n\n                  curl -sSL \\\n           https://hub.mcs.mail.ru/repository/client-keystone-auth/latest/linux/client-install.sh \\\n         | bash\n\n         This will start installing the keystone-auth utility.\n\n      If you are using a shell other than bash or zsh, add the path to the utility to the PATH environment variable manually.\n\n      Restart the shell session or run the source command.\n\n         A tip with the necessary commands will be displayed when the installation is complete.\n\n\n   Determine on behalf of which user you want to connect to the cluster. Then:\n\n      Assign necessary role to this user.\n      Activate API access for this user.\n\nConnecting to the cluster\n\nOn the host from which you plan to connect to the cluster:\n\nDownload the configuration file of the cluster you want to connect to to the local computer:\n\n   Go to personal account VK Cloud under the account of the user who will connect to the cluster.\n   Select the project where the necessary cluster is located.\n   Go to Containers → Kubernetes Clusters.\n   Click   for the required cluster and select Get Kubeconfig to access the cluster.\n\n   Such a file is automatically created for each new cluster and is named in the format _kubeconfig.yaml.\n\n\n    Further, it is assumed that the downloaded file has the name kubernetes-cluster-1234_kubeconfig.yaml and is located in the directory C:\\Users\\user\\.kube (for Windows) or /home/user/.kube (for Linux and macOS). Correct the commands below if necessary.\n\n\nThe configuration file contains sensitive information that should not be accessible to other users. Therefore, restrict the access rights to this file:\n\n   Windows (PowerShell)\n   Linux (bash)/macOS (zsh)\n\n\n      icacls.exe 'C:\\Users\\user\\.kube\\kubernetes-cluster-1234_kubeconfig.yaml' `\n     /c /t `\n     /Inheritance:d `\n     /Remove:g BUILTIN\\Administrators Everyone Users `\n     /Grant:r ${env:UserName}:RW\n\n\n      sudo chmod 0600 /home/user/.kube/kubernetes-cluster-1234_kubeconfig.yaml\n\n\nPlace the path to the configuration file in the $KUBECONFIG environment variable:\n\n   Windows (PowerShell)\n   Linux (bash)/macOS (zsh)\n\n\n      $env:KUBECONFIG = 'C:\\Users\\user\\.kube\\kubernetes-cluster-1234_kubeconfig.yaml'\n\n\n      export KUBECONFIG=/home/user/.kube/kubernetes-cluster-1234_kubeconfig.yaml\n\n\nChecking the connection to the cluster\n\n\n\nVersion of Kubernetes 1.23 and higher\nVersion of Kubernetes 1.22 and lower\n\n\n\nOn the host:\n\nRun the command:\n\n      kubectl cluster-info\n\nEnter the user's password for the VK Cloud account.\n\n   This is necessary for authentication when connecting to the cluster.\n\n\n\n\nRun the command on the host:\n\nkubectl cluster-info\n\n\n\n\nIf the cluster is in the nomal operation state and kubectl is configured to work with it, similar information will be dispalyed:\n\nKubernetes control plane is running at...\nCoreDNS is running at...\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n","url":"/en/kubernetes/k8s/connect/kubectl","description":"How to connect to a Kubernetes cluster using kubectl.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"a7d4320e-5761-4af2-9551-6d41244bb0db","title":"Connecting to cluster using Kubernetes Dashboard","data":"Kubernetes Dashboard is a universal web interface for Kubernetes clusters available into all VK Cloud clusters. It allows users to manage both the cluster itself and the applications running in it. Read more in official Kubernetes documentation.\n\nThe way to connect to Kubernetes Dashboard depends on the IP address of the cluster:\n\nIf an external IP address is assigned to the cluster, then you can connect from any host with Internet access.\nIf the cluster is assigned only an internal IP address, then you can connect only from a host in VK Cloud — a virtual machine that is located in the same subnet as the cluster.\n\nTo connect to Kubernetes Dashboard, a browser must be installed on the host.\n\nPreparatory steps\n\nMake sure that you can connect to the cluster using kubectl.\n\nOn the host from which you plan to connect to the cluster, install kauthproxy if the utility is not already installed:\n\n   Download the archive of the correct version from release page:\n\n      for Linux: kauthproxy_linux_....zip;\n      for macOS: kauthproxy_darwin....zip;\n      For Windows: kauthproxy_windows....zip.\n\n   Unpack the archive.\n\n   Place the executable file in the directory contained in the PATH environment variable, for example:\n\n      in /usr/local/bin for Linux/macOS;\n      In C:\\ for Windows.\n\nConnect to cluster\n\n\n\nVersion of Kubernetes 1.23 and higher\nVersion of Kubernetes 1.22 and lower\n\n\n\nOn the host in a separate terminal session, run the command:\n\n      kauthproxy -n kubernetes-dashboard https://kubernetes-dashboard.svc\n\n\n   Do not close this terminal session, or you will lose access to the Kubernetes Dashboard web interface.\n\n\nEnter the password for your personal VK Cloud account, if it is not present in the kubeconfig configuration file.\n\n   kauthproxy will periodically ask for this password again.\n\n   The browser will open and you will be directed to the Kubernetes Dashboard web interface.\n\n\n\n\nGet the secret:\n\n   Go to personal account VK Cloud under the account of the user who will connect to the cluster.\n   Select the project where the required cluster is located.\n   Go to Containers → Kubernetes Clusters.\n   Click   for the required cluster and select Get Secret to enter Kubernetes Dashboard.\n   Click the Copy button.\n\n      The secret will be copied to the clipboard.\n\nOn the host in a separate terminal session, run the command:\n\n          kauthproxy -n kube-system https://kubernetes-dashboard.svc\n\n\n   Do not close this terminal session, otherwise you will lose access to the Kubernetes Dashboard web interface.\n\n\n   The browser will open and you will be directed to the Kubernetes Dashboard authorization page.\n\nSelect the Token option and paste the previously copied secret.\n\nClick the Sign In button.\n\n   The Kubernetes Dashboard web interface will open.\n\n\n\n","url":"/en/kubernetes/k8s/connect/k8s-dashboard","description":"How to connect to a Kubernetes cluster using Kubernetes Dashboard.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"816df8dd-1bcb-424d-bf87-f451d47d4a1d","title":"Connecting to Docker Registry","data":"You can connect to the Docker registry if the appropriate add-on is installed in the cluster.\n\nWhen installing an add-on, a standard load balancer with a floating IP address is created for it. Therefore, you can connect to the Docker registry from any host that has Internet access.\n\nPreparatory steps\n\nMake sure that the Docker registry add-on (docker-registry) is installed in the cluster.\nMake sure that you can connect to the cluster using kubectl.\nGet the data to access the Docker registry.\n\nConnecting to the Docker Registry\n\nOn the host from which you plan to connect to the registry:\n\nInstall Docker Engine if not already installed. There is a choice of either Docker Desktop or a server-side version of Docker Engine without a GUI.\n\n   Docker Engine must be installed on the host from which the registry will be used. Perform the next steps on that host.\n\nAdd the Docker registry to the list of trusted registries:\n\n   Add the insecure-registries parameter with the address of the Docker registry endpoint to the Docker daemon.json configuration file.\n\n      The address is specified in the format :.\n\n            {\n        ...\n\n        { \"insecure-registries\": [\n          \"https://192.0.2.2:5000\"\n        ],\n\n        ...\n      }\n\n      The location of this file for different Docker Engine installations is given in official Docker documentation.\n\n   Restart the Docker Engine.\n\n      Linux\n      Windows\n      macOS\n\n\n      For the server version of the Docker Engine, run one of the commands to restart:\n\n                sudo systemd restart docker\n\n                sudo service docker restart\n\n      For Docker Desktop, use GUI.\n\n\n      Use GUI Docker Desktop.\n\n\n      Use GUI Docker Desktop.\n\n\nSign in into the registry:\n\n      docker login  --username\n\n   Enter the password for the Docker registry.\n\nNow you can do any operations with the registry, for example, to push Docker images there.\n\nRead more about registry operations in official Docker documentation.\n\nUsing Docker registry in Kubernetes cluster\n\nIn order to deploy workloads in a cluster using images from the Docker registry:\n\nCreate the k8s-registry-creds secret which contains the data to access the registry:\n\n   If the --namespace parameter is not provided, then the secret will be created in the default namespace (default).\n\n\n   The secret must reside in the same namespace the workload is planned to be deployed in.\n\n   Linux (bash) / macOS (zsh)\n   Windows (PowerShell)\n\n\n      kubectl create secret docker-registry k8s-registry-creds \\\n     --docker-server=:5000 \\\n     --docker-username= \\\n     --docker-password= \\\n     --namespace=\n\n\n      kubectl create secret docker-registry k8s-registry-creds `\n     --docker-server=:5000 `\n     --docker-username= `\n     --docker-password= `\n     --namespace=\n\n\nSpecify in the workload manifest:\n\n   Name of the created secret in the ìmagePullSecrets parameter.\n\n   Path to the image from the registry in the containers.image parameter.\n\n     The path should be specified in the :5000//: format.\n\n   Examples of manifests:\n\n   Pod\n   Deployment\n   ReplicaSet\n   StatefulSet\n   DaemonSet\n   Job\n   CronJob\n\n\n      apiVersion: v1\n   kind: Pod\n   metadata:\n     name: my-app\n   spec:\n     imagePullSecrets:\n     name: k8s-registry-creds\n     containers:\n     name: my-app\n       image: :5000//:\n\n\n      apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: my-app\n   spec:\n     template:\n       spec:\n         imagePullSecrets:\n         name: k8s-registry-creds\n         containers:\n         name: my-app\n           image: :5000//:\n\n\n      apiVersion: apps/v1\n   kind: ReplicaSet\n   metadata:\n     name: my-app\n   spec:\n     template:\n       spec:\n         imagePullSecrets:\n         name: k8s-registry-creds\n         containers:\n         name: my-app\n           image: :5000//:\n\n\n      apiVersion: apps/v1\n   kind: StatefulSet\n   metadata:\n     name: my-app\n   spec:\n     template:\n       spec:\n         imagePullSecrets:\n         name: k8s-registry-creds\n         containers:\n         name: my-app\n           image: :5000//:\n\n\n      apiVersion: apps/v1\n   kind: DaemonSet\n   metadata:\n     name: my-app\n   spec:\n     template:\n       spec:\n         imagePullSecrets:\n         name: k8s-registry-creds\n         containers:\n         name: my-app\n           image: :5000//:\n\n\n      apiVersion: batch/v1\n   kind: Job\n   metadata:\n     name: my-app\n   spec:\n     template:\n       spec:\n         imagePullSecrets:\n         name: k8s-registry-creds\n         containers:\n         name: my-app\n           image: :5000//:\n\n\n      apiVersion: batch/v1\n   kind: CronJob\n   metadata:\n     name: my-app\n   spec:\n     jobTemplate:\n       spec:\n         template:\n           spec:\n             imagePullSecrets:\n             name: k8s-registry-creds\n             containers:\n             name: my-app\n               image: :5000//:\n\n","url":"/en/kubernetes/k8s/connect/docker-registry","description":"If the Kubernetes cluster has the Docker registry enabled, you can connect to the registry using the procedure described here.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"1db777eb-c3bf-4d9f-8ce3-2e94811b74a0","title":"Connecting to add-on web interfaces","data":"Some add-ons that can be installed in the cluster have a web interface. The method of connecting to the web interface depends on the IP address of the cluster:\n\nIf an external IP address is assigned to the cluster, then you can connect from any host with Internet access.\nIf the cluster is assigned only an internal IP address, then you can connect only from a host in VK Cloud - a virtual machine that is located on the same subnet as the cluster.\n\nA browser must be installed on the host to connect.\n\nPreparatory steps\n\nOn the host from which you plan to connect to the add-on web interface, make sure that you can connect to the cluster using kubectl.\n\nInstall kauthproxy on the same host if the utility is not already installed:\n\n   Download the archive of the required version from the releases page:\n\n      for Linux: kauthproxy_linux_....zip;\n      for macOS: kauthproxy_darwin_....zip;\n      for Windows: kauthproxy_windows_....zip.\n\n   Unpack the archive.\n\n   Place the executable file in the directory that is contained in the environment variable PATH, for example:\n\n      at /usr/local/bin for Linux/macOS;\n      at C:\\ for Windows.\n\nConnecting to the add-on web interface\n\n\n\nKiali\nJaeger\n\n\n\nKiali is a web interface for working with Istio. To connect to it:\n\nMake sure that kiali add-on is installed to the cluster.\nOn the host, in a separate terminal session, run the command:\n\n      kauthproxy -n istio-system https://kiali.svc\n\n   If you selected a service name other than kiali or a namespace other than istio-system when adding the add-on, adjust the command.\n\n   A browser will open and you will be directed to the Kiali web interface. Learn more about working with Kiali in the official documentation.\n\n\n   Do not close this terminal session, otherwise access to the Kiali web interface will be lost.\n\n\nMake sure that jaeger add-on is installed to the cluster.\nOn the host, in a separate terminal session, run the command:\n\n      kauthproxy -n jaeger http://jaeger-query.svc\n\n   If you selected a service name other than jaeger or a namespace other than jaeger when adding the add-on, adjust the command.\n\n   A browser will open and you will be directed to the Query UI web interface. The web interface is available only via HTTP.\n\n\n   Do not close this terminal session, otherwise access to the web interface will be lost.\n\n","url":"/en/kubernetes/k8s/connect/addons-ui","description":"If an add-on is installed in the Kubernetes cluster that has a web interface, then you can connect to it using the procedure described here.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"43cacb8f-b9de-4108-9011-85e2bc24335b","title":"Creating a cluster from personal account","data":"The following describes how to create a cluster from your personal account. It is also possible to create a cluster with Terraform.\n\n\n\nWhen installing the cluster, a service load balancer will be created.\n\nUsage of this load balancer is charged.\n\nBefore you create a cluster\n\nFamiliarize yourself with the available resources and quotas for the region where the cluster is to be created. Different quotas may be configured for different regions.\n\n   If you want to increase the quotas, write to technical support.\n\nRun the cluster creation wizard:\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   If there are no clusters in the selected project, click the Create cluster button.\n\n      Otherwise, click the Add button.\n\n   The Create a new Kubernetes cluster wizard will open.\n\n1. Set the cluster configuration\n\nSelect the Kubernetes version and cluster configuration. See Kubernetes version support policy for a list of the available Kubernetes versions.\n\n   You will not be able to select another version when going through the cluster creation wizard.\n\n   The cluster configuration affects the default settings, which you will be able to change in the following steps. The following configurations are available:\n\n   Dev environment\n   Staging environment\n   Production\n   Other\n\n\n   A configuration of one master node and one worker node:\n\n   Minimum number of master nodes: one;\n   Minimum disk size for the master nodes: 20 GB.\n\n   This cluster is suitable for use in application development.\n\n\n   A configuration of one master node and one worker node:\n\n   minimum number of master nodes: one;\n   minimum disk size for the master nodes: 40 GB.\n\n   This cluster is suitable for use when testing or piloting applications.\n\n\n   A configuration of multiple master nodes and one worker node:\n\n   Minimum number of master nodes: three;\n   Minimum disk size for the master nodes: 40 GB.\n\n   Such a cluster is suitable for any task, including the routine operation of applications in a production environment.\n\n\n   Configuration of your choice:\n\n   minimum number of master nodes: one;\n   Minimal size of the disk for the master nodes: 20 GB.\n\n   Select this option to manually configure all settings for the cluster you are creating.\n\n\n   For more information about cluster topologies, see Architecture.\n\nClick the Next step button.\n\n2. Configure the cluster\n\nSet:\n\n   Cluster name: must begin with a letter. Can only consist of lowercase latin letters, numbers and hyphens - as a separator.\n\n   Master node settings:\n\n     Category of virtual machine: select the flavor of the VM. For more information, see Cloud Servers service overview.\n\n     Virtual machine type - Master: virtual machine template for master nodes.\n\n       Templates with high-performance CPUs are available upon request to support. See Available computing resources for details.\n\n     Availability zone: availability zone for nodes.\n     Master disk type: storage type which will be used by nodes.\n\n\n       The disk type you select affects the performance of the cluster. It is recommended to use an SSD or High-IOPS disk type for clusters that operate in a production environment or under heavy load.\n\n\n     Number of Master nodes: must be an odd number. One node does not provide cluster high availability at the master node level, three nodes or more do.\n\n       For more information about cluster topologies, see Architecture.\n\n     Disk size on Master node: the larger the disk size, the better its performance in some disk operations.\n\n   Network settings:\n\n     Network: select the network and subnet where the cluster's master and worker nodes will be located. If the required network and subnet are not on the list, create them.\n\n     Use load balancer network: enable this option to use a separate subnet on the selected network for load balancers created by the cluster. If the required subnet is not on the list, create it.\n\n       By default, the option is disabled and the load balancers use the same subnet as the cluster nodes.\n\n     Use pod subnet: enable this option to specify the subnet that the pods will use to communicate with each other.\n\n       By default, the pods use the 10.100.0.0./16 subnet for communication. If such a subnet already exists in the cluster network, specify a different subnet that is not part of the cluster network, to be used by the pods. This is necessary so that there is no address space overlap.\n\n     Assign external IP: enable this option so that external IP addresses are assigned to the cluster API endpoint and the pre-installed Ingress controller (if selected in the previous step). Otherwise IP addresses will be assigned from the cluster subnet.\n\n       By default, the option is enabled, which allows access to the cluster and the Ingress controller from the Internet.\n\n   Miscellaneous settings:\n\n     Trusted Docker Registry: add Docker Registry addresses to the trusted list to disable HTTPS connection check when connecting to them.\n\n       This comes in handy if the Docker registry uses a self-signed SSL or TLS certificate that cannot be validated by the cluster.\n\n       See Docker documentation for more information on disabling the validation (see the description of the insecure-registries setting).\n\n     Virtual machine key: SSH key, with which you can connect to the cluster hosts. The recommended value is No key.\n\n\n       Limited scope\n\n       This setting is only available for clusters with Kubernetes version 1.22.9 and lower.\n\n\n     Enable monitoring: enable this option to install a metrics collection agent in the cluster.\n\n       By default, the option is enabled and allows you to monitor the state of the cluster using the Cloud Monitoring service.\n\nClick the Next step button.\n\n3. Configure the node groups\n\nSet the settings for the worker node group.\n\nIf necessary, add one or more worker-node groups by clicking Add node group, and configure them.\n\nIf more than one worker node group is configured at this step, you can delete a node group by clicking Delete node group under the appropriate group.\n\nClick the Create cluster button.\n\n   This will start creating the Kubernetes cluster. This process may take a long time, depending on the size of the cluster.\n\nWhat's next?\n\nSet up the environment on the host from which you plan to connect to the cluster.\nFamiliarize yourself with the usage scenarios of the cluster.\nFamiliarize yourself with the concepts of the container service.\n","url":"/en/kubernetes/k8s/service-management/create-cluster/create-webui","description":"How to create a cluster from VK Cloud personal account.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"8f9de6cc-852b-4c03-928d-53f48a3f807a","title":"Creating a cluster with Terraform","data":"Description of how to create a cluster using Terraform is provided below. It is also possible to create a cluster via VK Cloud personal account.\n\nReady-to-use examples of configuration files to create different clusters are listed in the Terraform section.\n\n\n\nWhen installing the cluster, a service load balancer will be created. When you select add-on NGINX Ingress Controller, a standard load balancer will be created for it.\n\nUsage of this load balancer is charged.\n\nBefore creating cluster\n\nFamiliarize yourself with the available resources and quotas for the region in which you plan to create the cluster. Different quotas may be configured for different regions.\n\n   If you want to increase the quotas, write to technical support.\n\nFamiliarize yourself with Terraform features in the container service.\n\nInstall Terraform and configure the environment if it is not already done.\n\nInstall the OpenStack CLI and authorize, if not already done.\n\nCreate a Terraform configuration file.\n\n\n   The following steps list only the basic Terraform resource parameters you need to specify in this file. For a complete list of parameters, see Terraform provider documentation for Kubernetes cluster.\n\n\n1. Prepare the necessary data sources\n\nDetermine what type of virtual machine will be used for the cluster master nodes:\n\n   Run the command:\n\n            openstack flavor list\n\n      The available virtual machine types will be displayed.\n\n   Select the required virtual machine type and write its name from the Name column.\n\nDetermine the version of Kubernetes you want to create the cluster with:\n\n   Add the following lines to the configuration file:\n\n            data \"vkcs_kubernetes_clustertemplates\" \"k8s-template-list\" {}\n\n      output \"k8s-version-list\" {\n          value = data.vkcs_kubernetes_clustertemplates.k8s-template-list.cluster_templates.*.version\n      }\n\n   Run the command:\n\n            terraform refresh\n\n   Run the command:\n\n            terraform output k8s-version-list\n\n      A list of available Kubernetes versions will be displayed.\n\n   Select the necessary Kubernetes version and write down its version number.\n\nAdd data sources to the configuration file:\n\n   Virtual machine template for master nodes. Example:\n\n            data \"vkcs_compute_flavor\" \"k8s-master-flavor\" {\n          name = \"STD3-2-6\"\n      }\n\n      As the name of the template, specify the name obtained earlier.\n\n   Cluster template. Example:\n\n            data \"vkcs_kubernetes_clustertemplate\" \"k8s-template\" {\n          version = \"\"\n      }\n\n      As a version, specify the version number obtained earlier.\n\n2. Describe the cluster configuration\n\nAdd the cluster resource to the configuration file:\n\nresource \"vkcs_kubernetes_cluster\" \"k8s-cluster\" {\n  name                = \"k8s-cluster\"\n  cluster_template_id = data.vkcs_kubernetes_clustertemplate.k8s-template.id\n  master_flavor       = data.vkcs_compute_flavor.k8s-master-flavor.id\n  master_count        =\n  network_id          = \"\"\n  subnet_id           = \"\"\n  availability_zone   = \"\"\n  floating_ip_enabled =\n}\n\nSome clarification:\n\nThe number of master nodes master_count must be an odd number (1, 3, 5, and so on). See Architecture for details.\n\nThe network_id and subnet_id identifiers can be specified in different ways:\n\n  Setting IDs explicitly\n  Using data sources\n  Using created resources\n\n\n  If the required network and subnet already exist and you know their identifiers, specify the identifiers explicitly.\n\n  Example:\n\n    resource \"vkcs_kubernetes_cluster\" \"k8s-cluster\" {\n    name                = \"k8s-cluster\"\n    ...\n    network_id          = \"sample-id-4212-a090-9f30519275e5\"\n    subnet_id           = \"sample-id-4bd6-bda4-f66dc7fbaa4f\"\n    ...\n  }\n\n  If the required networks and subnets already exist, but you do not know their identifiers, specify the appropriate data sources and get the identifiers.\n\n  Example:\n\n    ...\n  data \"vkcs_networking_network\" \"k8s-network\" {\n      name = \"default\"\n  }\n  data \"vkcs_networking_subnet\" \"k8s-subnet\" {\n      name = \"default_subnet\"\n      network_id = data.vkcs_networking_network.k8s-network.id\n  }\n  ...\n  resource \"vkcs_kubernetes_cluster\" \"k8s-cluster\" {\n    name                = \"k8s-cluster\"\n    ...\n    network_id          = data.vkcs_networking_network.k8s-network.id\n    subnet_id           = data.vkcs_networking_subnet.k8s-subnet.id\n    ...\n  }\n\n  If the required network and subnet do not already exist, create them and get the IDs:\n\n    ...\n  resource \"vkcs_networking_network\" \"k8s-network\" {\n  name           = \"default\"\n  admin_state_up = \"true\"\n  }\n  resource \"vkcs_networking_subnet\" \"k8s-subnet\" {\n      name       = \"default_subnet\"\n      network_id = vkcs_networking_network.k8s-network.id\n      cidr       = \"\"\n  }\n  ...\n  resource \"vkcs_kubernetes_cluster\" \"k8s-cluster\" {\n    name                = \"k8s-cluster\"\n    ...\n    network_id          = vkcs_networking_network.k8s-network.id\n    subnet_id           = vkcs_networking_subnet.k8s-subnet.id\n    ...\n  }\n\n\nFor the Moscow region, specify one of three availability zones in the availability_zone parameter: ME1, MS1 or GZ1.\n\nIt is recommended to assign a public IP address to the cluster when creating it, so that you can access the cluster from the Internet (floating_ip_enabled = true). To assing such an IP address, it is necessary for the subnet with the subnet_id identifier to be connected to the router which has access to the external network.\n\nIf some of the add-ons are not needed, delete the corresponding lines from the labels block. See Addons for details.\nTo install add-ons in the cluster via Terraform, get the list of availible add-ons and install that you need.\n\n3. Describe the configuration of one or more worker node groups\n\n\n\nThis is an optional step.\nYou can use Terraform to create a cluster of master nodes only, and add worker node groups later.\n\n\n\nThis operation is described in detail in Worker node group management.\n\n4. Run the cluster creation procedure\n\nCheck the Terraform configuration file for correctness:\n\n      terraform validate\n\nFamiliarize yourself with the planned changes:\n\n      terraform plan\n\nApply the planned changes:\n\n      terraform apply\n\n   This will start creating the Kubernetes cluster. This process can take a long time, depending on the size of the cluster.\n\nWhat's next?\n\nSet up the environment on the host from which you plan to connect to the cluster.\nFamiliarize yourself with the usage scenarios of the cluster.\nFamiliarize yourself with the concepts of the container service.\n","url":"/en/kubernetes/k8s/service-management/create-cluster/create-terraform","description":"How to create a cluster using Terraform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"1d9dae9f-b231-4be6-818d-4b4422548001","title":"Managing cluster","data":"\n\nBefore performing any operation on a cluster from Terraform, read the information in Using Terraform.\n\nStart or stop the cluster\n\nStart cluster\n\n\n\nPersonal account\n\n\n\nThis is a group operation: if necessary, you can start several stopped clusters at once by selecting them with checkboxes.\n\nTo start a cluster:\n\nGo to VK Cloud personal account.\nSelect the project where the necessary cluster is located.\nGo to Containers → Kubernetes Clusters.\nUse the checkbox to select the necessary cluster.\nClick the Launch button.\nConfirm the operation.\n\nStop cluster\n\n\n\nPersonal account\n\n\n\nThis is a group operation: if necessary, you can stop several started clusters at once by selecting them with checkboxes.\n\nTo stop a cluster:\n\nGo to VK Cloud personal account.\nSelect the project where the necessary cluster is located.\nGo to Containers → Kubernetes Clusters.\nUse the checkbox to select the necessary cluster.\nClick the Stop button.\nConfirm the operation.\n\nGet cluster information\n\nDifferent information is available for running and stopped clusters. Terraform allows you to get only part of the information about the cluster.\n\n\n\nPersonal account\nTerraform\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary cluster is located.\nGo to Containers → Kubernetes Clusters.\nClick on the name of the necessary cluster. A page with information will open.\n\n   General information about the cluster and the event log are available on the corresponding tab at the beginning of the page. Information about connecting to and managing the cluster is available on the corresponding tab at the end of the page.\n\n\n\n\nRun the command:\n\n      terraform state show vkcs_kubernetes_cluster.\n\nExamine the available information in the output of the command.\n\nGet props for connecting to the cluster\n\nAvailable props:\n\nkubectl configuration file;\nKubernetes Dashboard secret (it can only be obtained when the cluster is running).\n\nThe operations to get these props are described in detail in Connecting section.\n\nInvalidate cluster key pair\n\nWhen creating clusters of versions 1.22.9 and lower, you can specify a key pair to connect to the cluster hosts directly.\n\nIf the private part of the specified key pair has been compromised or lost, you can revoke the cluster key pair by performing the invalidate procedure.\n\nThe operation can only be performed when the cluster is running.\n\n\n\nAfter performing this operation, the existing public keys on the cluster will be removed.\n\n\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary cluster is located.\nGo to Containers → Kubernetes Clusters.\nClick   for the required cluster and select Invalidate.\nIn the window that appears, perform one of the following actions:\n   Upload the public part of the new key pair, if you have created it yourself;\n   Create a new key pair using the service;\n   Select another existing key pair.\n\nChange virtual machine type for master nodes\n\nThis operation is described in detail in Cluster node scaling.\n\nThe operation can be performed only when the cluster is running.\n\nDelete cluster\n\n\n\nPersonal account\n\n\n\nThis is a group operation: if necessary, you can delete multiple clusters at once by selecting them using the checkboxes.\n\nTo delete a cluster:\n\nGo to VK Cloud personal account.\nSelect the project where the necessary cluster is located.\nGo to Containers → Kubernetes Clusters.\nPerform one of the actions for the required cluster:\n   Select the cluster using the checkbox, then click Delete.\n   Click   for the cluster and select Delete Cluster.\nIn the window that appears:\n   Select the option if you want to delete the cluster along with its disks.\n   Click the Delete cluster button.\n\n\n\n","url":"/en/kubernetes/k8s/service-management/manage-cluster","description":"This section describes the operations that can be used to manage a Kubernetes cluster. Each operation is accompanied by step-by-step instructions.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"a45f5d9a-12a5-43d3-a073-c604c1c90be2","title":"Managing worker node group","data":"\n\nBefore performing any operation on a cluster from Terraform, read the information in Using Terraform.\n\nAdd worker node group\n\n\n\nPersonal account\nTerraform\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary cluster is located.\nGo to Containers → Kubernetes Clusters.\nClick   for the required cluster and select Add node group.\nSet settings for the node group.\nClick the Add node group button.\n\n\n\n\nDetermine what types of virtual machines will be used for the cluster node group:\n\n   Run the command:\n\n            openstack flavor list\n\n      The available virtual machine types will be displayed.\n\n   Select the necessary virtual machine types and write their names from the Name column.\n\nAdd the vkcs_kubernetes_node_group resource and the necessary data sources to the Terraform configuration file:\n\n      ...\n\nAlready described cluster configuration\n   resource \"vkcs_kubernetes_cluster\" \"k8s-cluster\" { ... }\n\n   ...\n\nNew data source: virtual machine type for the nodes\n   data \"vkcs_compute_flavor\" \"k8s-node-group-flavor\" {\n    name = \"\"\n   }\n\nNew resource: worker node group\n   resource \"vkcs_kubernetes_node_group\" \"k8s-node-group\" {\n     name = \"k8s-node-group\"\n     node_count =\n     cluster_id = vkcs_kubernetes_cluster.k8s-cluster.id\n     flavor_id = data.vkcs_compute_flavor.k8s-node-group-flavor.id\n   }\n\n   If necessary, specify additional settings given in the resource documentation.\n\nCheck correctness of the Terraform configuration file:\n\n      terraform validate\n\nFamiliarize yourself with the planned changes:\n\n      terraform plan\n\nApply the planned changes:\n\n      terraform apply\n\n\nCustomize scaling options\n\nYou can change the size of a group of worker nodes manually or configure auto scaling.\n\nThese operations are described in detail in Scaling cluster nodes section.\n\nCustomise labels and taints\n\n\n\nConfigure taints with caution if the node already hosts a workload.\n\nRe-configuring taints can cause pods to be evicted to other nodes. If they do not have enough resources to host the pods, it can lead to partial or complete inaccessibility of applications that use the pods.\n\n\n\nLabels and taints can be set both with the interfaces supported by the VK Cloud platform (personal account and Terraform) and with kubectl. When assigning labels and taints, keep in mind that labels and taints set via the platform interfaces are periodically synchronized with the Kubernetes cluster (in one direction only). During synchronization, labels and taints set with the platform will overwrite labels and taints that were set with kubectl if their keys match. Other labels and taints that were set with kubectl and were not overwritten by values from the platform are valid in the cluster, but are not displayed, e.g. in the Terraform state or personal account.\n\n\n\nPersonal account\nTerraform\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary cluster is located.\nGo to Containers → Kubernetes Clusters.\nLocate the necessary cluster and node group.\nClick   for the required node group and select Labels and Taints.\nDo the necessary actions.\n\n   Manage labels:\n     Add a new label as a key/value pair.\n     Change the key or value of the existing label.\n     Delete existing label.\n\n   Manage taints:\n     Add a new taint by specifying its effect and label as a key/value pair.\n     Modify existing taint.\n     Delete existing taint.\n\n\n\n\nChange the necessary vkcs_kubernetes_node_group resource in the Terraform configuration file:\n\n      ...\n\nAlready described worker node group configuration\n   resource \"vkcs_kubernetes_node_group\" \"k8s-node-group\" {\n     name = \"k8s-node-group\"\n\n     ...\n\nLabels\n\nExisting label\n     labels {\n        key = \"my_awesome_value\"\n        value = \"my_another_awesome_value\"\n     }\n\nNew label\n     labels {\n        key = \"\"\n        value = \"\"\n     }\n\nTaints\n\nExisting taint\n     taints {\n        key = \"taint_key_1\"\n        value = \"taint_value_1\"\n        effect = \"PreferNoSchedule\"\n     }\n\nNew taint\n     taints {\n        key = \"\"\n        value = \"\"\n        effect = \"\"\n     }\n\n     ...\n   }\n\n   ...\n\n   If necessary, change or delete existing labels and taints.\n\n   If the labels and taints blocks do not already exist, create the corresponding blocks.\n\nCheck correctness of the Terraform configuration file:\n\n      terraform validate\n\nFamiliarize yourself with the planned changes:\n\n      terraform plan\n\nApply the planned changes:\n\n      terraform apply\n\n\nSee Labels and taints for details.\n\nConfigure update settings\n\nTo increase update speed, the container service updates multiple nodes in a group at once. To keep your applications and services available during the update, specify the maximum percentage of unavailable nodes for the node group before updating cluster.\n\n\n\nPersonal account\nTerraform\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary cluster is located.\nGo to Containers → Kubernetes Clusters.\nLocate the necessary cluster and node group.\nClick   for the required node group and select Node update.\nSpecify the necessary percentage.\nClick the Save changes button.\n\n\n\n\nAdd or change the max_node_unavailable parameter for the necessary vkcs_kubernetes_node_group resource in the Terraform configuration file:\n\n      ...\n\nAlready described cluster node group configuration\n   resource \"vkcs_kubernetes_node_group\" \"k8s-node-group\" {\n     name = \"k8s-node-group\"\n\n     ...\n\nPercentage of nodes that may be unavailable\n     max_node_unavailable =\n   }\n\n   ...\n\nCheck correctness of the Terraform configuration file:\n\n      terraform validate\n\nFamiliarize yourself with the planned changes:\n\n      terraform plan\n\nApply the planned changes:\n\n      terraform apply\n\n\nMore details about the update procedure in Cluster version update.\n\nDelete node group\n\nThis operation can only be performed when the cluster is running.\n\nThe single cluster node group cannot be deleted from the personal account. However, you can do it with Terraform.\n\n\n\nPersonal account\nTerraform\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary cluster is located.\nGo to Containers → Kubernetes Clusters.\nLocate the necessary cluster and node group.\nClick   for the required node group and select Delete.\nClick the Confirm button.\n\n\n\n\nDelete the necessary vkcs_kubernetes_node_group resource from the Terraform configuration file.\n\nCheck correctness of the Terraform configuration file:\n\n      terraform validate\n\nFamiliarize yourself with the planned changes:\n\n      terraform plan\n\nApply the planned changes:\n\n      terraform apply\n\n","url":"/en/kubernetes/k8s/service-management/manage-node-group","description":"This section lists operations that can be used to manage Kubernetes worker node groups. Each operation is accompanied by step-by-step instructions.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"173f9e9b-77c4-4bbe-9cae-eda492c65380","title":"Managing cluster resources","data":"It is possible to manage Kubernetes resources in Kubernetes cluster via VK Cloud personal account web interface. This functionality serves as alternative to kubectl and Kubernetes Dashboard.\n\nViewing information about cluster resources\n\nGo to VK Cloud personal account.\nSelect the project where the necessary cluster is located.\nGo to Containers → Kubernetes Clusters.\nClick on the name of the required cluster.\nGo to the Cluster resources tab.\n\n   All the cluster's Kubernetes resources are listed here. They are grouped into Sections.\n\nSelect the section from the drop-down list and click on the necessary category.\n\nAdditionally it is possible to:\n\nView the resources' data by clicking the corresponding element in the Data parameter.\n\nSpecify a search request in the Search field.\n\n  The entered search request is saved when moving between the sections.\n\nFilter resources by Namespaces.\n\n  The filter is saved when moving between the sections.\n\n  Some Kubernetes resources can not be filtered by the namespace filter. See the information in the list of available resources below.\n\n\nList of available resources\n\n\n| Resource                                            | Comment                                       |\n| --------------------------------------------------- | --------------------------------------------- |\n| Cluster section                                 |                                               |\n| Nodes                                               |                                               |\n| Namespaces                                          | Namespace filter is not applicable            |\n| Events                                              |                                               |\n| Work load section                               |                                               |\n| Pods                                                |                                               |\n| Deployments                                         |                                               |\n| DaemonSets                                          |                                               |\n| StatefulSets                                        |                                               |\n| ReplicaSets                                         |                                               |\n| HPA                                                 |                                               |\n| Jobs                                                |                                               |\n| Cron Jobs                                           |                                               |\n| Configuration section                           |                                               |\n| ConfigMaps                                          |                                               |\n| Secrets                                             |                                               |\n| Network section                                 |                                               |\n| Services                                            |                                               |\n| Endpoins                                            |                                               |\n| Ingresses                                           |                                               |\n| Storage section                                 |                                               |\n| Persistent Volume Claims                            |                                               |\n| Persistent Volumes                                  | Namespace filter is not applicable            |\n| Storage classes                                     | Namespace filter is not applicable            |\n| Access management section                       |                                               |\n| Service Accounts                                    |                                               |\n| Cluster Roles                                       | Namespace filter is not applicable            |\n| Roles                                               |                                               |\n| Cluster Role Bindings                               | Namespace filter is not applicable            |\n| Role Bindings                                       |                                               |\n\n\n","url":"/en/kubernetes/k8s/service-management/manage-resources","description":"Managing cluster's Kubernetes resources via personal account.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"90b8ebf7-4164-4e6f-b389-8fd206b357b4","title":"Add-ons management","data":"Add-ons for Cloud Containers clusters, you can also install it when creating a cluster using Terraform and into an existing cluster. Installed add-ons can be viewed and deleted.\n\n{heading(Viewing add-ons)[id=viewing_addons]}\n\n{heading(Add-ons available for installation)[id=addons_available_for_installation]}\n\n\n\nPersonal account\nTerraform\n\n\n\nGo to VK Cloud personal account.\nSelect project, where the cluster will be placed.\nGo to Containers → Kubernetes clusters.\nClick on the name of the required cluster.\nGo to Addons tab.\n\n   If there are no add-ons installed in the cluster yet, the cards of available add-ons will be shown on this tab in the Available addons block.\n   If there are already installed add-ons in the cluster, click on the Add addon button and look at the cards of available add-ons.\n\nClick to the icon Information on the add-on card to view detailed information about it.\n\n\n\n\nInstall Terraform and configure the environment if it is not already done.\nCreate a Terraform configuration file by specifying the cluster ID in the vkcs_kubernetes_addons block.\nApply the configuration for the vcs_kubernetes_addons data source using the command:\n\n      terraform apply -target=\"data.vkcs_kubernetes_addons.\"\n\nRun the command:\n\n      terraform state show data.vkcs_kubernetes_addons.\n\nSee the available information in the output of the command.\n\n{heading(Installed add-ons)[id=installed_addons]}\n\n\n\nPersonal account\nTerraform\n\n\n\nGo to VK Cloud personal account.\nSelect project, where the cluster will be placed.\nGo to Containers → Kubernetes clusters.\nClick on the name of the required cluster.\nGo to Addons tab.\n\n   The installed add-ons will be listed in the table. The table also shows:\n\n   add-on status information: Installing, Installed, Error, Deleting.\n   Additional information about the add-on.\n\n\n\n\nInstall Terraform and configure the environment if it is not already done.\nCreate a Terraform configuration file by specifying the cluster ID in the vkcs_kubernetes_addon block.\nApply the configuration for the vcs_kubernetes_addon data source using the command:\n\n      terraform apply -target=\"data.vkcs_kubernetes_addon.\"\n\nRun the command:\n\n      terraform state show data.vkcs_kubernetes_addon.\n\nSee the available information in the output of the command.\n\n{heading(Installing add-on)[id=installing_addon]}\n\n\n\nWhen installing the Docker Registry and Ingress NGINX add-ons, standard load balancers will be created for them.\n\nUsage of this load balancer is charged.\n\n\n\nThe procedure for installing add-ons is discussed in the relevant section.\n\n{heading(Editing add-on code)[id=editing_addon_code]}\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect project, where the cluster will be placed.\nGo to Containers → Kubernetes clusters.\nClick on the name of the required cluster.\nGo to Addons tab.\nClick   for the required add-on and select Edit.\nMake the necessary changes to the add-on setup code.\nClick the Change settings button.\nIn the window that opens, confirm the operation.\n\n\n\n\nIf the edit failed with an error:\n\n\n\nPersonal account\n\n\n\nIn the general list of installed add-ons, click the Retry edit button.\nSelect one of the troubleshooting options:\n\n   Restore: make changes to the latest add-on settings.\n   Reset: reset the add-on parameters to the last working state.\n\nIn the window that opens, make the necessary changes to the add-on settings.\nClick the Change settings button.\nIn the window that opens, confirm the operation.\n\n{heading(Updating add-on version)[id=updating_addon_version]}\n\nOnly an increase in the add-on version is available.\n\n\n\nTo update some add-ons, you will first need to uninstall the previous version.\n\n\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect project, where the cluster will be placed.\nGo to Containers → Kubernetes clusters.\nClick on the name of the required cluster.\nGo to Addons tab.\nClick   for the required add-on and select Update.\nIn the window that opens, review the changes.\n(Optional) Update the cluster version for compatibility with the add-on.\nMake the necessary changes to the add-on settings.\nClick the Update button.\n\n\n\n\nIf the update failed with an error:\n\n\n\nPersonal account\n\n\n\nIn the general list of installed add-ons, click the Retry update button.\nSelect one of the troubleshooting options:\n\n   Restore: make changes to the latest add-on update settings.\n   Reset: reset the add-on update settings to the last working state.\n\nIn the window that opens, make the necessary changes to the add-on settings.\nClick the Update button.\n\n{heading(Removing add-on)[id=removing_addon]}\n\n\n\nPersonal account\nTerraform\n\n\n\nThis is a group operation: if necessary, you can delete several add-ons at once by selecting them using the checkboxes.\n\nTo remove the add-on:\n\nGo to VK Cloud personal account.\nSelect project, where the cluster will be placed.\nGo to Containers → Kubernetes clusters.\nClick on the name of the required cluster.\nGo to Addons tab.\nPerform one of the actions for the required add-on:\n\n   Select the add-on using the checkbox, then click Delete.\n   Click   for the required add-on and select Remove addon.\n\nConfirm the deletion.\n\n\n\n\nInstall Terraform and configure the environment if it is not already done.\nIn the Terraform configuration file, delete or comment out the block with the add-ons to be deleted.\nCheck out the planned changes:\n\n      terraform plan\n\nApply the changes:\n\n      terraform apply\n\n","url":"/en/kubernetes/k8s/service-management/addons/manage-addons","description":"This section describes the operations that can be used to manage Kubernetes cluster add-ons. Each operation is accompanied by step-by-step instructions.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"715d8560-a802-49e9-80ea-f3d3275801bb","title":"Capsule","data":"{heading(Installing add-on)[id=installing_addon]}\n\nSeveral installation options are available for the add-on.\n\nTake into account the total maximum system requirements of add-ons that will be placed on groups of worker nodes. If necessary, perform manual scaling groups of worker nodes or set up automatic scaling before installation.\n\n\n\nStandard installation\nInstallation on dedicated worker nodes\nQuick installation\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n    Go to your VK Cloud personal account.\n    Select the project where the needed cluster is located.\n    Go to Containers → Kubernetes Clusters.\n    Click on the name of the cluster.\n    Go to the Addons tab.\n    If there are already installed add-ons in the cluster, click the Add addon button.\n    Click the Install button on the capsule add-on card.\n    Select the needed add-on version from the drop-down list.\n    Click the Install addon button.\n    Edit if necessary:\n\n      selected version\n      application name\n      namespace where the add-on will be installed\n      add-on settings code\n\n\n        An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Install Terraform and configure the environment if it is not already done.\n   Add to your Terraform configuration files that describe the cluster:\n\n      The vkcs_kubernetes_addon resource.\n      The vkcs_kubernetes_addon data source.\n      The vkcs_kubernetes_addons data source.\n\n      If necessary, adapt the usage examples given for resource and data sources to your task and Terraform configuration (see the links above).\n\n   Make sure the configuration files are correct and contain the necessary changes:\n\n            terraform validate && terraform plan\n\n   Apply the changes:\n\n            terraform apply\n\n\n(Optional) Check out the official Capsule documentation on working with the add-on.\n\n\n\n\nPrepare a dedicated group of worker nodes to install the add-on, if it has not already been done:\n\n   Personal account\n\n\n   Go to your VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Find the cluster you need in the list.\n   Make sure that the cluster has a dedicated group of worker nodes that will host add-ons.\n\n      If there is no such group — add it.\n\n   Customise for this node group, if it hasn't already been done:\n\n      Kubernetes labels: key addonNodes, value dedicated.\n      Node taints: effect NoSchedule, key addonNodes, value dedicated.\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to your VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install button on the capsule add-on card.\n   Выберите нужную версию аддона из выпадающего списка.\n   Нажмите кнопку Установить аддон.\n   Select the necessary add-on version from the drop-down list.\n   Click the Install addon button.\n   Edit if necessary:\n\n      selected version\n      application name\n      the name of the namespace where the add-on will be installed\n      add-on settings code\n\n   Set the necessary tolerations and nodeSelector in the add-on setup code:\n\n      Tolerations\n      nodeSelector\n\n\n            tolerations:\n        key: \"addonNodes\"\n          operator: \"Equal\"\n          value: \"dedicated\"\n          effect: \"NoSchedule\"\n\n      Set this toleration for the tolerations field.\n\n\n            nodeSelector:\n        add-onNodes: dedicated\n\n      Set this selector for the nodeSelector field.\n\n\n      An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Install Terraform and configure the environment if it is not already done.\n   Add to your Terraform configuration files that describe the cluster:\n\n      The vkcs_kubernetes_addon resource.\n      The vkcs_kubernetes_addon data source.\n      The vkcs_kubernetes_addons data source.\n\n      If necessary, adapt the usage examples given for resource and data sources to your task and Terraform configuration (see the links above).\n\n   Make sure the configuration files are correct and contain the necessary changes:\n\n            terraform validate && terraform plan\n\n   Apply the changes:\n\n            terraform apply\n\n\n(Optional) Check out the official Capsule documentation on working with the add-on.\n\n\n\n\n\n\nEditing the add-on settings code is not supported during the quick installation.\n\nIf this is not suitable for you, perform a standard installation or installation on dedicated worker nodes.\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to your VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install button on the capsule add-on card.\n   Select the necessary add-on version from the drop-down list.\n   Click the Install addon button.\n   Edit if necessary:\n\n      selected version\n      application name\n      the name of the namespace where the add-on will be installed\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Install Terraform and configure the environment if it is not already done.\n   Add to your Terraform configuration files that describe the cluster:\n\n      The vkcs_kubernetes_addon resource.\n      The vkcs_kubernetes_addon data source.\n      The vkcs_kubernetes_addons data source.\n\n      If necessary, adapt the usage examples given for resource and data sources to your task and Terraform configuration (see the links above).\n\n   Make sure the configuration files are correct and contain the necessary changes:\n\n            terraform validate && terraform plan\n\n   Apply the changes:\n\n            terraform apply\n\n\n(Optional) Check out the official Capsule documentation on working with the add-on.\n\n{heading(Editing add-on settings code during installation)[id=editing_addon_settings_code_during_installation]}\n\nEditing the add-on code is applicable for standard installation and installation on dedicated worker nodes.\n\nThe full add-on setup code along with the description of the fields is available on GitHub.\n\n\n\nDo not delete the podAnnotations.timestamp fields or the values set in them. These fields are required for correct installation and operation of the add-on.\n\n\n\nAfter editing the add-on code continue installing the add-on.\n","url":"/en/kubernetes/k8s/service-management/addons/advanced-installation/install-advanced-capsule","description":"The configuration and installation of the Capsule add-on is described here.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"bf1cac1e-9c56-43e8-a51a-f07551cba251","title":"Cert-manager","data":"{heading(Installing add-on)[id=installing_addon]}\n\nSeveral installation options are available for the add-on.\n\nTake into account the total maximum system requirements of add-ons that will be placed on groups of worker nodes. If necessary, perform manual scaling groups of worker nodes or set up automatic scaling before installation.\n\n\n\nStandard installation\nInstallation on dedicated worker nodes\nQuick installation\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install button on the cert-manager add-on card.\n   Select the necessary add-on version from the drop-down list.\n   Click the Install addon button.\n   Edit if necessary:\n\n      selected version;\n      application name;\n      the name of the namespace where the add-on will be installed;\n      add-on settings code.\n\n\n        An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Install Terraform and configure the environment if it is not already done.\n   Add to your Terraform configuration files that describe the cluster:\n\n      The vkcs_kubernetes_addon resource.\n      The vkcs_kubernetes_addon data source.\n      The vkcs_kubernetes_addons data source.\n\n      If necessary, adapt the usage examples given for resource and data sources to your task and Terraform configuration (see the links above).\n\n   Make sure the configuration files are correct and contain the necessary changes:\n\n            terraform validate && terraform plan\n\n   Apply the changes:\n\n            terraform apply\n\n\nVerify that the add-on is installed correctly by issuing a test self-signed certificate.\n\n\n\n\nPrepare a dedicated group of worker nodes to install the add-on, if it has not already been done:\n\n   Personal account\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Find the cluster you need in the list.\n\n   Make sure that the cluster has a dedicated group of worker nodes that will host add-ons.\n\n      If there is no such group — add it.\n\n   Customise for this node group, if it hasn't already been done:\n\n      Kubernetes labels: key addonNodes, value dedicated.\n      Node taints: effect NoSchedule, key addonNodes, value dedicated.\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install button on the cert-manager add-on card.\n   Select the necessary add-on version from the drop-down list.\n   Click the Install addon button.\n   Edit if necessary:\n\n      selected version;\n      application name;\n      the name of the namespace where the add-on will be installed;\n      add-on settings code.\n\n   Set the necessary tolerations and nodeSelector in the add-on setup code:\n\n      Tolerations\n      nodeSelector\n\n\n            tolerations:\n        key: \"addonNodes\"\n          operator: \"Equal\"\n          value: \"dedicated\"\n          effect: \"NoSchedule\"\n\n      Set this toleration for fields:\n\n      tolerations;\n      webhook.tolerations;\n      cainjector.tolerations.\n\n\n            nodeSelector:\n        addonNodes: dedicated\n\n      Set this selector for fields:\n\n      nodeSelector;\n      webhook.nodeSelector;\n      cainjector.nodeSelector.\n\n\n      An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Install Terraform and configure the environment if it is not already done.\n   Add to your Terraform configuration files that describe the cluster:\n\n      The vkcs_kubernetes_addon resource.\n      The vkcs_kubernetes_addon data source.\n      The vkcs_kubernetes_addons data source.\n\n      If necessary, adapt the usage examples given for resource and data sources to your task and Terraform configuration (see the links above).\n\n   Make sure the configuration files are correct and contain the necessary changes:\n\n            terraform validate && terraform plan\n\n   Apply the changes:\n\n            terraform apply\n\n\nVerify that the add-on is installed correctly by issuing a test self-signed certificate.\n\n\n\n\n\n\nEditing the add-on settings code is not supported during the quick installation.\n\nIf this is not suitable for you, perform a standard installation or installation on dedicated worker nodes.\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install button on the cert-manager add-on card.\n   Select the necessary add-on version from the drop-down list.\n   Click the Install addon button.\n   Edit if necessary:\n\n      selected version;\n      application name;\n      the name of the namespace where the add-on will be installed;\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Install Terraform and configure the environment if it is not already done.\n   Add to your Terraform configuration files that describe the cluster:\n\n      The vkcs_kubernetes_addon resource.\n      The vkcs_kubernetes_addon data source.\n      The vkcs_kubernetes_addons data source.\n\n      If necessary, adapt the usage examples given for resource and data sources to your task and Terraform configuration (see the links above).\n\n   Make sure the configuration files are correct and contain the necessary changes:\n\n            terraform validate && terraform plan\n\n   Apply the changes:\n\n            terraform apply\n\n\nVerify that the add-on is installed correctly by issuing a test self-signed certificate.\n\n{heading(Editing add-on settings code during installation)[id=editing_addon_settings_code_during_installation]}\n\nEditing the add-on code is applicable for standard installation and installation on dedicated worker nodes.\n\nThe full add-on setup code along with the description of the fields is available on GitHub.\n\n\n\nDo not delete the podAnnotations.timestamp fields or the values set in them. These fields are required for correct installation and operation of the add-on.\n\n\n\nAfter editing the add-on code continue installing the add-on.\n","url":"/en/kubernetes/k8s/service-management/addons/advanced-installation/install-advanced-cert-manager","description":"The configuration and installation of the cert-manager add-on is described here.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"c5ff4587-e97f-4ac5-9e6b-7e8c96181f30","title":"Docker Registry","data":"Preparatory steps\n\nCreate in the object storage bucket, which will be used to store Docker images.\n\n   When creating, select:\n\n   Storage class: Hotbox.\n   Default ACL: private.\n\n   Write down the bucket's name.\n\nAdd a key to access this bucket:\n\n   Personal account\n\n\n   Go to VK Cloud personal account.\n   Select project.\n   Go to Object storage → Buckets.\n   Click on the name of the created bucket.\n   Go to Keys tab.\n   Click the Add key button.\n   Specify any key name.\n   Leave the other settings unchanged.\n   Click the Create button.\n\n\n   Write down the values Access Key ID and Secret Key.\n\nCreate an encrypted login/password pair for authorization in the Docker registry by running the command:\n\n      docker run --entrypoint htpasswd registry:2.7.0 -Bbn\n\n   Write down the output of the command (in the format :).\n\nAdd floating IP or find an existing unbound floating IP address.\n\n   Write down this IP address. It will be used to access the Docker registry.\n\n{heading(Installing add-on)[id=installing_addon]}\n\n\n\nWhen installing the add-on, standard load balancers will be created for them.\n\nUsage of this load balancer is charged.\n\n\n\nSeveral installation options are available for the add-on:\n\nstandard installation;\ninstallation on dedicated worker nodes.\n\nTake into account the total maximum system requirements of add-ons that will be placed on groups of worker nodes. If necessary, perform manual scaling groups of worker nodes or set up automatic scaling before installation.\n\n\n\nStandard installation\nInstallation on dedicated worker nodes\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install addon button on the docker-registry add-on card.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed.\n\n   Edit the add-on settings code.\n\n\n      An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Install Terraform and configure the environment if it is not already done.\n   Create a Terraform configuration file with data about the add-on being installed in the vkcs_kubernetes_addon block:\n\n      Get list of add-ons available for installation.\n      Get the add-on settings from the configuration_values parameter using the data source vcs_kubernetes_addon.\n      (Optional) To dynamically change the add-on parameters (for example, via CI), add the add-on settings to a separate yaml file. Use the templatefile function to add the required values.\n\n         Example of specifying an add-on\n\n                  resource \"vkcs_kubernetes_addon\" \"docker-registry\" {\n            cluster_id = vkcs_kubernetes_cluster.k8s-cluster.id\n            addon_id = data.vkcs_kubernetes_addon.docker-registry.id\n            namespace = \"kube-system\"\n            configuration_values = templatefile(\"./docker-registry-all.yaml\",{htpasswd = \"\", accessKey = \"\", secretKey = \"\", loadBalancerIP = \"\"})\n\n            depends_on = [\n               vkcs_kubernetes_node_group.default_ng\n            ]\n         }\n\n\n   Check the Terraform configuration file for correctness:\n\n            terraform validate\n\n   Check out the planned changes:\n\n            terraform plan\n\n   Apply the changes:\n\n            terraform apply\n\n\nGet the data to access the registry.\n\n\n\n\nPrepare a dedicated group of worker nodes to install the add-on, if it has not already been done:\n\n   Personal account\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Find the cluster you need in the list.\n\n   Make sure that the cluster has a dedicated group of worker nodes that will host add-ons.\n\n      If there is no such group — add it.\n\n   Customise for this node group, if it hasn't already been done:\n\n      Kubernetes labels: key addonNodes, value dedicated.\n      Node taints: effect NoSchedule, key addonNodes, value dedicated.\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install addon button on the docker-registry add-on card.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed.\n\n   Edit the add-on settings code.\n\n   Set the necessary tolerations and nodeSelector in the add-on settings code:\n\n      Tolerations\n      nodeSelector\n\n\n            tolerations:\n        key: \"addonNodes\"\n          operator: \"Equal\"\n          value: \"dedicated\"\n          effect: \"NoSchedule\"\n\n      Set this exception for the tolerations field.\n\n\n            nodeSelector:\n        addonNodes: dedicated\n\n      Set this selector for the nodeSelector field.\n\n\n      An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Use the instructions from the standard add-on installation. In the add-on settings, set the necessary exceptions (tolerations) and node selectors (nodeSelector).\n\n\nGet the data to access the registry.\n\n{heading(Editing add-on settings code during installation)[id=editing_addon_settings_code_during_installation]}\n\n\n\nWhen editing the add-on settings code, use the information obtained earlier.\nThe full add-on settings code along with the description of the fields is available on GitHub.\n\n\n\nSpecify:\n\nDetails for authorization in the Docker registry:\n\n      secrets:\n     htpasswd: \":\"\n\nDetails for accessing the bucket for storing Docker images:\n\n      secrets:\n     s3:\n       secretRef: \"\"\n       accessKey: \"\"\n       secretKey: \"\"\n\n      s3:\n     bucket:\n\nIP address for the load balancer through which access to the service will be provided:\n\n      service:\n     name: registry\n     type: LoadBalancer\n     loadBalancerIP:\n\nAfter editing the add-on code continue installing the add-on.\n\nConnecting to registry\n\nWrite down the data that was used in the add-on settings code when installing it:\n\n   Login.\n   Password.\n   The IP address of the registry. The Docker registry URL will look like this: :5000.\n\nConnect to Docker Registry.\n","url":"/en/kubernetes/k8s/service-management/addons/advanced-installation/install-advanced-registry","description":"This describes how to configure and install the Docker Registry add-on.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"52f24096-a851-44f9-b485-6591f743b784","title":"Fluent Bit","data":"{heading(Installing add-on)[id=installing_addon]}\n\nOnly standard installation is available for the add-on.\n\nThe add-on will be installed as a DaemonSet controller on all nodes in the cluster, including master nodes.\n\nTake into account the total maximum system requirements of add-ons that will be placed on groups of worker nodes. If necessary, perform manual scaling groups of worker nodes or set up automatic scaling before installation.\n\n   Personal account\n   Terraform\n\n\n   Go to your VK Cloud personal account.\n   Select the project where the required cluster is located.\n   Go to Containers → Kubernetes Clusters.\n   Click the name of the cluster.\n   Go to the Addons tab.\n   If the cluster already has add-ons installed, click Add Addon.\n   Click Install on the fluent-bit add-on card.\n   Select the required add-on version from the drop-down list.\n   Click Install addon.\n   (Optional) Edit:\n\n      The selected version\n      The application name\n      The namespace where the add-on will be installed\n\n   Edit the add-on settings code: in the Output section set the parameters of logs delivery to the selected service. Leave other parameters at your discretion.\n\n\n        An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Install Terraform and configure the environment, if it is not already done.\n   Add to your Terraform configuration files that describe the cluster the following resource and data sources:\n\n      The vkcs_kubernetes_addon resource\n      The vkcs_kubernetes_addon data source\n      The vkcs_kubernetes_addons data source\n\n      If necessary, adapt the given examples of the resource and the data sources usage to your task and Terraform configuration. For example, you can edit the add-on settings code by changing the vkcs_kubernetes_addon resource.\n\n      An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Make sure the configuration files are correct and contain the necessary changes:\n\n            terraform validate && terraform plan\n\n   Apply the changes:\n\n            terraform apply\n\n\n{heading(Editing add-on settings code during installation))[id=editing_addon_settings_code_during_installation]}\n\nThe full add-on settings code along with the description of the fields is available:\n\nIn your personal account.\nIn the configuration_values attribute from the vkcs_kubernetes_addon data source if Terraform is used.\n\nInstallation of the add-on is impossible without editing the settings code. It is required to set the parameters of log delivery:\n\nIn the Output section of the code, set the delivery parameters to the selected service.\n\n\n   Example for configuring log delivery to Elasticsearch\n\n      outputs: |\n      [OUTPUT]\n         Name es\n         Match k8s.*\n         Host XX.XX.XX.XX\n         Logstash_Format On\n         Logstash_Prefix k8s\n         Logstash_Prefix_Key $kubernetes['pod_name']\n         Retry_Limit False\n         TLS off\n         TLS.debug 4\n         TLS.verify off\n         Suppress_Type_Name on\n         Trace_Error On\n         Trace_Output Off\n         Replace_Dots On\n\n      [OUTPUT]\n         Name es\n         Match host.*\n         Host XX.XX.XX.XX\n         Logstash_Format On\n         Logstash_Prefix host\n         Logstash_Prefix_Key $_HOSTNAME\n         Retry_Limit False\n         TLS off\n         TLS.debug 4\n         TLS.verify off\n         Suppress_Type_Name on\n         Trace_Error On\n         Trace_Output Off\n         Replace_Dots On\n      See the official documentation for details on parameters for Elasticsearch.\n\n\n   Example for configuring log delivery to Loki\n\n      outputs: |\n      [OUTPUT]\n         Name loki\n         host XX.XX.XX.XX\n         match k8s.*\n         labels source=kubernetes, pod=$kubernetes['pod_name'], namespace=$kubernetes['namespace_name']\n\n      [OUTPUT]\n         Name loki\n         host XX.XX.XX.XX\n         match host.*\n         labels source=systemd, host=$_HOSTNAME, service=$_SYSTEMD_UNIT\n\n   See the official Fluent Bit documentation for details on parameters for Loki.\n\n\n(Optional) Edit other parameters of the settings code. Read more about configuration file parameters in the official Fluent Bit documentation. Also, an example of the Fluent Bit settings code is available on GitHub.\n\n\n   Do not delete fields that are required for correct installation and operation of the add-on, as well as the values set in these fields.\n\n   Comments in the add-on settings code allow you to find such fields.\n\n\nWhen you have finished editing the code, continue installing the add-on.\n\nYou can read more about the pipeline in the official documentation.\n","url":"/en/kubernetes/k8s/service-management/addons/advanced-installation/install-advanced-fluent-bit","description":"This section describes how to configure and install the Fluent Bit add-on.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"e772a76c-16b6-4280-8feb-f25a907a5631","title":"Fluent Bit for Cloud Logging (logaas-integration)","data":"Preparatory steps\r\n\r\nConnect the Cloud Logging service to the project, if it has not been done yet. To do this, contact technical support.\r\n{heading(Installing add-on)[id=installing_addon]}\r\n\r\nSeveral installation options are available for the add-on:\r\n\r\nstandard installation;\r\nquick installation.\r\n\r\nRegardless of the selected installation option, the add-on will be installed as DaemonSet to all nodes of the cluster, including the master nodes.\r\n\r\nTake into account the total maximum system requirements of add-ons that will be placed on groups of worker nodes. If necessary, perform manual scaling groups of worker nodes or set up automatic scaling before installation.\r\n\r\n\r\n\r\nStandard installation\r\nQuick installation\r\n\r\n\r\n\r\nInstall the add-on:\n   Personal account\r\n   Terraform\n   Go to VK Cloud personal account.\r\n   Select project, where the cluster will be placed.\r\n   Go to Containers → Kubernetes clusters.\r\n   Click on the name of the required cluster.\r\n   Go to Addons tab.\r\n   If there are already installed add-ons in the cluster, click on the Add addon button.\r\n   Click the Install addon button on the logaas-integration add-on card.\r\n   Select the required add-on version from the drop-down list.\r\n   Edit if necessary:\r\n\r\n      the selected version;\r\n      application name;\r\n      the name of the namespace where the add-on will be installed;\r\n      add-on settings code.\n        An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n   Click the Install addon button.\r\n\r\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n   Documentation of the Terraform provider VK Cloud contains an example of using the resource vkcs_kubernetes_addon, which describes a single add-on. Data sources related to add-ons are also documented:\r\n\r\n   vkcs_kubernetes_addon;\r\n   vkcs_kubernetes_addons.\r\n\r\n   For more information about working with the provider, see Terraform.\n(Optional) View logs in the Cloud Logging service to make sure that the add-on is working properly.\r\n\r\n\r\n\r\n\r\n\r\n\r\nDuring quick installation, the add-on settings code is not edited.\r\n\r\nIf this does not suit you, perform the standard installation.\r\n\r\n\r\n\r\nInstall the add-on:\n   Personal account\r\n   Terraform\n   Go to VK Cloud personal account.\r\n   Select project, where the cluster will be placed.\r\n   Go to Containers → Kubernetes clusters.\r\n   Click on the name of the required cluster.\r\n   Go to Addons tab.\r\n   If there are already installed add-ons in the cluster, click on the Add addon button.\r\n   Click the Install button on the logaas-integration add-on card.\r\n   Select the necessary add-on version from the drop-down list.\r\n   Click the Install addon button.\r\n   Edit if necessary:\r\n\r\n      selected version;\r\n      application name;\r\n      the name of the namespace where the add-on will be installed.\r\n\r\n   Click the Install addon button.\r\n\r\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n   Documentation of the Terraform provider VK Cloud contains an example of using the resource vkcs_kubernetes_addon, which describes a single add-on. Data sources related to add-ons are also documented:\r\n\r\n   vkcs_kubernetes_addon;\r\n   vkcs_kubernetes_addons.\r\n\r\n   For more information about working with the provider, see Terraform.\n(Optional) View logs in the Cloud Logging service to make sure that the add-on is working properly.\r\n{heading(Editing add-on settings code during installation)[id=editing_addon_settings_code_during_installation]}\r\n\r\nEditing the add-on code is applicable for a standard installation.\r\n\r\nThe full add-on settings code along with the description of the fields is available:\r\n\r\nin your personal account;\r\nin the configuration_values attribute from the data source vkcs_kubernetes_addon if Terraform is used.\r\n\r\nAlso on GitHub the Fluent Bit settings code is available, which serves as the basis for this add-on.\r\n\r\n\r\n\r\nDo not delete the fields that are required for the correct installation and operation of the add-on, or the values specified in these fields.\r\n\r\nThere are comments in the add-on settings code that allow you to find such fields.\r\n\r\n\r\n\r\nRead more about pipeline and configuration file settings in the official documentation of Fluent Bit.\r\nFine-tuning the behavior of the add-on when working with different severity levels\r\n\r\nBefore sending logs to the Cloud Logging service, the add-on performs the following actions:\r\n\r\nDetermines the severity level of individual log entries. This is done using Fluent Bit parsers.\n   Table of supported levels\r\n\r\n   The levels are sorted in ascending order of severity.\n   | Level | The numerical equivalent | Decoding                     |\r\n   | ------- | ------------------- | ------------------------------- |\r\n   | DEBUG | 4                   | Debugging messages            |\r\n   | INFO  | 3                   | Informational messages        |\r\n   | WARN  | 2                   | Warning messages       |\r\n   | ERROR | 1                   | Error messages            |\r\n   | FATAL | 0                   | Critical error messages |\nAdds additional metadata to the logs, which makes it easier to work with Cloud Containers cluster logs (for example, searching for the necessary logs in Cloud Logging). Special Fluent Bit filters are used for this, written in Lua. This metadata contains, among other things, the severity level of logged events.\n   Learn more about metadata for determining the source of logs\r\n\r\n   For each log entry, identifiers are set that allow you to determine the source of logs when using Cloud Logging:\r\n\r\n   Resource group ID (group-id): the name of the cluster.\r\n   The resource ID (stream-id) in one of the formats:\r\n\r\n     For kubelet component: ..\r\n\r\n       Example:\r\n\r\n       `text\r\n       my-cluster-node-0.kubelet.service\r\n       `\r\n\r\n     For pod: ..\r\n\r\n       First, the name is searched in the following labels:\r\n\r\n       app.kubernetes.io/instance (more about label);\r\n       app.kubernetes.io/name (more about label);\r\n       app;\r\n       k8s-app.\r\n\r\n       If there are no such labels, then the name of the pod is used.\r\n\r\n       Example:\r\n\r\n       Let:\r\n\r\n       In the namespace kube-system there is a pod named kube-controller-manager-my-cluster-master-0.\r\n       The label k8s-app=kube-controller-manager is set for this pod.\r\n\r\n       Then the resource identifier will look like:\r\n\r\n       `text\r\n       kube-system.kube-controller-manager\r\n       `\n   Learn more about other metadata with additional information about logs\r\n\r\n   Metadata fields with additional information about the logged event are added to log entries, which allows you to search in Cloud Logging, including by log metadata:\r\n\r\n   node_name: the name of the cluster node where the kubelet service or pod is located.\r\n   severity: the severity level of the logged event in text form.\r\n   severity_num: the severity level of the logged event in number form.\r\n   All fields of the JSON message: if logs are written in JSON format, then all fields of the JSON message, except msg, are added as metadata.\nYou can fine-tune the behavior of the add-on when working with severity levels using:\r\n\r\n\r\n\r\nCustom filters\r\nCustom patterns\r\n\r\n\r\n\r\nSet one or more rules for custom CustomFilter filters in the add-on code so that only logs with the specified minimum severity level get into Cloud Logging. These rules can be configured at the level of a specific namespace and at the level of specific pods in the namespace:\r\n\r\n\r\n`yaml\r\ncustomFilter:\r\n  namespace:\n    rules: # One rule for the namespace\r\n      min_level:\n  namespace:\n    rules: # A few rules for the pods in the namespace\r\n      podprefix:\n        min_level:\n      podprefix:\n        min_level:\n  namespace:\n    rules: # A combination of namespace rules and pod rules\r\n      min_level:\n      podprefix:\n        min_level:\n      podprefix:\n        min_level:\n`\r\n\r\nIt is the prefix that is configured for the pods so that you can receive logs from several replica pods that relate to the same workload.\r\n\r\n\r\nExamples\r\n\r\nThe only rule applies at the level of the kube-system namespace.\r\n\r\n   Only those logs from this namespace that have a severity level of WARN or higher (ERROR, FATAL) will get into Cloud Logging.\r\n\r\nThe only rule applies at the level of the pods that are in the default namespace, and whose names begin with the prefix test-pod.\r\n\r\n   Only those logs from these pods that have a severity level of ERROR or higher (FATAL) will get into Cloud Logging. Logs are not filtered at the default namespace level.\r\n\r\nSeveral rules apply at the level of the my-namespace namespace and at the level of the example-pod prefixed pods that are located in this namespace.\r\n\r\n   Only the listed logs will be included in Cloud Logging:\r\n\r\n   Logs from the namespace that do not belong to pods with the prefix example-pod and have a severity level of ERROR or higher (FATAL).\r\n   Logs from the pods with the prefix example-pod, which have a severity level of WARN or higher (ERROR, FATAL).\r\n\r\n\r\n`yaml\r\ncustomFilter:\r\n  namespace: kube-system\r\n    rules:\r\n      min_level: WARN\r\n  namespace: default\r\n    rules:\r\n      podprefix: test-pod\r\n        min_level: ERROR\r\n  namespace: my-namespace\r\n    rules:\r\n      min_level: ERROR\r\n      podprefix: example-pod\r\n        min_level: WARN\r\n`\r\n\r\n\r\n\r\n\r\n\r\n\r\nSet one or more rules for custom customRegexp pattern in the add-on code to set the required severity level for logs bypassing the standard add-on mechanisms. If part of the log entry matches the specified Lua pattern, then this record is assigned the severity level specified in the rule. If there are no matches, then the record is assigned a severity level determined using the Fluent Bit parsers. This can be useful if the automatic level detection mechanism used in the add-on incorrectly determines the log level of a particular application.\r\n\r\nThese rules apply at the level of a certain namespace and at the level of certain pods in the namespace:\r\n\r\n\r\n`yaml\r\ncustomRegexp:\r\n  namespace:\n    rules:\r\n    levels: # One rule for a namespace: setting up multiple levels\r\n      level:\n        reg_exp: \"\"\r\n      level:\n        reg_exp: \"\"\r\n  namespace:\n    rules: # Several rules for the pods in the namespace: setting up multiple levels\r\n    podprefix:  # The first rule for pods\r\n      levels:\r\n      level:\n        reg_exp: \"\"\r\n      level:\n        reg_exp: \"\"\r\n    podprefix:  # The second rule for pods\r\n      levels:\r\n      level:\n        reg_exp: \"\"\r\n  namespace:\n    rules:\r\n    levels: # The first rule for a namespace is to set up multiple levels\r\n      level:\n        reg_exp: \"\"\r\n      level:\n        reg_exp: \"\"\r\n    podprefix:  # The second rule for pods: setting up multiple levels\r\n      levels:\r\n      level:\n        reg_exp: \"\"\r\n      level:\n        reg_exp: \"\"\r\n`\r\n\r\nIt is the prefix that is configured for the pods so that you can receive logs from several replica pods that relate to the same workload.\r\n\r\n\r\nExamples\r\n\r\nThe only rule applies at the level of the kube-system namespace.\r\n\r\n   If part of the log entry from this namespace matches the specified pattern [Ww]%d%d%d%d%s+, then this entry will be assigned the ERROR severity level.\r\n\r\nThe only rule applies at the level of the pods that are in the default namespace, and whose names begin with the prefix test-pod.\r\n\r\n   If a part of the log entry from these pods matches the specified pattern this is a plain text message, then this entry will be assigned the WARN severity level.\r\n\r\nSeveral rules apply at the level of the my-namespace namespace and at the level of the example-pod prefixed pods that are located in this namespace.\r\n\r\n   The severity levels will be assigned according to the following rules:\r\n\r\n   If a part of the log record from this namespace (which does not belong to the pods with the prefix example-pod) matches the specified pattern [Ww]%s+, then this record will be assigned the ERROR severity level.\r\n   If a part of the log record from the pods with the prefix example-pod matches the specified pattern Debug trace, then this record will be assigned the DEBUG severity level.\r\n\r\nIf no part of the log record matches the specified patterns, then such a record is assigned a severity level determined using Fluent Bit parsers.\r\n\r\n\r\n`yaml\r\ncustomRegexp:\r\n  namespace: kube-system\r\n    rules:\r\n    levels:\r\n      level: ERROR\r\n        reg_exp: \"[Ww]%d%d%d%d%s+\"\r\n  namespace: default\r\n    rules:\r\n    podprefix: test-pod\r\n      levels:\r\n      level: WARN\r\n        reg_exp: \"this is a plain text message\"\r\n  namespace: my-namespace\r\n    rules:\r\n    levels:\r\n      level: ERROR\r\n        reg_exp: \"[Ww]%s+\"\r\n    podprefix: example-pod\r\n      levels:\r\n      level: DEBUG\r\n        reg_exp: \"Debug trace\"\r\n`\r\n\r\n\r\n\r\n\r\n\r\n","url":"/en/kubernetes/k8s/service-management/addons/advanced-installation/install-advanced-logaas-integration","description":"This section describes how to configure and install the Fluent Bit add-on for Cloud Logging (logaas-integration).","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"2b6ffc85-38a5-4a34-8021-13ac4fafc0b5","title":"Ingress NGINX","data":"{heading(Installing add-on)[id=installing_addon]}\n\n\n\nWhen installing the add-on, standard load balancers will be created for them.\n\nUsage of this load balancer is charged.\n\n\n\nSeveral installation options are available for the add-on.\n\nTake into account the total maximum system requirements of add-ons that will be placed on groups of worker nodes. If necessary, perform manual scaling groups of worker nodes or set up automatic scaling before installation.\n\n\n\nStandard installation\nInstallation on dedicated worker nodes\nQuick installation\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install addon button on the ingress-nginx add-on card.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n      add-on settings code.\n\n\n        An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Install Terraform and configure the environment if it is not already done.\n   Create a Terraform configuration file with data about the add-on being installed in the vkcs_kubernetes_addon block:\n\n      Get list of add-ons available for installation.\n      Get the add-on settings from the configuration_values parameter using the data source vcs_kubernetes_addon.\n      (Optional) To dynamically change the add-on parameters (for example, via CI), add the add-on settings to a separate yaml file. Use the templatefile function to add the required values.\n\n         Example of specifying an add-on\n\n                  resource \"vkcs_kubernetes_addon\" \"kube-ingress\" {\n            cluster_id = vkcs_kubernetes_cluster.k8s-cluster.id\n            addon_id = data.vkcs_kubernetes_addon.kube-ingress.id\n            namespace = \"kube-ingress\"\n            configuration_values = templatefile(\"./ingress-all.yaml\",{openstack-internal-load-balancer= \"false\"})\n\n            depends_on = [\n               vkcs_kubernetes_node_group.default_ng\n            ]\n         }\n\n\n   Check the Terraform configuration file for correctness:\n\n            terraform validate\n\n   Check out the planned changes:\n\n            terraform plan\n\n   Apply the changes:\n\n            terraform apply\n\n\nGet the IP address of the load balancer.\n\n\n\n\nPrepare a dedicated group of worker nodes to install the add-on, if it has not already been done:\n\n   Personal account\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Find the cluster you need in the list.\n\n   Make sure that the cluster has a dedicated group of worker nodes that will host add-ons.\n\n      If there is no such group — add it.\n\n   Customise for this node group, if it hasn't already been done:\n\n      Kubernetes labels: key addonNodes, value dedicated.\n      Node taints: effect NoSchedule, key addonNodes, value dedicated.\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click the Add addon button.\n   Click the Install addon button on the ingress-nginx add-on.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n      add-on settings code.\n\n   Set the necessary tolerations and nodeSelector in the add-on settings code:\n\n      Tolerations\n      nodeSelector\n\n\n            tolerations:\n        key: \"addonNodes\"\n          operator: \"Equal\"\n          value: \"dedicated\"\n          effect: \"NoSchedule\"\n\n      Set this toleration for fields:\n\n      controller.tolerations;\n      defaultBackend.tolerations.\n\n\n            nodeSelector:\n        addonNodes: dedicated\n\n      Set this selector for fields:\n\n      controller.nodeSelector;\n      defaultBackend.nodeSelector.\n\n\n      An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Use the instructions from the standard add-on installation. In the add-on settings, set the necessary exceptions (tolerations) and node selectors (nodeSelector).\n\n\nGet the IP address of the load balancer.\n\n\n\n\n\n\nDuring quick installation, the add-on settings code is not edited. A load balancer with a floating IP address will be created, and the Ingress controller will be accessible from the Internet.\n\nIf this does not suit you, perform a standard installation or installation on dedicated worker nodes.\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click the Add addon button.\n   Click the Install addon button on the ingress-nginx add-on.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Use the instructions from the standard add-on installation.\n\n\nGet the IP address of the load balancer.\n\n{heading(Editing add-on settings code during installation)[id=editing_addon_settings_code_during_installation]}\n\n\n\nEditing the add-on code is applicable for standard installation and installation on dedicated worker nodes.\nThe full add-on settings code along with the description of the fields is available on GitHub.\n\nChanging the load balancer type for the Ingress controller\n\nWhen installing an add-on with default parameters, a load balancer with a floating IP address is created, and the Ingress controller will be accessible from the Internet.\n\nTo prevent the Ingress controller from being accessible from the Internet, specify an annotation according to which an internal load balancer will be created:\n\nservice:\n  annotations:\n    {\n      \"loadbalancer.openstack.org/proxy-protocol\": \"true\",\n      \"service.beta.kubernetes.io/openstack-internal-load-balancer\": \"true\",\n    }\n\nAfter editing the add-on code continue installing the add-on.\n\nProhibition of deleting an Ingress controller node by the Autoscaler module\n\nThe Autoscaler module automatically scales the cluster: it adds nodes when the load increases, and removes when it decreases. To prevent a module from deleting the node on which the add-on is running, you need to specify a ban on deletion in the pod annotation:\n\ncontroller:\n  podAnnotations:\n    cluster-autoscaler.kubernetes.io/safe-to-evict: \"false\"\n\nAfter editing the add-on code continue installing the add-on.\n\nGetting the IP address of the load balancer\n\n\n\nThe following uses the service name ingress-nginx and the ingress-nginx namespace. If other parameters were selected when adding the add-on, adjust the commands.\n\n\n\n\n\nKubernetes Dashboard\nkubectl\n\n\n\nConnect to the cluster using Kubernetes Dashboard.\nIn the drop-down list next to the left of the search bar, select a namespace ingress-nginx.\nGo to Service → Services.\nFind in the list of services ingress-nginx-controller type LoadBalancer.\n\n   The External Endpoints column will display the floating IP address assigned to the load balancer.\n\n\n\n\nMake sure that you can connect to the cluster using kubectl.\n\nRun the command:\n\n      kubectl get svc ingress-nginx-controller -n ingress-nginx\n\n   The EXTERNAL-IP column will display the floating IP address assigned to the load balancer.\n\n\n\n","url":"/en/kubernetes/k8s/service-management/addons/advanced-installation/install-advanced-ingress","description":"The configuration and installation of the Ingress controller add-on is described here.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"ca36f6da-245d-4916-ab63-454216b11c22","title":"Istio","data":"Preparatory steps\n\nInstall the kube-prometheus-stack add-on.\n\n{heading(Installing add-on)[id=installing_addon]}\n\nSeveral installation options are available for the add-on.\n\nTake into account the total maximum system requirements of add-ons that will be placed on groups of worker nodes. If necessary, perform manual scaling groups of worker nodes or set up automatic scaling before installation.\n\n\n\nStandard installation\nInstallation on dedicated worker nodes\nQuick installation\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install addon button on the istio add-on card.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n      add-on settings code.\n\n\n        An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Install Terraform and configure the environment if it is not already done.\n   Create a Terraform configuration file with data about the add-on being installed in the vkcs_kubernetes_addon block:\n\n      Get list of add-ons available for installation.\n      Get the add-on settings from the configuration_values parameter using the data source vcs_kubernetes_addon.\n      (Optional) To dynamically change the add-on parameters (for example, via CI), add the add-on settings to a separate yaml file. Use the templatefile function to add the required values.\n\n         Example of specifying an add-on\n\n                  resource \"vkcs_kubernetes_addon\" \"istiod\" {\n            cluster_id = vkcs_kubernetes_cluster.k8s-cluster.id\n            addon_id = data.vkcs_kubernetes_addon.istiod.id\n            namespace = \"istio-system\"\n            configuration_values = templatefile(\"./istiod-all.yaml\",{istioNamespace = \"istio-system\"})\n\n            depends_on = [\n               vkcs_kubernetes_node_group.default_ng\n            ]\n         }\n\n\n   Check the Terraform configuration file for correctness:\n\n            terraform validate\n\n   Check out the planned changes:\n\n            terraform plan\n\n   Apply the changes:\n\n            terraform apply\n\n\nIf necessary, install the kiali add-on in addition.\n\n\n\n\nPrepare a dedicated group of worker nodes to install the add-on, if it has not already been done:\n\n   Personal account\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Find the cluster you need in the list.\n\n   Make sure that the cluster has a dedicated group of worker nodes that will host add-ons.\n\n      If there is no such group — add it.\n\n   Customise for this node group, if it hasn't already been done:\n\n      Kubernetes labels: key addonNodes, value dedicated.\n      Node taints: effect NoSchedule, key addonNodes, value dedicated.\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click the Add addon button.\n   Click the Install addon button on the istio add-on.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n      add-on settings code.\n\n   Set the necessary tolerations and nodeSelector in the add-on settings code:\n\n      Tolerations\n      nodeSelector\n\n\n            tolerations:\n        key: \"addonNodes\"\n          operator: \"Equal\"\n          value: \"dedicated\"\n          effect: \"NoSchedule\"\n\n      Set this toleration for the tolerations field.\n\n\n            nodeSelector:\n        addonNodes: dedicated\n\n      Set this selector for the nodeSelector field.\n\n\n      An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Use the instructions from the standard add-on installation. In the add-on settings, set the necessary exceptions (tolerations) and node selectors (nodeSelector).\n\n\nIf necessary, install the kiali add-on in addition.\n\n\n\n\n\n\nDuring quick installation, the add-on settings code is not edited.\n\nIf this does not suit you, perform a standard installation or installation on dedicated worker nodes.\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click the Add addon button.\n   Click the Install addon button on the istio add-on.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Use the instructions from the standard add-on installation.\n\n\nIf necessary, install the kiali add-on in addition.\n\n\n\n","url":"/en/kubernetes/k8s/service-management/addons/advanced-installation/install-advanced-istio","description":"The configuration and installation of the Istio add-on is described here.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"e8a92c6b-b604-4499-a28e-3c6922f1e08e","title":"Jaeger","data":"As part of the add-on there is Jaeger collector, which requires storage to work. As a storage backend the Jaeger add-on from VK Cloud uses Elasticsearch, which is deployed in the form of several replicas.\n\n{heading(Installing add-on)[id=installing_addon]}\n\nSeveral installation options are available for the add-on.\n\nTake into account the total maximum system requirements of add-ons that will be placed on groups of worker nodes.\n\nJaeger add-on system requirements depend on the selected number of Elasticsearch replicas and the cluster environment. The minimum number of replicas is two, the default is three. Their number can be changed during the standard installation or installation on dedicated worker nodes.\n\nIf necessary, perform manual scaling for groups of worker nodes or configure automatic scaling before install.\n\n\n\nStandard installation\nInstallation on dedicated worker nodes\nQuick installation\n\n\n\nInstall the add-on:\n\n   Personal account\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install addon button on the jaeger add-on card.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed.\n\n   Edit the add-on settings code if:\n\n      you need a non-standard number of Elasticsearch replicas;\n      the master nodes and worker nodes are located in different availability zones.\n\n\n      An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n(Optional) Connect to the Query UI.\n(Optional) Get to know the practical guide for using Jaeger with the Hot R.O.D. microservice application, the manual shows:\n\n   Integration of OpenTelemetry into a microservice application so that it sends the data needed for query tracing to Jaeger.\n   Visualization and interpretation of the data collected by Jaeger using the Query UI.\n\n\n\n\nPrepare a dedicated group of worker nodes to install the add-on, if it has not already been done:\n\n   Personal account\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Find the cluster you need in the list.\n\n   Make sure that the cluster has a dedicated group of worker nodes that will host add-ons.\n\n      If there is no such group — add it.\n\n   Customise for this node group, if it hasn't already been done:\n\n      Kubernetes labels: key addonNodes, value dedicated.\n      Node taints: effect NoSchedule, key addonNodes, value dedicated.\n\n\nInstall the add-on:\n\n   Personal account\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click the Add addon button.\n   Click the Install addon button on the jaeger add-on.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed.\n\n   Edit the add-on settings code if:\n\n      you need a non-standard number of Elasticsearch replicas;\n      the master nodes and worker nodes are located in different availability zones.\n\n   Set the necessary tolerations and nodeSelector in the add-on settings code:\n\n      Tolerations\n      nodeSelector\n\n\n            tolerations:\n        key: \"addonNodes\"\n          operator: \"Equal\"\n          value: \"dedicated\"\n          effect: \"NoSchedule\"\n\n      Set this exception for fields:\n\n      elasticsearch.tolerations;\n      agent.tolerations;\n      collector.tolerations;\n      query.tolerations.\n\n\n            nodeSelector:\n        addonNodes: dedicated\n\n      Set this selector for fields:\n\n      elasticsearch.nodeSelector;\n      agent.nodeSelector;\n      collector.nodeSelector;\n      query.nodeSelector.\n\n\n      An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n(Optional) Connect to the Query UI.\n(Optional) Get to know the practical guide for using Jaeger with the Hot R.O.D. microservice application, the manual shows:\n\n   Integration of OpenTelemetry into a microservice application so that it sends the data needed for query tracing to Jaeger.\n   Visualization and interpretation of the data collected by Jaeger using the Query UI.\n\n\n\n\n\n\nTo install the add-on in this way, it is necessary that the master nodes and the worker nodes are in the same availability zone.\n\nDuring quick installation, the add-on settings code is not edited. Three Elasticsearch replicas will be used as the storage backend.\n\nIf this does not suit you, perform a standard installation or installation on dedicated worker nodes.\n\n\n\nInstall the add-on:\n\n   Personal account\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click the Add addon button.\n   Click the Install addon button on the jaeger add-on.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n(Optional) Connect to the Query UI.\n(Optional) Get to know the practical guide for using Jaeger with the Hot R.O.D. microservice application, the manual shows:\n\n   Integration of OpenTelemetry into a microservice application so that it sends the data needed for query tracing to Jaeger.\n   Visualization and interpretation of the data collected by Jaeger using the Query UI.\n\n{heading(Editing add-on settings code during installation)[id=editing_addon_settings_code_during_installation]}\n\nEditing the add-on code is applicable for standard installation and installation on dedicated worker nodes.\n\nThe full add-on settings code along with the description of the fields is available on GitHub.\n\n\n\nDo not delete the podAnnotations.timestamp fields or the values set in them. These fields are required for correct installation and operation of the add-on.\n\nChanging the number of Elasticsearch replicas\n\nTo set the required number of replicas, change the value of the field in the add-on settings code:\n\nelasticsearch:\n  replicas:\n\nMake sure that the number of worker nodes in the cluster is not less than the selected number of replicas.\n\nChanging Elasticsearch storage settings\n\nElasticsearch replicas are hosted on the worker nodes of the cluster and use persistent volumes as a storage. By default, these persistent volumes are located in the same availability zone in which the cluster's master nodes are located. If the cluster worker nodes and persistent volumes are located in different availability zones, then replicas on these nodes will not be able to work with volumes.\n\nTo ensure that persistent volumes work with Elasticsearch replicas, set the storage class, the availability zone of which coincides with the availability zone of worker nodes:\n\nelasticsearch:\n  volumeClaimTemplate:\n    accessModes:\n    ReadWriteOnce\n    storageClassName: \"\"\n\nAfter editing the add-on code continue installing the add-on.\n","url":"/en/kubernetes/k8s/service-management/addons/advanced-installation/install-advanced-jaeger","description":"This section describes the setup and installation of the Jaeger add-on.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"ba0e9837-0664-4382-aa74-de8184aec77d","title":"Kiali","data":"Preparatory steps\n\nInstall the istio add-on.\n\n{heading(Installing add-on)[id=installing_addon]}\n\nSeveral installation options are available for the add-on.\n\nTake into account the total maximum system requirements of add-ons that will be placed on groups of worker nodes. If necessary, perform manual scaling groups of worker nodes or set up automatic scaling before installation.\n\n\n\nStandard installation\nInstallation on dedicated worker nodes\nQuick installation\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install addon button on the kiali add-on card.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n      add-on settings code.\n\n\n        An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Install Terraform and configure the environment if it is not already done.\n   Create a Terraform configuration file with data about the add-on being installed in the vkcs_kubernetes_addon block:\n\n      Get list of add-ons available for installation.\n      Get the add-on settings from the configuration_values parameter using the data source vcs_kubernetes_addon.\n      (Optional) To dynamically change the add-on parameters (for example, via CI), add the add-on settings to a separate yaml file. Use the templatefile function to add the required values.\n\n         Example of specifying an add-on\n\n                  resource \"vkcs_kubernetes_addon\" \"kiali-server\" {\n            cluster_id = vkcs_kubernetes_cluster.k8s-cluster.id\n            addon_id = data.vkcs_kubernetes_addon.kiali-server.id\n            namespace = \"istio-system\"\n            configuration_values = templatefile(\"./kiali-server-all.yaml\",{grafana_username = \"\", grafana_password = \"\", istio_namespace = \"istio-system\"})\n\n            depends_on = [\n               vkcs_kubernetes_node_group.default_ng\n            ]\n         }\n\n\n   Check the Terraform configuration file for correctness:\n\n            terraform validate\n\n   Check out the planned changes:\n\n            terraform plan\n\n   Apply the changes:\n\n            terraform apply\n\n\nConnect to Kiali.\n\n\n\n\nPrepare a dedicated group of worker nodes to install the add-on, if it has not already been done:\n\n   Personal account\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Find the cluster you need in the list.\n\n   Make sure that the cluster has a dedicated group of worker nodes that will host add-ons.\n\n      If there is no such group — add it.\n\n   Customise for this node group, if it hasn't already been done:\n\n      Kubernetes labels: key addonNodes, value dedicated.\n      Node taints: effect NoSchedule, key addonNodes, value dedicated.\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click the Add addon button.\n   Click the Install addon button on the kiali add-on.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n      add-on settings code.\n\n   Set the necessary tolerations and nodeSelector in the add-on settings code:\n\n      Tolerations\n      nodeSelector\n\n\n            tolerations:\n        key: \"addonNodes\"\n          operator: \"Equal\"\n          value: \"dedicated\"\n          effect: \"NoSchedule\"\n\n      Set this toleration for the tolerations field.\n\n\n            nodeSelector:\n        addonNodes: dedicated\n\n      Set this selector for the nodeSelector field.\n\n\n      An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Use the instructions from the standard add-on installation. In the add-on settings, set the necessary exceptions (tolerations) and node selectors (nodeSelector).\n\n\nConnect to Kiali.\n\n\n\n\n\n\nDuring quick installation, the add-on settings code is not edited. Grafana integration will not be available.\n\nIf this does not suit you, perform a standard installation or installation on dedicated worker nodes.\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click the Add addon button.\n   Click the Install addon button on the kiali add-on.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Use the instructions from the standard add-on installation.\n\n\nConnect to Kiali.\n\n{heading(Editing add-on settings code during installation)[id=editing_addon_settings_code_during_installation]}\n\n\n\nEditing the add-on code is applicable for standard installation and installation on dedicated worker nodes.\n\nSetting the password to integrate with Grafana\n\nWhen installing an add-on with default parameters, integration with Grafana will be unavailable.\n\nTo allow the integration, specify the admin Grafana user's password during the add-on installation. To do this, change the value of the field in the add-on settings code:\n\nexternal_services:\n  grafana:\n    auth:\n      password: \"\"\n\nAfter editing the add-on code continue installing the add-on.\n","url":"/en/kubernetes/k8s/service-management/addons/advanced-installation/install-advanced-kiali","description":"The configuration and installation of the Kiali add-on is described here.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"28fa4aa2-45fb-4d94-b6f2-aa1d1d556b4e","title":"Kube Prometheus Stack","data":"{heading(Installing add-on)[id=installing_addon]}\n\nSeveral installation options are available for the add-on.\n\nTake into account the total maximum system requirements of add-ons that will be placed on groups of worker nodes. If necessary, perform manual scaling groups of worker nodes or set up automatic scaling before installation.\n\n\n\nStandard installation\nInstallation on dedicated worker nodes\nQuick installation\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install addon button on the kube-prometheus-stack add-on card.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n      add-on settings code.\n\n\n        An incorrectly set settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Install Terraform and configure the environment if it is not already done.\n   Create a Terraform configuration file with data about the add-on being installed in the vkcs_kubernetes_addon block:\n\n      Get list of add-ons available for installation.\n      Get the add-on settings from the configuration_values parameter using the data source vcs_kubernetes_addon.\n      (Optional) To dynamically change the add-on parameters (for example, via CI), add the add-on settings to a separate yaml file. Use the templatefile function to add the required values.\n\n         Example of specifying an add-on\n\n                  resource \"vkcs_kubernetes_addon\" \"kube-prometheus-stack\" {\n            cluster_id = vkcs_kubernetes_cluster.k8s-cluster.id\n            addon_id = data.vkcs_kubernetes_addon.kube-prometheus-stack.id\n            namespace = \"prometheus-monitoring\"\n            configuration_values = templatefile(\"./kube-prometheus-stack-all.yaml\",{openstack-internal-load-balancer= \"false\"})\n\n            depends_on = [\n               vkcs_kubernetes_node_group.default_ng\n            ]\n         }\n\n\n   Check the Terraform configuration file for correctness:\n\n            terraform validate\n\n   Check out the planned changes:\n\n            terraform plan\n\n   Apply the changes:\n\n            terraform apply\n\n\nIf necessary change the Prometheus disk size.\nIf necessary get the password for Grafana from the Kubernetes secret.\n\n\n\n\nPrepare a dedicated group of worker nodes to install the add-on, if it has not already been done:\n\n   Personal account\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Find the cluster you need in the list.\n\n   Make sure that the cluster has a dedicated group of worker nodes that will host add-ons.\n\n      If there is no such group — add it.\n\n   Customise for this node group, if it hasn't already been done:\n\n      Kubernetes labels: key addonNodes, value dedicated.\n      Node taints: effect NoSchedule, key addonNodes, value dedicated.\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install addon button on the kube-prometheus-stack add-on card.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n      add-on settings code.\n\n   Set the necessary tolerations and nodeSelector in the add-on settings code:\n\n      Ttolerations\n      nodeSelector\n\n\n            tolerations:\n        key: \"addonNodes\"\n          operator: \"Equal\"\n          value: \"dedicated\"\n          effect: \"NoSchedule\"\n\n      Set this toleration for fields:\n\n      grafana.tolerations;\n      alertmanager.alertmanagerSpec.tolerations;\n      prometheusOperator.tolerations;\n      prometheusOperator.admissionWebhooks.patch.tolerations;\n      prometheus.prometheusSpec.tolerations;\n      kube-state-metrics.tolerations.\n\n\n            nodeSelector:\n        addonNodes: dedicated\n\n      Set this selector for fields:\n\n      grafana.nodeSelector;\n      alertmanager.alertmanagerSpec.nodeSelector;\n      prometheusOperator.nodeSelector;\n      prometheusOperator.admissionWebhooks.patch.nodeSelector;\n      prometheus.prometheusSpec.nodeSelector;\n      kube-state-metrics.nodeSelector.\n\n\n      An incorrectly specified settings code can lead to errors during installation or the add-on is inoperable.\n\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Use the instructions from the standard add-on installation. In the add-on settings, set the necessary exceptions (tolerations) and node selectors (nodeSelector).\n\n\nIf necessary change the Prometheus disk size.\nIf necessary get the password for Grafana from the Kubernetes secret.\n\n\n\n\n\n\nDuring quick installation, the add-on settings code is not edited. A Kubernetes secret will be created, containing a permanent password to sign in to the Grafana web interface.\n\nIf this does not suit you, perform a standard installation or installation on dedicated worker nodes.\n\n\n\nInstall the add-on:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select project, where the cluster will be placed.\n   Go to Containers → Kubernetes clusters.\n   Click on the name of the required cluster.\n   Go to Addons tab.\n   If there are already installed add-ons in the cluster, click on the Add addon button.\n   Click the Install addon button on the kube-prometheus-stack add-on card.\n   Edit if necessary:\n\n      application name;\n      the name of the namespace where the add-on will be installed;\n\n   Click the Install addon button.\n\n      The installation of the add-on in the cluster will begin. This process can take a long time.\n\n\n   Use the instructions from the standard add-on installation.\n\n\nIf necessary change the Prometheus disk size.\nGet the password for Grafana from the Kubernetes secret.\n\n{heading(Editing add-on settings code during installation)[id=editing_addon_settings_code_during_installation]}\n\n\n\nEditing the add-on code is applicable for standard installation and installation on dedicated worker nodes.\nThe full add-on settings code along with the description of the fields is available on GitHub.\n\nSetting temporary password for Grafana web interface\n\nWhen installing an add-on with default parameters, a Kubernetes secret will be created containing a permanent password to sign in to the Grafana web interface.\n\nAlso, when installing the add-on, you can specify a temporary user password. In this case, the first authorization to the Grafana web interface is performed with this password, then you will be prompted to change it. To do this, change the value of the field in the add-on settings code:\n\ngrafana:\n  adminPassword: \"\"\n\nAfter editing the add-on code continue installing the add-on.\n\nChanging Prometheus disk size\n\nThis operation is available if the monitoring add-on kube-prometheus-stack is installed in the cluster.\n\nThe Prometheus disk stores cluster monitoring data. If there is not enough space for them, or you want to increase the performance of the Prometheus disk, increase the disk size.\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect project, where the cluster will be placed.\nGo to Containers → Kubernetes clusters.\nClick on the name of the required cluster.\nGo to Addons tab.\nClick   for the kube-prometheus-stack add-on and select Change Prometheus disk size.\nSet the required disk size. The operation works only in the direction of increase.\nClick the Confirm button.\n\nGetting Grafana password from Kubernetes secret\n\nIf the add-on was installed without specifying a temporary password, the password value for entering the Grafana web interface can be obtained from the Kubernetes secret.\n\n\n\nIf, when adding an add-on, a service name other than kube-prometheus-stack or a namespace other than prometheus-monitoring were selected, adjust the steps below.\n\n\n\n\n\nKubernetes Dashboard\nkubectl\n\n\n\nConnect to the cluster via Kubernetes Dashboard.\nIn the drop-down list next to the left of the search bar, select a namespace prometheus-monitoring.\nGo to Config and Storage → Secrets.\nFind kube-prometheus-stack-grafana in the list of secrets and click on the secret name.\nIn the Data block, click on the eye icon next to the admin-password parameter.\n\n   The password will be displayed.\n\n\n\n\nMake sure that you can connect to the cluster using kubectl.\n\nGet the password to sign in to Grafana from the Kubernetes secret:\n\n   Windows (PowerShell)\n   Linux (bash)/macOS (zsh)\n\n\n      $ENCODED = kubectl -n prometheus-monitoring get secret kube-prometheus-stack-grafana -o jsonpath='{.data.admin-password}'; `\n   [System.Text.Encoding]::Utf8.GetString([System.Convert]::FromBase64String($ENCODED)) | Write-Output\n\n\n      kubectl -n prometheus-monitoring get secret kube-prometheus-stack-grafana -o jsonpath='{.data.admin-password}' | base64 --decode\n\n\nResetting Grafana password\n\nIf the add-on was installed without specifying a temporary password, the password value for entering the Grafana web interface can be obtained from the Kubernetes secret. If this secret has been lost, you can reset the password to access Grafana again.\n\n\n\nFurther the kube-prometheus-stack service name and the prometheus-monitoring namespace are used. If other parameters have been selected when adding the add-on, modify the commands accordingly.\n\n\n\nGet the name of the Grafana pod:\n\n      kubectl -n prometheus-monitoring get pod -l app.kubernetes.io/name=grafana\n\n   The format of the pod name from the command output:\n\n      kube-prometheus-stack-grafana-XXXXXXXXX-XXXXX\n\nReset the password by executing the command inside the Grafana pod:\n\n      kubectl -n prometheus-monitoring exec  -- sh -c \"grafana cli --debug admin reset-admin-password \"\n\n   If the password is successfully reset, the command output will contain the following message: Admin password changed successfully ✔.\n","url":"/en/kubernetes/k8s/service-management/addons/advanced-installation/install-advanced-monitoring","description":"It describes the configuration and installation of an add-on with monitoring tools.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"5920466e-99d4-474a-b19a-d9a286815876","title":"Scaling cluster","data":"You can perform Cloud Containers cluster scaling manually or set up automatic scaling to adapt the cluster to the changing needs of workloads.\n\nVertical scaling\n\nThis type of scaling is applicable for master nodes and groups of worker nodes. During the scaling process, virtual machine templates are changed for cluster nodes, the number of nodes remains the same. If you need to change the number of worker nodes in a group, perform horizontal scaling\n\n{heading(Scaling of master nodes)[id=scale_master_nodes]}\n\nLearn how the vertical scaling mechanism works.\n\nMake sure that there are enough quotas for scaling.\n\nPerform the scaling.\n\n\n   During the scaling process, the virtual machines hosting the master nodes will be restarted sequentially.\n\n   If the cluster contains one master node, then the Kubernetes API will be unavailable during scaling.\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select the project where the necessary cluster is located.\n   Go to Containers → Kubernetes Clusters.\n   Make sure that cluster needed is running.\n   Click   for the required cluster and select Change type of master virtual machine.\n   Select the required VM template from the drop-down list.\n\n\n      Templates with high-performance CPUs are available upon request to support. To take advantage of these templates, select the \"Show high performance CPUs only\" option.\n\n      See Available computing resources for details.\n\n\n   Click the Save button.\n\n\n   Make sure that OpenStack client is installed and authenticate to the project.\n\n   Install Terraform and configure the environment if it is not already done.\n\n   Make sure that cluster needed is running.\n\n   Determine a new virtual machine type to be used for the cluster master nodes:\n\n      Run the command:\n\n                  openstack flavor list\n\n         The available virtual machine types will be displayed.\n\n      Select the required virtual machine type and write its name from the Name column.\n\n   Change the vkcs_compute_flavor data source in the Terraform configuration file:\n\nAlready existing data source with virtual machine type for the cluster\n      data \"vkcs_compute_flavor\" \"k8s-master-flavor\" {\n         name = \"\"\n      }\n\nAlready described cluster configuration\n      resource \"vkcs_kubernetes_cluster\" \"k8s-cluster\" {\n         name                = \"k8s-cluster\"\n         master_flavor       = data.vkcs_compute_flavor.k8s-master-flavor.id\n        ...\n\n      }\n\n   Check correctness of the Terraform configuration file:\n\n            terraform validate\n\n   Familiarize yourself with the planned changes:\n\n            terraform plan\n\n   Apply the planned changes:\n\n            terraform apply\n\n\n{heading(Scaling groups of worker nodes)[id=scale_worker_nodes]}\n\nLearn how the vertical scaling mechanism works.\n\nPrepare to scale:\n\n   Make sure that there are enough quotas for scaling.\n   If you plan to reduce the amount of computing resources, then make sure that the total amount of resources in the worker node group will be enough to accommodate the workload.\n   Make sure that replication is configured for the workload and replicas are distributed across multiple worker nodes from the node group.\n\n      If there is only one worker node in the node group, increase the number of nodes in the group and configure replication if possible.\n\nPerform the scaling.\n\n\n   During the scaling process, the virtual machines hosting the worker nodes will be restarted sequentially.\n\n   Workloads for which replication has not been configured will not be available during scaling.\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select the project where the necessary cluster is located.\n   Go to Containers → Kubernetes Clusters.\n   Make sure that cluster needed is running.\n   Locate the necessary cluster and node group.\n   Click   for the required node group and select Change type of master virtual machine.\n   Select the required VM template from the drop-down list.\n\n\n      Templates with high-performance CPUs are available upon request to support. To take advantage of these templates, select the \"Show high performance CPUs only\" option.\n\n      See Available computing resources for details.\n\n\n   Click the Save button.\n\n\n   Make sure that OpenStack client is installed and authenticate to the project.\n\n   Install Terraform and configure the environment if it is not already done.\n\n   Make sure that cluster needed is running.\n\n   Define a new type of VM to be used for worker nodes in the cluster node group:\n\n      Run the command:\n\n                  openstack flavor list\n\n         The available virtual machine types will be displayed.\n\n      Select the required virtual machine type and write its name from the Name column.\n\n   Change the vkcs_compute_flavor data source in the Terraform configuration file:\n\nAn existing data source with a VM type for a group of worker nodes\n      data \"vkcs_compute_flavor\" \"k8s-node-group-flavor\" {\n         name = \"\"\n      }\n\nThe configuration already described for the node group\n      resource \"vkcs_kubernetes_node_group\" \"k8s-node-group\" {\n        name = \"k8s-node-group\"\n        cluster_id = vkcs_kubernetes_cluster.k8s-cluster.id\n        flavor_id = data.vkcs_compute_flavor.k8s-node-group-flavor.id\n        ...\n\n      }\n\n   Check correctness of the Terraform configuration file:\n\n            terraform validate\n\n   Familiarize yourself with the planned changes:\n\n            terraform plan\n\n   Apply the planned changes:\n\n            terraform apply\n\n\nHorizontal scaling\n\nThis type of scaling is applicable for groups of worker nodes. During the scaling process, the number of worker nodes in the group changes, virtual machine templates for worker nodes remain the same. If you need to change these templates for master nodes or worker nodes, perform vertical scaling.\n\nScaling groups of worker nodes\n\nLearn how the horizontal scaling mechanism works.\n\nMake sure that there are enough quotas for scaling.\n\nPerform the scaling.\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select the project where the necessary cluster is located.\n   Go to Containers → Kubernetes Clusters.\n   Make sure that cluster needed is running.\n   Find the appropriate node group in this cluster.\n   Click   for the required node group and select Scaling settings.\n   In the window that appears:\n\n      Make sure that the Enable autoscaling option is disabled.\n      Set the required number of nodes. It can be changed both up and down.\n      Click the Save changes button.\n\n\n   Install Terraform and configure the environment if it is not already done.\n\n   Make sure that cluster needed is running.\n\n   Change the vkcs_kubernetes_node_group data source in the Terraform configuration file:\n\n            ...\n\nThe configuration already described for the node group\n      resource \"vkcs_kubernetes_node_group\" \"k8s-node-group\" {\n        name = \"k8s-node-group\"\n\n        ...\n\nMake sure that the autoscaling option is disabled (false).\n        autoscaling_enabled = false\n\n        node_count =\n\n        ...\n\n      }\n      ...\n\n   Check correctness of the Terraform configuration file:\n\n            terraform validate\n\n   Familiarize yourself with the planned changes:\n\n            terraform plan\n\n   Apply the planned changes:\n\n            terraform apply\n\n\n{heading(Configuring automatic scaling for worker node groups)[id=autoscale_worker_nodes]}\n\nLearn how the horizontal scaling mechanism works.\n\nMake sure that there are enough quotas for scaling.\n\nSet up automatic scaling:\n\n   Personal account\n   Terraform\n\n\n   Go to VK Cloud personal account.\n   Select the project where the necessary cluster is located.\n   Go to Containers → Kubernetes Clusters.\n   Make sure that cluster needed is running.\n   Find the appropriate node group in this cluster.\n   Click   for the required node group and select Scaling settings.\n   In the dialog window:\n\n      Make sure that the Enable autoscaling option is enabled.\n      Set the necessary number of nodes. It can be changed both upward and downward.\n      Click the Save changes button.\n\n\n   Install Terraform and configure the environment if it is not already done.\n\n   Make sure that cluster needed is running.\n\n   Change the vkcs_kubernetes_node_group data source in the Terraform configuration file:\n\n            ...\n\nThe configuration already described for the node group\n      resource \"vkcs_kubernetes_node_group\" \"k8s-node-group\" {\n        name = \"k8s-node-group\"\n\n        ...\n\nMake sure that the autoscaling option is enabled (true)\n        autoscaling_enabled = true\n\nSet the number of nodes within which the scaling will be performed\n        min_nodes =\n        max_nodes =\n\n        ...\n\n      }\n      ...\n\n   Check correctness of the Terraform configuration file:\n\n            terraform validate\n\n   Familiarize yourself with the planned changes:\n\n            terraform plan\n\n   Apply the planned changes:\n\n            terraform apply\n\n","url":"/en/kubernetes/k8s/service-management/scale","description":"How to change the VM type for a node or the number of nodes in a cluster.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"0f284c49-60f7-480e-8283-529ee9808251","title":"Updating cluster","data":"If the cluster uses not the latest supported version of Kubernetes, it can be upgraded. At this, installed add-ons and a part of its components are also upgraded.\n\nYou cannot downgrade to a lower version of Kubernetes when upgrading. Clusters of versions 1.16 and below can only be upgraded by transferring a backup of data to a new cluster of the correct version, for example using Velero.\n\n\n\nIf a component, that is to be updated with the cluster, is deleted, then it will be restored during the next cluster update.\n\nWhen the cluster is updating, the current CoreDNS Corefile is overwritten with the new one with default settings.\n\n  If the cluster uses a modified Corefile, then back it up prior to updating the cluster.\n\nBefore updating\n\nExamine update procedure.\nBack up the cluster you plan to update, for example, using Velero.\nDeploy the new cluster from the backup which is identical to the one you plan to update. Update it and verify that all cluster data and applications remain available, and applications behave as expected.\nSet threshold of the number of unavailable worker nodes in the node groups.\n\nDo update\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect the project where the necessary cluster is located.\nGo to Containers → Kubernetes Clusters.\nClick   for the required cluster and select Update version.\nIn the window that appears, select the required version.\nReview the version changelog.\nClick the Change version button.\n\n\n\n","url":"/en/kubernetes/k8s/service-management/update","description":"A Kubernetes cluster can be updated in two ways: via personal account or via Terraform. This section discusses both methods in detail.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"85f4300c-63a8-4e52-8dc6-58f02a0c0e83","title":"Using Terraform","data":"How to start using Terraform\n\nInstall Terraform and configure the environment if it is not already done.\nCreate a new cluster with one or more worker node groups.\nManage the created resources using Terraform.\n\nSee the Terraform provider documentation for more information on working with the container service.\n\nFeatures of using Terraform to manage the container service\n\nSome cluster operations are performed only in personal account. For each operation, the step-by-step instructions indicate whether it can also be performed using Terraform.\n\nChanging some parameters of an existing cluster will create a new cluster. The container service documentation lists only operations that can be performed on an existing cluster without creating a new cluster.\n\n  See Terraform provider documentation for details.\n\nIf you change the settings of a cluster that is managed by Terraform in your personal account, those changes will not be reflected in the Terraform configuration file.\n\n  In this case, the current Terraform configuration will become obsolete, and applying it with terraform apply will cause the settings made to be rolled back or even render the cluster inoperable.\n\n  To manage such a cluster again using Terraform:\n\n  Update the local infrastructure state (stored in a file with the extension .tfstate):\n\n     Run the command:\n\n                terraform apply -refresh-only\n\n        You will be prompted for infrastructure changes that will overwrite the local state.\n\n     If you are satisfied with these changes, confirm them.\n\n  Update the configuration file to match the Terraform state. This procedure is described in detail in the Create configuration section of the Terraform documentation.\n","url":"/en/kubernetes/k8s/service-management/helpers/terraform-howto","description":"How to use Terraform when performing basic operations.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"25670d2d-c331-4ae0-a577-0949c153988a","title":"Worker node group settings","data":"These settings are set when creating a cluster or when adding a worker node group to an existing cluster:\n\nName groups:** must start with a letter. Can only consist of lowercase Latin letters, numbers, and hyphens - as a separator.\nWorker node settings:\n\n  Node type: virtual machine template for worker-nodes.\n\n    Templates with high-performance CPUs are available upon request to support. To take advantage of these templates, select the \"Show high performance CPUs only\" option.\n\n    See Available computing resources for details.\n\n  Availability zone: availability zone for nodes.\n\n    For high availability, it is recommended to create several groups of nodes in different availability zones and place application replicas on these nodes so that the replicas are also in different availability zones.\n\n  Storage settings:\n\n    Disk type: storage type to be used by nodes.\n\n\n      The disk type you select affects the performance of the cluster. It is recommended to use SSD or High-IOPS disk type for clusters that run in a production environment or in high load environments.\n\n\n    Disk size: the larger the disk size, the better its performance in some disk operations.\n\n  Number of nodes: at least one node. One node does not provide high availability on the level of an individual group of nodes, two nodes or more do.\n\n  Enable autoscaling: enable this option to allow auto-scaling the number of nodes in the group. Then set the minimum and maximum number of nodes. Within these limits the scaling will be performed.\n\n  Percentage of unavailable nodes when updating the cluster version: the percentage number of nodes that can be taken out of the node group when cluster update procedure is performed.\n\n  Kubernetes parameters: labels, taints and tolerations.\n\n    More about labels and taints.\n","url":"/en/kubernetes/k8s/service-management/helpers/node-group-settings","description":"List of all worker node group settings.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"4f192b57-fd6e-427e-a304-1910904dee8a","title":"Deploying Ingress controller with TCP load balancer","data":"The Ingress controller can be deployed in conjunction with the TCP load balancer of the VK Cloud platform. As an example, a simple demo application and an Ingress resource will be deployed to test the operation of the controller.\n\n\n\nIt is assumed throughout the document that the NGINX Ingress Controller will be deployed. However, the proposed approaches can be adapted to other Ingress controllers such as Traefik.\n\n1. Preparatory steps\n\nCreate a Kubernetes cluster of the most current version.\n\n   When creating the cluster select the Assign external IP option.\n\n   Select other cluster settings at your discretion.\n\nMake sure that the NGINX Ingress add-on (ingress-nginx) is not installed in the cluster. For demonstration purposes, the Ingress controller will be installed manually.\n\nMake sure that you can connect to the cluster using `kubectl'.\n\nInstall Helm if the utility is not already installed.\n\nInstall curl if the utility is not already installed.\n\n2. Deploy demo applications\n\nThese applications will be accessed through the Ingress controller using the Ingress resource.\n\nThe tea and coffee applications from NGINX's Cafe example will be used for the demo. Each application consists of a ReplicaSet, a Deployment and a Service corresponding to that Deployment.\n\nTo deploy the demo applications:\n\nDownload the [cafe.yaml] manifest(https://raw.githubusercontent.com/nginxinc/kubernetes-ingress/v2.4.0/examples/ingress-resources/complete-example/cafe.yaml).\n\nApply this manifest to the cluster:\n\n      kubectl apply -f ./cafe.yaml\n\nTo check the status of the application components, run the command:\n\nkubectl get svc,rs,deployment -n default\n\nThe output of the command should be similar to this one:\n\nNAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\nservice/coffee-svc   ClusterIP   ...                      80/TCP    ...\nservice/tea-svc      ClusterIP   ...                      80/TCP    ...\n\nNAME                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/coffee-7c86d7d67c   2         2         2       ...\nreplicaset.apps/tea-5c457db9        3         3         3       ...\n\nNAME                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/coffee   2/2     2            2           ...\ndeployment.apps/tea      3/3     3            3           ...\n\n3. Install Ingress Controller\n\nWhen installing, select PROXY operation mode as it is necessary for full interaction with the TCP load balancer. If you install an Ingress controller without support for this protocol, the controller will not be able to handle headers that contain information about the request source.\n\nTo install an NGINX Ingress Controller with PROXY protocol support:\n\nAdd the NGINX Helm repository:\n\n   Linux/macOS\n   Windows\n\n\n      helm repo add nginx-stable https://helm.nginx.com/stable\n   helm repo update\n\n\n      helm repo add nginx-stable https://helm.nginx.com/stable; `\n   helm repo update\n\n\nInstall a PROXY-enabled Ingress controller by running the command:\n\n      helm install nginx-ingress-tcp nginx-stable/nginx-ingress --set-string 'controller.config.entries.use-proxy-protocol=true' --create-namespace --namespace example-nginx-ingress-tcp\n\nWait until the installation of the Ingress controller is complete and the controller receives an external IP address.\n\n   To check the status of the Ingress controller, run the command:\n\n      kubectl get svc -n example-nginx-ingress-tcp\n\n   The output of the command should be similar to this one:\n\n      NAME                              TYPE           CLUSTER-IP    EXTERNAL-IP                                   PORT(S)    AGE\n   nginx-ingress-tcp-nginx-ingress   LoadBalancer   ...               ...        ...\n\nCheck if the Ingress controller is working by browsing to http://. If the controller is configured correctly, a page with the status of HTTP 404 will be displayed.\n\n4. Create an Ingress resource\n\nThe Ingress resource will publish the coffee-svc and tea-svc services through the Ingress controller, thus providing access to the applications.\n\nThe following is a demonstration of how to create an Ingress resource with SSL/TLS sessions termination on the Ingress controller.\n\nTo create an Ingress resource:\n\nCreate a Kubernetes secret that will contain the certificate data. This will be used by the Ingress controller when handling HTTPS traffic.\n\n   This secret contains the public and private parts of the NGINX self-signed certificate, which is used to access the published applications in the cafe.example.com domain.\n\n\n   The private part of this certificate is publicly available on the Internet, so do not use this certificate to protect real applications in a production environment.\n\n   .\n\nDownload the [cafe-secret.yaml] manifest (https://raw.githubusercontent.com/nginxinc/kubernetes-ingress/v2.4.0/examples/ingress-resources/complete-example/cafe-secret.yaml).\n\nApply this manifest to the cluster:\n\n      kubectl apply -f ./cafe-secret.yaml\n\n   The cafe-secret secret will be created.\n\nCheck that the secret has been successfully created by running the command:\n\n      kubectl describe secret cafe-secret\n\n   The basic information about the secret will be shown.\n\nCreate an Ingress resource which will handle incoming requests to host cafe.example.com:\n\n   Download the cafe-ingress.yaml manifest.\n\n   Apply this manifest to the cluster:\n\n            kubectl apply -f ./cafe-ingress.yaml\n\n        The cafe-ingress Ingress resource will be created.\n\n   Check that the resource was successfully created by running the command:\n\n      kubectl describe ingress cafe-ingress\n\n   The output of the command should be similar to this one:\n\n      Name:             cafe-ingress\n   Labels:\n   Namespace:        default\n   Address:\n   Ingress Class:    nginx\n   Default backend:\n   TLS:\n     cafe-secret terminates cafe.example.com\n   Rules:\n     Host              Path  Backends\n     ----              ----  --------\n     cafe.example.com\n                       /tea      tea-svc:80 (...)\n                       /coffee   coffee-svc:80 (...)\n\n    Note that the IP address assigned to Ingress must be the same as the IP address assigned to the Ingress controller. This address belongs to the VK Cloud platform's TCP balancer, which routes incoming traffic to the Ingress controller.\n\n5. Check application availability\n\nCheck that pods named tea and coffee exist by getting a list of all pods in the default namespace:\n\n      kubectl get pods\n\nRun the command:\n\n   Coffee\n   Tea\n\n\n      curl -k --resolve cafe.example.com:443: https://cafe.example.com/coffee\n\n   The request should be answered by one of the two coffee pods. The response will contain the name of the pod that responded (Server name), for example:\n\n      Server address: ...:8080\n   Server name: coffee-7c86d7d67c-zsmwz\n   Date: ...\n   URI: /coffee\n   Request ID: ...\n\n   Receiving such responses means that the Ingress controller is configured correctly:\n\n   interacts with VK Cloud TCP load balancer;\n   terminates SSL\\TLS sessions;\n   provides access to services corresponding to the deployed applications.\n\n\n      curl -k --resolve cafe.example.com:443: https://cafe.example.com/tea\n\n   The request should be answered by one of the three tea pods. The response will contain the name of the pod that responded (Server name), for example:\n\n      Server address: ...:8080\n   Server name: tea-5c457db9-gjkgk\n   Date: ...\n   URI: /tea\n   Request ID: ...\n\n   Receiving such responses means that the Ingress controller is configured correctly:\n\n   interacts with VK Cloud TCP load balancer;\n   terminates SSL\\TLS sessions;\n   provides access to services corresponding to the deployed applications.\n\n\nDelete unused resources\n\nIf the Kubernetes resources you created are no longer needed, delete them.\n\n\n   The TCP load balancer created for the Ingress controller will also be removed.\n\n   Linux/macOS\n   Windows\n\n\n      kubectl delete -f ./cafe-ingress.yaml\n   kubectl delete -f ./cafe-secret.yaml\n   kubectl delete -f ./cafe.yaml\n   helm uninstall nginx-ingress-tcp -n example-nginx-ingress-tcp\n   kubectl delete namespace example-nginx-ingress-tcp\n\n\n      kubectl delete -f ./cafe-ingress.yaml; `\n   kubectl delete -f ./cafe-secret.yaml; `\n   kubectl delete -f ./cafe.yaml; `\n   helm uninstall nginx-ingress-tcp -n example-nginx-ingress-tcp; `\n   kubectl delete namespace example-nginx-ingress-tcp\n\n\nA running cluster consumes computing resources. If you no longer need it:\n\n   stop it to use it later;\n   delete it permanently.\n","url":"/en/kubernetes/k8s/how-to-guides/ingress/ingress-tcp","description":"Demonstration of Ingress controller interaction with VK Cloud platform TCP load balancer.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"10731d23-74b9-494b-b9e1-339beb4c550c","title":"Deploying Ingress controller with HTTP load balancer","data":"The Ingress controller can be deployed in conjunction with the HTTP load balancer of the VK Cloud platform. As an example, a simple demo application and an Ingress resource will be deployed to test the operation of the controller.\n\n\n\nIt is assumed throughout the document that the NGINX Ingress Controller will be deployed. However, the proposed approaches can be adapted to other Ingress controllers such as Traefik.\n\nIn this Ingress deployment, you need to manually add worker nodes to the load balancer rules. This is true both when manually resizing the worker group and when enabling autoscaling.\n\n1. Preparatory steps\n\nCreate a Kubernetes cluster of the most current version.\n\n   When creating the cluster select the Assign external IP option.\n\n   Select other cluster settings at your discretion.\n\nMake sure that the NGINX Ingress add-on (ingress-nginx) is not installed in the cluster. For demonstration purposes, the Ingress controller will be installed manually.\n\nMake sure that you can connect to the cluster using `kubectl'.\n\nInstall Helm if the utility is not already installed.\n\nInstall curl if the utility is not already installed.\n\n2. Deploy demo applications.\n\nThese applications will be accessed through the Ingress controller using the Ingress resource.\n\nThe tea and coffee applications from NGINX's Cafe example will be used for the demo. Each application consists of a ReplicaSet, a Deployment and a Service corresponding to that Deployment.\n\nTo deploy the demo applications:\n\nDownload the cafe.yaml manifest.\n\nApply this manifest to the cluster:\n\n      kubectl apply -f ./cafe.yaml\n\nTo check the status of the application components, run the command:\n\nkubectl get svc,rs,deployment -n default\n\nThe output of the command should be similar to this one:\n\nNAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\nservice/coffee-svc   ClusterIP   ...                      80/TCP    ...\nservice/tea-svc      ClusterIP   ...                      80/TCP    ...\n\nNAME                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/coffee-7c86d7d67c   2         2         2       ...\nreplicaset.apps/tea-5c457db9        3         3         3       ...\n\nNAME                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/coffee   2/2     2            2           ...\ndeployment.apps/tea      3/3     3            3           ...\n\n3. Install the Ingress Controller\n\nAdd the NGINX Helm repository:\n\n   Linux/macOS\n   Windows\n\n\n      helm repo add nginx-stable https://helm.nginx.com/stable\n   helm repo update\n\n\n      helm repo add nginx-stable https://helm.nginx.com/stable; `\n   helm repo update\n\n\nInstall the Ingress controller with the NodePort service by running the command:\n\n   Linux/macOS\n   Windows\n\n\n      helm install nginx-ingress-http nginx-stable/nginx-ingress \\\n    --create-namespace --namespace example-nginx-ingress-http \\\n    --set controller.service.type=NodePort \\\n    --set controller.service.httpsPort.enable=false \\\n    --set controller.service.externalTrafficPolicy=Local\n\n\n      helm install nginx-ingress-http nginx-stable/nginx-ingress `\n    --create-namespace --namespace example-nginx-ingress-http `\n    --set controller.service.type=NodePort `\n    --set controller.service.httpsPort.enable=false `\n    --set controller.service.externalTrafficPolicy=Local\n\n\nWait until the installation of the Ingress-Controller is completed and the port is assigned to the controller.\n\n   To check the status of the Ingress Controller, run the command:\n\n      kubectl get svc -n example-nginx-ingress-http\n\n   The output of the command should be similar to this one:\n\n      NAME                               TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                     AGE\n   nginx-ingress-http-nginx-ingress   NodePort   ...                    80:/TCP   ...\n\n4. Create HTTP load balancer\n\nThe HTTP load balancer will terminate SSL/TLS connections and redirect HTTP traffic to the Ingress controller.\n\nTo configure the load balancer:\n\nGather the necessary data:\n\n   The names of the network and subnet where the cluster nodes reside.\n   Names of Cloud Servers service instances that correspond to cluster's master nodes and worker nodes.\n   The port number that was assigned to the Ingress controller in the previous step.\n\nCreate a balancer:\n\n   Go to personal account VK Cloud.\n   Select the project where the required cluster is located.\n   Go to Virtual networks → Load balancers.\n   Click the Add button.\n   In the window that appears:\n      Set Loadbalancer name (any).\n      Select the network and subnet that match that of the cluster.\n      Set the DNS-name (any).\n      Make sure the Assign external IP option is enabled.\n      Set the processing parameters for each type of traffic:\n\n         For HTTP traffic\n         For HTTPS traffic\n\n\n         In the Balancer rules block, click the + Add rule link.\n         Select Assignment protocol HTTP, set for it the port that was assigned to the Ingress controller.\n         In the Allowed CIDRs block, click the link + Add address. Enter 0.0.0.0.0/0.\n         Select the Send X-Forwarded-For header option.\n         In the Apply to the following instances block, add all Cloud Servers service instances that correspond to the cluster master nodes and worker nodes.\n\n            Set the same weights for all instances equal to 1.\n\n         Click the Next step button.\n         Click the Add button.\n\n\n         In the Balancer rules block, click the + Add rule link.\n         Select Balancing protocol HTTPS.\n         Select Assignment protocol HTTP, set for it the port that has been assigned to the Ingress controller.\n         In the Allowed CIDRs block, click the link + Add address. Enter 0.0.0.0.0/0.\n         Select the Send X-Forwarded-For header option.\n         In the Apply to the following instances block, add all Cloud Servers service instances that correspond to the cluster master nodes and worker nodes.\n\n            Set the same weights for all instances equal to 1.\n\n         In the Certificate block:\n\n            Select Load new certificate.\n            Set Certificate name (any).\n\n            Insert the contents below the spoiler in the Certificate and chain of certificates field.\n\n               Public part (certificate.pub) of the NGINX self-signed certificate\n\n                              -----BEGIN CERTIFICATE-----\n               MIIDLjCCAhYCCQDAOF9tLsaXWjANBgkqhkiG9w0BAQsFADBaMQswCQYDVQQGEwJV\n               UzELMAkGA1UECAwCQ0ExITAfBgNVBAoMGEludGVybmV0IFdpZGdpdHMgUHR5IEx0\n               ZDEbMBkGA1UEAwwSY2FmZS5leGFtcGxlLmNvbSAgMB4XDTE4MDkxMjE2MTUzNVoX\n               DTIzMDkxMTE2MTUzNVowWDELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAkNBMSEwHwYD\n               VQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQxGTAXBgNVBAMMEGNhZmUuZXhh\n               bXBsZS5jb20wggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCp6Kn7sy81\n               p0juJ/cyk+vCAmlsfjtFM2muZNK0KtecqG2fjWQb55xQ1YFA2XOSwHAYvSdwI2jZ\n               ruW8qXXCL2rb4CZCFxwpVECrcxdjm3teViRXVsYImmJHPPSyQgpiobs9x7DlLc6I\n               BA0ZjUOyl0PqG9SJexMV73WIIa5rDVSF2r4kSkbAj4Dcj7LXeFlVXH2I5XwXCptC\n               n67JCg42f+k8wgzcRVp8XZkZWZVjwq9RUKDXmFB2YyN1XEWdZ0ewRuKYUJlsm692\n               skOrKQj0vkoPn41EE/+TaVEpqLTRoUY3rzg7DkdzfdBizFO2dsPNFx2CW0jXkNLv\n               Ko25CZrOhXAHAgMBAAEwDQYJKoZIhvcNAQELBQADggEBAKHFCcyOjZvoHswUBMdL\n               RdHIb383pWFynZq/LuUovsVA58B0Cg7BEfy5vWVVrq5RIkv4lZ81N29x21d1JH6r\n               jSnQx+DXCO/TJEV5lSCUpIGzEUYaUPgRyjsM/NUdCJ8uHVhZJ+S6FA+CnOD9rn2i\n               ZBePCI5rHwEXwnnl8ywij3vvQ5zHIuyBglWr/Qyui9fjPpwWUvUm4nv5SMG9zCV7\n               PpuwvuatqjO1208BjfE/cZHIg8Hw9mvW9x9C+IQMIMDE7b/g6OcK7LGTLwlFxvA8\n               7WjEequnayIphMhKRXVf1N349eN98Ez38fOTHTPbdJjFA/PcC+Gyme+iGt5OQdFh\n               yRE=\n               -----END CERTIFICATE-----\n\n\n            Insert the contents below the spoiler in the Private key field.\n\n               Private part (private.key) of the NGINX self-signed certificate\n\n                              -----BEGIN RSA PRIVATE KEY-----\n               MIIEowIBAAKCAQEAqeip+7MvNadI7if3MpPrwgJpbH47RTNprmTStCrXnKhtn41k\n               G+ecUNWBQNlzksBwGL0ncCNo2a7lvKl1wi9q2+AmQhccKVRAq3MXY5t7XlYkV1bG\n               CJpiRzz0skIKYqG7Pcew5S3OiAQNGY1DspdD6hvUiXsTFe91iCGuaw1Uhdq+JEpG\n               wI+A3I+y13hZVVx9iOV8FwqbQp+uyQoONn/pPMIM3EVafF2ZGVmVY8KvUVCg15hQ\n               dmMjdVxFnWdHsEbimFCZbJuvdrJDqykI9L5KD5+NRBP/k2lRKai00aFGN684Ow5H\n               c33QYsxTtnbDzRcdgltI15DS7yqNuQmazoVwBwIDAQABAoIBAQCPSdSYnQtSPyql\n               FfVFpTOsoOYRhf8sI+ibFxIOuRauWehhJxdm5RORpAzmCLyL5VhjtJme223gLrw2\n               N99EjUKb/VOmZuDsBc6oCF6QNR58dz8cnORTewcotsJR1pn1hhlnR5HqJJBJask1\n               ZEnUQfcXZrL94lo9JH3E+Uqjo1FFs8xxE8woPBqjZsV7pRUZgC3LhxnwLSExyFo4\n               cxb9SOG5OmAJozStFoQ2GJOes8rJ5qfdvytgg9xbLaQL/x0kpQ62BoFMBDdqOePW\n               KfP5zZ6/07/vpj48yA1Q32PzobubsBLd3Kcn32jfm1E7prtWl+JeOFiOznBQFJbN\n               4qPVRz5hAoGBANtWyxhNCSLu4P+XgKyckljJ6F5668fNj5CzgFRqJ09zn0TlsNro\n               FTLZcxDqnR3HPYM42JERh2J/qDFZynRQo3cg3oeivUdBVGY8+FI1W0qdub/L9+yu\n               edOZTQ5XmGGp6r6jexymcJim/OsB3ZnYOpOrlD7SPmBvzNLk4MF6gxbXAoGBAMZO\n               0p6HbBmcP0tjFXfcKE77ImLm0sAG4uHoUx0ePj/2qrnTnOBBNE4MvgDuTJzy+caU\n               k8RqmdHCbHzTe6fzYq/9it8sZ77KVN1qkbIcuc+RTxA9nNh1TjsRne74Z0j1FCLk\n               hHcqH0ri7PYSKHTE8FvFCxZYdbuB84CmZihvxbpRAoGAIbjqaMYPTYuklCda5S79\n               YSFJ1JzZe1Kja//tDw1zFcgVCKa31jAwciz0f/lSRq3HS1GGGmezhPVTiqLfeZqc\n               R0iKbhgbOcVVkJJ3K0yAyKwPTumxKHZ6zImZS0c0am+RY9YGq5T7YrzpzcfvpiOU\n               ffe3RyFT7cfCmfoOhDCtzukCgYB30oLC1RLFOrqn43vCS51zc5zoY44uBzspwwYN\n               TwvP/ExWMf3VJrDjBCH+T/6sysePbJEImlzM+IwytFpANfiIXEt/48Xf60Nx8gWM\n               uHyxZZx/NKtDw0V8vX1POnq2A5eiKa+8jRARYKJLYNdfDuwolxvG6bZhkPi/4EtT\n               3Y18sQKBgHtKbk+7lNJVeswXE5cUG6EDUsDe/2Ua7fXp7FcjqBEoap1LSw+6TXp0\n               ZgrmKE8ARzM47+EJHUviiq/nupE15g0kJW3syhpU9zZLO7ltB0KIkO9ZRcmUjo8Q\n               cpLlHMAqbLJ8WYGJCkhiWxyal6hYTyWY4cVkC0xtTl/hUE9IeNKo\n               -----END RSA PRIVATE KEY-----\n\n\n            The NGINX self-signed certificate is used to access the published applications on the cafe.example.com domain.\n\n\n         Click the Next step button.\n         Click the Add button.\n\n\n   The load balancer creation operation will start, which will take some time.\n\nOnce the balancer is created, copy its public IP address, you will need it to access the resources published through Ingress.\n\n5. Create an Ingress resource\n\nThe Ingress resource will publish the coffee-svc and tea-svc services on the cafe.example.com domain through the Ingress controller, thus providing access to the applications.\n\nThe following will demonstrate how to create an Ingress resource that works exclusively with HTTP traffic that comes from the HTTP balancer configured earlier:\n\nCreate a cafe-ingress.yaml manifest file with the following contents:\n\n   cafe-ingress.yaml\n\n      apiVersion: networking.k8s.io/v1\n   kind: Ingress\n   metadata:\n     name: cafe-ingress\n   spec:\n     ingressClassName: nginx\n     rules:\n     host: cafe.example.com\n       http:\n         paths:\n         path: /tea\n           pathType: Prefix\n           backend:\n             service:\n               name: tea-svc\n               port:\n                 number: 80\n         path: /coffee\n           pathType: Prefix\n           backend:\n             service:\n               name: coffee-svc\n               port:\n                 number: 80\n\n\nApply this manifest to the cluster:\n\n      kubectl apply -f ./cafe-ingress.yaml\n\n   Будет создан ресурс Ingress cafe-ingress.\n\nVerify that the resource was created successfully by running the command:\n\n      kubectl describe ingress cafe-ingress\n\n   The output of the command should be similar to this one:\n\n      Name:             cafe-ingress-http\n   Labels:\n   Namespace:        default\n   Address:\n   Ingress Class:    nginx\n   Default backend:\n   Rules:\n     Host              Path  Backends\n     ----              ----  --------\n     cafe.example.com\n                       /tea      tea-svc:80 (10.100.54.15:8080,10.100.54.16:8080,10.100.54.17:8080)\n                       /coffee   coffee-svc:80 (10.100.54.13:8080,10.100.54.14:8080)\n\n6. Check application availability\n\nCheck that pods named tea and coffee exist by getting a list of all pods in the default namespace:\n\n      kubectl get pods\n\nRun the command:\n\n   Coffee\n   Tea\n\n\n      curl -k --resolve cafe.example.com:443: https://cafe.example.com/coffee\n\n   The request should be answered by one of the two coffee pods. The response will contain the name of the pod that responded (Server name), for example:\n\n      Server address: ...:8080\n   Server name: coffee-7c86d7d67c-zsmwz\n   Date: ...\n   URI: /coffee\n   Request ID: ...\n\n   Receiving such responses means that the Ingress controller is configured correctly:\n\n   interacts with VK Cloud HTTP load balancer (which terminates SSL\\TLS sessions);\n   provides access to services corresponding to the deployed applications.\n\n\n      curl -k --resolve cafe.example.com:443: https://cafe.example.com/tea\n\n   The request should be answered by one of the three tea pods. The response will contain the name of the pod that responded (Server name), for example:\n\n      Server address: ...:8080\n   Server name: tea-5c457db9-gjkgk\n   Date: ...\n   URI: /tea\n   Request ID: ...\n\n   Receiving such responses means that the Ingress controller is configured correctly:\n\n   interacts with VK Cloud HTTP load balancer (which terminates SSL\\TLS sessions);\n   provides access to services corresponding to the deployed applications.\n\n\nDelete unused resources\n\nIf the Kubernetes resources you created are no longer needed, delete them.\n\n\n   The HTTP load balancer created for the Ingress controller will not be removed. If necessary, remove it manually from the VK Cloud personal account interface.\n\n   Linux/macOS\n   Windows\n\n\n      kubectl delete -f ./cafe-ingress.yaml\n   kubectl delete -f ./cafe.yaml\n   helm uninstall nginx-ingress-http -n example-nginx-ingress-http\n   kubectl delete namespace example-nginx-ingress-http\n\n\n      kubectl delete -f ./cafe-ingress.yaml; `\n   kubectl delete -f ./cafe.yaml; `\n   helm uninstall nginx-ingress-http -n example-nginx-ingress-http; `\n   kubectl delete namespace example-nginx-ingress-http\n\n\nA running cluster consumes computing resources. If you no longer need it:\n\n   stop it to use it later;\n   delete it permanently.\n","url":"/en/kubernetes/k8s/how-to-guides/ingress/ingress-http","description":"Demonstration of Ingress controller interaction with VK Cloud platform HTTP load balancer.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"6f0a2aef-cf60-456d-92ec-d20dcf43c31d","title":"Configuring Canary Deployment Ingress","data":"This article will help you deploy a Kubernetes cluster and configure Canary Deployment on it using Nginx Ingress Annotations: execute the Canary Deployment script for the echo server and make sure that traffic is distributed according to the configuration file.\n\nPreparatory steps\n\nCreate Kubernetes cluster in VK Cloud.\nConnect to the cluster using kubectl.\nCreate a test application:\n\n   Create a new namespace for the project:\n\n            kubectl create ns echo-production\n\n   Create a Kubernetes resource based on the manifest, for example, http-svc:\n\n      kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/docs/examples/http-svc.yaml -n echo-production\n\n1. Create an Ingress resource\n\nCreate a manifest file http-svc.ingress with the following contents:\n\n    http-svc.ingress\n\n       apiVersion: extensions/v1beta1\n    kind: Ingress\n    metadata:\n      name: http-svc\n      annotations:\n        kubernetes.io/ingress.class: nginx\n    spec:\n      rules:\n      host: echo.com\n        http:\n          paths:\n          backend:\n              serviceName: http-svc\n              servicePort: 80\n\n\nApply the manifest in the cluster:\n\n      kubectl apply -f http-svc.ingress -n echo-production\n\n   As a result, an application will be created and the server will respond to all requests from echo.com.\n\n2. Create a copy of the deployed application\n\nCreate a Canary version of the namespace for the application:\n\n      kubectl create ns echo-canary\n\nDeploy the Canary version of the app:\n\n      kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/docs/examples/http-svc.yaml -n echo-canary\n\nCreate a Canary version of the Ingress configuration file:\n\n    http-svc.ingress.canary\n\n      apiVersion: extensions/v1beta1\n   kind: Ingress\n   metadata:\n     name: http-svc\n     annotations:\n       kubernetes.io/ingress.class: nginx\n       nginx.ingress.kubernetes.io/canary: \"true\"\n       nginx.ingress.kubernetes.io/canary-weight: \"10\"\n   spec:\n     rules:\n     host: echo.com\n       http:\n         paths:\n         backend:\n             serviceName: http-svc\n             servicePort: 80\n\n   Decryption of some parameters:\n\n   nginx.ingress.kubernetes.io/canary: \"true\" — Kubernetes will not consider this Ingress as independent and will mark it as Canary, associating it with the main Ingress.\n   nginx.ingress.kubernetes.io/canary-weight: \"10\" — Canary will account for approximately 10% of all requests.\n\n\nApply the manifest in the cluster:\n\n      kubectl apply -f http-svc.ingress.canary -n echo-canary\n\n3. Check the performance of traffic distribution\n\nConnect to the cluster using Kubernetes Dashboard.\nGo to Namespaces.\nSwitch the Namespace filter to All.\nAt the bottom of the side menu, select Ingresses.\n\n   A list of all available Ingresses will be displayed.\n\nMake sure that http-svc has one IP address specified in the Endpoints column.\nCheck the distribution of requests according to the installed configuration by executing the script count.rb:\n\n    count.rb\n\n      counts = Hash.new(0)\n   1000.times do\n     output = \\curl -s -H \"Host: echo.com\" http:// | grep 'pod namespace'\\\n     counts[output.strip.split.last] += 1\n   end\n   puts counts\n\n\n      ruby count.rb\n\nExample of a successful result:\n\n{\"echo-production\"=>896, \"echo-canary\"=>104}\n\nDelete unused resources\n\nA running cluster consumes computing resources. If you no longer need it:\n\nstop it to use it later;\ndelete it permanently.\n","url":"/en/kubernetes/k8s/how-to-guides/ingress/k8s-canary","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"8cbc1166-a1d4-4e2c-9d2c-cd01d38a0db6","title":"Working with persistent volumes","data":"Persistent volumes can be connected to simple demo applications in various ways. Next, Persistent Volume Claims (PVCs) will be used to connect them. An Ingress resource will be created to test the functionality of the applications and the volumes connected to them.\n\n1. Preparatory steps\n\nCreate a Kubernetes cluster of the most current version.\n\n   When creating the cluster:\n\n   Select the Assign external IP option.\n   Create one group of worker nodes with virtual machine type STD3-2-8 in the MS1 availability area with total computing resources: 2 vCPU, 8 GB RAM (or more). This is necessary to be able to schedule all the required objects.\n\n    For example, you can create one group of nodes with virtual machine type STD3-2-8.\n\n   Other cluster parameters are at your discretion.\n\nMake sure that the NGINX Ingress add-on (ingress-nginx) is installed in a cluster with default parameters. It will be required to provide access to demo applications.\n\n\n    When installing the add-on, a standard load balancer will be created.\n\n    Usage of this load balancer is charged.\n\n\nMake sure that you can connect to the cluster using kubectl.\n\nInstall curl if the utility is not already installed.\n\n2. Create demo applications and connect persistent volumes to them\n\nThe following will demonstrate how to create several NGINX-based web applications to display web pages written to the persistent volumes connected to those applications.\nThe NGINX nginxdemos/nginx-hello image is used, which displays web pages from the /usr/share/nginx/html directory, so all persistent volumes will be mounted in the application pods via this path.\n\nYou can create one or more demo applications, depending on which way you want to connect the persistent volumes.\n\nConnecting block storages\n\nBlock stores are connected to the cluster with Cinder CSI.\n\nWhen using this type of storage:\n\nonly one pod can access storage (multiple pods cannot use block storage at the same time);\nas a consequence, the ReadWriteOnce mode must be used to access the storage.\n\n\n\nConnecting via static PVC\nConnecting via dynamic PVC\nConnecting to several pods via dynamic PVC\n\n\n\nThis example will create:\n\nDisk in the cloud compute service of the VK Cloud platform.\nA persistent volume corresponding to this disk.\nStatic PVC, using a persistent volume that has already been created.\nApplication tea as a single pod deployment, and its corresponding service.\n\n   For this application there will also be an initialization container (initContainer) which will write the web page to the persistent volume.\n\nTo connect a persistent volume using static PVC:\n\nCreate a network HDD.\n\n   When creating, specify:\n\n   Disk name: any name, such as disk-tea.\n   Source: empty disk.\n   Disk type: network HDD (ceph-hdd).\n   Availability zone: MS1.\n   Size: 1 GB.\n\n   Leave other options and settings unchanged.\n\nCopy the ID of the created disk, for example f6d8bf3b-aaaa-bbbb-cccc-4ece8e353246.\n\nExamine the connection features:\n\n   The storage sizes specified in the parameters spec.capacity.storage for the PersistentVolume resource and spec.resources.requests.storage for the PersistentVolumeClaim resource must match the size of the corresponding disk. In this example it is 1 GB.\n   For the PersistentVolumeClaim resource, use an empty value in the storageClassName storage class parameter.\n   The storage access mode is specified in the spec.accessModes parameter for the PersistentVolume resource.\n   The availability zones of the disk and the worker node on which the pod (application) will be located must match. Otherwise an attempt to mount a persistent volume corresponding to the disk on this node will fail. In this example, the pod will be placed on a group of worker nodes in the MS1 availability zone and use a disk from the same zone.\n   ReclaimPolicy Retain is used for the permanent volume. The Delete policy is not used so that you can monitor the state of the disk manually and not accidentally delete it.\n\nCreate a manifest for the tea application.\n\n    For the PersistentVolume resource, specify the ID of the created disk in the spec.cinder.volumeID parameter.\n\n   tea.yaml\n\n      ---\n   apiVersion: v1\n   kind: PersistentVolume\n   metadata:\n     name: tea-pv\n   spec:\n     capacity:\n       storage: 1Gi\n     cinder:\n       volumeID:\n     accessModes:\n       ReadWriteOnce\n     persistentVolumeReclaimPolicy: Retain\n\n   ---\n   apiVersion: v1\n   kind: PersistentVolumeClaim\n   metadata:\n     name: tea-pvc\n   spec:\n     accessModes:\n       ReadWriteOnce\n     resources:\n       requests:\n         storage: 1Gi\n     volumeName: tea-pv\n     storageClassName: \"\"\n\n   ---\n   apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: tea\n   spec:\n     replicas: 1\n     selector:\n       matchLabels:\n         app: tea\n     template:\n       metadata:\n         labels:\n           app: tea\n       spec:\n         volumes:\n           name: tea-volume\n             persistentVolumeClaim:\n               claimName: tea-pvc\n         initContainers:\n           name: write-html-for-nginx\n             image: busybox\n             volumeMounts:\n               name: tea-volume\n                 mountPath: /usr/share/nginx/html\n             command: [\"/bin/sh\", \"-c\"]\n             args:\n               [\n                 'echo \"The tea pod says Hello World to everyone! This file is located on the statically claimed Cinder ReadWriteOnce persistent volume.\" > /usr/share/nginx/html/index.html',\n               ]\n         containers:\n           name: tea\n             image: nginxdemos/nginx-hello\n             volumeMounts:\n               name: tea-volume\n                 mountPath: /usr/share/nginx/html\n             ports:\n               containerPort: 8080\n\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: tea-svc\n     labels:\n   spec:\n     ports:\n       port: 80\n         targetPort: 8080\n         protocol: TCP\n         name: http\n     selector:\n       app: tea\n\n\nApply this manifest to the cluster to create all necessary resources:\n\n      kubectl apply -f ./tea.yaml\n\n\nThis example will create:\n\nA dynamic PVC that will create a permanent volume based on the given parameters.\nA coffee application as a single pod deployment, and its corresponding service.\n\n   For this application there will also be an initialization container (initContainer) which will write the web page to the persistent volume.\n\nTo connect a persistent volume using dynamic PVC:\n\nExamine the connection features:\n\n   The required storage size is specified in the spec.resources.requests.storage parameter of the PersistentVolumeClaim resource. In this example it is 1 GB.\n   For the PersistentVolumeClaim resource, specify the storage class in the spec.storageClassName parameter. The storage class must use the same availability zone as the worker node on which the pod (application) will reside. Otherwise, an attempt to connect a persistent volume corresponding to the PVC to the pod on that node will fail. In this example, the pod will be placed on a group of worker nodes in the MS1 availability zone and use the csi-ceph-hdd-ms1 storage class from the same zone.\n   The storage access mode is set in the spec.accessModes parameter for the PersistentVolumeClaim resource.\n   ReclaimPolicy Delete is used for persistent volume (it follows from selected storage class). When the PVC is deleted, the disk corresponding to the persistent volume will automatically be deleted.\n\nCreate a manifest for the coffee application.\n\n   coffee.yaml\n\n      ---\n   apiVersion: v1\n   kind: PersistentVolumeClaim\n   metadata:\n     name: coffee-pvc\n   spec:\n     accessModes:\n       ReadWriteOnce\n     resources:\n       requests:\n         storage: 1Gi\n     storageClassName: \"csi-ceph-hdd-ms1\"\n\n   ---\n   apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: coffee\n   spec:\n     replicas: 1\n     selector:\n       matchLabels:\n         app: coffee\n     template:\n       metadata:\n         labels:\n           app: coffee\n       spec:\n         volumes:\n           name: coffee-volume\n             persistentVolumeClaim:\n               claimName: coffee-pvc\n         initContainers:\n           name: write-html-for-nginx\n             image: busybox\n             volumeMounts:\n               name: coffee-volume\n                 mountPath: /usr/share/nginx/html\n             command: [\"/bin/sh\", \"-c\"]\n             args:\n               [\n                 'echo \"The coffee pod says Hello World to everyone! This file is located on the dynamically claimed Cinder ReadWriteOnce persistent volume.\" > /usr/share/nginx/html/index.html',\n               ]\n         containers:\n           name: coffee\n             image: nginxdemos/nginx-hello\n             volumeMounts:\n               name: coffee-volume\n                 mountPath: /usr/share/nginx/html\n             ports:\n               containerPort: 8080\n\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: coffee-svc\n     labels:\n   spec:\n     ports:\n       port: 80\n         targetPort: 8080\n         protocol: TCP\n         name: http\n     selector:\n       app: coffee\n\n\nApply this manifest to the cluster to create all necessary resources:\n\n      kubectl apply -f ./coffee.yaml\n\n\nThis example will create:\n\nApplication juice as a StatefulSet of two pods, as well as the corresponding services.\n\n   For this application there will also be an initialization container (initContainer) which will write the web page to the persistent volume.\n\nA dynamic PVC that will create persistent volumes based on the parameters you specify.\n\nTo connect a persistent volume to multiple pods using dynamic PVC:\n\nExamine the connection features:\n\n   When you use a StatefulSet PVC, it is not configured separately as in the other examples, but as part of the StatefulSet resource.\n   The PVC will create one persistent volume for each StatefulSet replica, and these replicas will be numbered in order.\n\n\n      When deploying an application of multiple replicas as a Deployment resource, you must also ensure that a persistent volume is created for each replica using the PVC. Such volumes will have random identifiers instead of sequential numbers.\n\n\n   The required storage size is specified in the spec.volumeClaimTemplates.spec.resources.requests.storage parameter of the StatefulSet resource. In this example it is 1 GB.\n   The storage class is specified in the spec.volumeClaimTemplates.spec.storageClassName parameter of the StatefulSet resource. The storage class must use the same availability zone as the worker node on which the sub-application will reside. Otherwise, an attempt to connect a persistent volume corresponding to the PVC to the pad on that node will fail. In this example, the pod will be placed on a group of worker nodes in the MS1 availability zone and use the csi-ceph-hdd-ms1 storage class from the same zone.\n   The storage access mode is set in the spec.volumeClaimTemplates.spec.accessModes parameter of the StatefulSet resource.\n   ReclaimPolicy Delete is used for permanent volume (it follows from selected storage class). When the PVC is deleted, the disk corresponding to the permanent volume will be automatically deleted.\n\nCreate a manifest for the juice application.\n\n   juice.yaml\n\n      ---\n   apiVersion: apps/v1\n   kind: StatefulSet\n   metadata:\n     name: juice\n   spec:\n     serviceName: juice\n     replicas: 2\n     selector:\n       matchLabels:\n         app: juice\n     volumeClaimTemplates:\n       metadata:\n           name: juice-pvc\n         spec:\n           accessModes: [\"ReadWriteOnce\"]\n           storageClassName: \"csi-ceph-hdd-ms1\"\n           resources:\n             requests:\n               storage: 1Gi\n     template:\n       metadata:\n         labels:\n           app: juice\n       spec:\n         initContainers:\n           name: write-html-for-nginx\n             image: busybox\n             volumeMounts:\n               name: juice-pvc\n                 mountPath: /usr/share/nginx/html\n             command: [\"/bin/sh\", \"-c\"]\n             args:\n               [\n                 'echo \"The juice StatefulSet pod says Hello World to everyone! This file is located on the dynamically claimed Cinder ReadWriteOnce persistent volume.\" > /usr/share/nginx/html/index.html',\n               ]\n         containers:\n           name: juice\n             image: nginxdemos/nginx-hello\n             volumeMounts:\n               name: juice-pvc\n                 mountPath: /usr/share/nginx/html\n             ports:\n               containerPort: 8080\n\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: juice-svc\n     labels:\n   spec:\n     ports:\n       port: 80\n         targetPort: 8080\n         protocol: TCP\n         name: http\n     selector:\n       app: juice\n\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: juice-0-svc\n     labels:\n   spec:\n     ports:\n       port: 80\n         targetPort: 8080\n         protocol: TCP\n         name: http\n     selector:\n       statefulset.kubernetes.io/pod-name: juice-0\n\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: juice-1-svc\n     labels:\n   spec:\n     ports:\n       port: 80\n         targetPort: 8080\n         protocol: TCP\n         name: http\n     selector:\n       statefulset.kubernetes.io/pod-name: juice-1\n\n\nApply this manifest to the cluster to create all necessary resources:\n\n      kubectl apply -f ./juice.yaml\n\n\nConnecting file storages\n\nFile storages are connected to the cluster using a persistent volume that is configured to use the existing storage via the required protocol, such as NFS.\n\nWhen using this type of storage:\n\nMultiple pods can access the storage at once;\nas a consequence, the ReadWriteMany mode must be used to access the storage.\n\n\n\nConnecting NFS storage via static PVC\n\n\n\nThis example will create:\n\nNFS file storage in the Cloud Servers service.\nA persistent volume corresponding to this storage.\nStatic PVC using an already created persistent volume.\nApplication milkshake as a StatefulSet of two pods, as well as the corresponding services.\n\nTo connect an NFS persistent volume using a static PVC:\n\nCreate file storage.\n\n   When creating it, specify:\n\n   Name of file storage: any name, such as storage-milkshake.\n   Storage size: `10 GB'.\n   Protocol: NFS.\n   Network: network and subnet where the Kubernetes cluster is located. This information can be found on the cluster page.\n   File storage network: existing network. If a suitable network is not on the list, select Create new network.\n\nView information about the created file storage.\n\n   Save the value of the Connection point parameter.\n\nExamine the specifics of the connection:\n\n   The storage sizes specified in the spec.capacity.storage and spec.resources.requests.storage parameters for the PersistentVolumeClaim resource must match the size of the created file storage. In this example it's 10 GB.\n   For the PersistentVolumeClaim resource, use an empty value in the storageClassName storage class parameter.\n   For the PersistentVolume resource:\n\n      The storage access mode is specified in the spec.accessModes parameter for the PersistentVolume resource.\n      The spec.mountOptions parameter set must contain an nfsvers entry with version 4.0.\n\n   Instead of an initialization container to write a web page to a persistent volume, a single-run Kubernetes task (job) is used. This approach works because in this case all pods will have access to the same persistent volume.\n\n   ReclaimPolicy Retain is used for the persistent volume because the Recycle policy will not allow instant removal of the volume when it becomes unnecessary. Clearing a volume of data takes a long time. The Delete policy is not used so that you can monitor the state of the storage manually and not accidentally delete it.\n\nCreate a manifest for the milkshake application.\n\n   For the PersistentVolume resource, specify:\n\n   IP address from the Connection point of the file storage as the value of the spec.nfs.server parameter.\n   Data after the IP address (/shares/...) as a value of the spec.nfs.path parameter.\n\n   milkshake.yaml\n\n      apiVersion: v1\n   kind: PersistentVolume\n   metadata:\n     name: milkshake-pv\n   spec:\n     accessModes:\n       ReadWriteMany\n     mountOptions:\n       hard\n       nfsvers=4.0\n       timeo=60\n       retrans=10\n     capacity:\n       storage: 10Gi\n     nfs:\n       server:\n       path: \"\"\n     persistentVolumeReclaimPolicy: \"Retain\"\n\n   ---\n   apiVersion: v1\n   kind: PersistentVolumeClaim\n   metadata:\n     name: milkshake-pvc\n   spec:\n     volumeMode: Filesystem\n     accessModes:\n       ReadWriteMany\n     resources:\n       requests:\n         storage: 10Gi\n     volumeName: \"milkshake-pv\"\n     storageClassName: \"\"\n\n   ---\n   apiVersion: batch/v1\n   kind: Job\n   metadata:\n     name: write-html-for-nginx-on-nfs-volume\n   spec:\n     template:\n       spec:\n         restartPolicy: Never\n         volumes:\n           name: milkshake-volume\n             persistentVolumeClaim:\n               claimName: milkshake-pvc\n         containers:\n           name: write-html-for-nginx\n             image: busybox\n             volumeMounts:\n               name: milkshake-volume\n                 mountPath: /usr/share/nginx/html\n             command: [\"/bin/sh\", \"-c\"]\n             args:\n               [\n                 'echo \"The milkshake StatefulSet pod says Hello World to everyone! This file is located on the dynamically claimed NFS ReadWriteMany persistent volume.\" > /usr/share/nginx/html/index.html',\n               ]\n\n   ---\n   apiVersion: apps/v1\n   kind: StatefulSet\n   metadata:\n     name: milkshake\n   spec:\n     serviceName: milkshake\n     replicas: 2\n     selector:\n       matchLabels:\n         app: milkshake\n     template:\n       metadata:\n         labels:\n           app: milkshake\n       spec:\n         volumes:\n           name: milkshake-volume\n             persistentVolumeClaim:\n               claimName: milkshake-pvc\n         containers:\n           name: milkshake\n             image: nginxdemos/nginx-hello\n             volumeMounts:\n               name: milkshake-volume\n                 mountPath: /usr/share/nginx/html\n             ports:\n               containerPort: 8080\n\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: milkshake-svc\n     labels:\n   spec:\n     ports:\n       port: 80\n         targetPort: 8080\n         protocol: TCP\n         name: http\n     selector:\n       app: milkshake\n\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: milkshake-0-svc\n     labels:\n   spec:\n     ports:\n       port: 80\n         targetPort: 8080\n         protocol: TCP\n         name: http\n     selector:\n       statefulset.kubernetes.io/pod-name: milkshake-0\n\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: milkshake-1-svc\n     labels:\n   spec:\n     ports:\n       port: 80\n         targetPort: 8080\n         protocol: TCP\n         name: http\n     selector:\n       statefulset.kubernetes.io/pod-name: milkshake-1\n\n\nApply this manifest to the cluster to create all necessary resources:\n\n      kubectl apply -f ./milkshake.yaml\n\n\n3. Check the functionality of demo applications and persistent volumes\n\nCreate a manifest for the Ingress resource through which application requests will go.\n\n   cafe-ingress.yaml\n\n      ---\n   apiVersion: networking.k8s.io/v1\n   kind: Ingress\n   metadata:\n     name: cafe-ingress\n   spec:\n     ingressClassName: nginx\n     rules:\n     host: cafe.example.com\n       http:\n         paths:\n         path: /tea\n           pathType: Prefix\n           backend:\n             service:\n               name: tea-svc\n               port:\n                 number: 80\n         path: /coffee\n           pathType: Prefix\n           backend:\n             service:\n               name: coffee-svc\n               port:\n                 number: 80\n         path: /juice\n           pathType: Prefix\n           backend:\n             service:\n               name: juice-svc\n               port:\n                 number: 80\n         path: /juice/0\n           pathType: Prefix\n           backend:\n             service:\n               name: juice-0-svc\n               port:\n                 number: 80\n         path: /juice/1\n           pathType: Prefix\n           backend:\n             service:\n               name: juice-1-svc\n               port:\n                 number: 80\n         path: /milkshake\n           pathType: Prefix\n           backend:\n             service:\n               name: milkshake-svc\n               port:\n                 number: 80\n         path: /milkshake/0\n           pathType: Prefix\n           backend:\n             service:\n               name: milkshake-0-svc\n               port:\n                 number: 80\n         path: /milkshake/1\n           pathType: Prefix\n           backend:\n             service:\n               name: milkshake-1-svc\n               port:\n                 number: 80\n\n\nApply this manifest to the cluster to create all necessary resources:\n\n      kubectl apply -f ./cafe-ingress.yaml\n\nDefine the public IP address of the Ingress controller.\n\nCheck the availability of the applications with curl using the IP address of the Ingress controller.\n\n\n   If some of the applications have not been deployed, the message Service Unavailable will be displayed for them.\n\n   Tea\n   Coffee\n   Juice\n   Milkshake\n\n\n   Run the command:\n\n      curl --resolve cafe.example.com:80: http://cafe.example.com/tea\n\n   A response should be displayed:\n\n      The tea pod says Hello World to everyone! This file is located on the statically claimed persistent volume.\n\n\n   Run the command:\n\n      curl --resolve cafe.example.com:80: http://cafe.example.com/coffee\n\n   A response should be displayed:\n\n      The coffee pod says Hello World to everyone! This file is located on the statically claimed persistent volume.\n\n\n   Run the commands:\n\n      curl --resolve cafe.example.com:80: http://cafe.example.com/juice\n   curl --resolve cafe.example.com:80: http://cafe.example.com/juice/0\n   curl --resolve cafe.example.com:80: http://cafe.example.com/juice/1\n\n   The same response should be output for the application and each of its replicas:\n\n      The juice StatefulSet pod says Hello World to everyone! This file is located on the dynamically claimed Cinder ReadWriteOnce persistent volume.\n\n\n   Run the commands:\n\n      curl --resolve cafe.example.com:80: http://cafe.example.com/milkshake\n   curl --resolve cafe.example.com:80: http://cafe.example.com/milkshake/0\n   curl --resolve cafe.example.com:80: http://cafe.example.com/milkshake/1\n\n   The same response should be output for the application and each of its replicas:\n\n      The milkshake StatefulSet pod says Hello World to everyone! This file is located on the dynamically claimed NFS ReadWriteMany persistent volume.\n\n\nDelete unused resources\n\nIf the Kubernetes resources you created are no longer needed, delete them.\n\n   Linux/macOS\n   Windows\n\n\n      kubectl delete -f ./cafe-ingress.yaml\n   kubectl delete -f ./milkshake.yaml\n   kubectl delete -f ./juice.yaml\n   kubectl delete -f ./coffee.yaml\n   kubectl delete -f ./tea.yaml\n\n\n      kubectl delete -f ./cafe-ingress.yaml; `\n   kubectl delete -f ./milkshake.yaml; `\n   kubectl delete -f ./juice.yaml; `\n   kubectl delete -f ./coffee.yaml; `\n   kubectl delete -f ./tea.yaml\n\n\nRemove unused storage:\n\n   If the disk used by the tea application is no longer needed — delete it.\n\n   If the NFS repository used by the milkshake application is no longer needed — delete it.\n\n   All other Cinder stores created with dynamic PVCs will be deleted automatically.\n\nA running cluster consumes computing resources. If you no longer need it:\n\n   stop it to use it later;\n   delete it permanently.\n","url":"/en/kubernetes/k8s/how-to-guides/storage","description":"In the VK Cloud Kubernetes cluster, you can connect various types of persistent volumes. Here you will find information about the available connection options and limitations.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"f0b66c19-8370-4dee-a8d8-4451caf200da","title":"Configuring mandatory labels","data":"With Gatekeeper you can set a constraint that will require a specific label for created Kubernetes resources. For example, this constraint can be useful if company policies require the creator name of any Kubernetes resources to facilitate auditing and parsing of potential incidents.\n\nTo demonstrate how Gatekeeper works, the following will be created:\n\nA constraint template and its corresponding constraint. It will require a creator-name label in Kubernetes resources that are created in any namespaces (except system namespaces).\nA few Kubernetes resources to test how the constraint works.\n\nPreparatory steps\n\nCreate a Kubernetes cluster of the most current version.\n\n   Choose the cluster parameters at your own discretion.\n\nMake sure that you can connect to the cluster with kubectl.\n\nCreate a constraint that checks labels\n\nCreate a constraint template:\n\n   Create a constraint template manifest.\n\n      Use the contents of this file. This is a ready-made template K8sRequiredLabels from Gatekeeper library, which checks if the Kubernetes resource has the specified labels.\n\n   Create a restriction template based on the template manifest:\n\n            kubectl apply -f template.yaml\n\nCreate a constraint:\n\n   Create a restriction manifest based on the template created earlier:\n\n      constraint.yaml\n\n            apiVersion: constraints.gatekeeper.sh/v1beta1\n      kind: K8sRequiredLabels\n      metadata:\n        name: require-creator-label\n      spec:\n        match:\n          kinds:\n            apiGroups: [\"\"]\n              kinds: [\"Namespace\"]\n          excludedNamespaces: [\"kube-system\"]\n        parameters:\n          labels:\n            key: creator-name\n              allowedRegex: \"((A-Za-z0-9*)?[A-Za-z0-9])?\"\n\n\n   Create a constraint based on the constraint manifest:\n\n            kubectl apply -f constraint.yaml\n\nMake sure that the constraint template and the constraint have been successfully created by running the command:\n\n      kubectl get constraints,constrainttemplates\n\n   Output should give you the similar information:\n\n      NAME                                                              ENFORCEMENT-ACTION TOTAL-VIOLATIONS\n   k8srequiredlabels.constraints.gatekeeper.sh/require-creator-label ...                ...\n\n   NAME                                                              AGE\n   ...\n   constrainttemplate.templates.gatekeeper.sh/k8srequiredlabels      ...\n\nTest the restriction by trying to create several namespaces:\n\n   Namespace that satisfies the constraint\n   Namespace that does not satisfy the constraint\n\n\n   Create a manifest for the namespace:\n\n      example-allowed.yaml\n\n            apiVersion: v1\n      kind: Namespace\n      metadata:\n        name: allowed-namespace\n        labels:\n          creator-name: john.doe\n\n   Try to create a namespace based on the manifest:\n\n            kubectl apply -f example-allowed.yaml\n\n      The operation should be completed successfully.\n\n   Make sure that the namespace has been successfully created by running the command:\n\n            kubectl get ns allowed-namespace\n\n      Output should give you the similar information:\n\n            NAME                STATUS   AGE\n      allowed-namespace   Active   ...\n\n\n   Create a manifest for the namespace:\n\n      example-disallowed.yaml\n\n            apiVersion: v1\n      kind: Namespace\n      metadata:\n        name: disallowed-namespace\n        labels:\n          my-label: sample\n\n   Try to create a namespace based on the manifest:\n\n            kubectl apply -f example-disallowed.yaml\n\n      The operation should end with an error:\n\n            Error from server (Forbidden): error when creating \".\\\\example-disallowed.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [require-creator-label] you must provide labels: {\"creator-name\"}\n\n   Make sure that the namespace has not been created by running the command:\n\n            kubectl get ns disallowed-namespace\n\n      Output should give you the similar information:\n\n            Error from server (NotFound): namespaces \"disallowed-namespace\" not found\n\n\nDelete unused resources\n\nIf the Kubernetes resources you created are no longer needed, delete them.\n\n   Linux/macOS\n   Windows\n\n\n      kubectl delete ns allowed-namespace\n   kubectl delete k8srequiredlabels.constraints.gatekeeper.sh/require-creator-label\n   kubectl delete constrainttemplate.templates.gatekeeper.sh/k8srequiredlabels\n\n\n      kubectl delete ns allowed-namespace; `\n   kubectl delete k8srequiredlabels.constraints.gatekeeper.sh/require-creator-label; `\n   kubectl delete constrainttemplate.templates.gatekeeper.sh/k8srequiredlabels\n\n\nA running cluster consumes computing resources. If you no longer need it:\n\n   stop it to use it later;\n   delete it permanently.\n","url":"/en/kubernetes/k8s/how-to-guides/gatekeeper/labels","description":"Using Gatekeeper to enforce constraints that will check mandatory labels on Kubernetes resources.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"9aad2849-1640-4016-9dcb-d588c5d27e90","title":"Configuring allowed repositories","data":"With Gatekeeper you can set a constraint that requires only allowed repositories to be used to download images. For example, this constraint can be useful if company policies require downloading images only from trusted repositories, so that cluster operators do not accidentally run an untrusted application from an untrusted source.\n\nTo demostrate how Gatekeeper works several objects will be created:\n\nA constraint template and its corresponding constraint. It will only allow images to be downloaded from the Docker Hub repository.\nA few Kubernetes resources to test how the constraint works.\n\nPreparatory steps\n\nCreate a Kubernetes cluster of the most current version.\n\n   Choose the cluster parameters at your own discretion.\n\nMake sure that you can connect to the cluster with kubectl.\n\nCreate a constraint that checks the repositories\n\nCreate a constraint template:\n\n   Create a constraint template manifest.\n\n      Use the contents of this file. This is a ready-made template K8sAllowedRepos from the Gatekeeper library, which checks that images are only downloaded from trusted repositories.\n\n   Create a constraint template based on the template manifest:\n\n            kubectl apply -f template.yaml\n\nCreate a constraint:\n\n   Create a constraint manifest constraint based on the template you created earlier:\n\n      constraint.yaml\n\n            apiVersion: constraints.gatekeeper.sh/v1beta1\n      kind: K8sAllowedRepos\n      metadata:\n        name: require-docker-hub\n      spec:\n        match:\n          kinds:\n            apiGroups: [\"\"]\n              kinds: [\"Pod\"]\n          namespaces:\n            \"default\"\n        parameters:\n          repos:\n            \"docker.io/\"\n\n\n   Create a constraint based on the constraint manifest:\n\n            kubectl apply -f constraint.yaml\n\nMake sure that the constraint template and the constraint have been successfully created by running the command:\n\n      kubectl get constraints,constrainttemplates\n\n   Output should give you the similar information:\n\n      NAME                                                          ENFORCEMENT-ACTION TOTAL-VIOLATIONS\n   k8sallowedrepos.constraints.gatekeeper.sh/require-docker-hub  ...                ...\n\n   NAME                                                          AGE\n   constrainttemplate.templates.gatekeeper.sh/k8sallowedrepos    ...\n\nTest the restriction by trying to create several pods:\n\n   Pod that satisfies the constraint\n   Pod that does not satisfy the constraint\n\n\n   Create a manifest for the pod:\n\n      example-allowed.yaml\n\n            apiVersion: v1\n      kind: Pod\n      metadata:\n        name: allowed-pod\n      spec:\n        containers:\n          name: nginx\n            image: docker.io/jitesoft/nginx:latest\n\n   Try to create a pod based on the manifest:\n\n            kubectl apply -f example-allowed.yaml\n\n      The operation should be completed successfully.\n\n   Make sure that the pod has been successfully created by running the command:\n\n            kubectl get pod allowed-pod\n\n      Output should give you the similar information:\n\n            NAME          READY   STATUS    RESTARTS   AGE\n      allowed-pod   1/1     Running   ...        ...\n\n\n   Create a manifest for the pod:\n\n      example-disallowed.yaml\n\n            apiVersion: v1\n      kind: Pod\n      metadata:\n        name: disallowed-pod\n      spec:\n        containers:\n          name: nginx\n            image: quay.io/jitesoft/nginx:latest\n\n   Try to create a pod based on the manifest:\n\n            kubectl apply -f example-disallowed.yaml\n\n      The operation should end with an error:\n\n            Error from server (Forbidden): error when creating \"example-disallowed.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [require-docker-hub] container  has an invalid image repo , allowed repos are [\"docker.io/\"]\n\n   Make sure that the pod has not been created by running the command:\n\n            kubectl get pod disallowed-pod\n\n      Output should give you the similar information:\n\n            Error from server (NotFound): pods \"disallowed-pod\" not found\n\n\nDelete unused resources\n\nIf the Kubernetes resources you created are no longer needed, delete them.\n\n   Linux/macOS\n   Windows\n\n\n      kubectl delete pod allowed-pod\n   kubectl delete k8sallowedrepos.constraints.gatekeeper.sh/require-docker-hub\n   kubectl delete constrainttemplate.templates.gatekeeper.sh/k8sallowedrepos\n\n\n      kubectl delete pod allowed-pod; `\n   kubectl delete k8sallowedrepos.constraints.gatekeeper.sh/require-docker-hub; `\n   kubectl delete constrainttemplate.templates.gatekeeper.sh/k8sallowedrepos\n\n\nA running cluster consumes computing resources. If you no longer need it:\n\n   stop it to use it later;\n   delete it permanently.\n","url":"/en/kubernetes/k8s/how-to-guides/gatekeeper/repositories","description":"Using Gatekeeper to enforce constraints that will check that images are downloaded only from allowed repositories.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"3ac53ce3-1959-438f-b441-f830c9207eeb","title":"Using load balancers","data":"For all Kubernetes services of the spec.type: LoadBalancer type, the VK Cloud platform can automatically create an appropriate TCP load balancer with the specified behavior. The load balancer can:\n\nHave a public IP address that is accessible from the Internet, or have a private IP address that is not accessible from the Internet. The IP address can be assigned to the balancer manually or automatically.\n\nUse different algorithms for balancing connections between application instances:\n\n  Selecting random replica (default).\n\n    The balancer behaves this way because kube-proxy in Cloud Containers clusters works in iptables mode.\n    See official Kubernetes documentation for details.\n\n  Bounding a replica to a specific IP address.\n\n    In this case, the balancer will assign a specific application replica to the IP address from which the first request came. As long as that replica remains available, all requests from that address will be forwarded to it.\n\nAllow access only from specific IP addresses.\n\nPreparatory steps\n\nCreate a Kubernetes cluster with the most current version.\n\n   Select the cluster parameters at your own discretion.\n\nMake sure you can connect to the cluster with kubectl.\n\n1. Create application\n\nRequests to this application will be served by the load balancer. To demonstrate the behavior of the load balancer, the application will be deployed as a StatefulSet of two replicas. In this case, all the sub-applications will be numbered, and it will be easy to determine which replica the load balancer will send the request to.\n\nTo create such an application:\n\nCreate a manifest file:\n\n   coffee.yaml\n\n      apiVersion: apps/v1\n   kind: StatefulSet\n   metadata:\n     name: coffee\n   spec:\n     serviceName: coffee\n     replicas: 2\n     selector:\n       matchLabels:\n         app: coffee\n     template:\n       metadata:\n         labels:\n           app: coffee\n       spec:\n         containers:\n         name: coffee\n           image: nginxdemos/nginx-hello:plain-text\n           ports:\n           containerPort: 8080\n\n\nCreate the Kubernetes resource based on the manifest:\n\n      kubectl apply -f coffee.yaml\n\n2. Create load balancers\n\nCreate several load balancers with different behaviors that serve the deployed coffee application:\n\n\n\nManual assignment of static public address, random replica\nAutomatic assignment of dynamic public address, bounded replica\nAutomatic assignment of dynamic public address, random replica, access limiting\nAutomatic assignment of dynamic public address, random replica\n\n\n\nSelect the public IP address you want to assign to the balancer or create a new one. You can do this in VK Cloud personal account.\n\n   There should be no internal IP address attached to the IP address.\n\nCreate the lb-static-public-ip.yaml manifest file.\n\n   In the spec.loadBalancerIP parameter, specify the selected IP address.\n\n      apiVersion: v1\n   kind: Service\n   metadata:\n     name: coffee-svc-public-static-ip\n   spec:\n     type: LoadBalancer\n     loadBalancerIP:\n     ports:\n     port: 80\n       targetPort: 8080\n       protocol: TCP\n       name: http\n     selector:\n       app: coffee\n\nCreate the Kubernetes resource based on the manifest:\n\n      kubectl apply -f lb-static-public-ip.yaml\n\nPeriodically check the status of the service with the command:\n\n      kubectl get svc coffee-svc-public-static-ip\n\n   Wait for the service to be assigned a static public IP address that was set manually earlier: in the EXTERNAL-IP table column, an IP address will appear instead of ``.\n\n\n\n\nCreate the lb-session-affinity.yaml manifest file.\n\n   In the spec.sessionAffinity parameter, specify the value ClientIP. It is responsible for assigning a user session to a specific replica.\n\n      apiVersion: v1\n   kind: Service\n   metadata:\n     name: coffee-svc-session-affinity\n   spec:\n     type: LoadBalancer\n     sessionAffinity: ClientIP\n     ports:\n     port: 80\n       targetPort: 8080\n       protocol: TCP\n       name: http\n     selector:\n       app: coffee\n\nCreate the Kubernetes resource based on the manifest:\n\n      kubectl apply -f lb-session-affinity.yaml\n\nPeriodically check the status of the service with the command:\n\n      kubectl get svc coffee-svc-session-affinity\n\n   Wait for the service to be assigned a dynamic public IP address: in the EXTERNAL-IP table column, an IP address will appear instead of ``.\n\n\n\n\nFind a public IP address of the host that should be allowed access to the application.\n\n   For example, run the command:\n\n      curl icanhazip.com\n\nCreate the lb-restrict-access-by-ip.yaml manifest file.\n\n   In the spec.loadBalancerSourceRanges parameter, specify the list of IP addresses from which access is allowed in IP address/prefix format. Access from other IP addresses will be denied.\n\n   Specify the IP address obtained in the previous step:\n\n      apiVersion: v1\n   kind: Service\n   metadata:\n     name: coffee-svc-restrict-access-by-ip\n   spec:\n     type: LoadBalancer\n     loadBalancerSourceRanges:\n       /32\n     ports:\n     port: 80\n       targetPort: 8080\n       protocol: TCP\n       name: http\n     selector:\n       app: coffee\n\nCreate the Kubernetes resource based on the manifest:\n\n      kubectl apply -f lb-restrict-access-by-ip.yaml\n\nPeriodically check the status of the service with the command:\n\n      kubectl get svc coffee-svc-restrict-access-by-ip\n\n   Wait for the service to be assigned a dynamic public IP address: in the EXTERNAL-IP table column, an IP address will appear instead of ``.\n\n\n\n\nCreate the lb-private-ip.yaml manifest file.\n\n   In the service metadata, specify the service.beta.kubernetes.io/openstack-internal-load-balancer: \"true\" annotation. This annotation is responsible for creating a load balancer with a private IP address.\n\n      apiVersion: v1\n   kind: Service\n   metadata:\n     name: coffee-svc-private-ip\n     annotations:\n       service.beta.kubernetes.io/openstack-internal-load-balancer: \"true\"\n   spec:\n     type: LoadBalancer\n     ports:\n     port: 80\n       targetPort: 8080\n       protocol: TCP\n       name: http\n     selector:\n       app: coffee\n\nCreate the Kubernetes resource based on the manifest:\n\n      kubectl apply -f lb-private-ip.yaml\n\nPeriodically check the status of the service with the command:\n\n      kubectl get svc coffee-svc-private-ip\n\n   Wait for the service to be assigned a private IP address: in the EXTERNAL-IP table column, an IP address will appear instead of ``.\n\n\n\n\nRead more about services and load balancers in official Kubernetes documentation.\n\n3. Check the operation of the load balancers\n\n\n\nLoad balancer with manual assignment of static public address, random replica\nLoad balancer with automatic assignment of dynamic public address, bounded replica\nLoad balancer with automatic assignment of dynamic public address, random replica, access limiting\nLoad balancer with automatic assignment of dynamic public address, random replica\n\n\n\nGet the IP address assigned to the load balancer:\n\n      kubectl get svc coffee-svc-public-static-ip\n\n   The required address is contained in the EXTERNAL-IP column of the table.\n\nRequest the application several times via the load balancer:\n\n      curl http://\n\n   Different pods may respond: both coffee-0 and coffee-1. This behavior means that the load balancer is sending requests to random replicas of the application.\n\n\n\n\nGet the IP address assigned to the load balancer:\n\n      kubectl get svc coffee-svc-session-affinity\n\n   The required address is contained in the EXTERNAL-IP column of the table.\n\nRequest the application several times via the load balancer:\n\n      curl http://\n\n   All requests will be answered by a single selected pod: coffee-0 or coffee-1. This behavior means that the balancer sends requests from a specific IP address to the same application replica.\n\n\n\n\nGet the IP address assigned to the load balancer:\n\n      kubectl get svc coffee-svc-restrict-access-by-ip\n\n   The required address is contained in the EXTERNAL-IP column of the table.\n\nRequest the application several times via the load balancer (from the allowed IP address):\n\n      curl http://\n\n   Different pods may respond: both coffee-0 and coffee-1. This behavior means that the load balancer is sending requests to random replicas of the application.\n\nTry to make the same request from a host with a different IP address. The request will fail.\n\n\n\n\nGet the IP address assigned to the load balancer:\n\n      kubectl get svc coffee-svc-private-ip\n\n   The required address is contained in the EXTERNAL-IP column of the table.\n\nCreate Linux virtual machine in the same subnet where the balancer IP address resides.\n\nConnect to this virtual machine via SSH.\n\nRequest the application several times via the load balancer:\n\n      curl http://\n\n   Different pods may respond: both coffee-0 and coffee-1. This behavior means that the load balancer is sending requests to random replicas of the application.\n\nDelete unused resources\n\nIf the Kubernetes resources you created are no longer needed, delete them.\n\n\n   Together with the services, their respective balancers will be removed. This process can take a long time.\n\n   Linux/macOS\n   Windows\n\n\n      kubectl delete svc coffee-svc-public-static-ip\n   kubectl delete svc coffee-svc-session-affinity\n   kubectl delete svc coffee-svc-restrict-access-by-ip\n   kubectl delete svc coffee-svc-private-ip\n   kubectl delete statefulset coffee\n\n\n      kubectl delete svc coffee-svc-public-static-ip; `\n   kubectl delete svc coffee-svc-session-affinity; `\n   kubectl delete svc coffee-svc-restrict-access-by-ip; `\n   kubectl delete svc coffee-svc-private-ip; `\n   kubectl delete statefulset coffee\n\n\nIf you no longer need the static public IP address that was assigned to the coffee-svc-public-static-ip service, delete it.\n\nA running cluster consumes computing resources. If you no longer need it:\n\n   stop it to use it later;\n   delete it permanently.\n","url":"/en/kubernetes/k8s/how-to-guides/load-balancer","description":"How to create load balancers for different purposes.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"931afbb1-a3dc-49b9-8be8-55492fbe0756","title":"Working with cert-manager using Helm 3","data":"You can manage certificates in Kubernetes clusters using cert-manager:\n\nIssue certificates (including self-signed ones) by sending requests to the sources that act as Certificate Authority (CA).\n\n  Examples of the sources:\n\n  Cybersecurity solutions providers such as Venafi.\n  Certificate providers, such as Let’s Encrypt.\n  Storage for secrets, such as HashiCorp Vault.\n  Local containers containing the public part of a certificate and private key.\n\nAutomatically reissue expiring certificates.\n\nA certificate issued with cert-manager will be available to other Kubernetes resources. For example, it can be used by Ingress.\n\nIt will be shown how to install and upgrade cert-manager using Helm 3 in Kubernetes clusters. A self-signed certificate will also be issued to confirm cert-manager operability.\n\nPreparatory steps\n\nIf you do not already have a Kubernetes cluster, then create one.\n\n\n   To save time, you can create a Kubernetes cluster in a few minutes on the VK Cloud platform.\n\n\nDetermine the cluster's version.\n\nInstall the kubectl utility on the host from which you plan to connect to the cluster, if the utility is not already installed.\n\n   Select the version of kubectl that is compatible with the cluster.\n\nMake sure that you can connect to the cluster using kubectl.\n\nInstall Helm 3.0.0 or higher on the host from which you plan to connect to the cluster, if the utility is not already installed.\n\n   Select the version of Helm that is compatible with the cluster.\n\n1. Add the repository and select the version to install\n\nAdd the cert-manager repository:\n\n      helm repo add jetstack https://charts.jetstack.io\n\nUpdate the charts cache:\n\n      helm repo update\n\nList available cert-manager charts and their versions:\n\n      helm search repo jetstack -l\n\nSelect the cert-manager version to install on the cluster.\n\n   See the compatibility table for the cert-manager and Kubernetes in the official cert-manager documentation.\n\n\n   Next, the 1.11.3 version of cert-manager will be installed to additionally demonstrate the upgrade to 1.12.3 version.\n\n   You could select any version that suits your needs. Adjust the commands listed below to match the selected version.\n\n\n2. Install cert-manager\n\nInstall the Custom Resource Definitions (CRDs) required by cert-manager to operate.\n\n   The CRDs will be installed manually using kubectl. It is the advised approach as it is the most secure.\n\n   Execute the command:\n\n      kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.3/cert-manager.crds.yaml\n\nInstall the selected vesion of cert-manager.\n\n   This command installs a release named cert-manager in the cert-manager namespace. If such a namespace does not exist in the cluster, then it will be created automatically.\n\n   Linux (bash) / macOS (zsh)\n   Windows (PowerShell)\n\n\n      helm install cert-manager jetstack/cert-manager \\\n     --version v1.11.3 \\\n     --namespace cert-manager \\\n     --create-namespace\n\n\n      helm install cert-manager jetstack/cert-manager `\n     --version v1.11.3 `\n     --namespace cert-manager `\n     --create-namespace\n\n\n   On successful completion of the installation Helm will display the message with:\n\n   STATUS: deployed.\n   NOTES: cert-manager v1.11.3 has been deployed successfully!.\n\n   Example of the output\n\n      NAME: cert-manager\n   LAST DEPLOYED: Thu Aug 17 15:06:35 2023\n   NAMESPACE: cert-manager\n   STATUS: deployed\n   REVISION: 1\n   TEST SUITE: None\n   NOTES:\n   cert-manager v1.11.3 has been deployed successfully!\n\n   In order to begin issuing certificates, you will need to set up a ClusterIssuer\n   or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).\n\n   More information on the different types of issuers and how to configure them\n   can be found in our documentation:\n\n   https://cert-manager.io/docs/configuration/\n\n   For information on how to configure cert-manager to automatically provision\n   Certificates for Ingress resources, take a look at the ingress-shim\n   documentation:\n\n   https://cert-manager.io/docs/usage/ingress/\n\n\n3. Confirm cert-manager operability\n\nVerify that all the necessary pods have been successfully created in the cert-manager namespace. The pods should be in the Running state:\n\n      kubectl get pods -n cert-manager\n\n   Example of the output\n\n      NAME                                       READY   STATUS    RESTARTS   AGE\n   cert-manager-...                           1/1     Running   0          3m20s\n   cert-manager-cainjector-...                1/1     Running   0          3m20s\n   cert-manager-webhook-...                   1/1     Running   0          3m20s\n\n\nIssue a self-signed certificate for testing:\n\n   Create a manifest:\n\n      cert-manager-test-resources.yaml\n\n            apiVersion: v1\n      kind: Namespace\n      metadata:\n        name: cert-manager-test\n      ---\n      apiVersion: cert-manager.io/v1\n      kind: Issuer\n      metadata:\n        name: test-selfsigned\n        namespace: cert-manager-test\n      spec:\n        selfSigned: {}\n      ---\n      apiVersion: cert-manager.io/v1\n      kind: Certificate\n      metadata:\n        name: selfsigned-cert\n        namespace: cert-manager-test\n      spec:\n        dnsNames:\n          example.com\n        secretName: selfsigned-cert-tls\n        issuerRef:\n          name: test-selfsigned\n\n\n      This manifest contains the descriptions of:\n\n      The cert-manager-test namespace. The Issuer and Certificate resources will be placed in this namespace.\n      The Issuer resource that is responsible for issuing self-signed certificates.\n      The Certificate resource that holds the parameters of the self-signed certificate to be issued.\n\n   Apply the manifest:\n\n            kubectl apply -f cert-manager-test-resources.yaml\n\n      The described resources will be created. In addition, cert-manager will automatically create other necessary resources.\n\n   Verify that all the necessary resources have been successfully created in the cert-manager-test namespace:\n\n            kubectl get issuers,clusterissuers,certificates,certificaterequests,orders,challenges,secrets -n cert-manager-test\n\n      The command's output should contain:\n\n      Issuer and Certificate in the READY: True status. Their configuration was described in the manifest.\n      CertificateRequest in the READY: True status.\n      Secret containing the certificate's data.\n\n      Example of the output\n\n            NAME                                     READY   AGE\n      issuer.cert-manager.io/test-selfsigned   True    39m\n\n      NAME                                          READY   SECRET                AGE\n      certificate.cert-manager.io/selfsigned-cert   True    selfsigned-cert-tls   39m\n\n      NAME                                                       APPROVED   DENIED   READY   ISSUER            REQUESTOR                                         AGE\n      certificaterequest.cert-manager.io/selfsigned-cert-...     True                True    test-selfsigned   system:serviceaccount:cert-manager:cert-manager   39m\n\n      NAME                         TYPE                DATA   AGE\n      secret/selfsigned-cert-tls   kubernetes.io/tls   3      39m\n\n\n   Verify the certificate's status:\n\n            kubectl describe certificate selfsigned-cert -n cert-manager-test\n\n      If the certificate has been successfully issued:\n\n      Status information (Status) will contain the Certificate is up to date and has not expired string.\n      The Events list will contain the event with the The certificate has been successfully issued message.\n\n      Example of the partial output\n\n            ...\n\n      Status:\n        Conditions:\n          Last Transition Time:  2023-08-17T08:11:27Z\n          Message:               Certificate is up to date and has not expired\n          Observed Generation:   1\n          Reason:                Ready\n          Status:                True\n          Type:                  Ready\n        Not After:               2023-11-15T08:11:27Z\n        Not Before:              2023-08-17T08:11:27Z\n        Renewal Time:            2023-10-16T08:11:27Z\n        Revision:                1\n      Events:\n        Type    Reason     Age    From                                       Message\n        ----    ------     ----   ----                                       -------\n        Normal  Issuing    3m16s  cert-manager-certificates-trigger          Issuing certificate as Secret does not exist\n        Normal  Generated  3m16s  cert-manager-certificates-key-manager      Stored new private key in temporary Secret resource \"selfsigned-cert-...\"\n        Normal  Requested  3m16s  cert-manager-certificates-request-manager  Created new CertificateRequest resource \"selfsigned-cert-...\"\n        Normal  Issuing    3m16s  cert-manager-certificates-issuing          The certificate has been successfully issued\n\n\n   If the certificate has been successfully issued, then cert-manager is correctly installed and operates normally.\n\n4. (Optional) Back up the cert-manager resources\n\n\n\nDue to security reasons, it is advised to create a backup before upgrading cert-manager\n\n\n\nA backup of the Issuer, ClusterIssuer and Certificate reosurces will be created. The following resources are not the part of the backup:\n\nCertificateRequests resources. It is not advised to include such resources in the backup, as it may complicate restoring from the backup.\n\nSecrets that directly store the certificate's data, including private key.\n\n\n  During restoring from the backup, if no matching secret is found for the Certificate resource, then the certificate will be reissued.\n\n\nTo create a backup, execute the command:\n\n\n\nLinux (bash) / macOS (zsh)\nWindows (PowerShell)\n\n\n\nkubectl get -o yaml \\\n  --all-namespaces \\\n  issuer,clusterissuer,certificate \\\ncert-manager-backup.yaml\n\n\n\n\nkubectl get -o yaml `\n  --all-namespaces `\n  issuer,clusterissuer,certificate `\ncert-manager-backup.yaml\n\n\n\n\nRead about advanced backups and restoring from backup in the official cert-manager documentation.\n\n5. Upgrade cert-manager\n\nCheck the version of the installed cert-manager release:\n\n      helm list --namespace cert-manager\n\nUpdate the charts cache:\n\n      helm repo update\n\nList available cert-manager charts and their versions:\n\n      helm search repo jetstack -l\n\nExplore the official cert-manager documentation about the upgrade. It contains upgrade recommendations, a list of breaking changes, and other useful information.\n\n   In particular, it is advised to upgrade one minor version at a time (for example, 1.11.3 → 1.12.3).\n\nSelect the version to upgrade to.\n\n   See the compatibility table for the cert-manager and Kubernetes in the official cert-manager documentation.\n\n\n   Next, the 1.11.3 version of cert-manager will be upgraded to 1.12.3 version.\n\n   If a different version of cert-manager has been installed earlier, then select the correct version to ugrade to, taking into account the recommendations given above.\n\n\nUpgrade the CRDs installed in the cluster.\n\n   Since these CRDs have been installed manually, upgrade them manually as well before upgrading the cert-manager itself.\n\n      kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.3/cert-manager.crds.yaml\n\nUpgrade the cert-manager release to the selected version:\n\n   Linux (bash) / macOS (zsh)\n   Windows (PowerShell)\n\n\n      helm upgrade cert-manager jetstack/cert-manager \\\n     --version v1.12.3 \\\n     --namespace cert-manager\n\n\n      helm upgrade cert-manager jetstack/cert-manager `\n     --version v1.12.3 `\n     --namespace cert-manager\n\n\n   On successful completion of the upgrade, Helm will display the message with:\n   Release \"cert-manager\" has been upgraded. Happy Helming!.\n   STATUS: deployed.\n   NOTES: cert-manager v1.12.3 has been deployed successfully!.\n\n   Example of the output\n\n      Release \"cert-manager\" has been upgraded. Happy Helming!\n   NAME: cert-manager\n   LAST DEPLOYED: Thu Aug 17 15:17:35 2023\n   NAMESPACE: cert-manager\n   STATUS: deployed\n   REVISION: 2\n   TEST SUITE: None\n   NOTES:\n   cert-manager v1.12.3 has been deployed successfully!\n\n   In order to begin issuing certificates, you will need to set up a ClusterIssuer\n   or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).\n\n   More information on the different types of issuers and how to configure them\n   can be found in our documentation:\n\n   https://cert-manager.io/docs/configuration/\n\n   For information on how to configure cert-manager to automatically provision\n   Certificates for Ingress resources, take a look at the ingress-shim\n   documentation:\n\n   https://cert-manager.io/docs/usage/ingress/\n\n\nDelete unused resources\n\nIf the Kubernetes resources, created to confirm cert-manager operability, are the test ones and you no longer need them, then delete them:\n\n      kubectl delete -f cert-manager-test-resources.yaml\n\n\n   The cert-manager-test namespace with all contents, including additional resources automatically created by cert-manager, will be deleted.\n\n\nIf cert-manager has been installed for testing purposes and you no longer need it, then delete all resources associated with it:\n\n   Make sure that there are no resources created by cert-manager in the cluster:\n\n            kubectl get issuers,clusterissuers,certificates,certificaterequests,orders,challenges --all-namespaces\n\n      If there are such resources, delete them.\n\n   Delete the cert-manager release:\n\n            helm delete cert-manager --namespace cert-manager\n\n   Delete the cert-manager namespace:\n\n            kubectl delete ns cert-manager\n\n   Delete the CRDs, installed in the cluster for cert-manager:\n\n            kubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.3/cert-manager.crds.yaml\n","url":"/en/kubernetes/k8s/how-to-guides/case-certmanager-helm3","description":"Installing and upgrading cert-manager using Helm 3 in Kubernetes clusters.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"d79731dc-c029-4c6a-8137-b3ab9d10c99f","title":"Local DNS cache configuration","data":"A local CoreDNS-based caching DNS server can be configured on each cluster node. This is also known as NodeLocal DNS. This will improve the stability and performance of the DNS service in the cluster, without changing existing applications.\n\nOverview of the DNS architecture in a cluster\n\nLet some pod initiate a DNS query.\n\nWithout using caching DNS servers, the following will happen:\n\n  the DNS query will be sent to the IP address of the kube-dns service of the cluster.\n  This IP address will be translated by kube-proxy into the IP address of the kube-dns endpoint using iptables rules. In doing so, iptables will use conntrack to track connections.\n  When a response is received from kube-dns, the reverse process will be performed.\n\n  If DNS queries are mostly sent via UDP protocol, then high load on kube-dns (for example, if any application is actively sending DNS queries) may lead to issues:\n\n  Race condition for conntrack. This results in significant (up to several times) slowdown of responses to DNS queries.\n  Overflow of service tables for conntrack. Records for UDP are removed from these tables only by time-out (default — 30 seconds). If tables are full, new DNS queries sent via UDP will be dropped.\n\nWhen using caching DNS servers, the following will happen:\n\n  Pods will refer to the local caching DNS-server, which is located on the same node as the pods.\n\n     This will avoid address translation (Dynamic NAT, DNAT), the use of iptables and conntrack. The issues described above will be eliminated.\n\n  The caching DNS-server itself will address to service kube-dns using iptables and conntrack, but via TCP protocol.\n\n     In this case load on kube-dns is reduced, because it is queried directly by a limited number of DNS-servers and not by all services of the cluster that require DNS for their work. Also, when using TCP, the latency associated with UDP packet loss and time-outs is reduced.\n\nSee official Kubernetes documentation for more details.\n\n1. Preparatory steps\n\nCreate a Kubernetes cluster of the most current version.\n\n   Choose the cluster parameters at your own discretion.\n\nMake sure that you can connect to the cluster using kubectl.\n\n2. Deploy a caching DNS server on each node\n\nThe DNS server will be deployed in the following configuration:\n\nDeployment type: DaemonSet so that the server is available on all nodes in the cluster.\nIP addresses that the server listens to:\n  A local 169.254.0.10 IP address on each node. This link-local address is specifically chosen so that there is no overlap with addresses from other subnets used by the cluster.\n  The IP address of the kube-dns service. In Cloud Containers clusters, this is always 10.254.0.10.\n\n  This configuration is used because kube-proxy in Cloud Containers clusters operates in iptables mode.\n\nThe port to which Prometheus will connect to collect metrics: 9153.\nLabel for service selection: kube-dns: coredns.\nCluster domain: cluster.local.\n\nSee official Kubernetes documentation for more details about configurations and settings.\n\nTo deploy a DNS server:\n\nCreate a manifest file for NodeLocal DNS:\n\n   nodelocaldns.yaml\n\nCopyright 2018 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\n   apiVersion: v1\n   kind: ServiceAccount\n   metadata:\n     name: node-local-dns\n     namespace: kube-system\n     labels:\n       kubernetes.io/cluster-service: \"true\"\n       addonmanager.kubernetes.io/mode: Reconcile\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: kube-dns-upstream\n     namespace: kube-system\n     labels:\n       k8s-app: coredns\n       kubernetes.io/cluster-service: \"true\"\n       addonmanager.kubernetes.io/mode: Reconcile\n       kubernetes.io/name: \"KubeDNSUpstream\"\n   spec:\n     ports:\n     name: dns\n       port: 53\n       protocol: UDP\n       targetPort: 53\n     name: dns-tcp\n       port: 53\n       protocol: TCP\n       targetPort: 53\n     selector:\n       k8s-app: coredns\n   ---\n   apiVersion: v1\n   kind: ConfigMap\n   metadata:\n     name: node-local-dns\n     namespace: kube-system\n     labels:\n       addonmanager.kubernetes.io/mode: Reconcile\n   data:\n     Corefile: |\n       cluster.local:53 {\n           errors\n           cache {\n                   success 9984 30\n                   denial 9984 5\n           }\n           reload\n           loop\n           bind 169.254.0.10 10.254.0.10\n           forward . PILLAR__CLUSTER__DNS {\n                   force_tcp\n           }\n           prometheus :9153\n           health 169.254.0.10:8080\n           }\n       in-addr.arpa:53 {\n           errors\n           cache 30\n           reload\n           loop\n           bind 169.254.0.10 10.254.0.10\n           forward . PILLAR__CLUSTER__DNS {\n                   force_tcp\n           }\n           prometheus :9153\n           }\n       ip6.arpa:53 {\n           errors\n           cache 30\n           reload\n           loop\n           bind 169.254.0.10 10.254.0.10\n           forward . PILLAR__CLUSTER__DNS {\n                   force_tcp\n           }\n           prometheus :9153\n           }\n       .:53 {\n           errors\n           cache 30\n           reload\n           loop\n           bind 169.254.0.10 10.254.0.10\n           forward . PILLAR__UPSTREAM__SERVERS\n           prometheus :9153\n           }\n   ---\n   apiVersion: apps/v1\n   kind: DaemonSet\n   metadata:\n     name: node-local-dns\n     namespace: kube-system\n     labels:\n       k8s-app: node-local-dns\n       kubernetes.io/cluster-service: \"true\"\n       addonmanager.kubernetes.io/mode: Reconcile\n   spec:\n     updateStrategy:\n       rollingUpdate:\n         maxUnavailable: 10%\n     selector:\n       matchLabels:\n         k8s-app: node-local-dns\n     template:\n       metadata:\n         labels:\n           k8s-app: node-local-dns\n         annotations:\n           prometheus.io/port: \"9153\"\n           prometheus.io/scrape: \"true\"\n       spec:\n         priorityClassName: system-node-critical\n         serviceAccountName: node-local-dns\n         hostNetwork: true\n         dnsPolicy: Default  # Don't use cluster DNS.\n         tolerations:\n         key: \"CriticalAddonsOnly\"\n           operator: \"Exists\"\n         effect: \"NoExecute\"\n           operator: \"Exists\"\n         effect: \"NoSchedule\"\n           operator: \"Exists\"\n         containers:\n         name: node-cache\n           image: registry.k8s.io/dns/k8s-dns-node-cache:1.22.13\n           resources:\n             requests:\n               cpu: 25m\n               memory: 5Mi\n           args: [ \"-localip\", \"169.254.0.10,10.254.0.10\", \"-conf\", \"/etc/Corefile\", \"-upstreamsvc\", \"kube-dns-upstream\" ]\n           securityContext:\n             capabilities:\n               add:\n               NET_ADMIN\n           ports:\n           containerPort: 53\n             name: dns\n             protocol: UDP\n           containerPort: 53\n             name: dns-tcp\n             protocol: TCP\n           containerPort: 9153\n             name: metrics\n             protocol: TCP\n           livenessProbe:\n             httpGet:\n               host: 169.254.0.10\n               path: /health\n               port: 8080\n             initialDelaySeconds: 60\n             timeoutSeconds: 5\n           volumeMounts:\n           mountPath: /run/xtables.lock\n             name: xtables-lock\n             readOnly: false\n           name: config-volume\n             mountPath: /etc/coredns\n           name: kube-dns-config\n             mountPath: /etc/kube-dns\n         volumes:\n         name: xtables-lock\n           hostPath:\n             path: /run/xtables.lock\n             type: FileOrCreate\n         name: kube-dns-config\n           configMap:\n             name: kube-dns\n             optional: true\n         name: config-volume\n           configMap:\n             name: node-local-dns\n             items:\n               key: Corefile\n                 path: Corefile.base\n   ---\nA headless service is a service with a service IP but instead of load-balancing it will return the IPs of our associated Pods.\nWe use this to expose metrics to Prometheus.\n   apiVersion: v1\n   kind: Service\n   metadata:\n     annotations:\n       prometheus.io/port: \"9153\"\n       prometheus.io/scrape: \"true\"\n     labels:\n       k8s-app: node-local-dns\n     name: node-local-dns\n     namespace: kube-system\n   spec:\n     clusterIP: None\n     ports:\n       name: metrics\n         port: 9153\n         targetPort: 9153\n     selector:\n       k8s-app: node-local-dns\n\n\n   This file is based on the manifest from the official Kubernetes repository, taking into account the configuration above.\n\nCreate the Kubernetes resources required for NodeLocal DNS based on the manifest:\n\n      kubectl apply -f nodelocaldns.yaml\n\n   The following information about the created resources should be displayed:\n\n      serviceaccount/node-local-dns created\n   service/kube-dns-upstream created\n   configmap/node-local-dns created\n   daemonset.apps/node-local-dns created\n   service/node-local-dns created\n\n3. Check the operation of the caching DNS server\n\nCreate pod that includes utilities to work with DNS:\n\n      kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml\n\nVerify that the pod is successfully created by running the command:\n\n      kubectl get pod dnsutils\n\n   Output should give you the similar information:\n\n      NAME       READY   STATUS    RESTARTS    AGE\n   dnsutils   1/1     Running   ...         ...\n\nConnect to a bash session inside this pod:\n\n      kubectl exec -it dnsutils -- bash\n\nCheck that the DNS queries are successful:\n\n   Without specifying a DNS server\n   With explicitly specified DNS server\n\n\n      nslookup cloud.vk.com && \\\n   nslookup kubernetes.default\n\n   Output should give you the similar information:\n\n      Server:         10.254.0.10\n   Address:        10.254.0.10#53\n\n   Non-authoritative answer:\n   Name:   cloud.vk.com\n   Address: 95.163.254.194\n   Name:   cloud.vk.com\n   Address: 95.163.254.192\n   Name:   cloud.vk.com\n   Address: 95.163.254.193\n\n   Server:         10.254.0.10\n   Address:        10.254.0.10#53\n\n   Name:   kubernetes.default.svc.cluster.local\n   Address: 10.254.0.1\n\n\n      nslookup cloud.vk.com 169.254.0.10 && \\\n   nslookup kubernetes.default 169.254.0.10\n\n   Output should give you the similar information:\n\n      Server:         169.254.0.10\n   Address:        169.254.0.10#53\n\n   Non-authoritative answer:\n   Name:   cloud.vk.com\n   Address: 95.163.254.192\n   Name:   cloud.vk.com\n   Address: 95.163.254.193\n   Name:   cloud.vk.com\n   Address: 95.163.254.194\n\n   Server:         169.254.0.10\n   Address:        169.254.0.10#53\n\n   Name:   kubernetes.default.svc.cluster.local\n   Address: 10.254.0.1\n\n\nEnd the bash session in the dnsutils pod:\n\n      exit\n\nDelete unused resources\n\nIf the Kubernetes resources you created are no longer needed, delete them.\n\n   Linux/macOS\n   Windows\n\n\n      kubectl delete -f https://k8s.io/examples/admin/dns/dnsutils.yaml\n   kubectl  -f nodelocaldns.yaml\n\n\n      kubectl delete -f https://k8s.io/examples/admin/dns/dnsutils.yaml; `\n   kubectl  -f nodelocaldns.yaml\n\n\nA running cluster consumes computing resources. If you no longer need it:\n\n   stop it to use it later;\n   delete it permanently.\n","url":"/en/kubernetes/k8s/how-to-guides/local-dns-cache","description":"How to configure a local DNS cache on each cluster node to reduce the load on kube-dns and improve the speed and stability of DNS queries processing.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"9e12acd2-4d24-4b59-8eb4-73a642a19312","title":"Backup with Velero","data":"With Velero you can back up and restore cluster data.\n\nPreparatory steps\n\nCreate a cluster Kubernetes of the most current version.\n\n   Place one or more groups of worker nodes in the GZ1 availability area.\n\n   The rest of the cluster parameters are at your discretion.\n\nMake sure that you can connect to the cluster with kubectl.\nMake sure that Velero is installed and configured.\nInstall OpenStack CLI if it is not already installed. Make sure you can authorize in the cloud using it.\n\n1. Deploy application\n\nTo get familiar with creating a backup and restoring from it, deploy a demo application coffee. A persistent volume will be attached to this application.\n\nCreate a manifest file:\n\n   coffee.yaml\n\n      apiVersion: v1\n   kind: Namespace\n   metadata:\n     name: example-app\n\n   ---\n   apiVersion: v1\n   kind: PersistentVolumeClaim\n   metadata:\n     name: coffee-pvc\n     namespace: example-app\n   spec:\n     accessModes:\n       ReadWriteOnce\n     resources:\n       requests:\n         storage: 1Gi\n     storageClassName: \"csi-ceph-hdd-gz1\"\n\n   ---\n   apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: coffee\n     namespace: example-app\n   spec:\n     replicas: 1\n     selector:\n       matchLabels:\n         app: coffee\n     template:\n       metadata:\n         labels:\n           app: coffee\n       spec:\n         volumes:\n           name: coffee-volume\n             persistentVolumeClaim:\n               claimName: coffee-pvc\n         initContainers:\n           name: write-html-for-nginx\n             image: busybox\n             volumeMounts:\n               name: coffee-volume\n                 mountPath: /usr/share/nginx/html\n             command: [\"/bin/sh\", \"-c\"]\n             args:\n               [\n                 'echo \"The coffee pod says Hello World to everyone! This file is located on the dynamically claimed Cinder ReadWriteOnce persistent volume.\" > /usr/share/nginx/html/index.html',\n               ]\n         containers:\n           name: coffee\n             image: nginxdemos/nginx-hello\n             volumeMounts:\n               name: coffee-volume\n                 mountPath: /usr/share/nginx/html\n             ports:\n               containerPort: 8080\n\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: coffee-svc\n     namespace: example-app\n   spec:\n     type: LoadBalancer\n     ports:\n       port: 80\n         targetPort: 8080\n         protocol: TCP\n         name: http\n     selector:\n       app: coffee\n\n\nCreate the necessary Kubernetes resources based on the manifest:\n\n      kubectl apply -f coffee.yaml\n\n   All the Kubernetes resources needed for the application will be placed in a separate namespace example-app.\n\nMake sure that a disk is created for the persistent volume:\n\n   Locate the persistent volume created for the application:\n\n            kubectl get pv -n example-app\n\n       In the table displayed, find the ID of the persistent volume for which example-app/coffee-pvc is listed in the CLAIM column:\n\n            NAME                                       ...   STATUS   CLAIM                    ...\n      ...                                        ...   ...      ...                      ...\n                           ...   Bound    example-app/coffee-pvc   ...\n\n   Get the disk ID for the permanent volume created:\n\n            kubectl describe pv  -n example-app\n\n      The output of the command will contain the disk ID in the `VolumeHandle` parameter:\n\n            ...\n      Source:\n          Type:              CSI (a Container Storage Interface (CSI) volume source)\n          Driver:            cinder.csi.openstack.org\n          FSType:            ...\n          VolumeHandle:\n          ...\n\n   Get detailed information about the disk with this ID using the OpenStack CLI:\n\n            openstack volume show  --fit-width\n\nWait until the load balancer is assigned a public IP address.\n\n    Check the status of the load balancer periodically:\n\n      kubectl get svc -n example-app\n\n   The EXTERNAL-IP column should show the public IP address assigned to the load balancer.\n\nMake sure that NGINX is responding to requests:\n\n      curl\n\n   The following should be output:\n\n      The coffee pod says Hello World to everyone! This file is located on the dynamically claimed Cinder ReadWriteOnce persistent volume.\n\n2. Create a backup of the application\n\nCreate a manual backup of the entire example-app namespace, which contains the resources you need for the application:\n\n      velero backup create coffee-backup --include-namespaces example-app\n\nLoad the backup details:\n\n      velero backup describe coffee-backup\n\n\n   The default backup lifetime is 720 hours. After that time, the backup will be deleted.\n\n\nCheck the logs of the backup operation (if necessary):\n\n      velero backup logs coffee-backup\n\nIt's also possible to do automatic scheduled backups. For more information about scheduled backups, see Velero's help:\n\nvelero help\n\n3. Restore the application from the backup\n\nSimulate application failure. To do this, delete the example-app namespace, which contains the resources needed for the application to work:\n\n      kubectl delete ns example-app\n\nPerform a restore from the backup that was created earlier:\n\n      velero restore create --from-backup coffee-backup\n\n   The command will restore the data to the same cluster that was backed up. If you need to restore data to a new cluster:\n\n   Create cluster.\n   Install Velero in the cluster.\n   Run the above command.\n\nWait until the load balancer is assigned a public IP address.\n\n   Check the status of the load balancer periodically:\n\n      kubectl get svc -n example-app\n\n   The EXTERNAL-IP column should show the public IP address assigned to the load balancer.\n\nMake sure that NGINX is responding to requests:\n\n      curl\n\n   The following should be output:\n\n      The coffee pod says Hello World to everyone! This file is located on the dynamically claimed Cinder ReadWriteOnce persistent volume.\n\nDelete unused resources\n\nIf the Kubernetes resources you created are no longer needed, delete them.\n\n   Linux/macOS\n   Windows\n\n\n      kubectl delete ns example-app\n   velero backup delete coffee-backup\n\n\n      kubectl delete ns example-app; `\n   velero backup delete coffee-backup\n\n\nIf you no longer need Velero, delete it:\n\n      velero uninstall\n\nIf you don't need the backups anymore, delete them from the bucket that Velero used.\n\n   If necessary, also delete the bucket itself.\n\nA running cluster consumes computing resources. If you no longer need it:\n\n   stop it to use it later;\n   delete it permanently.\n","url":"/en/kubernetes/k8s/how-to-guides/velero-backup","description":"How to use Velero to back up and restore Kubernetes cluster resources.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"3c085d42-5e20-4f56-80ce-5f06511aa899","title":"Installing and using ExternalDNS","data":"ExternalDNS allows you to automate DNS record management when working with the Ingress or Service resources. If these resources are configured according to ExternalDNS requirements, they will be accessible by their domain names as soon as they are created.\n\nExternalDNS integrates with the VK Cloud DNS service via a plugin. Below is an example of installing ExternalDNS to a cluster and using this tool with the Ingress and Service resources.\n\nPreparatory steps\n\nCreate a DNS zone for ExternalDNS to work with, if not already done.\n\n   In the example below, the example.com zone is used.\n\nCreate a Cloud Containers cluster of the latest version that has an external IP address and is accessible from the Internet.\n\n   Select other cluster parameters at your discretion.\n\nMake sure that you can connect to the cluster using kubectl.\n\n   To connect, use the cluster configuration file (kubeconfig) downloaded from your VK Cloud personal account.\n\nInstall Helm version 3.0.0 or higher if the utility is not already installed.\n\n   To install, select a version of Helm that is compatible with the cluster.\n\nSet an environment variable that points to kubeconfig for the cluster. This will make it easier to work with the kubectl and helm utilities when installing ExternalDNS.\n\n   The path to your kubeconfig file may differ from the example below.\n\n   Linux (bash)/macOS (zsh)\n   Windows (PowerShell)\n\n\n      export KUBECONFIG=\"/home/user/.kube/kubernetes-cluster-1234_kubeconfig.yaml\"\n\n\n      $Env:KUBECONFIG=\"C:\\Users\\user\\.kube\\kubernetes-cluster-1234_kubeconfig.yaml\"\n\n\n1. Prepare a user for ExternalDNS\n\nExternalDNS will use this VK Cloud user's credentials to cooperate with the VK Cloud API and manage DNS resource records.\n\nPrepare the user and get all the necessary credentials:\n\nSelect an existing user or invite a new user to the project.\n\n   User Requirements:\n\n   API access must be enabled.\n   One of the following roles must be assigned in order for ExternalDNS to operate resource records within the DNS zone:\n\n     Network Administrator (a minimum required role).\n     Project Administrator.\n     Superadministrator.\n     Project Owner.\n\n\n     To work with ExternalDNS, it is recommended to assign a dedicated user with the Network administrator role. This will minimize possible damage if an attacker gains access to this user's credentials.\n\n\nGet the credentials you need to access the VK Cloud API:\n\n   Go to your VK Cloud personal account using the credentials of the user assigned to ExternalDNS.\n   Click the username in the page header and select Project Settings.\n   Go to the API access tab and note the values of the following parameters:\n\n      Project ID (Project ID for OpenStack);\n      User Domain Name;\n      Username;\n      Region Name;\n      Auth URL (Authentication URL).\n\nWrite down the password for this user: it is also required to access the API.\n\n2. Install ExternalDNS\n\nCreate a namespace where ExternalDNS will be installed:\n\n      kubectl create ns external-dns\n\nIn this namespace, create a secret that contains the VK Cloud API access credentials obtained during user preparation:\n\n   Linux (bash)/macOS (zsh)\n   Windows (PowerShell)\n\n\n      kubectl -n external-dns create secret generic vkcs-auth \\\n     --from-literal=ProjectID=\"\" \\\n     --from-literal=UserDomainName=\"\" \\\n     --from-literal=Username=\"\" \\\n     --from-literal=RegionName=\"\" \\\n     --from-literal=AuthURL=\"\" \\\n     --from-literal=Password=\"\"\n\n\n      kubectl -n external-dns create secret generic vkcs-auth `\n     --from-literal=ProjectID=\"\" `\n     --from-literal=UserDomainName=\"\" `\n     --from-literal=Username=\"\" `\n     --from-literal=RegionName=\"\" `\n     --from-literal=AuthURL=\"\" `\n     --from-literal=Password=\"\"\n\n\nAdd the Bitnami Helm repository:\n\n      helm repo add bitnami https://charts.bitnami.com/bitnami\n\nCreate a file that contains the values needed to install ExternalDNS with Helm:\n\n   external-dns-vkcs-values.yaml\n\n      policy: upsert-only\n   txtPrefix: externaldns-\n\n   provider: webhook\n\n   extraArgs:\n     webhook-provider-url: http://localhost:8888\n\n   sidecars:\n     name: vkcs-plugin\n       image: registry.infra.mail.ru:5010/external-dns-vkcs-plugin:latest\n       imagePullPolicy: Always\n       ports:\n         name: http\n           containerPort: 8888\n       livenessProbe:\n         httpGet:\n           path: /healthz\n           port: http\n         initialDelaySeconds: 10\n         timeoutSeconds: 5\n       readinessProbe:\n         httpGet:\n           path: /healthz\n           port: http\n         initialDelaySeconds: 10\n         timeoutSeconds: 5\n       env:\n         name: OS_AUTH_URL\n           valueFrom:\n             secretKeyRef:\n               name: vkcs-auth\n               key: AuthURL\n         name: OS_USERNAME\n           valueFrom:\n             secretKeyRef:\n               name: vkcs-auth\n               key: Username\n         name: OS_PASSWORD\n           valueFrom:\n             secretKeyRef:\n               name: vkcs-auth\n               key: Password\n         name: OS_PROJECT_ID\n           valueFrom:\n             secretKeyRef:\n               name: vkcs-auth\n               key: ProjectID\n         name: OS_DOMAIN_NAME\n           valueFrom:\n             secretKeyRef:\n               name: vkcs-auth\n               key: UserDomainName\n         name: OS_REGION_NAME\n           valueFrom:\n             secretKeyRef:\n               name: vkcs-auth\n               key: RegionName\n         name: SERVER_HOST\n           value: \"0.0.0.0\"\n         name: SERVER_PORT\n           value: \"8888\"\n\n\n   The behavior of the ExternalDNS Helm chart is influenced by many values. The created file contains a minimum set of values that is sufficient to start working with ExternalDNS. The most important values affecting the behavior of ExternalDNS with the VK Cloud DNS are described below. Descriptions of values that do not apply to the VK Cloud plugin (all values except sidecars[]) are given in README.md for the chart.\n\n\n   Do not change or delete the required values listed below. This may cause ExternalDNS to work incorrectly.\n\n   Description of important values that affect the behavior of ExternalDNS\n\n   (Required value) provider: the webhook value indicates that an external plugin with webhooks support is required to work with DNS.\n\n   (Required value) extraArgs:webhook-provider-url: the URL to be used to interact with the plugin.\n\n   policy: the resource record synchronization policy. The default policy is upsert-only: ExternalDNS will only add resource records, not delete them.\n\n     If you want ExternalDNS to remove resource records when deleting Kubernetes resources for which the records were created, use the sync policy.\n\n   txtPrefix: prefix for TXT records that are created by ExternalDNS.\n\n     ExternalDNS can automatically add both A records and CNAME records for Kubernetes resources.\n\n     ExternalDNS keeps track of which DNS zone resource records it manages by placing service information in TXT records. In particular, with the default settings, it will create a service TXT record with the same name as the record being added: for example, for the A record example.com, a corresponding TXT record with the same name example.com will be created.\n\n     However, according to RFC 1912, CNAME records cannot coexist with other records with the same name. That is why ExternalDNS is configured to prefix the name of TXT records with the value specified in txtPrefix. This allows you to avoid possible collisions when working with CNAME records: for example, for the CNAME record example.com a corresponding TXT record with the name externaldns-example.com will be created.\n\n     You can specify a prefix different from externaldns-, if necessary.\n\n\n   The plugin for ExternalDNS, which provides its integration with the VK Cloud DNS, has many settings that affect the plugin's behavior. The settings are set using environment variables in sidecars[].env. In the created file, only the required settings are set. If necessary, you can specify additional settings for the plugin by adding the appropriate environment variables.\n\n\n   Do not modify or delete the required plugin settings listed below. This may cause ExternalDNS to work incorrectly.\n\n    Description of values that affect plugin behavior\n\n   (Required settings) The settings corresponding to the variables with the OS_ prefix are used to authenticate the plugin when interacting with the VK Cloud API.\n\n     The values of these variables are stored in the Kubernetes secret that was created earlier.\n\n   (Required settings) The settings corresponding to the SERVER_HOST and SERVER_PORT variables have fixed values and are necessary for the plugin to work correctly.\n\n   DNS zone filtering settings:\n\n     Filters for DNS zones in which resource records are allowed to be created:\n\n       DOMAIN_FILTERS: a string with a list of domain names separated by commas. For example, example.com,contoso.com.\n       REGEX_DOMAIN_FILTER: a string with a regular expression (RE2 syntax). For example, .*.com$.\n\n       If both filters are configured, the REGEX_DOMAIN_FILTER filter takes precedence over DOMAIN_FILTERS. By default, no filtering is performed.\n\n     Filters for DNS zones in which resource records are not allowed to be created:\n\n       EXCLUDE_DOMAINS: a string with a list of domain names separated by commas. For example, example.org,foo.bar.com.\n       REGEX_DOMAIN_FILTER_EXCLUSION: a string with a regular expression (RE2 syntax). For example, ^stage-.*.com$.\n\n       If both filters are configured, the REGEX_DOMAIN_FILTER_EXCLUSION filter takes precedence over EXCLUDE_DOMAINS. By default, no filtering is performed.\n\n     SERVER_READ_TIMEOUT: timeout for reading when the connection to the server is open (in seconds). Default value: 30.\n     SERVER_WRITE_TIMEOUT: timeout for writing when the connection to the server is open (in seconds). Default value: 30.\n\n     LOG_LEVEL: level of logging of events that occur during plugin operation.\n\n       The error, warn, info, debug, and trace levels are supported. Default value: info.\n\n     DRY_RUN: flag that allows the plugin to be run in \"dry run\" mode.\n\n       false (default): \"dry run\" mode is disabled. The plugin runs and manipulates resource records in the DNS zone as configured.\n       true: \"dry run\" mode is enabled. The plugin runs but does not manipulate resource records in the DNS zone: no resource records will be created or deleted.\n\n\nInstall ExternalDNS:\n\n      helm -n external-dns install external-dns-vkcs bitnami/external-dns -f external-dns-vkcs-values.yaml\n\nVerify that the Helm chart has been successfully deployed:\n\n      helm -n external-dns list && kubectl -n external-dns get all\n\n   Example of partial output of the command\n\n      NAME                    NAMESPACE       ...        ...   STATUS          CHART                 ...\n   external-dns-vkcs       external-dns    ...        ...   deployed        external-dns-6.32.1   ...\n\n   NAME                                     READY   STATUS    RESTARTS   AGE\n   pod/external-dns-vkcs-NNNNNNNNNN-MMMMM   2/2     Running   0          ...\n\n   NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\n   service/external-dns-vkcs   ClusterIP   10.254.169.195           7979/TCP   ...\n\n   NAME                                READY   UP-TO-DATE   AVAILABLE   AGE\n   deployment.apps/external-dns-vkcs   1/1     1            1           87s\n\n   NAME                                           DESIRED   CURRENT   READY   AGE\n   replicaset.apps/external-dns-vkcs-NNNNNNNNNN   1         1         1       ...\n\n\n3. Verify that External DNS is working\n\nNext, several demo applications based on NGINX's Cafe example will be deployed. These applications will be published (made available from the Internet) using Service and Ingress configured to work with ExternalDNS.\n\n3.1. Publish the application using a service like LoadBalancer\n\nCreate a manifest for the tea application:\n\n   tea-app.yaml\n\n      apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: tea\n     labels:\n       app: tea\n   spec:\n     replicas: 3\n     selector:\n       matchLabels:\n         app: tea\n     template:\n       metadata:\n         labels:\n           app: tea\n       spec:\n         containers:\n         name: tea\n           image: nginxdemos/nginx-hello:plain-text\n           ports:\n           containerPort: 8080\n\n\nApply this manifest to the cluster to deploy the application:\n\n      kubectl apply -f tea-app.yaml\n\nVerify that the application has been successfully deployed as a ReplicaSet of three replicas:\n\n      kubectl get rs,pod -l app==tea\n\n   Example of partial output of the command\n\n      NAME                           DESIRED   CURRENT   READY   AGE\n   replicaset.apps/tea-XXXXXXXXX  3         3         3       ...\n\n   NAME                      READY   STATUS    RESTARTS   AGE\n   pod/tea-XXXXXXXXX-AAAAA   1/1     Running   0          ...\n   pod/tea-XXXXXXXXX-BBBBB   1/1     Running   0          ...\n   pod/tea-XXXXXXXXX-CCCCC   1/1     Running   0          ...\n\n\nCreate a tea-service.yaml manifest for the Kubernetes service (Service).\n\n   This service will be used to publish the deployed application. The application will be accessible by the domain name tea.example.com.\n\n   To have ExternalDNS create resource records for the service:\n\n   The external-dns.alpha.kubernetes.io/hostname annotation must be set for the service.\n   The service must have the LoadBalancer type.\n\n   Specify the external-dns.alpha.kubernetes.io/ttl annotation if you want to set a non-standard TTL for resource records (default: 86400 seconds, 24 hours).\n\n      apiVersion: v1\n   kind: Service\n   metadata:\n     annotations:\n       external-dns.alpha.kubernetes.io/hostname: \"tea.example.com\"\n       external-dns.alpha.kubernetes.io/ttl: \"3600\"\n     name: tea-svc\n     labels:\n       app: tea\n   spec:\n     type: LoadBalancer\n     ports:\n     port: 80\n       targetPort: 8080\n       protocol: TCP\n       name: http\n     selector:\n       app: tea\n\n   Here:\n\n   The required external-dns.alpha.kubernetes.io/hostname annotation is specified: the domain name to use for the service.\n   The optional external-dns.alpha.kubernetes.io/ttl annotation is specified: TTL in seconds for the resource record to be created by ExternalDNS.\n\n   The LoadBalancer service type is selected. A standard load balancer will be created for such a service. Since the load balancer is created with a public IP address, the application associated with the service will be accessible from the Internet.\n\n\n     Using the load balancer is charged.\n\n\nApply this manifest to the cluster to create the service:\n\n      kubectl apply -f tea-service.yaml\n\nCheck the status of the service:\n\n      kubectl get svc tea-svc\n\n   Wait until the service is assigned the public IP address of the load balancer. Creating the load balancer can take a long time.\n\n    Example of partial output of the command\n\n   The balancer creation is in progress:\n\n          NAME      TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)        AGE\n     tea-svc   LoadBalancer   10.254.170.195         80:32314/TCP   ...\n\n   The balancer has been successfully created:\n\n          NAME      TYPE           CLUSTER-IP       EXTERNAL-IP        PORT(S)        AGE\n     tea-svc   LoadBalancer   10.254.170.195   203.0.113.111      80:32314/TCP   ...\n\n\nVerify that ExternalDNS has created the necessary resource records:\n\n   Get a list of resource records for the example.com zone.\n   Find the entries created by ExternalDNS in the list:\n\n      One A record tea.example.com.\n      Two TXT records externaldns-tea.example.com and externaldns-a-tea.example.com.\n\n        These TXT records are service records used by ExternalDNS to track the status of the tea-svc A record created for the tea-svc service.\n\n\n        Such records are easily distinguished by the externaldns- prefix in their name. Their values have the standard structure of the form heritage=.../owner=.../resource=....\n\n        If you specified a different prefix value when installing ExternalDNS, the names of the service TXT records will be different.\n\n\n      If the required resource records are not present, wait a few more minutes. ExternalDNS will start creating resource records after the service has been assigned an IP address. This will take some time.\n\nCheck that the application is accessible by domain name. To do this, go to http://tea.example.com in your browser.\n\n   You should see a page with a response from the application like this:\n\n      Server address: 10.100.184.219:8080\n   Server name: tea-XXXXXXXXX-AAAAA\n   Date: 09/Feb/2024:10:09:51 +0000\n   URI: /\n   Request ID:\n\n   Successful interaction with the application at this address indicates that ExternalDNS works correctly with a service like LoadBalancer.\n\n3.2. Publish the application using Ingress\n\nInstall the Ingress NGINX add-on of the latest version to the cluster.\n\n   Perform a standard installation. Do not change any parameters, only edit the add-on configuration code:\n\n   Make sure that the service.beta.kubernetes.io/openstack-internal-load-balancer annotation is set to false:\n\n            controller:\n        ...\n        service:\n          annotations: {\"loadbalancer.openstack.org/proxy-protocol\": \"true\", \"service.beta.kubernetes.io/openstack-internal-load-balancer\": \"false\"}\n          ...\n\n      This is necessary to create a load balancer with a public IP address for the Ingress controller. Then the application using Ingress will be accessible from the Internet.\n\n   Set the controller.publishService.enabled field to true:\n\n            controller:\n        ...\n        publishService:\n          enabled: true\n          pathOverride: \"\"\n          ...\n\n      This is necessary to assign a public IP address of the Ingress controller to the Ingress resource. This will allow ExternalDNS to create the correct resource records for Ingress.\n\n   Wait for the add-on installation to complete. This process may take a long time.\n\nCreate a manifest for the coffee application:\n\n   coffee-app.yaml\n\n      apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: coffee\n     labels:\n       app: coffee\n   spec:\n     replicas: 2\n     selector:\n       matchLabels:\n         app: coffee\n     template:\n       metadata:\n         labels:\n           app: coffee\n       spec:\n         containers:\n         name: coffee\n           image: nginxdemos/nginx-hello:plain-text\n           ports:\n           containerPort: 8080\n\n\nCreate a manifest for the Kubernetes service that the application will use.\n\n   coffee-service.yaml\n\n      apiVersion: v1\n   kind: Service\n   metadata:\n     name: coffee-svc\n     labels:\n      app: coffee\n   spec:\n     type: ClusterIP\n     ports:\n     port: 80\n       targetPort: 8080\n       protocol: TCP\n       name: http\n     selector:\n       app: coffee\n\n\n   Here, the service type is set to ClusterIP, which is sufficient since the application will be published using Ingress. Such a service is accessible only from within the cluster and does not have a dedicated load balancer, unlike the service created earlier.\n\nApply these manifests to the cluster to create all necessary resources:\n\n      kubectl apply -f coffee-app.yaml -f coffee-service.yaml\n\nCheck that the application has been successfully deployed as a ReplicaSet of two replicas along with the corresponding service:\n\n      kubectl get rs,pod,svc -l app==coffee\n\n   Example of partial output of the command\n\n      NAME                                DESIRED   CURRENT   READY   AGE\n   replicaset.apps/coffee-YYYYYYYYY    2         2         2       ...\n\n   NAME                         READY   STATUS    RESTARTS   AGE\n   pod/coffee-YYYYYYYYY-DDDDD   1/1     Running   0          ...\n   pod/coffee-YYYYYYYYY-EEEEE   1/1     Running   0          ...\n\n   NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\n   service/coffee-svc   ClusterIP   10.254.243.13           80/TCP    ...\n\n\nCreate the cafe-ingress.yaml manifest for the Ingress resource.\n\n   This Ingress resource will be used to publish the deployed application. The application will be available on the cafe.example.com domain at the http://cafe.example.com/coffee URL.\n\n   For ExternalDNS to create resource records for the service, no additional values need to be specified in the manifest: having an Ingress controller configured in the right way is sufficient. The domain name values will be taken from the host fields for the Ingress spec.rules[] rules.\n\n   Specify the external-dns.alpha.kubernetes.io/ttl annotation if you want to set a non-standard TTL for resource records (default: 86400 seconds, 24 hours).\n\n      apiVersion: networking.k8s.io/v1\n   kind: Ingress\n   metadata:\n     name: cafe-ingress\n     annotations:\n       external-dns.alpha.kubernetes.io/ttl: \"3600\"\n   spec:\n     ingressClassName: nginx\n     rules:\n     host: cafe.example.com\n       http:\n         paths:\n         path: /coffee\n           pathType: Prefix\n           backend:\n             service:\n               name: coffee-svc\n               port:\n                 number: 80\n\n   Here:\n\n   The optional external-dns.alpha.kubernetes.io/ttl annotation is specified: TTL in seconds for the resource records to be created by ExternalDNS.\n   The cafe.example.com host for the Ingress rule is specified. Since only one host is specified, one resource record will be created for this domain name.\n   According to the Ingress rule, the coffee application that is behind the coffee-svc service will be accessible at the URL http://cafe.example.com/coffee.\n\nApply this manifest to the cluster to create the resource:\n\n      kubectl apply -f cafe-ingress.yaml\n\nCheck the status of Ingress:\n\n      kubectl get ingress cafe-ingress\n\n   Wait for the Ingress resource to be assigned a public IP address. This address will be the same as the address of the Ingress controller.\n\n   Example of partial output of the command\n\n   Ingress has not yet been assigned an IP address:\n\n          NAME           CLASS   HOSTS               ADDRESS   PORTS   AGE\n     cafe-ingress   nginx   cafe.example.com              80      ...\n\n   Ingress is assigned an IP address:\n\n          NAME           CLASS   HOSTS               ADDRESS                PORTS   AGE\n     cafe-ingress   nginx   cafe.example.com    203.0.113.222.nip.io   80      ...\n\n\nVerify that ExternalDNS has created the necessary resource records:\n\n   Get a list of resource records for the example.com zone.\n   Find the entries created by ExternalDNS in the list:\n\n      One CNAME record cafe.example.com.\n      Two TXT records externaldns-cafe.example.com and externaldns-cname-cafe.example.com.\n\n        These TXT records are service records used by ExternalDNS to track the status of the cafe-ingress CNAME record created for Ingress.\n\n      If the required resource records are not present, wait a few more minutes. ExternalDNS will begin creating resource records after the Ingress resource has been assigned an IP address. This will take some time.\n\nVerify that the application is accessible by domain name. To do this, go to http://cafe.example.com/coffee in your browser.\n\n   A page should open with a response from the application like:\n\n      Server address: 10.100.184.220:8080\n   Server name: coffee-YYYYYYYYY-DDDDD\n   Date: 09/Feb/2024:13:07:11 +0000\n   URI: /coffee\n   Request ID:\n\n   Successful interaction with the application at this address indicates that ExternalDNS works correctly with the Ingress resource.\n\nRemove unused resources\n\nIf you no longer need the Kubernetes resources created for ExternalDNS validation, delete them:\n\n   Delete all resources associated with the tea application:\n\n            kubectl delete -f cafe-ingress.yaml -f coffee-service.yaml -f coffee-app.yaml\n\n      It can take a long time to remove the load balancer associated with the service.\n\n   Remove all resources associated with the coffee application:\n\n            kubectl delete -f tea-service.yaml -f tea-app.yaml\n\n   Delete the Ingress NGINX add-on.\n\n      It may take a long time to remove the add-on and its associated resources.\n\n   Delete the resource records created by ExternalDNS.\n\n      This must be done if you did not modify the external-dns-vkcs-values.yaml file when installing ExternalDNS: in this case ExternalDNS uses the upsert-only policy and does not remove resource records from the DNS zone when removing Kubernetes resources. If you modified this file and selected the sync policy, then these records will be deleted automatically.\n\n      List of resource records:\n\n      A record tea.example.com.\n      TXT records externaldns-tea.example.com and externaldns-a-tea.example.com.\n      CNAME record cafe.example.com.\n      TXT records externaldns-cafe.example.com and externaldns-cname-cafe.example.com.\n\nIf you no longer need ExternalDNS, delete it:\n\n   Remove the Helm chart from ExternalDNS:\n\n            helm -n external-dns uninstall external-dns-vkcs\n\n   Remove the external-dns namespace.\n\n\n      The vkcs-auth secret, which contains the credentials to access the VK Cloud API, will also be removed.\n\n\n            kubectl delete ns external-dns\n\nA running Cloud Containers cluster consumes compute resources and is charged. If you no longer need it:\n\n   stop it to use it later;\n   delete it permanently.\n\nDelete the example.com DNS zone if you no longer need it.\n","url":"/en/kubernetes/k8s/how-to-guides/external-dns","description":"How to install ExternalDNS integrated with the VK Cloud DNS and use this tool in a cluster.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"3b2112f6-08f1-4142-9ac6-cfbbed6c96a3","title":"Creating a kubeconfig file for a service account","data":"When connecting using kubectl to a Cloud Containers cluster, kubeconfig is used, this is cluster configuration file. kubeconfig is usually used to work with the cluster from the VK Cloud personal account, which is configured to use single sign-on technology. Therefore, when working with kubectl you periodically need to enter the user's password.\n\nThis authentication process is inconvenient when working with automated tools that need access to the cluster. To work with them, it is more convenient to use the kubeconfig file for the service account. This kubeconfig allows you to authenticate with a token with an infinite lifetime, without entering a password.\n\nPreparatory steps\n\nCreate the Cloud Containers cluster is the most up-to-date version.\n\n   When creating a cluster, select the Assign external IP option. Select other cluster parameters at your discretion.\n\nMake sure that you can connect to the created cluster using kubectl.\n\n   In this case, kubeconfig will be used, downloaded from the VK Cloud personal account.\n\nSet the environment variables pointing to kubeconfig:\n\n   VKCLOUD_KUBECONFIG: the path to the kubeconfig uploaded from the VK Cloud personal account.\n   SA_KUBECONFIG: the path to kubeconfig for the service account (the file itself will be created later).\n\n   This will simplify further work with kubectl.\n\n\n   The path to your kubeconfig files may differ from the example below.\n\n   Linux (bash)/macOS (zsh)\n   Windows (PowerShell)\n\n\n      export VKCLOUD_KUBECONFIG=\"/home/user/.kube/kubernetes-cluster-1234_kubeconfig.yaml\"\n   export SA_KUBECONFIG=\"/home/user/.kube/sa_kubeconfig.yaml\"\n\n\n      $VKCLOUD_KUBECONFIG=\"C:\\Users\\user\\.kube\\kubernetes-cluster-1234_kubeconfig.yaml\"\n   $SA_KUBECONFIG=\"C:\\Users\\user\\.kube\\sa_kubeconfig.yaml\"\n\n\nMake sure that after connecting to the cluster, you have the rights to create the necessary Kubernetes resources:\n\n      kubectl --kubeconfig $VKCLOUD_KUBECONFIG auth can-i create serviceaccount\n   kubectl --kubeconfig $VKCLOUD_KUBECONFIG auth can-i create secret\n   kubectl --kubeconfig $VKCLOUD_KUBECONFIG auth can-i create clusterrolebinding\n\n   The answer yes should be output for each of the commands.\n\n   If you do not have the rights to create any of these resources (the answer is no), adjust the VK Cloud user role on behalf of which the connection to the cluster is being performed.\n\n   For more information about the role model and available roles, see Access management.\n\n1. Create a service account and link it to the role\n\nCreate an example-sa service account in the kube-system namespace:\n\n   Linux (bash)/macOS (zsh)\n   Windows (PowerShell)\n\n\n      kubectl --kubeconfig $VKCLOUD_KUBECONFIG \\\n     create serviceaccount example-sa -n kube-system\n\n\n      kubectl --kubeconfig $VKCLOUD_KUBECONFIG `\n     create serviceaccount example-sa -n kube-system\n\n\n   Example of command output:\n\n      serviceaccount/example-sa created\n\nSelect the cluster role to assign to the service account.\n\n   To get a list of all cluster roles with a detailed description, run the command:\n\n      kubectl --kubeconfig $VKCLOUD_KUBECONFIG describe clusterroles\n\n   When choosing a role, follow the principle of least privilege to increase security when working with the cluster. Read more about the role model in the official Kubernetes documentation.\n\n   As an example, the edit role will be assigned next. It corresponds to to role Kubernetes operator from personal account.\n\nLink the created service account to the selected cluster role. To do this, create a ClusterRoleBinding resource named example-binding.\n\n   The service account must be specified along with the namespace to which it belongs.\n\n   Linux (bash)/macOS (zsh)\n   Windows (PowerShell)\n\n\n      kubectl --kubeconfig $VKCLOUD_KUBECONFIG \\\n     create clusterrolebinding example-binding \\\n       --serviceaccount=kube-system:example-sa \\\n       --clusterrole=edit\n\n\n      kubectl --kubeconfig $VKCLOUD_KUBECONFIG `\n     create clusterrolebinding example-binding `\n       --serviceaccount=kube-system:example-sa `\n       --clusterrole=edit\n\n\n   Example of command output:\n\n      clusterrolebinding.rbac.authorization.k8s.io/example-binding created\n\n2. Get a token for a service account\n\nCreate an 'example-token` secret containing a token for the service account:\n\n   Create a manifest file:\n\n      example-token.yaml\n\n            apiVersion: v1\n      kind: Secret\n      type: kubernetes.io/service-account-token\n      metadata:\n        name: example-token\n        namespace: kube-system\n        annotations:\n          kubernetes.io/service-account.name: example-sa\n\n\n      Explanations of the manifest fields:\n\n      type: a special type of secret kubernetes.io/service-account-token. This secret holds a token for the service account.\n      metadata.namespace: the namespace for the secret. The secret must be placed in the same namespace as the service account.\n      metadata.annotations: special abstract kubernetes.io/service-account.name with the name of the service account. The token from the created secret will be associated with this account.\n\n   Apply the manifest file:\n\n            kubectl --kubeconfig $VKCLOUD_KUBECONFIG apply -f example-token.yaml\n\n      A secret will be created with the specified parameters. Example of command output:\n\n            secret/example-token created\n\nMake sure that the service account has been assigned a token from the created secret:\n\n   Linux (bash)/macOS (zsh)\n   Windows (PowerShell)\n\n\n      kubectl --kubeconfig $VKCLOUD_KUBECONFIG \\\n     describe serviceaccount example-sa -n kube-system\n\n\n      kubectl --kubeconfig $VKCLOUD_KUBECONFIG `\n     describe serviceaccount example-sa -n kube-system\n\n\n   The output should contain an indication of the secret in the Tokens field.\n\n   Example of command output\n\n      Name:                example-sa\n   Namespace:           kube-system\n   Labels:\n   Annotations:\n   Image pull secrets:\n   Mountable secrets:\n   Tokens:              example-token\n   Events:\n\n\nGet the token value.\n\n   The secret stores the token in encoded form (encoding scheme Base64). The token must be decoded so that it can be used in kubeconfig:\n\n   Linux (bash)/macOS (zsh)\n   Windows (PowerShell)\n\n\n      kubectl --kubeconfig $VKCLOUD_KUBECONFIG \\\n     get secret example-token -n kube-system \\\n     --template={{.data.token}} | base64 --decode\n\n\n      [System.Text.Encoding]::UTF8.GetString( `\n     [System.Convert]::FromBase64String( `\n       (kubectl --kubeconfig $VKCLOUD_KUBECONFIG `\n          get secret example-token -n kube-system -o json `\n          | ConvertFrom-Json).data.token))\n\n\n   The token value will be displayed. Save it.\n\n\n   The value of the token is confidential information. If it is compromised, revoke the token.\n\n\n4. Create a kubeconfig for a service account\n\nCreate the basis for this kubeconfig by copying the kubeconfig downloaded from the VK Cloud personal account.\n\n      cp $VKCLOUD_KUBECONFIG $SA_KUBECONFIG\n\n(Optional) Get to know the kubeconfig structure:\n\n      kubectl --kubeconfig $SA_KUBECONFIG config view\n\n   The contents of kubeconfig will be output in a compressed form: the values of some fields will be skipped.\n\n   Simplified kubeconfig example\n\n      apiVersion: v1\n   clusters: # Clusters\n     cluster:\n       name:\n   contexts: # The contexts in which the cluster is being worked on\n     context:\n         cluster:\n         user:\n       name:\n   current-context:\n   kind: Config\n   preferences: {}\n   users: # Users\n     name:\n       user:\n\n\n   Kubeconfig contains all the parameters necessary to work with the cluster:\n\n   clusters: a list of clusters and data to connect to them.\n\n     The Kubeconfig for the Cloud Containers cluster contains an entry about a single cluster.\n\n   users: a list of users and data for their authentication in the cluster.\n\n     Kubeconfig for the Cloud Containers cluster contains a record of a single user who authenticates using keystone-auth.\n\n   contexts: the context in which kubectl works. In the simplest case, the context is a combination of the cluster name and the user name.\n\n     Kubeconfig for the Cloud Containers cluster contains an entry about a single context. This context uses the cluster and user record that are already defined in kubeconfig.\n\n   When kubectl is running in the specified context, it is working with the cluster specified in the context on behalf of the specified user.\n\nChange the contents of kubeconfig for the service account so that this file contains the parameters associated with the previously configured service account:\n\n   Delete the existing user.\n\n      This user corresponds to the VK Cloud user and should not appear in kubeconfig, which will be used by automated tools.\n\n      Get a list of users:\n\n         Linux (bash)/macOS (zsh)\n         Windows (PowerShell)\n\n\n                  kubectl --kubeconfig $SA_KUBECONFIG \\\n           config get-users\n\n\n                  kubectl --kubeconfig $SA_KUBECONFIG `\n           config get-users\n\n\n      Delete the user using the required name from the list:\n\n         Linux (bash)/macOS (zsh)\n         Windows (PowerShell)\n\n\n                  kubectl --kubeconfig $SA_KUBECONFIG \\\n           config delete-user\n\n\n                  kubectl --kubeconfig $SA_KUBECONFIG `\n           config delete-user\n\n\n         Example of partial command output:\n\n                  deleted user kubernetes-cluster-1234 from ...sa_kubeconfig.yaml\n\n   Add a new user example-sa.\n\n      This user corresponds to a previously created service account. The previously received token will be used for authentication.\n\n      Linux (bash)/macOS (zsh)\n      Windows (PowerShell)\n\n\n            kubectl --kubeconfig $SA_KUBECONFIG \\\n        config set-credentials example-sa --token=\"\"\n\n\n            kubectl --kubeconfig $SA_KUBECONFIG `\n        config set-credentials example-sa --token=\"\"\n\n\n      Example of command output:\n\n            User \"example-sa\" set.\n\n   Configure the current context to use the added user:\n\n      Linux (bash)/macOS (zsh)\n      Windows (PowerShell)\n\n\n            kubectl --kubeconfig $SA_KUBECONFIG \\\n        config set-context --current --user=\"example-sa\"\n\n\n            kubectl --kubeconfig $SA_KUBECONFIG `\n        config set-context --current --user=\"example-sa\"\n\n\n      Example output:\n\n            Context \"default/kubernetes-cluster-1234\" modified.\n\n(Optional) Check the updated kubeconfig content for the service account:\n\n      kubectl --kubeconfig $SA_KUBECONFIG config view\n\n   This kubeconfig should not contain any other users except the previously added example-sa. The only context is to use this user.\n\n   Example of command output\n\n      apiVersion: v1\n   clusters:\n   cluster:\n       certificate-authority-data: DATA+OMITTED\n       server: https://203.0.113.123:6443\n     name: kubernetes-cluster-1234\n   contexts:\n   context:\n       cluster: kubernetes-cluster-1234\n       user: example-sa\n     name: default/kubernetes-cluster-1234\n   current-context: default/kubernetes-cluster-1234\n   kind: Config\n   preferences: {}\n   users:\n   name: example-sa\n     user:\n       token: REDACTED\n\n\n5. Check the operation of the created kubeconfig\n\nUse the kubectl commands and the previously created kubeconfig for the service account to get information about the cluster and its resources, for example:\n\nGet information about the cluster:\n\n      kubectl --kubeconfig $SA_KUBECONFIG cluster-info\n\n   Example of command output\n\n      Kubernetes control plane is running at https://203.0.113.123:6443\n   CoreDNS is running at https://203.0.113.123:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\n\nGet a list of the main resources in the default namespace:\n\n      kubectl --kubeconfig $SA_KUBECONFIG get all -n default\n\n   Example of command output\n\n      NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n   service/kubernetes   ClusterIP   10.254.0.1           443/TCP   3d1h\n\n\nIf the password was not requested when executing the commands, then the resulting kubeconfig can be used in combination with automated tools to access the Cloud Containers cluster.\n\n\n\nProvide the necessary measures to protect the kubeconfig file. It contains confidential information: the value of the token in plain text.\n\nIf kubeconfig is compromised revoke the token.\n\nRevoke the compromised token\n\nIf a previously created token or a kubeconfig containing it has been compromised, revoke the token to prevent unauthorized access to the cluster.\n\nTo do this, delete the secret that is used to store the token:\n\nkubectl --kubeconfig $VKCLOUD_KUBECONFIG delete secret example-token -n kube-system\n\nDelete unused resources\n\nIf you no longer need the created Kubernetes resources, delete them:\n\n   Linux (bash)/macOS (zsh)\n   Windows (PowerShell)\n\n\n      kubectl --kubeconfig $VKCLOUD_KUBECONFIG \\\n     delete clusterrolebinding example-binding\n   kubectl --kubeconfig $VKCLOUD_KUBECONFIG \\\n     delete secret example-token -n kube-system\n   kubectl --kubeconfig $VKCLOUD_KUBECONFIG \\\n     delete serviceaccount example-sa -n kube-system\n\n\n      kubectl --kubeconfig $VKCLOUD_KUBECONFIG `\n     delete clusterrolebinding example-binding\n   kubectl --kubeconfig $VKCLOUD_KUBECONFIG `\n     delete secret example-token -n kube-system\n   kubectl --kubeconfig $VKCLOUD_KUBECONFIG `\n     delete serviceaccount example-sa -n kube-system\n\n\nA running Cloud Containers cluster is charged and consumes computing resources. If you don't need it anymore:\n\n   stop it to use it later;\n   delete its forever.\n","url":"/en/kubernetes/k8s/how-to-guides/sa-kubeconfig","description":"How to create a cluster configuration file for use with service accounts.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"e04cf47f-a0ca-4951-b5d5-411854e6d11d","title":"Updating the Kube Prometheus Stack add-on","data":"There are multiple versions of the Kube Prometheus Stack add-on available in the Cloud Containers clusters. Update of this add-on using VK Cloud interfaces is not available, but you can update the add-on manually.\n\nTo upgrade the Kube Prometheus Stack add-on from version 36.2.0 to version 54.2.2, it is necessary to uninstall the current version of the add-on and then install the new one. Therefore, the upgrade process includes preparing the environment of the current version of the add-on for saving and further re-using it with the new version.\n\n\n\nIt is further assumed that the add-on is installed in the namespace (e.g., kube-prometheus-stack) that contains only those Kubernetes resources that relate to the add-on.\n\nIf there are other Kubernetes resources in the namespace, modify the commands and script so that they do not affect resources not related to the add-on.\n\nPreparation steps\n\nIf you already have an existing Cloud Containers cluster with the Kube Prometheus Stack add-on that needs to be upgraded, skip this step.\n\n   Otherwise, create a test cluster where the add-on update will be performed:\n\n   Create a Cloud Containers cluster version 1.26.5.\n\n      When you create the cluster, select the Assign external IP option. Other cluster parameters can be set at your discretion.\n\n   Install the Kube Prometheus Stack add-on version 36.2.0 in the cluster.\n\n      Perform a quick installation of the add-on (without editing the add-on configuration code).\n\nVerify that you can connect to the cluster via kubectl.\n\n   To connect, use the cluster configuration file (kubeconfig) downloaded from the VK Cloud personal account.\n\nMake sure the add-on is available and working. To do this, access the Grafana web interface.\n\n\n   Write down the password to access Grafana, even if it is stored as a Kubernetes secret. During the add-on upgrade process, the namespace where the add-on and the secret are located will be deleted along with all contents.\n\n\nInstall Helm version 3.0.0 or higher if the utility has not been already installed.\n\n   To install, select the Helm version that is compatible with the cluster.\n\nSet an environment variable pointing to kubeconfig for the cluster. This will simplify your work with kubectl and helm when updating the add-on.\n\n   The path to your kubeconfig files may differ from the example below.\n\n   Linux (bash)/macOS (zsh)\n   Windows (PowerShell)\n\n\n      export KUBECONFIG=\"/home/user/.kube/kubernetes-cluster-1234_kubeconfig.yaml\"\n\n\n      $Env:KUBECONFIG=\"C:\\Users\\user\\.kube\\kubernetes-cluster-1234_kubeconfig.yaml\"\n\n\n{heading(1. Get information you need to update your add-on)[id=1_get_information_you_need_to_update_your_addon]}\n\nGo to editing the add-on configuration code.\n\n   Do not modify the code.\n\n   Write down the following information:\n\n   Application name (kube-prometheus-stack by default).\n   Namespace name (prometheus-monitoring by default).\n   Full add-on configuration code.\n\nSet the environment variables that point to these application and namespace names. This will simplify further updating the add-on.\n\n   The values of your variables may differ from the example below.\n\n   Linux (bash)/macOS (zsh)\n   Windows (PowerShell)\n\n\n      export CHART_NAME=\"kube-prometheus-stack\"\n   export NAMESPACE=\"prometheus-monitoring\"\n\n\n      $CHART_NAME=\"kube-prometheus-stack\"\n   $NAMESPACE=\"prometheus-monitoring\"\n\n\nGet information about Persistent Volumes (PVs) and Persistent Volume Claims (PVCs). They are used to store the collected metrics, as well as other data necessary for the add-on to work.\n\n      kubectl -n $NAMESPACE get pvc\n\n   The output of the command will contain a list of PVCs (NAME) and their corresponding PVs (VOLUMES) with the size of the volumes (CAPACITY). Write this information down, you will need it later.\n\n   Example of the command partial output\n\n      NAME                                                                             STATUS   VOLUME                                     CAPACITY   ...\n   alertmanager-prometheus-alertmanager-db-alertmanager-prometheus-alertmanager-0   Bound    pvc-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX   1Gi        ...\n   kube-prometheus-stack-grafana                                                    Bound    pvc-YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY   1Gi        ...\n   prometheus-prometheus-prometheus-db-prometheus-prometheus-prometheus-0           Bound    pvc-ZZZZZZZZ-ZZZZ-ZZZZ-ZZZZ-ZZZZZZZZZZZZ   10Gi       ...\n\n\n{heading(2. Prepare add-on environment for update)[id=2_prepare_addon_environment_for_update]}\n\nVolumes for the add-on are created by default using the Retain reclaim policy if no changes related to the storage class have been made to the add-on configuration code. So, deleting the add-on will cause the deletion of the persistent volumes of data as well. This will result in the loss of metrics accumulated during the add-on operation, as well as other data necessary for the add-on to work.\n\nIn addition, the following Kubernetes resources, which are also used by the add-on, may prevent you from installing a new version of the add-on:\n\nA set of Custom Resource Definitions (CRDs) required for the add-on to work.\nA set of Persistent Volume Claims (PVCs) that allows the add-on to use persistent volumes.\n\nBefore you update the add-on, protect the persistent volumes used by this add-on from deletion, and remove the mentioned Kubernetes resources using one of the methods below.\n\n\n\nReady-to-use bash script for Linux\nSeparate commands for Linux/macOS/Windows\n\n\n\nCreate a file with the script code:\n\n   prepare-for-addon-update.sh\n\n      #!/bin/sh\n\n   set -e\n\n   DEFAULT_NAMESPACE=prometheus-monitoring\n   : \"${NAMESPACE:=prometheus-monitoring}\"\n   : \"${CHART_NAME:=kube-prometheus-stack}\"\n   : \"${DRY_RUN:=none}\"\n\n   usage() {\n   cat /dev/null 2>&1; then\n           helm uninstall \"${@}\" \"${CHART_NAME}\" --wait --timeout 600s \"${dry_run}\" >/dev/null 2>&1 || true\n       fi\n       helm list \"${@}\" -aA -f \"${CHART_NAME}\" -q\n   }\n\n   delete_prometheus_namespace() {\n       if kubectl get \"${@}\" ns \"${NAMESPACE}\" >/dev/null 2>&1; then\n           kubectl delete \"${@}\" ns \"${NAMESPACE}\" --dry-run=\"${DRY_RUN}\" --wait --timeout 600s --force --grace-period=0 --cascade=foreground\n       fi\n   }\n\n   echo \"Setting retain policy for the Prometheus PVs...\"\n   set_prometheus_pv_retain_reclaim_policy \"${@}\"\n   echo \"Deleting Prometheus chart...\"\n   delete_prometheus_chart \"${@}\"\n   echo \"Deleting Prometheus PVCs...\"\n   delete_prometheus_pvc \"${@}\"\n   echo \"Clearing Prometheus PV claim references...\"\n   clear_prometheus_pv_claim_reference \"${@}\"\n   echo \"Clearing Prometheus CRDs...\"\n   delete_prometheus_crd \"${@}\"\n   [ \"${NAMESPACE}\" = \"${DEFAULT_NAMESPACE}\" ] && {\n       echo \"Deleting Prometheus namespace...\"\n       delete_prometheus_namespace \"${@}\"\n   }\n   echo \"Completed!\"\n\n\n   The script performs the following actions:\n\n   Detects persistent volumes that are associated with PVCs created in the namespace in which the add-on is installed. For these persistent volumes, the script defines the Retain reclaim policy. This is necessary to prevent these volumes from being deleted when the current version of the add-on is uninstalled.\n   Deletes the Helm chart of the current version of the add-on. This is necessary for successfully removing the PVCs and CRDs sets that are used by the add-on.\n   Removes a set of PVCs from the namespace where the add-on has been installed. This is necessary for unbinding PVs from PVCs and then reusing these PVs with a new version of the add-on.\n   Removes references to the deleted PVCs from persistent volumes that have been linked to these PVCs. These persistent volumes will become available for linking (PVs status becomes Available) and will be reused by the new version of the add-on after it is installed.\n   Deletes the set of CRDs that have been used by the add-on.\n   Deletes the prometheus-monitoring namespace if the script has been run for this namespace.\n\nMake the script code file executable:\n\n      chmod +x prepare-for-addon-update.sh\n\nDefine the parameters to run the script with:\n\n      bash prepare-for-addon-update.sh -h\n\n   Help will be displayed, showing the available parameters and their default values:\n\n   -h: show the help.\n   -k: path to the kubeconfig file. The default value is retrieved from the $KUBECONFIG environment variable if specified.\n   -c: name of the kubeconfig context that should be used when working with the cluster. Default value: empty string.\n   -n: name of the namespace in which the add-on is installed. Default value is prometheus-monitoring.\n   -r: name of the application with which the add-on is installed. It matches the name of the Helm-chart for the add-on. Default value is kube-prometheus-stack.\n   -d: value of the --dry-run argument for kubectl. Default value is none. Use server or client for test running of the script: no changes will be made to the cluster.\n\n   Help output example\n\n      Usage:\n   -h help\n   -k kubeconfig           \"/home/user/.kube/kubernetes-cluster-1234_kubeconfig.yaml\"\n   -c kubeconfig context   \"\"\n   -n namespace            \"prometheus-monitoring\"\n   -r chart                \"kube-prometheus-stack\"\n   -d dry run              \"none\". [none,server,client]\n\n\nRun the script.\n\n\n   If you run the script with the prometheus-monitoring namespace explicitly (the -n parameter) or implicitly specified, this namespace will be removed.\n\n   If you run the script for a different namespace, it will not be removed.\n\n\n   Run the command, specifying the required parameters. You can omit the parameter if you are satisfied with its default value.\n\n        bash prepare-for-addon-update.sh \\\n       -k  \\\n       -c  \\\n       -n  \\\n       -r  \\\n       -d\n\n   Detailed messages about the script operation will be displayed. The output must include the following messages:\n\n      Setting retain policy for the Prometheus PVs...\n   Deleting Prometheus chart...\n   Deleting Prometheus PVCs...\n   Clearing Prometheus PV claim references...\n   Clearing Prometheus CRDs...\n   Deleting Prometheus namespace...\n   Completed!\n\n   Command output example\n\n      Settings retain policy for the prometheus PVs...\n   persistentvolume/pvc-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX patched\n   persistentvolume/pvc-YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY patched\n   persistentvolume/pvc-ZZZZZZZZ-ZZZZ-ZZZZ-ZZZZ-ZZZZZZZZZZZZ patched\n   Deleting prometheus chart...\n   Deleting prometheus PVCs...\n   persistentvolumeclaim \"prometheus-prometheus-prometheus-db-prometheus-prometheus-prometheus-0\" deleted\n   persistentvolumeclaim \"alertmanager-prometheus-alertmanager-db-alertmanager-prometheus-alertmanager-0\" deleted\n   Clearing prometheus PV claim references...\n   persistentvolume/pvc-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX patched\n   persistentvolume/pvc-YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY patched\n   persistentvolume/pvc-ZZZZZZZZ-ZZZZ-ZZZZ-ZZZZ-ZZZZZZZZZZZZ patched\n   Clearing prometheus CRDs...\n   Warning: deleting cluster-scoped resources, not scoped to the provided namespace\n   customresourcedefinition.apiextensions.k8s.io \"thanosrulers.monitoring.coreos.com\" deleted\n   customresourcedefinition.apiextensions.k8s.io \"servicemonitors.monitoring.coreos.com\" deleted\n   customresourcedefinition.apiextensions.k8s.io \"prometheusrules.monitoring.coreos.com\" deleted\n   customresourcedefinition.apiextensions.k8s.io \"prometheuses.monitoring.coreos.com\" deleted\n   customresourcedefinition.apiextensions.k8s.io \"probes.monitoring.coreos.com\" deleted\n   customresourcedefinition.apiextensions.k8s.io \"podmonitors.monitoring.coreos.com\" deleted\n   customresourcedefinition.apiextensions.k8s.io \"alertmanagers.monitoring.coreos.com\" deleted\n   customresourcedefinition.apiextensions.k8s.io \"alertmanagerconfigs.monitoring.coreos.com\" deleted\n   Deleting prometheus namespace...\n   Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n   Warning: deleting cluster-scoped resources, not scoped to the provided namespace\n   namespace \"prometheus-monitoring\" force deleted\n   Completed!\n\n\nGet the list of PVs that are used by the add-on via PVCs in the add-on namespace:\n\n      kubectl get pv -o jsonpath='{range .items[?(@.spec.claimRef.namespace==\"'\"${NAMESPACE}\"'\")]}{.metadata.name}{\"\\n\"}{end}'\n\n   The list of PVs must match the list of PVs received earlier.\n\nPatch these PVs so that they use the Retain reclaim policy. This is necessary to prevent these volumes from being deleted when the current version of the add-on is uninstalled.\n\n   This command patches an individual PV. Run it for all PVs in the list.\n\n      kubectl patch pv  -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}'\n\nGet a list of all PVs in the cluster and make sure that the PVs, which the add-on is working with, use the Retain reclaim policy and are associated with a PVC (Bound):\n\n      kubectl get pv\n\n   Example of partial command output\n\n      NAME                                       CAPACITY   ...   RECLAIM POLICY   STATUS  CLAIM    ...\n   pvc-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX   1Gi        ...   Retain           Bound   prometheus-monitoring/alertmanager-prometheus-alertmanager-db-alertmanager-prometheus-alertmanager-0 ...\n   pvc-YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY   1Gi        ...   Retain           Bound   prometheus-monitoring/kube-prometheus-stack-grafana ...\n   pvc-ZZZZZZZZ-ZZZZ-ZZZZ-ZZZZ-ZZZZZZZZZZZZ   10Gi       ...   Retain           Bound   prometheus-monitoring/prometheus-prometheus-prometheus-db-prometheus-prometheus-prometheus-0 ...\n\n\nDelete the Helm chart of the current version of the add-on. This is necessary for successfully removing the PVCs and CRDs sets that are used by the add-on.\n\n      helm uninstall -n $NAMESPACE $CHART_NAME --wait --timeout 600s\n\nGet a list of PVCs that have been used by the add-on:\n\n      kubectl -n $NAMESPACE get pvc\n\n   The final list may differ from previously obtained: part of the PVCs has been deleted during the Helm chart deletion.\n\nDelete the PVCs that are used by the add-on. This is necessary for unbinding PVs from PVCs and then reusing these PVs with a new version of the add-on.\n\n   This command deletes a separate PVC. Run it for all PVCs from the list.\n\n      kubectl -n $NAMESPACE delete pvc\n\nPatch the PVs that are used by the add-on to unbind them from the PVCs that have been deleted. These persistent volumes will become available for binding (the PVs status becomes Available) and will be reused by the new version of the add-on after its installation.\n\n   This command patches an individual PV. Run it for all PVs in the list you received earlier.\n\n      kubectl patch pv  --type json -p '[{\"op\": \"remove\", \"path\": \"/spec/claimRef\"}]'\n\nGet a list of all PVs in the cluster and make sure that the PVs, which the add-on is working with, use the Retain reclaim policy and are available for binding (Available):\n\n      kubectl get pv\n\n   Example of partial command output\n\n      NAME                                       CAPACITY   ...   RECLAIM POLICY   STATUS      ...\n   pvc-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX   1Gi        ...   Retain           Available   ...\n   pvc-YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY   1Gi        ...   Retain           Available   ...\n   pvc-ZZZZZZZZ-ZZZZ-ZZZZ-ZZZZ-ZZZZZZZZZZZZ   10Gi       ...   Retain           Available   ...\n\n\nGet a list of CRDs that the add-on has worked with:\n\n      kubectl get crd -o jsonpath='{range .items[?(@.spec.group==\"monitoring.coreos.com\")]}{.metadata.name}{\"\\n\"}{end}'\n\nDelete all CRDs that the add-on has worked with.\n\n   This command deletes an individual CRD. Execute it for all CRDs in the list obtained earlier.\n\n      kubectl delete crd\n\nDelete the namespace in which the add-on has been installed.\n\n      kubectl delete ns $NAMESPACE\n\n\n{heading(3. Update add-on version)[id=3_update_addon_version]}\n\nUninstall the current version of the add-on using VK Cloud interfaces.\nInstall the Kube Prometheus Stack add-on version 54.2.2 in the cluster.\n\n   Perform standard installation as follows:\n\n   Set the same application and namespace names that have been used when installing the previous version of the add-on.\n\n   Review the previous version of the add-on configuration code, obtained earlier. Find code fragments that are responsible for setting up the storage for the following add-on components:\n\n      Grafana\n      Alert Manager\n      Prometheus\n\n\n            grafana:\n\n        ...\n\n        persistence:\n          enabled: true\n          storageClassName: \"csi-ceph-hdd-gz1\"\n          accessModes:\n          ReadWriteOnce\n          size: 1Gi\n\n        ...\n\n\n            alertmanager:\n\n        ...\n\n        alertmanagerSpec:\n\n        ...\n\n          storage:\n            volumeClaimTemplate:\n              spec:\n                storageClassName: \"csi-ceph-hdd-gz1\"\n                accessModes:\n                ReadWriteOnce\n                resources:\n                  requests:\n                    storage: 1Gi\n\n\n            prometheus:\n\n        ...\n\n        prometheusSpec:\n\n          ...\n\n          storageSpec:\n            volumeClaimTemplate:\n              spec:\n                storageClassName: \"csi-ceph-ssd-gz1\"\n                accessModes:\n                ReadWriteOnce\n                resources:\n                  requests:\n                    storage: 10Gi\n\n\n   Review the configuration code for the new version of the add-on that you plan to install.\n\n      If the code fragments that are responsible for setting up the storage are different from those obtained earlier, correct them. The storage settings for Grafana, Alert Manager and Prometheus should exactly match the same settings used for the previous version of the add-on.\n\n   (Optional) Make other changes to the add-on configuration code.\n\n      If you specified the password for access to Grafana in the grafana.adminPassword field when installing the previous version of the add-on, you do not need to specify it again. The new version of the add-on will use the previous PVs as a storage, so the password will remain the same. Installing a new version of the add-on will not change the password for accessing Grafana, even if you leave this field empty: a secret with the password for Grafana will be generated, but it will not be used.\n\n\n      Incorrectly specified configuration code may cause errors during installation or inoperability of the add-on.\n\n\n   Install the add-on.\n\n      The installation process may take a long time. Wait until it is completed.\n\nGet information about the Persistent Volume Claims (PVCs) and persistent volumes (PVs) used by the add-on:\n\n      kubectl -n $NAMESPACE get pvc\n\n   The output of the command should be similar to the output of the previously executed command for the add-on of the previous version. Thus, the alertmanager... PVC must be associated with the same PV that Alert Manager used earlier. Similarly for Prometheus and Grafana.\n\n{heading(4. Verify add-on operability after updating)[id=4_verify_addon_operability_after_updating]}\n\nGet access to the Grafana web interface. To connect, use the same password that was used with the previous version of the add-on. If you forgot your Grafana password, reset it.\n\nA successful connection to Grafana indicates a successful add-on update.\n\nDelete unused resources\n\nA running Cloud Containers cluster is being charged and consumes computing resources. If you have created a cluster for testing purposes and no longer need it, do as follows:\n\nstop it, so you can use it later;\ndelete it permanently.\n","url":"/en/kubernetes/k8s/how-to-guides/update-monitoring-addon","description":"How to manually update an add-on with a large difference between its versions.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"d54d6b63-3d4f-43d3-93c4-e2f3f4cbcb19","title":"Cluster monitoring","data":"The following tools are available for monitoring the status of a Kubernetes cluster:\n\nTools built into the Kubernetes Dashboard.\n\n  They provide basic monitoring capabilities, allowing you to view information about Kubernetes resources.\n\nMonitoring add-on.\n\n  It provides advanced cluster monitoring capabilities, allowing you to view metrics for multiple Kubernetes resources, down to individual pods.\n  It is also possible to configure alerts when specified thresholds for metrics are exceeded. For example, you can monitor if the cluster resources are not available or if the computing power of the worker nodes is insufficient.\n\n  To find out if the monitoring add-on is installed in the cluster, see the list of installed add-ons.\n\nUsing Kubernetes Dashboard\n\nConnect to the cluster using Kubernetes Dashboard.\nGet data about the cluster resources:\n\n   Select the required namespace from the drop-down list at the top of the Kubernetes Dashboard interface. The default namespace selected is default.\n   From the side menu, select the Kubernetes resource of interest.\n\nUsing Grafana\n\nMake sure that the monitoring add-on (kube-prometheus-stack) is installed in a cluster.\nMake sure that you can connect to the cluster using kubectl.\n\nAccess the Grafana web interface:\n\n   In a separate terminal session, run the command:\n\n            kubectl -n prometheus-monitoring port-forward service/kube-prometheus-stack-grafana 8001:80\n\n\n      Do not close this session or you will lose access to the Grafana web interface.\n\n\n   From the output of the command identify the port opened by kubectl to access Grafana.\n\n      The port number is specified before the → symbol. For example, in this output the port 6637 is used to connect:\n\n            Forwarding from 127.0.0.1:6637 -> 3000\n      Forwarding from [::1]:6637 -> 3000\n\n   Open in your browser the URL for accessing the Grafana web interface:\n\n            http://localhost:/\n\n      The Grafana authorization page will appear.\n\nSign in to Grafana. Depending on the parameters with which the add-on was installed, use:\n\n   Login admin and temporary password. The password will need to be changed after the first authorization.\n   Login admin and a permanent password from the Kubernetes secret.\n\nNow you can work with Grafana. For example, it is possible to create and view dashboards. Preconfigured dashboards with information about various Kubernetes resources are available in the Dashboards → Browse side menu.\n\nRead more about working with Grafana in official documentation.\n","url":"/en/kubernetes/k8s/monitoring","description":"A Kubernetes cluster can be monitored to track its status. This can help prevent a possible shortage of computing resources or track an incorrect cluster configuration.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"b1c04296-66d0-483c-80ea-f3e9cbda8762","title":"Installing Helm","data":"Helm is a popular package manager for Kubernetes that can be used as an add-on to kubectl to quickly install and update applications.\n\nInstallation\n\nMake sure that you can connect to the cluster using kubectl from the host on which you plan to install the Helm client.\nInstall the Helm client on the host in any of the ways described in the official Helm documentation.\n\n   Choose the most current version of Helm that is compatible with the cluster in which you plan to use the client. See the Helm and Kubernetes version compatibility table in official Helm documentation.\n\nIf necessary, add the path to the Helm executable file to the PATH environment variable if this was not done during installation.\n\nMake sure that the correct version of Helm is installed by running the command:\n\n      helm version\n\nChecking Helm operation\n\nInstall NGINX from the Bitnami repository:\n\n   Windows (PowerShell)\n   Linux (bash)/macOS (zsh)\n\n\n      helm repo add bitnami https://charts.bitnami.com/bitnami; `\n   helm repo update; `\n   helm install demo-helm-nginx bitnami/nginx\n\n\n      helm repo add bitnami https://charts.bitnami.com/bitnami\n   helm repo update\n   helm install demo-helm-nginx bitnami/nginx\n\n\nVerify that the NGINX pod is working by running the command:\n\n      kubectl get pods\n\n   The output of the command should contain the demo-helm-nginx-... pod in the Running status.\n\n   Example of the output:\n\n      NAME                               READY   STATUS    RESTARTS   AGE\n   demo-helm-nginx-...                1/1     Running   0          ...\n\nUninstall NGINX installed with Helm and remove the Bitnami repository:\n\n   Windows (PowerShell)\n   Linux (bash)/macOS (zsh)\n\n\n      helm uninstall demo-helm-nginx; `\n   helm repo remove bitnami\n\n\n      helm uninstall demo-helm-nginx\n   helm repo remove bitnami\n\n\nDeletion\n\nDelete the Helm client executable file.\n\nIf necessary, remove Helm-related entries from the PATH environment variable.\n\nIf necessary, delete directories and files created by Helm:\n\n   Cache directory: corresponds to the $XDG_CACHE_HOME environment variable.\n   Configuration files directory: corresponds to the $XDG_CONFIG_HOME environment variable.\n   Data directory: corresponds to the $XDG_DATA_HOME environment variable.\n\n   The directories corresponding to these variables for different operating systems are listed in official Helm documentation.\n","url":"/en/kubernetes/k8s/install-tools/helm","description":"How to install the Helm package manager in the Kubernetes cluster.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"090dd7f7-3f53-4058-90cb-b306e6b53c3c","title":"Installing Gatekeeper","data":"Gatekeeper is a controller embedded between the Kubernetes API and the Open Policy Agent (OPA) policy engine to check that Kubernetes resources created, modified and deleted are compliant with policies. For more information about Gatekeeper, see Kubernetes reference and official Gatekeeper documentation.\n\n\n\nUse these instructions if your cluster is version 1.20 or lower. Starting with Kubernetes 1.21, Gatekeeper is already installed in the cluster along with preconfigured templates and restrictions.\n\nInstallation\n\nInstall Helm if the utility is not already installed.\n\nRun the commands\n\n   Windows (PowerShell)\n   Linux (bash)/macOS (zsh)\n\n\n      helm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts; `\n   helm install gatekeeper/gatekeeper --name-template=gatekeeper --namespace opa-gatekeeper --create-namespace\n\n\n      helm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts\n   helm install gatekeeper/gatekeeper --name-template=gatekeeper --namespace opa-gatekeeper --create-namespace\n\n\nChecking Gatekeeper operation\n\nCheck that the Gatekeeper pods have been created and are working by running the command:\n\nkubectl -n opa-gatekeeper get pods\n\nThe output of the command should contain gatekeeper-audit-... and gatekeeper-controller-manager-... pods in Running status.\n\nExample of the output:\n\nNAME                                             READY   STATUS    RESTARTS   AGE\ngatekeeper-audit-...                             1/1     Running   0          ...\ngatekeeper-controller-manager-...                1/1     Running   0          ...\n\nDeletion\n\nTo delete Gatekeeper, run the command:\n\n      helm delete gatekeeper --namespace opa-gatekeeper\n\nTo delete the CRD objects created for Gatekeeper, run the command:\n\n      kubectl delete crd -l gatekeeper.sh/system=yes\n\n\n   This operation will also delete the constraints and their templates.\n\n","url":"/en/kubernetes/k8s/install-tools/gatekeeper","description":"How to install a Gatekeeper controller in the Kubernetes cluster.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"ddc9a7ce-1cc7-498c-b64f-fd4e0e9f879b","title":"Installing Velero","data":"Velero is a client-server utility for backing up and restoring Kubernetes cluster resources.\n\nInstallation\n\nMake sure that:\n\n   Either the worker node groups have at least 2 vCPUs available;\n   Or automatic scaling is enabled.\n\nMake sure that you can connect to the cluster with kubectl.\nInstall OpenStack CLI if it is not already installed. Make sure that you can authorize in the cloud using it.\nCreate a Hotbox bucket to store backups in the Object Storage service.\nCreate an account in the Object Storage service.\n\n    Save the account's key ID and secret to the s3_creds file:\n\n      [default]\n   aws_access_key_id=\n   aws_secret_access_key=\n\nDetermine which version of Velero is compatible with the Kubernetes cluster version you want to install Velero in.\nDownload the correct version of the Velero client.\nDetermine the version of the AWS plugin that is compatible with Velero version.\nAdd the path to the client to the environment variable:\n\n   Path for Windows.\n   PATH for Linux/macOS.\n\nInstall the server part of Velero in the Kubernetes cluster. In the command, specify the name of the bucket created for Velero and the path to the s3_creds file with the account data:\n\n   Linux/macOS\n   Windows\n\n\n      velero install \\\n   --plugins \\\n     velero/velero-plugin-for-aws:v,registry.infra.mail.ru:5010/velero/velero-plugin-mcs:v1.2.2 \\\n   --provider aws \\\n   --bucket  \\\n   --secret-file  \\\n   --use-volume-snapshots=false \\\n   --backup-location-config \\\n     region=ru-msk,s3ForcePathStyle=\"true\",s3Url=https://hb.bizmrg.com:443\n\n\n      velero install `\n   --plugins `\n     velero/velero-plugin-for-aws:v,registry.infra.mail.ru:5010/velero/velero-plugin-mcs:v1.2.2 `\n   --provider aws `\n   --bucket  `\n   --secret-file  `\n   --use-volume-snapshots=false `\n   --backup-location-config `\n     region=ru-msk,s3ForcePathStyle=\"true\",s3Url=https://hb.bizmrg.com:443\n\n\n   After the installation is complete, a message will be displayed:\n\n      Velero is installed! ⛵ Use 'kubectl logs deployment/velero -n velero' to view the status.\n\nCreate a Kubernetes secret so that the server part of Velero can authorize in the VK Cloud:\n\n   Linux/macOS\n   Windows\n\n\n      kubectl -n velero create secret generic openstack-cloud-credentials \\\n     --from-literal OS_PROJECT_ID=$OS_PROJECT_ID \\\n     --from-literal OS_REGION_NAME=$OS_REGION_NAME \\\n     --from-literal OS_IDENTITY_API_VERSION=$OS_IDENTITY_API_VERSION \\\n     --from-literal OS_PASSWORD=$OS_PASSWORD \\\n     --from-literal OS_AUTH_URL=$OS_AUTH_URL \\\n     --from-literal OS_USERNAME=$OS_USERNAME \\\n     --from-literal OS_INTERFACE=$OS_INTERFACE \\\n     --from-literal OS_FILE_OPERATION_TIMEOUT=$OS_FILE_OPERATION_TIMEOUT \\\n     --from-literal OS_DOMAIN_NAME=$OS_USER_DOMAIN_NAME \\\n     -o yaml\n\n\n      kubectl -n velero create secret generic openstack-cloud-credentials `\n     --from-literal OS_PROJECT_ID=$env:OS_PROJECT_ID `\n     --from-literal OS_REGION_NAME=$env:OS_REGION_NAME `\n     --from-literal OS_IDENTITY_API_VERSION=$env:OS_IDENTITY_API_VERSION `\n     --from-literal OS_PASSWORD=$env:OS_PASSWORD `\n     --from-literal OS_AUTH_URL=$env:OS_AUTH_URL `\n     --from-literal OS_USERNAME=$env:OS_USERNAME `\n     --from-literal OS_INTERFACE=$env:OS_INTERFACE `\n     --from-literal OS_FILE_OPERATION_TIMEOUT=$env:OS_FILE_OPERATION_TIMEOUT `\n     --from-literal OS_DOMAIN_NAME=$env:OS_USER_DOMAIN_NAME `\n     -o yaml\n\n\n   Output should contain a similar information:\n\n      apiVersion: v1\n   data:\n     OS_AUTH_URL: ...\n     OS_DOMAIN_NAME: ...\n     ...\n   kind: Secret\n   metadata:\n     creationTimestamp: ...\n     name: openstack-cloud-credentials\n     namespace: velero\n     resourceVersion: ...\n     selfLink: ...\n     uid: ...\n   type: Opaque\n\nPatch the Velero deployment in the cluster. This is needed for:\n\n   limiting resource consumption by Velero plugins for AWS and VK Cloud;\n   specifying the secret with variables for authorization in VK Cloud.\n\n   Create a file with the patch:\n\n      velero-patch.yaml\n\n            spec:\n        template:\n          spec:\n            containers:\n              name: velero\n                envFrom:\n                  secretRef:\n                      name: openstack-cloud-credentials\n            initContainers:\n              name: velero-velero-plugin-for-aws\n                resources:\n                  limits:\n                    cpu: \"1\"\n                    memory: 512Mi\n                  requests:\n                    cpu: 500m\n                    memory: 256Mi\n              name: velero-velero-plugin-mcs\n                resources:\n                  limits:\n                    cpu: \"1\"\n                    memory: 512Mi\n                  requests:\n                    cpu: 500m\n                    memory: 256Mi\n\n\n   Apply the patch to the Velero deployment:\n\n            kubectl patch deployment velero -n velero --patch-file velero-patch.yaml\n\n      The Velero deployment will be patched and restarted.\n\nCreate a default snapshot location by running the command:\n\n      velero snapshot-location create default --provider openstack --config region=ru-msk\n\nChecking Velero operation\n\nRun the command to check the configured plugins:\n\n      velero plugin get\n\n   The output of the command should contain the following plugins:\n\n      NAME                                        KIND\n   ...                                         ...\n   velero.io/aws                               VolumeSnapshotter\n   velero.io/openstack                         VolumeSnapshotter\n\nRun the command to check the configured backup locations:\n\n      velero backup-location get\n\n   A similar information should be displayed:\n\n      NAME      PROVIDER   BUCKET/PREFIX   PHASE       LAST VALIDATED                  ACCESS MODE   DEFAULT\n   default   aws        ...             Available   ...                             ReadWrite     true\n\nRun the command to check the configured snapshot locations:\n\n      velero snapshot-location get\n\n   A similar information should be displayed:\n\n      NAME      PROVIDER\n   default   openstack\n\nDeletion\n\nRun the command:\n\nvelero uninstall\n","url":"/en/kubernetes/k8s/install-tools/velero","description":"How to install Velero in the Kubernetes cluster.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"07d0022d-ad51-4b35-8ccd-f9ed5dc58b65","title":"Pods","data":"A pod is the minimum computing unit that can be run in a Kubernetes cluster.\n\nA pod runs one or more application containers:\n\nTypically, a pod consists of a single container, being a wrapper on top of it. This is the most common scenario for using pods in a Kubernetes cluster.\nA rarer case: a pod can consist of multiple containers. These containers can communicate with each other and share resources assigned to the pod (e.g., storage).\n\n\n  It is not recommended to use multi-container pods unless explicitly needed.\n  It is recommended to limit compute resource consumption for the pod's container for stable cluster operation.\n\n\nFrom a Kubernetes perspective, one pod is one application instance. If you want to have multiple instances of an application for high availability, use workload controllers, which allow you to replicate pods and control their lifecycle. An example of such a controller is Deployment.\n","url":"/en/kubernetes/k8s/reference/pods","description":"General information about pods in Kubernetes.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"479cf863-74f7-4abf-bd1a-b91bab0b111d","title":"Resource limiting","data":"Any Kubernetes workload resource creates and runs a number of containers. These containers have requirements for the amount of computing resources of the cluster worker nodes. The Kubernetes scheduler takes into account the workload's computational resource requirements to place it on the most appropriate worker nodes.\n\nYou should limit resource consumption for any workload that runs in the cluster. Then workload containers cannot exhaust all available worker node resources (for example, if there is a memory leak).\n\nAlthough VK Cloud Kubernetes clusters already contain preconfigured limits, it is a good practice to limit resources manually. For example, preconfigured limits may not be appropriate for resource-intensive workloads.\n\nFor a container in Kubernetes, you can set:\n\nresources.requests: request of computational resources.\n\n  The container can use more computational resources than requested if the worker-node is willing to allocate additional resources. The lower threshold for resources allocated to a container is always limited to the values specified in resources.requests.\n\n  For example, let the container memory be given a required amount of RAM of 256M (256 MB). If this container is in a pod hosted on a node with 8GB of RAM, the container may try to use more RAM if the node has free memory.\n\nresources.limits: limitation on computing resources.\n\n  The container will not be allowed to use more computing resources than specified in this parameter.\n\nThe following computing resources can be limited by the requests and limits parameters:\n\ncpu: the number of vCPUs.\n\n  It is set either in integer cores (e.g., for 3 vCPUs - resources.requests.cpu: 3) or in hundredths of one core using suffix m (e.g., for 0.25 vCPU - resources.requests.cpu: 250m).\n\nmemory: the amount of RAM.\n\n  The amount is specified in bytes, but the following suffixes can be used for convenience:\n\n  Decimal suffixes: k, M, G, T, P, E. For example, 1k would be equivalent to 1000 bytes.\n  Binary suffixes corresponding to the power of two: Ki, Mi, Gi, Ti, Pi, Ei. For example, 1Ki will be equivalent to 1024 bytes.\n","url":"/en/kubernetes/k8s/reference/resource-limiting","description":"How to control the use of computing resources by containers that are part of pods and workloads.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"1c39dfa6-5cbb-4364-83cc-c35b3de2e844","title":"Labels and taints","data":"Labels are key/value pairs that are attached to Kubernetes objects, such as pods. They are designed to specify and use identifying attributes for objects relevant to the user. Taints can be used to organize and select subsets of objects.\n\nTaints apply at the worker node level, prohibiting the scheduler from placing pods on the worker nodes.\n\nTolerations apply at the individual pod level and allow them to run on the nodes where the taints are in effect.\n\nWhen creating a taint for a node, you must select the effect that will be applied to the pods:\n\nNoSchedule: prohibit a pod to run on a node if no corresponding exception is set for the pod. Do not evict from the node already running pods for which there are no exceptions.\nPreferNoSchedule: try to prevent a pod from running on a node if no corresponding exception is set for the pod. Do not evict already running pods with no exceptions from the node.\n\n  This effect is milder than NoSchedule: the scheduler can decide to place a pod on a node with this taint.\n\nNoExecute: evict running pods from the node if no exceptions are set for them.\n\nThus, using taints and tolerations allows to control the set of pods to be run on nodes.\n\nSee official Kubernetes documentation for details.\n","url":"/en/kubernetes/k8s/reference/labels-and-taints","description":"General information about labels and taints in Kubernetes.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"e032a1cc-3c3a-473b-a696-3e0d779608ab","title":"Gatekeeper","data":"Gatekeeper is a controller that provides Rego constraints and controls their execution. A set of constraints forms a CRD-based policy. Gatekeeper checks all CRD operations for Kubernetes resources (creation, modification, deletion) for compliance with the specified constraints and decides whether to deny or allow a particular action.\n\nThe Open Policy Agent (OPA) is responsible for applying constraints (in the form of policies).\n\nHow Gatekeeper works\n\nCompliance with policies is controlled via constraints. These constraints are created based on constraint templates and define:\n\nScope of the constraint: which Kubernetes resources will be checked for compliance with the rules defined in the constraint template.\nParameters that will be used when checking for compliance with the rules.\n\nThe constraint template, in turn, checks for compliance with the rules described in it with the scope and parameters specified in the constraint. These rules are described directly in the YAML file of the constraint template using Rego language. There is also a check on the correctness of parameters passed from the constraint.\n\nResource structure for Gatekeeper\n\nConstraint structure\n\nExample of a constraint:\n\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind:\n\nmetadata:\n  name:\n\nspec:\n  match:\n\n  parameters:\n\n  enforcementAction:\n\nHere:\n\nspec.match: a group of parameters that together define the scope of the constraint.\n\n  kinds: contains a list of objects with fields apiGroups and kinds. These objects list the groups and types of resources to which the constraint will be applied. If several groups and kinds of resources are specified, one match is enough for the resource to fall within the scope of the constraint (a logical OR operation is applied).\n\n  scope: defines whether to check for matches for all resources at once (default), for cluster-scoped resources, namespace-scoped resources.\n\n  namespaces: a list of namespaces. If this option is given, constraints apply only to resources in the namespaces specified.\n\n    This option may be configured as a prefix to select more than one namespace at a time. For example, namespaces: [kube-*] corresponds to kube-system and kube-public.\n\n  excludedNamespaces: a list of namespaces that serve as exceptions for namespaces. If this parameter is set, the constraints apply only to resources that do not belong to these namespaces.\n\n    This option may be configured as a prefix to select multiple namespaces at once. For example, namespaces: [kube-*] corresponds to kube-system and kube-public.\n\n  labelSelector: a combination of two optional fields matchLabels and matchExpressions. These two fields provide different methods to select or exclude Kubernetes resources based on the keys and labels values included in the resource's metadata.\n\n    For a resource to be selected, it must satisfy all the requirements specified here (the \"logical AND\" operation is applied).\n\n  namespaceSelector: a label selector for the namespace containing the resource, or the resource itself if it is a namespace.\n\n  name: the name of the resource. If this parameter is given, constraints apply only to resources with the specified name.\n\n    This parameter can be specified as a prefix to select multiple resources at once. For example, name: [pod-*] corresponds to pod-a and pod-b.\n\nspec.parameters: parameters that will first be checked in the constraint template for type matching and then substituted in Rego rules.\n\nspec.enforcementAction: action when handling constraint violation. Possible values:\n\n  deny (default): prohibit creation of a Kubernetes resource.\n  warn: create Kubernetes resource and issue a warning. This can be useful for debugging constraints.\n  dryrun: do not create a Kubernetes resource, perform a test run (dry run). This can be useful for debugging constraints.\n\nRead more about constraints in Gatekeeper documentation.\n\nConstraint template structure\n\nExample of a constraint template:\n\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\n\nmetadata:\n  name:\n\nspec:\n  crd:\n    spec:\n      names:\n        kind:\n      validation:\n        openAPIV3Schema:\n  targets:\n    target: admission.k8s.gatekeeper.sh\n      rego:\n\nThe spec.crd.spec.validation.openAPIV3Schema parameter provides a type correctness check on the parameters which are passed from the constraint to the constraint template to check the rules. Constraints with incorrectly passed parameters will not be created.\n\nMore about templates in Gatekeeper documentation.\n\nHow to use Gatekeeper\n\nTo use Gatekeeper:\n\nCreate a constraint template manifest with the required rules and input parameter checks.\nCreate a constraint template resource based on the manifest.\nCreate a constraint manifest based on the created constraint template. Set the scope and parameters in the constraint.\n\nExamples of how to use Gatekeeper constraints are given in use cases.\n","url":"/en/kubernetes/k8s/reference/gatekeeper","description":"How to use Gatekeeper to create and enforce constraints for Kubernetes resources.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"e7e0a184-d730-4593-80fe-af2bd453ddeb","title":"Persistent volumes and PVCs","data":"A persistent volume (PV) provides long-term data storage capabilities for Kubernetes clusters. Data stored on a PV is not lost when an individual container or entire pod fails.\n\nWorkloads cannot use PV directly. A Persistent Volume Claim (PVC) should be created, which allows you to request a persistent volume with the required parameters and then use it in workloads. The most common parameters that can be specified in a PVC are:\n\nStorage size (spec.resources.requests.storage).\nVolume access mode (spec.accessModes).\n\n  Kubernetes supports the following volume access modes:\n\n  ReadWriteOnce (RWO): a volume can be mounted in read and write mode by a single node in the cluster. Such a volume can be used by multiple pods at once if they are all hosted on the same node to which the volume is mounted.\n\n  ReadOnlyMany (ROX): a volume can be mounted in read-only mode by multiple nodes in a cluster.\n  ReadWriteMany (RWX): a volume can be mounted in read-write mode by multiple nodes in the cluster.\n\n  A PVC can be used to request a PV that is capable of multiple modes, but when mounting such a PV, you will have to select one of the modes. For example, if a PV with both RWO and RWX modes is requested using PVC, one of the following actions will be available:\n\n  Either mount this PV in RWO mode by a single cluster node.\n  Or mount this PV in RWX mode by multiple cluster nodes.\n\nPV and PVC lifecycle\n\nThe PV and PVC life cycle is independent of the pod life cycle and consists of four consecutive stages:\n\nProvisioning.\nBinding.\nUsing.\nReclaiming.\n\n1. Provisioning\n\nA PV must be prepared prior to requesting the PV via PVC. It can be done in one of the following ways:\n\nStatic provisioning: the PV is created manually, e.g. by a Kubernetes administrator.\nDynamic provisioning: the PV is created automatically after the PVC is created.\n\nFor dynamic provisioning, two conditions must be met:\n\nStorage classes must be configured in the Kubernetes cluster. Cloud Containers clusters already contain pre-configured storage classes.\n\nFor PVCs, no suitable PVs shall be found that already exist.\n\nDuring dynamic provisioning Kubernetes will try to create a PV matching the parameters of the PVC. One of the storage classes will be used:\n\nA storage class explicitly defined in the PVC.\nA default storage class if the class is not explicitly specified in the PVC.\n\nIn VK Cloud Kubernetes clusters, the default storage class is not configured. If you do not plan to explicitly set the storage class in the PVC, then manually set the default storage class before creating the PVC.\n\n2. Binding\n\nPV and PVC are bound on a one-to-one basis for use in workloads. An already bound PV cannot be used with other PVCs.\n\nWhen a new PVC is created in a Kubernetes cluster:\n\nKubernetes tries to find a suitable PV that is available for binding (in the Available status) and matches the parameters specified in the PVC as closely as possible.\n\n   If there is no available PV that exactly matches the parameters specified in the PVC, a PV with excessive characteristics may be selected. Keep an eye on the available PVs and the PVCs being created to avoid a situation where all PVs of large capacity are associated with PVCs that request small amounts of storage. An extra space on such PVs will be wasted and they will not be available for binding when PVCs requiring large amounts of storage appear in the cluster.\n\n   PV selection example\n\n   For example, let there exist in a cluster:\n\n   A PVC that requests 10Gi of storage in ROX mode.\n   The first PV of 100Gi in ROX, RWX modes.\n   The second PV of 5Gi in ROX mode.\n   The third PV of 100Gi in RWO, ROX, RWX modes.\n\n   In this case, the PVC will be bound to the first PV even though:\n\n   The PV size exceeds the requested size and the PV supports more modes than specified in the PVC.\n   There is the third PV with the same size as the first PV but with excessive set of access modes.\n\n\nKubernetes performs one of the actions:\n\n   If a suitable PV was found, Kubernetes binds it to the PVC.\n   If no suitable PV was found, Kubernetes attempts to dynamically provision a PV, then to bind the created PV to the PVC.\n\nIf no PV is bound to a PVC, then the PVC remains in the Unbound state. Workloads cannot use such PVC as a volume. Kubernetes will periodically try to bind such PVC to new PVs if they appear in the cluster.\n\n3. Using\n\nOnce Kubernetes has bound a PVC to a PV, that PVC can be used by a workload as a normal volume.\n\n4. Reclaiming\n\nWhen a PVC is no longer needed and is deleted, a specified reclaim policy is applied to the PV associated with that PVC.\n\nThe policy is set in one of the following ways:\n\nWhen creating a persistent volume manually.\nWhen setting a default storage class or setting a storage class in the PVC.\n\nAvailable policies:\n\nKeep the volume (Retain).\n\n  Use it for PVs with sensitive data to protect the data if the PVC is accidentally deleted. If necessary, you can manually clean up and delete PVs with this policy.\n\nDelete Volume (Delete).\n\n  Depending on the selected PV type, the underlying storage associated with the PV may also be deleted.\n\n\n  Use this policy and the storage classes that implement it with caution: data loss is possible.\n  In VK Cloud, due to integration with Cinder CSI, deleting a PV will also delete the VK Cloud disk associated with it.\n\n\nIn VK Cloud Kubernetes clusters, the storage type selected affects available reclaim policies.\n\nSee also\n\nHow storage is organized in Cloud Containers.\nList of pre-configured storage classes.\nUse-case that demonstrates using of various PVCs.\nOfficial Kubernetes documentation for more information about PVCs and PVs.\n","url":"/en/kubernetes/k8s/reference/pvs-and-pvcs","description":"General information about working with Persistent Volumes with use of Persistent Volume Claim.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Kubernetes","link":"/en/kubernetes","isActive":false,"evenSimpler":true},{"title":"Cloud Containers","link":"/en/kubernetes/k8s","isActive":false,"evenSimpler":true}]},{"uuid":"c71512d8-cb6b-4543-9588-49af1302a9ca","title":"About service","data":"To solve the problems of stream processing of large amounts of data, Cloud Big Data services allow you to create Cloud Streams clusters that provide the ability to safely and efficiently manage streams in real time.\n\nCloud Streams is a fault-tolerant and scalable streaming data processing system built on Apache Kafka and Apache NiFi. The main distinguishing features of Cloud Streams include:\n\nFault tolerance - ensuring consistency in real-time data streaming;\nScalability - adding new servers to the cluster as needed;\nDistribution - the ability to build a geo-distributed infrastructure;\nReal time - flow control, adding and configuring data sources in real time;\nSecurity - flexible mechanisms for controlling access to data;\nIntegration - connectors to various systems: Elasticsearch, SAP HANA, Vertica, Couchbase, Cassandra, CouchDB, IBM MQ and all DBaaS that are now in the cloud.\nA wide range of APIs for integration with other external systems;\nFlexibility - the ability to create a workflow using a graphical interface or develop your own applications using the SDK to improve work efficiency.\n","url":"/en/data-processing/cloud-streams/concepts/about","description":"Benefits of Cloud Streams","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Streams","link":"/en/data-processing/cloud-streams","isActive":false,"evenSimpler":true}]},{"uuid":"f0d5d42f-4d4e-4c6d-bf93-6b540d0c2d50","title":"General information","data":"To view information about an already created cluster:\n\nGo to personal account VK Cloud.\nGo to Big Data → Clusters.\n\nInformation about the cluster will be displayed by the tabs listed below.\n\nGeneral information\n\nThe page provides information about the cluster that is useful for administration and accounting. Among the parameters are:\n\n| Parameter name | Description |\n| --- | --- |\n| Name | The name of the cluster on the VK Cloud platform. |\n| Project ID | ID of the VK Cloud project in which this cluster was created. You can learn more about projects in the article “Projects”. |\n| ID | Cluster ID on the VK Cloud platform. |\n| Creation date | Timestamp of cluster creation in the VK Cloud project. |\n| Description | Description of the cluster on the VK Cloud platform. |\n| Status Description | Additional description of the cluster status. |\n| kafka_manager-SERVER | Account information for accessing the Kafka interface. |\n| nifi-server | Account information for accessing the NiFi server. |\n| nifi-REGISTRY | Account information for accessing the NiFi registry server. |\n| mininifi-C2_SERVER | Account information for accessing the minifi server. |\n\nParameters\n\nList of cluster parameters. To add a parameter, click Add parameter or Add.\n\nInstances\n\nThe tab lists the virtual hardware settings for cluster node instances. You can change them on the General Information tab.\n\nThe options include:\n\n| Parameter name | Description |\n| --- | --- |\n| Virtual CPUs | The number of vCPUs used by these instances. |\n| External address | If applicable. The external IP address of the instance. |\n| RAM | The amount of RAM used by these instances. |\n| Boot Disk Size | The size of the boot disk used by these instances. |\n| Instance Disk Size | The amount of disk used by these instances. |\n| Number of instances | The number of such instances in this cluster. |\n\nCluster events\n\nOn the tab you can view events on the Cloud Streams cluster nodes.\n\nEvent information includes the event that occurred, the date and time, the duration of the event, and its result.\n\nFunctional check\n\nOn the tab you can view Cloud Streams cluster component health check events.\n\nAmong the information about the event, the name, date and time of the check are indicated.\n","url":"/en/data-processing/cloud-streams/service-management/cluster-info","description":"Connection instructions.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Streams","link":"/en/data-processing/cloud-streams","isActive":false,"evenSimpler":true}]},{"uuid":"455aaae9-09c2-4c6c-bc93-42cab9271fb3","title":"Delete Cloud Streams Cluster","data":"To delete a Cloud Streams cluster in your VK Cloud account follow these steps:\n\nGo to the Big Data → Clusters section.\nClick   for the required cluster and select Delete cluster.\nConfirm the operation.\n\nAfter confirming the operation, the cluster will be removed from the VK Cloud project.\n","url":"/en/data-processing/cloud-streams/service-management/delete","description":"Delete Cloud Streams Cluster","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Streams","link":"/en/data-processing/cloud-streams","isActive":false,"evenSimpler":true}]},{"uuid":"ffe36397-af75-4ef6-9105-d3155fe1a8ac","title":"Connecting to Kafka","data":"To connect to a Kafka instance, you need to select its DNS name in your Personal Account and open it in a new browser tab.\n\nNext, you need to enter the Kafka username. By default, the admin user is created.\n\nAfter entering the username, UI Kafka will be available to you.\n","url":"/en/data-processing/cloud-streams/how-to-guides/connect-kafka","description":"Instructions for connecting to the NiFi cluster UI.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Streams","link":"/en/data-processing/cloud-streams","isActive":false,"evenSimpler":true}]},{"uuid":"cc0353b6-2d56-4725-ab26-b980bbfc69bd","title":"Connecting to NiFi","data":"To connect to a NiFi instance, you need to select its DNS name in your Personal Account and open it in a new browser tab.\n\nNext, you need to enter your NiFi username. By default, the admin user is created.\n\nAfter entering the username, the NiFi UI will be available to you.\n","url":"/en/data-processing/cloud-streams/how-to-guides/connect-nifi","description":"Instructions for connecting to the NiFi cluster UI.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Streams","link":"/en/data-processing/cloud-streams","isActive":false,"evenSimpler":true}]},{"uuid":"978242aa-5ce2-49d9-b1fd-9dfac7d406b7","title":"Quick start","data":"A quick start will help you get started with the service and get to know its capabilities.\n\nAfter completing all the steps of a quick start, you will:\n\nCreate an instance of Cloud Kafka in the Single configuration.\nLearn how to send messages to a topic and receive messages from the topic of the Cloud Kafka instance.\n\n\n\nThe instance of Cloud Kafka is charged and consumes computing resources. After passing the quick start delete the service instance if you don't need it anymore.\n\n1. Create an instance of the Cloud Kafka service\n\nGo to VK Cloud personal account.\nSelect the project where you want to create an instance of the service.\nGo to Data Platform → Экземпляры сервисов.\nClick the Создать экземпляр button.\nAt the “Configuration” step:\n\n   Select the type of service Kafka.\n   Select version 3.5.0.\n   Select the configuration Single.\n   Click the Next step button.\n\nAt the “Parameters” step:\n\n   Set a name and description for the instance: vkcloud-demo-kafka and The first instance of Cloud Kafka.\n   Set the node parameters for the broker:\n\n      Type of virtual machine: Standard-2-4.\n      Disk Type: High-IOPS SSD.\n      Disk size: 50 ГБ.\n\n   Кластер Kubernetes: choose Создать новый кластер.\n   Network: choose Create new network.\n   Choose Neutron as SDN.\n   Subnet address: type 10.0.1.0/24.\n   Availability zone: Москва (GZ1).\n   Click the Next step button.\n\nAt the “Учетные данные” step:\n\n   Логин администратора для доступа к Kafka: for example, kafkaadmin.\n   Пароль администратора для доступа к Kafka: click the Generate button and save the received password.\n\n\n      The password cannot be restored.\n\n\n   Click the Create button.\n\n      Wait for the operation to complete. It may take a long time to create an instance of Cloud Kafka.\n\n2. Prepare to work with a Cloud Kafka instance\n\nGet access to Kafka console clients on the host from which you plan to work with the service instance:\n\n   Linux (bash) / macOS (zsh)\n\n\n   Make sure that Java version 11 or higher is installed on the host:\n\n            java --version\n\n      If the output contains Command 'java'not found or the version is outdated, install the Java Development Kit (JDK), for example, OpenJDK. You can install both the entire JDK and only the Java Runtime Environment (JRE).\n\n     Example of installing OpenJDK 11 (JRE only) using apt for Ubuntu 22.04 LTS:\n\n            sudo apt update && sudo apt install openjdk-11-jre\n\n   Download the archive with the Kafka distribution version 3.5.0. The Kafka versions for the service instance and on the host must match. Example of a command to download a distribution using cURL:\n\n            curl -O https://archive.apache.org/dist/kafka/3.5.0/kafka_2.13-3.5.0.tgz\n\n   Unzip the archive and go to the directory with the unpacked files:\n\n            tar -xzf kafka_2.13-3.5.0.tgz && cd kafka_2.13-3.5.0\n\n   Get the version of the producer console client and the consumer console client:\n\n            bin/kafka-console-producer.sh --version\n      bin/kafka-console-consumer.sh --version\n\n      Successful output of the version indicates that the clients are working correctly.\n\n   Create a configuration file client-ssl.properties with the contents:\n\n            security.protocol=SSL\n\n      This file contains the setting, which requires the producer client and the consumer client to authenticate to the service instance using the SSL protocol.\n\n\nGet the address of the bootstrap server to connect to the service instance:\n\n   Go to VK Cloud personal account.\n   Select the project where the service instance is located.\n   Go to Data Platform → Экземпляры сервисов.\n   Open the vkcloud-demo-kafka cluster page by clicking on its name.\n   Go to the General information tab.\n   Write down the address from the Bootstrap parameter.\n\n\n   Below, the address is used as an example kafka-bootstrap.example.com:9093.\n\n   Replace it with the real address of the bootstrap server in the commands below.\n\n\n3. Send a message to the topic and receive it\n\n\n\nLinux (bash) / macOS (zsh)\n\n\n\nCreate a separate terminal session in which the console client will work.\nRun the client-consumer in this session and specify the test topic as the source. The client-consumer will connect to the Cloud Kafka instance using the previously received bootstrap server address and configuration file, and continuously read and output messages from the topic. A new topic will be created automatically the first time you access it.\n\n   To do this, run the command:\n\n      bin/kafka-console-consumer.sh \\\n     --bootstrap-server kafka-bootstrap.example.com:9093 \\\n     --consumer.config client-ssl.properties \\\n     --topic test\n\nCreate another terminal session in which another console client will be running.\nRun the client-producer in this session and specify the test topic as the receiver. The client-producer will connect to the Cloud Kafka instance using the previously received bootstrap server address and configuration file, and send the input messages to the topic.\n\n   To do this, run the command:\n\n      bin/kafka-console-producer.sh \\\n     --broker-list kafka-bootstrap.example.com:9093 \\\n     --producer.config client-ssl.properties\n     --topic test\n\nIn the terminal session with the manufacturer client, enter a test message and then press Enter:\n\n      This is the test message.\n\nCheck that the sent test message is displayed in the terminal session with the consumer client:\n\n      This is the test message.\n\n   The message means that the Cloud Kafka instance works correctly.\n\nShut down the clients by pressing the keyboard shortcut CTRL+C in both terminal sessions.\n\nDelete unused resources\n\nThe instance of Cloud Kafka is charged and consumes computing resources. If you don't need it anymore, delete it.\n\nYou can also delete the configuration file client-ssl.properties and the downloaded Kafka distribution.\n","url":"/en/data-processing/kafka/quick-start","description":"How to get started with the Cloud Kafka service in the VK Cloud project.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Kafka","link":"/en/data-processing/kafka","isActive":false,"evenSimpler":true}]},{"uuid":"1d4b25a0-8344-42e2-9d40-50b52363da87","title":"About the service","data":"Cloud Kafka is a distributed platform for transferring messages between applications. Each instance of the service is a Apache Kafka cluster deployed in a fault-tolerant configuration or in a single node configuration.\n\nThe service operates on the “publish/subscribe” principle: producers publish messages to topics, and consumers poll the service to receive new messages. Cloud Kafka does not track which records have already been read by the consumer, but stores the records for a specified period of time. Due to this, the same message can be processed by different consumers the required number of times.\n\nPossible scenarios for using the service:\n\ndata loading bus, including in data streaming scenarios;\nbuffer for collecting metrics, logs and other types of messages from application systems;\nqueue service for interaction of distributed applications;\ndata transfer between the service and external systems (Kafka Connect).\n\nCloud Kafka service automates processes:\n\ndeployment of infrastructure for a service instance;\nefficient allocation of cluster resources;\nscaling the resources of the service instance.\n\nThe user can manage Cloud Kafka resources through the VK Cloud personal account, while the standard Apache Kafka tool is available — Kafka UI.\n","url":"/en/data-processing/kafka/concepts/about","description":"General information about the Cloud Kafka service in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Kafka","link":"/en/data-processing/kafka","isActive":false,"evenSimpler":true}]},{"uuid":"f06f9912-b825-4f1b-8bb1-0021d6f75c1f","title":"Service architecture","data":"Service architecture\n\nThe Cloud Kafka cluster consists of several Zookeeper clusters and several brokers (Kafka Broker). The number of Zookeeper and Kafka Broker instances depends on the selected fault tolerance mode. To improve cluster performance, you can manually increase the number of brokers.\n\nThe service operates on the “publish/subscribe” principle: producers publish messages to topics, and consumers poll the service to receive new messages. To work with suppliers and consumers, a connection must be configured via load balancer bootstrap (Kafka bootstrap load balancer). The load balancer provides fault-tolerant connection between suppliers and consumers.\n\nA Kafka Connect can be created for a Cloud Kafka cluster is a data streaming tool between Apache Kafka and other systems. The Cloud Kafka service supports connectors:\n\nJDBC,\nS3,\nDebezium,\nClickHouse.\n\nThe Kafka UI tool is also deployed to work with the cluster. Its main functions are:\n\nview the list of Apache Kafka cluster topics and messages in them;\nlaunch connectors and monitor their operation in Kafka Connect;\ntrack the availability of brokers.\n","url":"/en/data-processing/kafka/concepts/architecture","description":"The architecture of the Cloud Kafka service in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Kafka","link":"/en/data-processing/kafka","isActive":false,"evenSimpler":true}]},{"uuid":"a24b57ac-a14c-45be-a1bd-3fcc8ae11bba","title":"Creating an instance of the service","data":"You can create an instance of Cloud Kafka in one of two configurations:\n\nSingle: single node configuration, not fault-tolerant. An instance in this configuration is suitable for development and testing tasks.\nCluster: fault-tolerant configuration of multiple nodes. An instance in this configuration is suitable for any task, including daily work with data in a production environment.\n\nCreating an instance in the Single configuration\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect the project where you want to create an instance of the service.\nGo to Data Platform → Экземпляры сервисов.\nClick the Добавить инстанс or Создать экземпляр button.\nAt the “Configuration” step:\n\n   Select the type of service Kafka.\n   Select the version of the service.\n   Select the configuration Single.\n   Click the Next step button.\n\nAt the “Parameters” step:\n\n   Specify the name and the description for the instance.\n   Set the node parameters for the broker:\n\n      Type of virtual machine: flavor for the node.\n      Disk Type: disk type for the node.\n      Disk size: disk size for the node.\n\n   Select the Kubernetes cluster. A worker node will be added to this Cloud Containers cluster to host an instance of Cloud Kafka.\n\n      If the required cluster is not in the list:\n\n      Choose Создать новый кластер option.\n      Select the network and subnet for the cluster from the drop-down list Network.\n\n         If the required network is not in the list:\n\n         Choose Create new network option.\n         Select the required SDN: Neutron or Sprut.\n         Enter the subnet address, for example, 10.0.1.0/24.\n\n   Select availability zone where the Cloud Containers cluster node will be located.\n   Click the Next step button.\n\nAt the “Учетные данные” step:\n\n   Set the administrator login to access Kafka.\n   Create or generate an administrator password.\n\n      Password requirements:\n\n      can contain only numbers, special characters !, \", #, $, %, &, (, ), *, +, ,, ., :, ;, `, ?, @, [, ], ^, _, {, |, }, ~, -`, uppercase and lowercase Latin letters;\n      must consist of at least 8 characters;\n      must contain at least one uppercase and one lowercase letter of the Latin alphabet, at least one digit.\n\n\n      The password cannot be restored.\n\n\n   Click the Create button.\n\n      Wait for the operation to complete. It may take a long time to create an instance of Cloud Kafka.\n\nCreating an instance in the Cluster configuration\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect the project where you want to create an instance of the service.\nGo to Data Platform → Экземпляры сервисов.\nClick the Создать инстанс or Создать экземпляр button.\nAt the “Configuration” step:\n\n   Select the type of service Kafka.\n   Select the required version of the service.\n   Select the configuration Cluster.\n   Click the Next step button.\n\nAt the “Parameters” step:\n\n   Specify the name and the description for the instance.\n   Select the Kubernetes cluster. A worker node will be added to this Cloud Containers cluster to host an instance of Cloud Kafka.\n\n      If the required cluster is not in the list:\n\n      Choose Создать новый кластер option.\n      Select the network and subnet for the cluster from the drop-down list Network.\n\n         If the required network is not in the list:\n\n         Choose Create new network option.\n         Select the required SDN: Neutron or Sprut.\n         Set the subnet address, for example, 10.0.1.0/24.\n\n   Select availability zone where the Cloud Containers cluster node will be located.\n   Configure the cluster node settings by selecting one of the options:\n\n      Basic\n      Advanced\n\n\n      Select one of the ready-made cluster templates. Flavor and disk type the cluster nodes have already been selected.\n\n      If you select any template, the Cloud Kafka instance will consist of three nodes for brokers and three nodes for ZooKeeper.\n\n\n      Configure the settings yourself:\n\n      Set the node parameters for brokers in the Broker block:\n\n         Type of virtual machine: flavor for the node.\n         Disk Type: disk type for the node.\n         Disk size: disk size for the node.\n         Number of brokers: the total number of nodes.\n\n           The minimum number of nodes is three.\n\n         Replication factor: default replication factor.\n\n           The minimum value of the coefficient coincides with the selected number of nodes for brokers.\n\n           This coefficient is applied when creating Kafka topics, unless a replication coefficient has been explicitly specified for them.\n\n      Get acquainted with the node parameters for ZooKeeper.\n\n         These parameters are preset and cannot be changed.\n\n\n   Click the Create button.\n\nAt the “Учетные данные” step:\n\n   Set the administrator login to access Kafka.\n   Create or generate an administrator password.\n\n      Password requirements:\n\n      can contain only numbers, special characters !, \", #, $, %, &, (, ), *, +, ,, ., :, ;, `, ?, @, [, ], ^, _, {, |, }, ~, -`, uppercase and lowercase Latin letters;\n      must consist of at least 8 characters;\n      must contain at least one uppercase and one lowercase letter of the Latin alphabet, at least one digit.\n\n\n      The password cannot be restored.\n\n\n   Click the Create button.\n\n      Wait for the operation to complete. It may take a long time to create an instance of Cloud Kafka.\n\n\n\n","url":"/en/data-processing/kafka/service-management/create","description":"How to create an instance of the Cloud Kafka service in the required configuration in the VK Cloud project.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Kafka","link":"/en/data-processing/kafka","isActive":false,"evenSimpler":true}]},{"uuid":"3bca8573-7e40-4113-a7a3-3309b1c22ea2","title":"Working with service components","data":"Changing the configuration of components\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to Data Platform → Экземпляры сервисов.\nClick on the name of the instance.\nGo to the Статус компонентов tab.\nClick the Управление компонентами button.\nUpdate the necessary parameters and click Save changes.\n\nEditing information about a service instance\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to Data Platform → Экземпляры сервисов.\nClick   for the instance and select Edit.\nIn the window that opens, specify the new name and description of the instance.\nClick the Save changes button.\n\nChanging the administrator password\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to Data Platform → Экземпляры сервисов.\nClick   for the instance and select Change password.\nIn the window that opens, enter the new administrator password.\n\n   Password requirements:\n\n   can contain only numbers and special characters !, \", #, $, %, &, (, ), *, +, ,, ., :, ;, `, ?, @, [, ], ^, _, {, |, }, ~, -`, uppercase and lowercase Latin letters;\n   must consist of at least 8 characters;\n   must contain at least one uppercase and one lowercase letter of the Latin alphabet, at least one digit.\n\n\n   The password cannot be restored.\n\n\n   To generate a new password, click Generate button.\n\nClick the Save button.\n\nViewing information about an associated Kubernetes cluster\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to Data Platform → Экземпляры сервисов.\nClick   for the required instance and select Перейти к кластеру k8s.\n\nDeleting a service instance\n\nThis is a group operation: if necessary, you can delete several instances of the service at once by selecting them with checkboxes.\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to Data Platform → Экземпляры сервисов.\nClick   for the instance and select Delete.\nConfirm the deletion and wait for the operation to complete. The deletion may take a long time.\n\n\n\n","url":"/en/data-processing/kafka/service-management/management","description":"Operations for single-node or cluster installation via VK Cloud personal account.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Kafka","link":"/en/data-processing/kafka","isActive":false,"evenSimpler":true}]},{"uuid":"087cd0f8-7d97-4264-8f21-629f2a99db89","title":"Tariffication","data":"About pricing\n\n\n\nThe service operates in BETA mode, so fees are charged only for infrastructure resources.\n\n\n\nThe cost of individual components of the service is shown in the price list. To calculate the total cost of the service, use the calculator.\n\nBilling is based on the “pay as you go” principle: the fee is charged only for the resources down to the minute.\n\nTo learn how to pay for the service, see the section Billing.\n\nCharged\n\nCPU — for each core.\nRAM — for each 1 GB of RAM.\nDisks — for each 1 GB of disk space, the price depends on the type of disk (SSD or High-IOPS SSD).\nIP addresses and load balancers, for more information, see Virtual networks.\nSystem disks.\n\nNot charged\n\nOutgoing and incoming traffic.\nUsing the DP Console.\n","url":"/en/data-processing/kafka/tariffication","description":"Features of billing services and applications in the Cloud Kafka, cost calculation.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Kafka","link":"/en/data-processing/kafka","isActive":false,"evenSimpler":true}]},{"uuid":"b17364d8-3143-4fa1-9b98-c8c377e55d03","title":"About the service","data":"Data Platform UI Console (DP Console) is a service to simplify working with Data Platform.\n\nThe service provides an additional user interface for centralized management and grouping of data services in Data Platform.\n\nWith the service you can:\n\nSearch for the required instance of a data service or a group of services.\nCombine data services into logical groups. Each group can include data services deployed on different clusters.\nNavigate through the groups of data services, view network connectivity between individual instances of services.\nAssign rights to individual data science teams at the service group level. This way, each team of data engineers gets access to a fixed set of data services and is responsible for a specific section of the data pipeline.\nView the event log, which records the actions with the groups, including the actions at the level of granting or revoking access rights to the groups.\n","url":"/en/data-processing/console/concepts/about","description":"Information about the Data Platform UI Console service in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Data Platform UI Console","link":"/en/data-processing/console","isActive":false,"evenSimpler":true}]},{"uuid":"6dff963b-ef6d-4689-bb2d-b8e22420520f","title":"Creating a logical group","data":"To make it easier to find and manage Data Platform service instances, organize them into logical groups.\n\nTo create a logical group:\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nGo to Data Platform → Console.\nGo to the Groups tab.\nClick Add or Create group button.\nSpecify the name and the description of the group.\nAdd service instances to the group:\n\n    Click the Добавить экземпляры сервисов button.\n    Select services from the list.\n    Click the Add button. The selected service instances will appear in the group.\n\nClick the Create button.\n\n\n\n","url":"/en/data-processing/console/service-management/create-group","description":"How to create a group of service instances via Data Platform UI Console in the VK Cloud project.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Data Platform UI Console","link":"/en/data-processing/console","isActive":false,"evenSimpler":true}]},{"uuid":"3776b604-2797-47d9-858d-bb703be06d24","title":"Working with logical groups","data":"Viewing groups of service instances\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nGo to Data Platform → Console.\nGo to the Groups tab.\nClick the name of the group.\n\nViewing network connectivity\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nGo to Data Platform → Console.\nGo to the Groups tab.\nClick the name of the group.\nGo to the Сетевая связность tab. A list of networks and services connected with them will open.\n\nEditing a group\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nGo to Data Platform → Console.\nGo to the Groups tab.\nOpen the group editing page by one of the following ways:\n\n    Click   for the group and select Edit.\n    On the group page:\n\n        Click on the name of the group.\n        On the page that opens, click the Редактировать группу button.\n\nMake changes:\n\n    To change the name or description of the group, edit the data in the fields Name or Description.\n    To add service instances:\n\n        Click the Добавить экземпляры сервисов button.\n        In the window that opens, select services from the list.\n        Click the Add button.\n\n    To remove an added instance of the service from the group, click on the icon Delete to the right of its name.\n\nClick the Save button.\n\nDeleting a group\n\n\n\nDeleting a group does not delete the service instances that are part of it.\n\n\n\n\n\nPersonal account\n\n\n\nThis is a group operation: you can delete several groups at once by selecting them using the checkboxes and clicking the Delete button above the list of groups.\n\nGo to VK Cloud personal account.\nGo to Data Platform → Console.\nGo to the Groups tab.\nDelete the group using one of the following methods:\n\n    Click   for the group and select Delete.\n    Click on the name of the group and then open the menu above the table and select Delete group.\n\nConfirm the deletion.\n\n\n\n","url":"/en/data-processing/console/service-management/groups-manage","description":"How to manage groups of service instances via Data Platform UI Console in the VK Cloud project.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Data Platform UI Console","link":"/en/data-processing/console","isActive":false,"evenSimpler":true}]},{"uuid":"9b31c17f-a906-4029-af99-34aa56036be3","title":"Quick start","data":"Быстрый старт поможет вам на примере кластера ADH Community начать работу с сервисом и познакомиться с его возможностями.\n\nПройдя все шаги быстрого старта, вы:\n\nСоздадите кластер больших данных в конфигурации ADH Community Test.\nДобавите новый компонент в развернутый кластер.\n\n1. Создайте кластер ADH\n\nПерейдите в личный кабинет VK Cloud.\nВыберите проект, где нужно создать кластер.\nПерейдите в раздел Большие данные → Кластеры.\nНажмите кнопку Добавить кластер.\nНа шаге «Конфигурация»:\n\n   Выберите конфигурацию кластера ADH Community Test.\n   Укажите версию конфигурации 2.1.8_b3.\n   Нажмите кнопку Следующий шаг.\n\nНа шаге «Создание кластера» задайте:\n\n   Имя кластера: например, ADH-cluster1039.\n   Зона доступности: Москва (GZ1).\n   Сеть: Создать новую сеть.\n   Адрес подсети: 10.0.1.0/24.\n   Настройка компонентов кластера: Продвинутая, выберите HIVE; зависимые компоненты будут выбраны автоматически.\n\nНажмите кнопку Следующий шаг.\nНа шаге «Настройка узлов» задайте:\n\n   Назначить внешний IP на Head-узел: убедитесь, что эта опция выбрана.\n   Автомасштабирование дисков: убедитесь, что эта опция не выбрана.\n\n   Остальные параметры выберите на свое усмотрение.\n\nНажмите кнопку Следующий шаг.\nОставьте настройки без изменений и нажмите кнопку Следующий шаг.\nВ полях Пароль ADCM и Пароль MySql укажите пароль 6B11Z2ah5DAX53~j.\nНажмите кнопку Создать кластер.\n\n   Дождитесь завершения создания кластера, процесс может занять около 25-30 минут.\n\n\n   Через 5-8 минут после начала запуска станет доступен интерфейс ADCM для отслеживания хода процесса установки.\n\n\n2. Добавьте в кластер компонент Airflow\n\nПерейдите в личный кабинет VK Cloud.\nПерейдите в раздел Большие данные → Кластеры.\nОткройте страницу кластера ADH-cluster1039, нажав на его имя.\nПерейдите по адресу, указанному в ADCM UI. Откроется консоль ADCM.\nВведите имя пользователя admin и пароль 6B11Z2ah5DAX53~j.\nПерейдите в раздел CLUSTERS.\nВыберите из списка нужный кластер, нажав на его название.\nПерейдите в раздел Services.\nНажмите кнопку Add services.\nВ открывшемся окне выберите Airflow2 и нажмите кнопку Add.\nПерейдите в раздел Host - Components.\nРаспределите обязательные части компонента по нужным нодам.\nНажмите кнопку Save.\nПерейдите в раздел Configuration и укажите пароли в обязательных полях.\nВ иерархии нажмите на значок запуска возле названия AIRFLOW2 и выберите опцию Install.\nДождитесь завершения установки и перезапустите сервисы.\n\nУдалите неиспользуемые ресурсы\n\nИспользование кластера тарифицируется и потребляет вычислительные ресурсы. Если он вам больше не нужен:\n\nУдалите кластер.\nПри необходимости удалите плавающий IP-адрес, назначенный узлам кластера.\n","url":"/en/data-processing/bigdata/quick-start","description":"How to get started with the Cloud Big Data service in the VK Cloud project.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Big Data","link":"/en/data-processing/bigdata","isActive":false,"evenSimpler":true}]},{"uuid":"7464d86d-1823-40b0-a750-6fdb177e9469","title":"About service","data":"Cloud Big Data is a series of approaches, tools and methods for processing structured and unstructured data of huge volumes and significant diversity to obtain results perceived by a person and effective in conditions of continuous growth, distribution over numerous nodes of a computer network.\n\nFeatures\n\nComplete offline installation utility package - provides a complete set of features and tools for automatic installation and configuration of components both on bare metal and on virtual machines (in the cloud). Cluster configuration monitoring and management tools allow you to optimize performance for all system components.\nOwn management system - provides an open system for automatic deployment and management of Arenadata Cluster Manager. It is a Multi-cloud system and can be deployed on any existing infrastructure, including public clouds.\nBuild based on open Apache projects - Arenadata Hadoop is a completely open-source distribution of Hadoop, so users will never have to deal with such a problem as a vendor lock-in.\nA set of typical package services for planning, installing and auditing the system.\nThe ability to influence the development of the system functionality - if the user needs to expand the functionality of the platform to solve business problems, then by agreement it is possible to add additional services to the ecosystem and take support as a full-fledged component of additional services, for example, Apache Flink, Zeppelin and Solr.\nOriginal documentation in Russian and English will help facilitate the process of planning, installing and configuring a Hadoop cluster.\nRefresher courses for the client's employees.\nSupport is available both remotely and on-site.\n","url":"/en/data-processing/bigdata/concepts/about","description":"Arenadata Hadoop Benefits","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Big Data","link":"/en/data-processing/bigdata","isActive":false,"evenSimpler":true}]},{"uuid":"20023db5-5bf9-4f48-a26b-235296753091","title":"Components","data":"Доступные компоненты\n\nПри ручной настройке вы можете выбрать набор компонентов, устанавливаемых в кластер:\n\nADCM — возможность подключения к Edge-узлу и к Head-узлу.\nHBASE — нереляционная (NoSQL) распределенная база данных.\nHDFS — распределённая файловая система Apache Hadoop.\nHIVE — система хранения данных (data warehouse) для анализа больших наборов данных и ad-hoс запросов при помощи SQL.\nMAPREDUCE — система на основе YARN для параллельной обработки больших наборов данных.\nSPARK — платформа для обработки больших массивов данных.\nMYSQL — реляционная система управления базами данных компании Oracle.\nSQOOP — инструмент для передачи данных между Hadoop и реляционными базами данных или мэйнфреймами.\nYARN — структура для планирования заданий и управления ресурсами кластера.\nZEPPELIN — интерактивный веб-блокнот, который поддерживает этапы работы с данными в Data Science, от извлечения до визуализации, в том числе интерактивный анализ и совместное использование документов.\nZOOKEEPER — сервис-координатор, который обеспечивает распределенную синхронизацию небольших по объему данных (конфигурационная информация, пространство имен) для группы приложений.\nSOLR — поисковая платформа с открытым исходным кодом, построенная на Apache Lucene.\nKERBEROS — сетевой протокол аутентификации, основанный на концепции мандатов (tickets). Позволяет узлам, обменивающимся данными по незащищенному каналу, безопасно идентифицировать друг друга.\nAIRFLOW — платформа для создания, планирования и мониторинга рабочих процессов.\nFLINK — фреймворк для statefull вычислений над потоками данных.\n\nШаблоны наборов компонентов\n\nВ рамках сервиса Cloud Big Data доступны шаблонные наборы компонентов для кластера.\n\n\n\nШаблоны недоступны для Test-конфигураций кластера.\n\n\n\nData Science:\n\n  HDFS,\n  YARN,\n  MapReduce 2,\n  Tez,\n  Hive,\n  Spark 2,\n  Sqoop,\n  flink,\n  Airflow,\n  Hbase,\n  Zeppelin.\n\nEDW Analytics:\n\n  HDFS,\n  YARN,\n  MapReduce 2,\n  Tez,\n  Hive,\n  Spark 2.\n\nEDW ETL:\n\n  HDFS,\n  YARN,\n  MapReduce 2,\n  Tez,\n  Hive,\n  Spark 2,\n  Sqoop,\n  Flink,\n  Airflow.\n\nDATA Science + GPU:\n\n  HDFS,\n  YARN,\n  MapReduce 2,\n  Tez,\n  Hive,\n  Spark 2,\n  Sqoop,\n  Flink,\n  Airflow,\n  Hbase,\n  Zeppelin.\n","url":"/en/data-processing/bigdata/concepts/components","description":"Available components for installation in the Cloud Big Data cluster, templates of component sets in the VK Cloud project.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Big Data","link":"/en/data-processing/bigdata","isActive":false,"evenSimpler":true}]},{"uuid":"7218e556-571c-49d2-a2da-935f03b8625e","title":"Arenadata Hadoop Licenses","data":"Конфигурации кластера (пресеты)\n\nVK Cloud предоставляет разные конфигурации кластера при его создании:\n\nADH Community:\nADH Community Test,\nADH Enterprise,\nADH Enterprise Test.\n\nВсе конфигурации доступны в версиях:\n\n2.1.4_b10,\n2.1.8_b3.\n\nЛицензии\n\nПри выборе одной из конфигурации кластера будет автоматически добавлена одна из лицензий:\n\nArenadata ADH Community,\nArenadata Hadoop Enterprise Full,\nArenadata Hadoop Enterprise Limited,\nArenadata Hadoop Enterprise Test.\n\n\n\nИспользование лицензий тарифицируется.\n\n\n","url":"/en/data-processing/bigdata/concepts/licensing","description":"Community and corporate configuration of big data cloud clusters. User Community, Enterprise Full, Enterprise Limited, Enterprise Test.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Big Data","link":"/en/data-processing/bigdata","isActive":false,"evenSimpler":true}]},{"uuid":"83229542-621a-4c5d-ad04-17b820ff8db8","title":"Installing components in a cluster","data":"\n\nЛичный кабинет\n\n\n\nПерейдите в интерфейс ADCM кластера.\nПерейдите в раздел CLUSTERS.\nВыберите из списка нужный кластер, нажав на его название.\nПерейдите в раздел Services.\nНажмите кнопку Add services.\nВ открывшемся окне выберите нужные компоненты и нажмите кнопку Add.\nРаспределите части компонента по нодам: последовательно выделяйте часть слева, затем выберите ноду, на которую хотите установить часть.\nНажмите кнопку Save.\nПри необходимости укажите дополнительные авторизационные данные для компонента в разделе Configuration.\nВ иерархии нажмите на значок запуска возле названия компонента и выберите опцию Install.\nДождитесь завершения установки.\nПри необходимости перезапустите сервисы.\n\n\n\n","url":"/en/data-processing/bigdata/service-management/add-component","description":"How to install new components in a deployed big data cluster in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Big Data","link":"/en/data-processing/bigdata","isActive":false,"evenSimpler":true}]},{"uuid":"31907aa6-1167-4915-88ce-ce6ad7149698","title":"Scaling the cluster","data":"\n\nPersonal account\n\n\n\nGo to personal account VK Cloud.\nGo to Big Data → Clusters.\nClick   for the required cluster and select Reconfigure cluster.\nOn the page that opens, specify the scaling parameters:\n\n   Scaling type: horizontal or vertical.\n   Node type: select the node from the list for which to scale.\n   Number of nodes: specify the required number of nodes.\n   Type of virtual machine: specify the VM type for the node to be created.\n\nClick the Save changes button.\n\n\n\n","url":"/en/data-processing/bigdata/service-management/scale/scale-cluster","description":"Vertical and horizontal scaling of VK Cloud cluster nodes.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Big Data","link":"/en/data-processing/bigdata","isActive":false,"evenSimpler":true}]},{"uuid":"1be7be97-27ad-4eb8-9143-dbcb5fe2f031","title":"Configuring the Replication Factor","data":"VK Cloud поддерживает изменение фактора репликации на разных уровнях:\n\n\n\nСуществующие файлы кластера\nНовые файлы кластера\nНовые кластера проекта\n\n\n\nПодключитесь к master-узлу с плавающем адресом с помощью SSH.\n(Опционально) Уточните текущее число копий:\n\n      hdfs dfs -stat\n\n   Пример вывода команды:\n\n      hdfs fsck /user/admin/data.csv\n   /user/admin/data.csv:  Under replicated BP-1014754436-192.168.99.119-1532095262675:blk_1073743175_2396. Target Replicas is 3 but found 1 live replica(s), 0 decommissioned replica(s) and 0 decommissioning replica(s).\n   Status: HEALTHY\n    Total size: 2878934 B\n    Total dirs: 0\n    Total files:   1\n    Total symlinks:             0\n    Total blocks (validated):   1 (avg. block size 2878934 B)\n    Minimally replicated blocks:   1 (100.0 %)\n    Over-replicated blocks:     0 (0.0 %)\n    Under-replicated blocks:    1 (100.0 %)\n    Mis-replicated blocks:      0 (0.0 %)\n    Default replication factor: 1\n    Average block replication:  1.0\n    Corrupt blocks:             0\n    Missing replicas:           2 (66.666664 %)\n    Number of data-nodes:       1\n    Number of racks:            1\n   FSCK ended at Mon Jul 23 17:20:46 UTC 2018 in 1 milliseconds\n   The filesystem under path '/user/admin/data.csv' is HEALTHY\nИзмените фактор репликации для отдельного файла с помощью команды:\n\n      hdfs dfs -setrep 3\n\n   Чтобы изменить параметр рекурсивно, добавьте в команду флаг -R.\n\n\n\n\nПерейдите в интерфейс ADCM кластера.\nПерейдите в раздел CLUSTERS.\nВыберите из списка нужный кластер, нажав на его название.\nПерейдите в раздел Services.\nВыберите из списка компонент HDFS, нажав на его имя.\nПерейдите в раздел Configuration.\nНайдите параметр dfs.replication и укажите для него нужное значение.\nНажмите кнопку Save.\n\n\n\n\nОбратитесь в техническую поддержку.\n\n\n\n","url":"/en/data-processing/bigdata/service-management/scale/change-replica-factor","description":"Configuring the replication factor of big data cluster files in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Big Data","link":"/en/data-processing/bigdata","isActive":false,"evenSimpler":true}]},{"uuid":"70b872db-0eb3-4d98-ba2e-2ce3a288fd43","title":"Configuring Java Memory Heap parameters","data":"Оптимальные параметры виртуальной машины Java подбираются автоматически при установке и настройке кластера, в зависимости от типа виртуальной машины. Если вы получаете ошибки «Out of memory», выполните удобное действие:\n\nсоздайте кластер с конфигурацией большего размера;\nизмените параметры в интерфейсе ADCM.\n\nЧтобы изменить параметры Java Memory Heap для компонента:\n\nПерейдите в интерфейс ADCM кластера.\nПерейдите в раздел CLUSTERS.\nВыберите из списка нужный кластер, нажав на его название.\nПерейдите в раздел Services.\nВыберите из списка нужный компонент, нажав на его имя.\nПерейдите в раздел Configuration.\nВ блоке Heap Memory settings установите нужные параметры и нажмите кнопку Save.\nПерезапустите измененный компонент.\n","url":"/en/data-processing/bigdata/service-management/scale/config-jmemory-heap","description":"How to configure Java memory settings for cluster components in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Big Data","link":"/en/data-processing/bigdata","isActive":false,"evenSimpler":true}]},{"uuid":"10ef4ef1-44b7-4c44-84af-dafde2b9c7dc","title":"Deleting","data":"\n\nPersonal account\nAPI\n\n\n\nGo to personal account VK Cloud.\nGo to Big Data → Clusters.\nClick   for the required cluster and select Delete.\nConfirm the operation.\n\n\n\n\nUse a DELETE /clusters/{cluster_id} REST API method.\n\n\n\n","url":"/en/data-processing/bigdata/service-management/delete","description":"Deleting a big data cluster from a VK Cloud project.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Big Data","link":"/en/data-processing/bigdata","isActive":false,"evenSimpler":true}]},{"uuid":"d4800688-e762-41e3-9150-33d763ddc529","title":"Connecting to cluster nodes","data":"После запуска кластера группы безопасности настроены согласно данным из таблицы ниже.\n\nИсходящий трафикБез ограниченийВходящий и исходящий трафик между узлами кластера в рамках внутренней сетиБез ограниченийВходящий трафик на TCP-порт 8080 головного узлаБез ограничений, используется для доступа в интерфейс ADCM (авторизация по логину и паролю)Входящий трафик на TCP-порт 22 головного узлаБез ограничений, используется для доступа по SSH (авторизация по SSH-ключу).Входящий ICMP-трафик на головной узелБез ограниченийОстальной трафикЗапрещен\n\n\n\nВнешний доступ к кластеру возможен через веб-интерфейс ADCM и SSH-доступ к Head-узлу.\n\nНастройка доступа к веб-интерфейсу ADCM\n\n\n\nЛичный кабинет\n\n\n\nПерейдите в личный кабинет VK Cloud.\nПерейдите в раздел Большие данные → Кластеры.\nНа шаге «Настройка узлов» создания кластера выберите опцию Назначить внешний IP на Head-узел.\nНа шаге указания пароля компонентов укажите пароль для входа в ADCM UI.\nДождитесь завершения развертывания кластера.\nОткройте страницу нужного кластера, нажав на его имя.\nПерейдите по адресу, указанному в ADCM UI. Откроется консоль ADCM.\nВведите имя пользователя admin и пароль, указанный при создании кластера.\n\nПодключение к Head-узлу по SSH\n\n\n\nЛичный кабинет\n\n\n\nПерейдите в личный кабинет VK Cloud.\nПерейдите в раздел Большие данные → Кластеры.\nНа шаге «Настройка узлов» создания кластера выберите опцию Назначить внешний IP на Head-узел.\nДождитесь завершения развертывания кластера. Сохраните SSH-ключ, сгенерированный при создании кластера.\nОткройте страницу нужного кластера, нажав на его имя.\nПодключитесь к Head-узлу по IP-адресу, указанному в ADCM UI по SSH-ключу.\n\nПодключение к Edge-узлу по SSH\n\n\n\nЛичный кабинет\n\n\n\nПерейдите в личный кабинет VK Cloud.\nПерейдите в раздел Большие данные → Кластеры.\nНа шаге «Настройка узлов» создания кластера выберите опцию Подключить Edge-узел.\nУкажите нужные параметры узла и выберите опцию Назначить внешний IP на Edge-узел.\nДождитесь завершения развертывания кластера. Сохраните SSH-ключ, сгенерированный при создании кластера.\nОткройте страницу нужного кластера, нажав на его имя.\nПодключитесь к Edge-узлу по SSH-ключу.\n\nНастройка доступа к веб-интерфейсам компонентов кластера\n\nЧтобы настроить доступ к веб-интерфейсам компонентов в сети, воспользуйтесь одним из способов:\n\n\n\nНастройте VPN-доступа\nНастройте группы безопасности\n\n\n\nСоздайте новую внутреннюю сеть и маршрутизатор.\nСоздайте кластер, указав созданную сеть.\nНастройте VPN-подключение к сервисам VK Cloud.\n\n\n\n\nНастройте группы безопасности нужным для вас образом, подробнее в статье Ограничение трафика.\n\n\n\n","url":"/en/data-processing/bigdata/connect","description":"Connecting to the cluster via the ADCM UI in VK Cloud. Connecting to nodes via SSH. Connecting to the components' web interfaces via an internal network.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Big Data","link":"/en/data-processing/bigdata","isActive":false,"evenSimpler":true}]},{"uuid":"be47b8cf-a531-4da4-99e8-13ff4bd14767","title":"Tariffication","data":"Tariffication is based on the “pay as you go” principle: you are charged only for the resources you consume to the nearest minute, the fee for using paid OS is charged once a day.\n\nThe cost of network services is given in price list. You can use calculator to calculate the total cost of other platform services that use services. See Billing for how the tools related to paying for platform services work.\n\nWhat is charged\n\nCPU (vCPU) — for each core. 1 vCPU corresponds to 1 physical core of the virtualization server.\nRandom access memory (RAM) — for each 1 GB of RAM.\nLicenses.\nDisks — for each 1 GB of disk space, the price depends on the type of disk (SSD, HDD, High-IOPS, Low Latency NVME).\nDisk snapshot (backup) — for every 1 GB of snapshot size.\nFloating IP addresses.\n\nWhat is not charged\n\nUsing ADCM UI.\nIncoming and outgoing traffic.\nMonitoring.\n","url":"/en/data-processing/bigdata/tariffication","description":"Billing of Cloud Big Data service resources in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Data processing and analysis","link":"/en/data-processing","isActive":false,"evenSimpler":true},{"title":"Cloud Big Data","link":"/en/data-processing/bigdata","isActive":false,"evenSimpler":true}]},{"uuid":"181aeab3-48f1-454e-a020-bac27a54175e","title":"Concepts","data":"The Cloud ML Platform from VK Cloud is a set of services and tools designed to solve typical Data Science and ML team tasks.\n\nMLOps can be defined as a discipline or set of practices with the goal of using and maintaining services based on ML models. This discipline was born at the intersection of Machine Learning, DevOps and Data Engineering.\n\nThe task of MLOps usually includes:\n\nConducting experiments with data;\nTraining of ML-models;\nDeploy income of ML-models in various environments, such as, for example, development and production;\nMonitoring the behavior of models in production;\nUpdate of used ML-models.\n\nTo solve MLOps tasks, VK Cloud provide the opportunity to receive the following services:\n\nJupyterHub — a tool for working with Jupyter Notebook in multi-user mode;\nMLflow is a tool for solving problems of tracking and versioning experiments, ML models.\n","url":"/en/ml/mlplatform/concepts","description":"The operating and design description of the Cloud ML Platform service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"13c22d91-b876-4e78-9bef-883c6e0ffa8b","title":"Creating a JupyterHub Instance","data":"To create a JupyterHub instance, go to the \"ML Platform\" section. Click Add Instance.\n\nWhen you click on the button, the virtual machine configurator will open in the window, consisting of several stages, as a result of which it determines the parameters of the created VM.\n\nAt all stages, the configurator informs about the cost of the created instance, additional features, and also allows you to contact support in case of questions.\n\nDuring the installation process, you need to enter the following settings:\n\n| Parameter | Description |\n| --- | --- |\n| Instance name | The display name of the instance. Also sets the hostname on the OS. |\n| Category of virtual machine | Flavors of the VMs. For more information, see Cloud Servers service overview. |\n| Type of virtual machine | Preinstalled VM configuration (CPU and RAM). |\n| Accessibility zone | Selecting the data center where the instance will be launched. |\n| Operating system | Operating system image (version, edition). |\n| Number of discs | The number of hard drives in the created instance. Only one disk per JupyterHub instance is currently supported. More than one default drive cannot be selected.|\n| Disk size | Specifies the size of the VM disk in GB. Growth or shrinkage of the disk size of a JupyterHub instance is currently not supported. We recommend storing large amounts of data in external storage, for example, in S3 object storage.|\n| Disk type | Type of instance disk being created, more. |\n| Choosing a domain name | Specifies the DNS name of the instance.|\n| Username | Guest OS username |\n| User password | User password in the guest OS. |\n\nThe next step is to configure the virtual network. You can select an existing network or create a new one (for more details, see the article \"Creating and deleting networks)\".\n\n| Parameter | Description |\n| --- | --- |\n| Network | Creating a VM in an external (ext-net) or private network. |\n| Virtual machine key | Used to decrypt the administrator password. |\n\nAfter entering the settings values, click \"Create Instance\".\n\nThe virtual machine will be created within 10-15 minutes. During this period, the operating system is deployed to the instance disk, and system tools are running to configure the virtual machine in accordance with the specified parameters.\n\n\n\nDo not close the New Instance window.\n\nUpon completion of the instance setup, a page with server characteristics and instructions for connecting to it will open.\n\n\n","url":"/en/ml/mlplatform/jupyterhub/quick-start/create","description":"Description of how to create a JupyterHub instance.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"a5311849-6bbd-4e88-8022-4f85638a5c01","title":"Creating new users","data":"To create a new user, follow these steps:\n\nFrom your VK Cloud personal account, go to the DNS name of the JupyterHub instance, and then follow the link \"Sign up to create a new user\".\nSet the name and password of the user to be created. If necessary, enable two-factor authentication to sign in to JupyterHub.\nSign in with an admin account and go to the Hub Control Panel. Select Authorize Users. Next to the new user, select Authorize.\n\n    In order to go to the Hub Control Panel, you must select this item in the File section of JupyterHub.\n\nAfter that, the user will be able to sign in to JupyterHub using the username and password chosen earlier.\n","url":"/en/ml/mlplatform/jupyterhub/quick-start/createuser","description":"Creating new users.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"edbad91d-ada5-4150-9493-f219828a4647","title":"Connecting to JupyterHub","data":"To connect to a JupyterHub instance, you need to select its DNS name in your Dashboard and open it in a new browser tab. After that, the UI JupyterHub will be available.\n\nNext, you need to enter the username and password from JupyterHub.\n\nBy default, the admin user is created, the password for it is set during the creation of the JupyterHub instance in the LC.\n\n\n\nWhen you connect for the first time, it is recommended that you change the administrator password.\n\nA user with administrator rights has the ability to bring the service to an unhealthy state, destroy the data of all users in the JupyterHub instance. Therefore, it is recommended to use a user with administrative rights only to grant access to other users by creating non-administrative users.\n\nIf the data is deleted or the JupyterHub instance is rendered unhealthy by a user with administrative privileges, the instance cannot be restored.\n\n\n","url":"/en/ml/mlplatform/jupyterhub/quick-start/connect","description":"Description of the process for connecting to a JupyterHub instance.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"d94f380b-fa77-46d2-a403-fcad09759dd0","title":"Creating a custom kernel","data":"To create an individual kernel, run the following commands in the JupyterHub terminal:\n\nCreate a virtual environment:\n\npython3 -m venv set-name-of-venv\n\nwhere set-name-of-venv is the name of the virtual environment.\n\nActivate the virtual environment:\n\nsource set-name-of-venv/bin/activate\n\nInstall ipykernel:\n\npip install ipykernel\n\nCreate a new kernel:\n\npython -m ipykernel install --user --name set-name-of-venv --display-name \"Python (myenv)\"\n\nwhere Python (myenv) is the name of the new kernel.\n\nYou can give an arbitrary name to display kernel and virtual environment.\n","url":"/en/ml/mlplatform/jupyterhub/how-to-guides/kernel","description":"Description of how to create a custom kernel on a JupyterHub instance.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"81547dc2-62dd-4677-a7f3-caf30053b322","title":"Deleting an instance","data":"To delete a JupyterHub instance, you need to:\n\nIn your VK Cloud personal account, go to the ML Platform → Instances section.\nClick   for the instance and select Delete.\nConfirm the operation.\n\n\n\nNote\nIf the JupyterHub instance was previously associated with MLflow, you first need to delete the MLflow instance.\n\n\n","url":"/en/ml/mlplatform/jupyterhub/service-management/delete","description":"Description of how to remove a JupyterHub instance.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"3cc5f830-46e6-4932-9ff2-c711caea484e","title":"Creating an MLflow Instance","data":"To create an MLflow instance, go to the \"ML Platform\" section. Click Add Instance.\n\nWhen you click on the button, the virtual machine configurator will open in the window, consisting of several stages, as a result of which it determines the parameters of the created VM.\n\nAt all stages, the configurator informs about the cost of the created instance, additional features, and also allows you to contact support in case of questions.\n\nDuring the installation process, you need to enter the following settings:\n\n| Parameter | Description |\n| --- | --- |\n| Instance name | The display name of the instance. Also sets the hostname on the OS |\n| Type of virtual machine | Preinstalled VM configuration (CPU and RAM) |\n| Accessibility zone | Selecting the data center where the instance will be launched |\n| Disk size | Sets the VM disk size in GB |\n| Disk type | Type of instance disk to be created, more |\n| Choosing a domain name | Specifies the DNS name of the instance |\n| JupyterHub Instance | Selecting the JupyterHub instance to which the MLflow instance will be connected |\n\nThe next step is to configure the virtual network. You can select an existing network or create a new one (more details can be found in the article \"Creating and deleting networks\".\n\n| Parameter | Description |\n| --- | --- |\n| Network | Creating a VM in an external (ext-net) or private network. Must match the network where the JupyterHub instance is running |\n| Virtual machine key | Used to decrypt the administrator password |\n\nAfter entering the settings values, click \"Create Instance\".\n\nThe virtual machine will be created within 10-15 minutes. During this period, the operating system is deployed to the disk of the instance, and system tools are running to configure the virtual machine in accordance with the specified parameters.\n\n\n\nDo not close the New Instance window.\n\nUpon completion of the instance setup, a page with server characteristics and instructions for connecting to it will open.\n\nAfter the installation of MLflow is completed, within 5-10 minutes, the JupyterHub instance will be automatically configured to work with MLflow.\n\nIn order for the settings to take effect, you must restart the JupyterHub server. This can be done from JupyterHub in the Hub Control Panel, or restart the VM from JupyterHub in the Cloud Computing -> Virtual Machines section.\n\n\n","url":"/en/ml/mlplatform/mlflow/quick-start/create","description":"Description of how to create an MLflow instance.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"a6b42ffc-f03e-4bfc-9f59-f01001b6a4dd","title":"Connecting to MLflow","data":"To connect to an MLflow instance, you need to select its DNS name in your Personal Account and open it in a new browser tab. After that, UI MLflow will be available.\n\nNext, you need to enter the username and password from MLflow.\n\nThe MLflow login and password are the same as the JupyterHub login and password. Each user who can sign in to JupyterHub also has access under the same credentials in MLflow.\n","url":"/en/ml/mlplatform/mlflow/quick-start/connect","description":"Description of the process for connecting to an MLflow instance.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"6e2c6a20-e88a-49e4-ba18-c05815962b08","title":"Creating new users","data":"To provide access to MLflow to new users, you must add them to the JupyterHub associated with this MLflow instance.\nInstructions for adding users to JupyterHub are available in the article “Creating a New JupyterHub User”.\n","url":"/en/ml/mlplatform/mlflow/quick-start/createuser","description":"Creating new users.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"0492cf8a-2c61-4e7e-a0b0-2e0031324764","title":"Deleting an instance","data":"To delete an MLflow instance, you must:\n\nIn your VK Cloud personal account, go to the ML Platform → Instances section.\nClick   for the instance and select Delete.\nConfirm the operation.\n","url":"/en/ml/mlplatform/mlflow/service-management/delete","description":"Description of how to delete an MLflow instance.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"1add8808-ac13-46e5-93fd-f843309379e1","title":"Creating an MLflow Deploy instance","data":"The MLflow Deploy service provides the ability to automatically package ML models into Docker containers and make them available via REST API to solve real-time service tasks.\n\nThe service is integrated with the Cloud ML Platform components: JupyterHub and MLflow.\n\nCreating MLflow Deploy instances is available both through your VK Cloud personal account and via MLflow Client.\n\nBefore you start\n\nCreate a JupyterHub instance.\nCreate an MLflow instance.\n\nCreating an instance\n\nGo to your VK Cloud personal account.\nGo to ML Platform.\nIn the MLflow Deploy Instance block, click the Create Instance button.\nSet up the instance configuration:\n\n   Instance name: a name of the instance. It also sets the OS hostname parameter.\n   Virtual machine category: a category of the preset VM configurations. More details in the review of the Cloud Servers service.\n   Virtual machine type: a preset VM configuration (CPU and RAM).\n   Availability zone: the data center where the instance will be launched.\n   Disk size: the VM disk size in GB.\n   Disk type: the VM disk type.\n   MLflow instance: the MLflow instance which will be connected with the MLflow Deploy instance.\n\nClick the Next Step button.\nSet up the network:\n\n   Network: select an existing network or create a new one.\n   Virtual machine key: a key for decrypting the administrator password. Select an existing key or create a new one.\n\n\n    The MLflow Deploy instance must be created on the network that hosts JupyterHub and MLflow.\n\n\nClick the Create Instance button.\n","url":"/en/ml/mlplatform/deploymlflow/quick-start/create","description":"How to create an instance in the MLflow Deploy service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"9b3a5770-12b3-4204-a10f-0cd6aad136d9","title":"Managing instances with MLflow Client","data":"Manage MLflow Deploy instances using MLflow Deployment Client, a python library from VK Cloud pre-installed in JupyterHub.\n\nBefore you start\n\nCreate a JupyterHub instance.\nCreate an MLflow instance.\nReboot the virtual machine with the JupyterHub instance:\n    Go to Cloud Servers → Virtual machines.\n    In the ML Platform Instances subsection, find the JupyterHub you need instance.\n    Click   for the instance and select Restart.\n\n\n\nJupyterHub, MLflow and MLflow Deploy instances must be created on the same network.\n\nPreparing the model\n\nConnect to the JupyterHub instance.\nOpen the tutorials directory.\n\n    JupyterHub from VK Cloud includes training Jupyter notebooks: mlflow_demo.ipynb and mlflow_deploy_demo.ipynb.\n\nPrepare a test model:\n    Double-click on mlflow_demo.ipynb.\n    In the window that opens, click on the cell with the code example and select Run → Run Selected Cells from the menu.\n    Repeat the operation for all cells with a code.\n\nInstance management\n\nTo go through the entire Jupyter tutorial script:\n\nDouble-click on mlflow_deploy_demo.ipynb.\nIn the window that opens, click on the cell with the code example and select Run → Run Selected Cells from the menu.\nRepeat the operation for all cells with a code.\n\nCreating MlflowClient\n\nCreate an MlflowClient from the MLflow Tracking module to receive the URI of the ML model. The URI will be needed later to deploy the model. The example shows accessing the model by URI.\n\nfrom mlflow.tracking import MlflowClient\ncli = MlflowClient()\n\nFor this example use the very first model:\n\nmodel_source_uri = cli.search_registered_models()[0].latest_versions[0].source\nprint(\"Model name: \", cli.search_registered_models()[0].latest_versions[0].name)\nprint(\"Model URI: \", model_source_uri)\n\nCreate a Client from the MLflow Deployments module to work in Cloud ML Platform:\n\nfrom mlflow.deployments import get_deploy_client\nclient = get_deploy_client('vk-cloud-mlplatform')\n\nCreating an endpoint\n\nendpoint in VK Cloud MLflow Deploy terminology is a virtual machine configured as a deployment server.\n\ndeploy_server_name = \"deploy_server_one\"\nclient.create_endpoint(name=deploy_server_name)\n\nclient.create_endpoint(name, performance=\"low\", disk_size=50, disk_type=\"ceph-ssd\", av_zone=None)\n\nThe full list of parameters is presented above. You can also create a deployment server by specifying only the server name. In this case, the value of theav_zone parameter will be the name of the zone in which the synchronized MLflow service is located.\n\nThe perfomance parameter in the create_enpoint method is responsible for the configuration of the virtual machine. The following values are available:\n\nlow - standard 4-4 (4 cores, 4 GB RAM)\nmid - advanced 8-8 (8 cores, 8 GB RAM)\nhigh - advanced 16-16 (16 cores, 16 GB RAM)\n\nGetting a list and status of servers\n\nThe deployment server is ready for operation when its status changes from CREATING to RUNNING. Typically, preparing a deployment server takes about five to ten minutes.\n\nclient.list_endpoints()\n\nGet information about the status of the deployment server by its name:\n\nclient.get_endpoint(deploy_server_name)\n\nCreating a deployment\n\ndeployment in VK Cloud MLflow Deploy terminology is a Docker container with a model running on the deployment server.\n\ndeployment_name=\"test_deployment\"\nclient.create_deployment(server_name=deploy_server_name, deployment_name=deployment_name, model_uri=model_source_uri, port_out = None)\n\nport_out can be omitted, the first free port in the range from 62000 to 65000 will be selected. Launching the model usually takes less than a minute.\n\nGetting the list of running models and the deployment status\n\nDisplay a list of running models on the deployment server:\n\n        client.list_deployments(deploy_server_name)\n\nGet information about a deployed model by its name and the name of the deployment server:\n\n        client.get_deployment(deploy_server_name, deployment_name)\n\nUse the predict method of the model in the Docker container:\n\n      data = {\"inputs\":[[0.045341,  0.050680,  0.060618,  0.031065,  0.028702, -0.047347, -0.054446, 0.071210,  0.133597, 0.135612],[0.075341,  0.010680,  0.030618,  0.011065,  0.098702, -0.007347, -0.014446, 0.071210,  0.093597, 0.115612]]}\n   client.predict(deploy_server_name, deployment_name, data)\n\nCreating a deployment available via public DNS\n\nSet a username and password.\n\n\n    It is recommended to set details different from VK Cloud and JupyterHub.\n\n\n        auth_value = \"user:PasswordDA@dvv//!123$\"\n    auth_deployment_name = \"test_deploy_auth\"\n    client.create_deployment(deploy_server_name, auth_deployment_name, model_source_uri, auth=auth_value)\n\nGet information about the deployed model:\n\n        deployment_info = client.get_deployment(deploy_server_name, auth_deployment_name)\n    print(deployment_info)\n\nGet a DNS name to access the model:\n\n        print(deployment_info['model_ref'])\n\nCreate a request with authorization data (an example is given below):\n\n        import requests\n    data = {\"inputs\":[[0.045341, 0.050680, 0.060618, 0.031065, 0.028702, -0.047347, -0.054446, 0.071210, 0.133597, 0.135612],[0.075341, 0.0 10680, 0.030618, 0.011065, 0.098702, -0.007347, -0.014446, 0.071210 , 0.093597, 0.115612]]}\n    response = requests.post('https://ml-platform-3bb5b04ebb82c0.ml.msk.vkcs.cloud/deploy/0e84f86c-b9f0-4102-861d-222c41a81452/test_deploy_auth/invocations', json=data, auth=(\" user\", \"PasswordDA@dvv//!123$\"))\n    print(response.text)\n\nUse the predict method of the model in the Docker container:\n\n        client.predict(deploy_server_name, auth_deployment_name, data)\n\nDeleting a deployment\n\nDelete the deployment from the server using the names of the server and the deployment:\n\nclient.delete_deployment(deploy_server_name, deployment_name)\n\nDeleting a deployment server\n\nclient.delete_endpoint(deploy_server_name)\n\nBrief list of methods\n\nfrom mlflow.deployments import get_deploy_client\nclient = get_deploy_client('vk-cloud-mlpatform')\n\nendpoint — the VM configured as deployment server\nclient.create_endpoint(name, performance=\"low\", disk_size=50, disk_type=\"ceph-ssd\", av_zone=None)\nclient.list_endpoints()\nclient.get_endpoint(server_name)\n\ndeployment — the running Docker container with a model on the deployment server\nclient.create_deployment(server_name, deployment_name, model_uri, port_out = None)\n\nport_out — can be omitted, the first free port in the range from 62000 to 65000 will be selected\nclient.list_deployments(server_name)\nclient.get_deployment(server_name, deployment_name)\n\ncall of the predict method of the model in the Docker container\nclient.predict(server_name, deployment_name, df_payload)\n\nclient.delete_deployment(server_name, deployment_name)\nclient.delete_endpoint(server_name)\n","url":"/en/ml/mlplatform/deploymlflow/service-management/manage-mlflow-client","description":"How to manage ML-model deploy via MLflow Client.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"b53f453d-3838-4ea5-9b7b-b2e576b5ae86","title":"Removing an instance","data":"This is a group operation: you can delete several instances at once by selecting them using the checkboxes.\n\nGo to your personal account VK Cloud.\nGo to ML Platform → Instances.\nDelete the instance in one of the following ways:\n\n    Click   for the instance and select Delete.\n    Select the instance using the checkbox, then click the Delete button above the table.\nConfirm the deletion.\n\nYou cal also delete the instance using MLflow Deployment Client.\n","url":"/en/ml/mlplatform/deploymlflow/service-management/delete","description":"How to remove an MLflow Deploy instance.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"99c158d7-f53c-4aa0-9b2a-0db6848a5830","title":"FAQ","data":"\n\n\nContact support\n\nWhen contacting support, be sure to include your Cloud ML Platform instance ID. The instance ID can be found under ML Platform -> Instances -> Instance Name.\n\nClicking on the instance name will open the instance information page. On the \"ID\" line, you can copy the instance ID by clicking the \"Copy\" icon.\n\n\n\n\n\nActions to take when a JupyterHub instance is unavailable\n\nIn case the JupyterHub interface stops responding, you should reboot the JupyterHub VM instance.\n\nTo restart the VM\n\nGo to the Cloud Computing -> Virtual Machines section.\nClick   for the JupyterHub VM instance and select Restart.\n\nYou can learn more about managing VM instances in \"Starting, Stopping, and Restarting a VM\".\n\n\n\n\n\nFix issues with Jupyter Kernel\n\nThe Jupyter Kernel needs to be restarted after installing the libraries or if it hangs. To restart the Kernel, in the JupyterHub interface, select the menu item \"Kernel\" -> \"Restart Kernel\".\n\nYou can learn more about connecting to the JupyterHub interface in the article “Connecting to JupyterHub”.\n\n\n","url":"/en/ml/mlplatform/faq","description":"Answers to the most frequently asked questions about Cloud ML Platform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud ML Platform","link":"/en/ml/mlplatform","isActive":false,"evenSimpler":true}]},{"uuid":"e620c5fa-a0c8-4d9c-a981-9734cd36830c","title":"About the service","data":"Apache Spark is a core for processing big data. It consists of APIs in Java, Scala, Python and R, as well as processing tools Spark SQL for SQL, pandas API, MLlib for machine learning, GraphX for processing graphs and Structured Streaming for streaming processing. Spark is most often used as part of a Hadoop cluster.\n\nCloud Spark — a solution based on Apache Spark Operator and PaaS Kubernetes from VK Cloud. It allows you to deploy Spark inside Kubernetes using an image from Docker Registry, without using a Hadoop cluster.\n\nWhat tasks is the service suitable for?\n\nDistributed processing of big data.\nReading data from S3 with their further export to the database for processing (ClickHouse / Greenplum / PostgreSQL). It is also possible to transfer data from the database to S3.\nDistributed training of ML models using big data.\nGraph calculations using the GraphX component.\n\nService features\n\nDeployment of the Spark cluster inside Kubernetes.\nAutomatic configuration of the master nodes of the deployed clusters.\nConnecting a pre-assembled Spark image via Docker Registry.\nHorizontal and vertical cluster scaling with autoscaling support.\nAccess differentiation using tokens and a role model.\nAuto-delete or put the cluster into sleep mode according to a schedule.\nService Management using the API.\n\nInteraction of service components\n\n\n","url":"/en/ml/spark-to-k8s/concepts/about","description":"Overview of Spark service capabilities in Kubernetes implemented on VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud Spark","link":"/en/ml/spark-to-k8s","isActive":false,"evenSimpler":true}]},{"uuid":"5c2421fe-fc39-4821-b770-a31c7ed7ca3b","title":"Creating a Spark cluster","data":"\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nSelect the project where you want to create a cluster.\nGo to ML Platform → Spark in k8s.\nGo to Clusters tab.\nClick the Add cluster or Add button.\nOn the «Create cluster» step:\n\n   Set general settings:\n\n      Cluster name: it can contain only Latin letters, numbers, and special characters ., -, _.\n\n      Availability zone: availability zone for the cluster.\n\n      Network: the network and subnet where the cluster nodes will be located. If the necessary networks and subnets are not in the list, create them.\n\n   Set the worker node settings in the Worker-node settings section:\n\n      Category of virtual machine: select the flavor of the VM. For more information, see Cloud Servers service overview.\n\n      Type of virtual machine: configuration template for worker nodes.\n\n        Templates with high-performance CPUs are available [on request to the support service] (/en/contacts). To use these templates, select the Show only high performance CPUs option.\n\n      Enable autoscaling: select this option so that the cluster automatically scales the number of worker nodes depending on the load.\n\n        Then set the minimum and maximum number of nodes within which scaling is allowed. Acceptable range of values: from 1 to 100.\n\n        This option is disabled by default. A cluster with autoscaling enabled will contain one worker node.\n\n   Set additional settings:\n\n      Registry selection: Docker registry, images from which will be used when running Spark jobs.\n\n        If the required registry is not in the list:\n\n        Choose Create new registry.\n\n           The registry will be placed on a dedicated virtual machine that is not part of the cluster and is charged separately.\n\n        Specify the details for accessing the registry:\n\n           Registry username: it can contain only Latin letters, numbers, and special characters ., -, _.\n           Registry password: you can come up with a password or generate it.\n\n\n             Save the password. If it is lost, it cannot be restored.\n\n\n             Password requirements:\n\n             it is allowed to use only uppercase and lowercase Latin letters, numbers, and special characters !, #, $, %, &, (, ), *, +, ,, ., :, ;, `, ?, @, [, ], ^, _, {, |, }, ~, -,   `, ;`.\n             the password must contain at least one letter of the Latin alphabet and at least one digit.\n\n      Virtual machine key: the key that is used for connecting to cluster nodes via SSH. Select an existing key or create a new one.\n\n\n        If you select No key, then SSH connection will not be possible.\n\n\n      Cluster mode: defines the configuration of the master node.\n\n        DEV: a virtual machine with 2 vCPUs and 4 GB RAM will be used for the master node.\n        PROD: a virtual machine with 6 vCPUs and 6 GB RAM will be used for the master node.\n\n   Select options that affect the lifecycle of an inactive cluster.\n\n      If there are no Spark jobs running in the cluster, it becomes inactive. The options listed below define the lifecycle of just such a cluster. The cluster returns to the active state when a new Spark job is started.\n\n      Available options:\n\n      Destroy after inactivity: when the specified time expires, the inactive cluster will be automatically deleted.\n\n        By default, the option is disabled, and the cluster exists until it is manually deleted. This option is useful in clusters used for one-time tasks.\n\n      Suspend cluster after inactivity: when the specified time expires, the inactive cluster will go into sleep mode. In this mode, only cluster node disks are charged, computing resources are not charged.\n\n        The cluster will be brought out of sleep mode when a new Spark job is started.\n\n        By default, the option is disabled, and the cluster runs until it is deleted, even if there are no Spark jobs running in it. This option is useful to save computing resources during long breaks between running Spark jobs.\n\n      Examples of the impact of options on the cluster lifecycle\n\n      Let only the inactivity time before destruction (120 minutes, 2 hours) be configured.\n\n        Then, if the cluster is inactive since 13:00, then at 15:00 it will be deleted.\n\n      Let's set only the time before switching to sleep mode (120 minutes, 2 hours).\n\n        Then, if the cluster is inactive since 13:00, then at 15:00 it will go into sleep mode.\n\n      Let's set the time before switching to sleep mode (60 minutes, 1 hour) and the time of inactivity before destruction (120 minutes, 2 hours).\n\n        Then, if the cluster is inactive since 13:00, then at 14:00 it will go into sleep mode, and at 15:00 it will be deleted.\n\n      Let's set the time before switching to sleep mode (120 minutes, 2 hours) and the time of inactivity before destruction (60 minutes, 1 hour).\n\n        Then, if the cluster is inactive since 13:00, then at 14:00 it will be deleted.\n\n\n   Click the Next step button.\n\nOn the «Spark settings» step:\n\n   (Optional) Set advanced Spark settings. These settings will be used by all Spark jobs that will run in the cluster.\n\n      You can configure:\n\n      Spark configuration: list of properties responsible for Spark configuration.\n      Environment variables: a list of environment variables for Spark.\n\n      Each property or variable should be placed on a separate line in the following format:\n\n            :\n\n      Property descriptions should be correct from the point of view of YAML syntax.\n\n      Examples of descriptions of properties and variables\n\n      A list of several Spark properties:\n\n                spark.eventLog.dir: s3a://spark-bucket\n        spark.eventLog.enabled: \"true\"\n        spark.hadoop.fs.s3a.endpoint: https://hb.vkcs.cloud\n        spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem\n\n      A list of several environment variables:\n\n                ENV_VAR_1: env_var_1_value\n        ENV_VAR_2: env_var_2_value\n        ENV_VAR_3: env_var_3_value\n\n\n   Click the Create cluster button.\n\n      Wait for the operation to complete. Creating a cluster can take a long time.\n\n\n\n","url":"/en/ml/spark-to-k8s/instructions/create","description":"Creating a new Cloud Spark cluster.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud Spark","link":"/en/ml/spark-to-k8s","isActive":false,"evenSimpler":true}]},{"uuid":"7ce53948-cc35-4fa6-9f96-639990c2b8f4","title":"Working with tokens","data":"To interact with Cloud Spark clusters, the Cloud ML Platform Python library is used. When working with the library, most actions require authorization using a token.\n\nThe library supports two types of tokens:\n\nAccess tokens (refresh tokens). They are used to interact with Spark clusters.\nRegistration tokens (register tokens). They are used to create access tokens using the library (without using your personal account).\n\nGetting a list of access tokens\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the tokens are located.\nGo to ML Platform → Tokens.\n\nA list of access tokens will be displayed.\n\n\n\n\n\n\nThere are no registration tokens in this list.\n\nCreating an access token\n\n\n\nPersonal account\nCloud ML Platform library\n\n\n\nGo to your VK Cloud personal account.\nSelect the project in which you want to create a token.\nGo to ML Platform → Tokens.\nClick the Create access token button.\nIn the window that opens, specify the token parameters:\n\n   Name: can be anything.\n   Role: one of the roles of the token.\n\n     User. This role gives the right to perform operations that require authorization when working with the Cloud ML Platform library (for example, send a task to the Spark cluster).\n     Administrator. This role gives the same rights as the User role and also provides additional rights to work with tokens.\n\nClick the Create button.\nIn the window that opens, copy the token value and save it on your device.\n\n\n   After closing the window, it will be impossible to restore the token value. If it is lost, create a new token.\n\n\nClick the Ready button.\n\n\n\n\n\n\nFor simplicity, the value of the registration token is contained directly in the Python script example, and the value of the access token is output using print().\n\nWhen working in a production environment, do not operate tokens in clear text. Use environment variables, secret stores, or other tools to handle sensitive data.\n\n\n\nPrepare the environment for working with Python in any convenient way, if you have not done it already:\n\n   Using VK Cloud\n   By yourself\n\n\n   Create a JupyterHub instance on the VK Cloud platform. It already contains configured Python 3.x and pip, which you can work with from JupyterHub notebook.\n\n\n   Install Python 3.x and pip.\n   If necessary, set up a virtual environment for Python.\n\n   For example, you can use conda or perform the installation and configuration manually.\n\n\nInstall the Cloud ML Platform library for Python if you have not done it already.\n\n   JupyterHub\n   pip\n\n\n   Connect to your JupyterHub instance.\n   In the JupyterHub notebook, create and execute a cell with the following content:\n\n            %pip install https://mlplatform.hb.ru-msk.vkcs.cloud/mlplatform_client.tar.gz\n\n\n   Run the command:\n\n      pip install https://mlplatform.hb.ru-msk.vkcs.cloud/mlplatform_client.tar.gz\n\n\n   The current version of the library is available at the link provided.\n\nAsk the owner of the access token with the role Administrator to create a registration token for you.\nCreate a new access token using the registration token by executing the Python script:\n\n      from mlplatform_client import MLPlatform\n\n   REGISTER_TOKEN = \"\"\n\n   mlp = MLPlatform()\n   refresh_token = mlp.create_refresh_token(REGISTER_TOKEN)\n\n   print(refresh_token)\n\n   After executing this script, the value of the created access token will be output.\n\n   The parameters of the access token (for example, its name, role, and lifetime) are determined by the registration token.\n\nCopy the token value and save it on your device.\n\nCreating a registration token\n\n\n\nCloud ML Platform library\n\n\n\n\n\nFor simplicity, the value of the access token is contained directly in the Python script example, and the value of the registration token is output using print().\n\nWhen working in a production environment, do not operate tokens in clear text. Use environment variables, secret stores, or other tools to handle sensitive data.\n\n\n\nPrepare the environment for working with Python in any convenient way, if you have not done it already:\n\n   Using VK Cloud\n   By yourself\n\n\n   Create a JupyterHub instance on the VK Cloud platform. It already contains configured Python 3.x and pip, which you can work with from JupyterHub notebook.\n\n\n   Install Python 3.x and pip.\n   If necessary, set up a virtual environment for Python.\n\n   For example, you can use conda or perform the installation and configuration manually.\n\n\nInstall the Cloud ML Platform library for Python if you have not done it already.\n\n   JupyterHub\n   pip\n\n\n   Connect to your JupyterHub instance.\n   In the JupyterHub notebook, create and execute a cell with the following content:\n\n            %pip install https://mlplatform.hb.ru-msk.vkcs.cloud/mlplatform_client.tar.gz\n\n\n   Run the command:\n\n      pip install https://mlplatform.hb.ru-msk.vkcs.cloud/mlplatform_client.tar.gz\n\n\n   The current version of the library is available at the link provided.\n\nCreate an access token with the Administrator role if you don't have one yet.\nCreate a registration token by executing a Python script:\n\n      from mlplatform_client import MLPlatform\n   from mlplatform_client.serializers.auth import MLPTokenType\n\n   ADMIN_REFRESH_TOKEN = \"\"\n\n   mlp = MLPlatform(ADMIN_REFRESH_TOKEN)\n\n   register_token = mlp.create_register_token(\n       client_name=\"\",\n       access_ttl=\"\",\n       refresh_ttl=\"\",\n       token_type=)\n\n   print(register_token)\n\n   Here:\n\n   client_name: the name of the access token that will be created using this registration token.\n   access_ttl: the lifetime of the registration token (in the format 2h45m30s). The expired registration token cannot be used to create an access token.\n   refresh_ttl: the lifetime of the access token (in the 2h format). If you do not specify this parameter, the access token will be valid indefinitely.\n   token_type: the role of the access token:\n\n     The value MLPTokenType.USER corresponds to the role User. This role gives the right to perform operations that require authorization when working with the Cloud ML Platform library (for example, to send a task to the Spark cluster).\n     The value of MLPTokenType.ADMIN corresponds to the role of Administrator. This role gives the same rights as the User role and also provides additional rights to work with tokens.\n\n   After executing this script, the value of the created registration token will be output.\n\nProvide this registration token to another user so that he or she can use this token to create an access token with the specified parameters.\n\n\n\n\n\n\nA list of created registration tokens is not maintained. You can only get a list of access tokens.\n\nDeleting an access token\n\n\n\nPersonal account\nCloud ML Platform library\n\n\n\nGo to your VK Cloud personal account.\nSelect the project where the required token is located.\nGo to ML Platform → Tokens.\nClick   for the required token and select Delete.\nClick the Confirm button.\n\n\n\n\n\n\nFor simplicity, the value of the access token is contained directly in the Python script example.\n\nWhen working in a production environment, do not operate tokens in clear text. Use environment variables, secret stores, or other tools to handle sensitive data.\n\n\n\nPrepare the environment for working with Python in any convenient way, if you have not done it already:\n\n   Using VK Cloud\n   By yourself\n\n\n   Create a JupyterHub instance on the VK Cloud platform. It already contains configured Python 3.x and pip, which you can work with from JupyterHub notebook.\n\n\n   Install Python 3.x and pip.\n   If necessary, set up a virtual environment for Python.\n\n   For example, you can use conda or perform the installation and configuration manually.\n\n\nInstall the Cloud ML Platform library for Python if you have not done it already.\n\n   JupyterHub\n   pip\n\n\n   Connect to your JupyterHub instance.\n   In the JupyterHub notebook, create and execute a cell with the following content:\n\n            %pip install https://mlplatform.hb.ru-msk.vkcs.cloud/mlplatform_client.tar.gz\n\n\n   Run the command:\n\n      pip install https://mlplatform.hb.ru-msk.vkcs.cloud/mlplatform_client.tar.gz\n\n\n   The current version of the library is available at the link provided.\n\nCreate an access token with the Administrator role if you don't have one yet.\n\nGet a list of access tokens and determine the name of the token to delete.\n\nDelete the access token by executing the Python script:\n\n      from mlplatform_client import MLPlatform\n\n   ADMIN_REFRESH_TOKEN = \"\"\n   REFRESH_TOKEN_NAME = \"\"\n\n   mlp = MLPlatform(ADMIN_REFRESH_TOKEN)\n   mlp.delete_token(token_name=REFRESH_TOKEN_NAME)\n\n   print(\"Token\", REFRESH_TOKEN_NAME, \"was deleted.\")\n\n   After executing this script, a message about deleting the access token will be displayed.\n\n\n\n","url":"/en/ml/spark-to-k8s/instructions/tokens","description":"How to create and manage various tokens.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud Spark","link":"/en/ml/spark-to-k8s","isActive":false,"evenSimpler":true}]},{"uuid":"edd4b89b-9592-4def-a7f0-4af3e2e87fba","title":"Basic working with Spark jobs","data":"Spark jobs can be submitted to the cluster in different ways:\n\nFor Spark applications that do not have dependencies, it is sufficient to pass the application code in the job manifest. This approach will be demonstrated below.\nFor Spark applications that require additional artifacts for their operation, you must manually add the required artifacts to the Cloud Storage bucket and edit the job manifest. This approach is illustrated using the example of working with ClickHouse.\n\nAs an example, an application will be used to calculate an approximate value of the number π.\n\nPreparatory steps\n\nPrepare the environment for working with Python in any convenient way:\n\n   Using VK Cloud\n   By yourself\n\n\n   Create a JupyterHub instance on the VK Cloud platform. It already contains configured Python 3.x and pip, which you can work with from JupyterHub notebook.\n\n\n   Install Python 3.x and pip.\n   If necessary, set up a virtual environment for Python.\n\n   For example, you can use conda or perform the installation and configuration manually.\n\n\nInstall the Cloud ML Platform library for Python:\n\n   JupyterHub\n   pip\n\n\n   Connect to the JupyterHub instance.\n   In the JupyterHub notebook, create and execute a cell with the following contents:\n\n            %pip install https://mlplatform.hb.ru-msk.vkcs.cloud/mlplatform_client.tar.gz\n\n\n   Execute the command:\n\n      pip install https://mlplatform.hb.ru-msk.vkcs.cloud/mlplatform_client.tar.gz\n\n\n   The up-to-date version of the library is always available at the link provided.\n\nCreate an access token, which is needed to work with the library.\n\n   A token with both the Administrator role and the User role is suitable.\n\n\n   For simplicity, the token value is specified in the Python script examples in plain text.\n\n   When working in a production environment, do not place the token in scripts in plain text. Use environment variables, vaults for secrets, or other tools to work with sensitive data.\n\n\nCreate a Spark cluster.\n\n   Choose the cluster parameters at your discretion.\n\nRun the script to get information about Spark clusters in the project:\n\n      from mlplatform_client import MLPlatform\n\n   REFRESH_TOKEN = \"\"\n\n   mlp = MLPlatform(REFRESH_TOKEN)\n   print(mlp.get_clusters())\n\n   Detailed information about clusters will be displayed.\n\nFind and write down the ID of the created cluster (contained in the id field).\n\n1. Create a file with the Spark application code\n\nThis application calculates a Monte Carlo approximation of the number π by distributing the computation across the nodes of the Spark cluster.\n\n\ncalculate-pi.py\n\nimport sys\nfrom random import random\nfrom operator import add\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"PythonPi\") \\\n    .getOrCreate()\n\npartitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\nn = 100000 * partitions\n\ndef f(_: int) -> float:\n    x = random() * 2 - 1\n    y = random() * 2 - 1\n    return 1 if x * 2 + y * 2\n\n2. Send the Spark job to the cluster\n\nSubmit a job to the cluster by executing the script:\n\nfrom mlplatform_client import MLPlatform\n\nREFRESH_TOKEN = \"\"\nCLUSTER_ID = \"\"\n\nPY_FILE = \"calculate-pi.py\"\nJOB_NAME = \"pi-spark-job\"\n\nmlp = MLPlatform(REFRESH_TOKEN)\n\nspark_job_manifest = mlp.get_default_manifest(CLUSTER_ID, JOB_NAME)\nspark_job_info = mlp.spark_submit_job(CLUSTER_ID, spark_job_manifest, PY_FILE)\n\nprint(spark_job_info)\n\nInformation about the submitted task will be displayed, for example:\n\nJob: pi-spark-job, status: SUBMITTED, created_at: ...\n\nBy default, the Spark manifest job does not contain a location of the files needed to run the Spark application.\n\nThe application that calculates the number π requires only one executable file, no additional artifacts are used.\n\nIn this simple case, you don't need to host the Spark application executable in an Object Storage bucket and then edit the default manifest to add the required information to it.\n\nIt is enough to pass the name of the executable file when sending a job to the cluster:\n\nspark_job_info = mlp.spark_submit_job(CLUSTER_ID, spark_job_manifest, PY_FILE)\n\nThe Cloud ML Platform library itself will adjust the manifest so that the code from the specified file can be executed.\n\n3. Track the status of the Spark job\n\nMake sure that the result of calculating the number π appears in the task logs. If the result does not appear, run the script to obtain the logs again: intermediate logs of the work can be displayed at the time when the task has not completed yet.\n\n      from mlplatform_client import MLPlatform\n\n   REFRESH_TOKEN = \"\"\n   CLUSTER_ID = \"\"\n   JOB_NAME = \"pi-spark-job\"\n\n   mlp = MLPlatform(REFRESH_TOKEN)\n\n   logs = mlp.spark_job_logs(CLUSTER_ID, JOB_NAME)\n   print(logs)\n\n   Example of partial output when the job completes successfully\n\n      Pi is roughly 3.146360\n\n\n(Optional) Get information about events in the cluster. Such information allows you to find out the current status of the cluster and jobs, for example, when investigating issues.\n\n      from mlplatform_client import MLPlatform\n\n   REFRESH_TOKEN = \"\"\n   CLUSTER_ID = \"\"\n\n   mlp = MLPlatform(REFRESH_TOKEN)\n\n   events = mlp.spark_events(CLUSTER_ID)\n   print(events)\n\nDelete unused resources\n\nIf you no longer need the created resources, delete them:\n\nDelete the Spark cluster.\nDelete the Docker registry for this Spark cluster.\nDelete the access token.\n","url":"/en/ml/spark-to-k8s/use-cases/submit-basic-job-pi","description":"How to create a task with a standard manifest on the example of calculating the number Pi.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud Spark","link":"/en/ml/spark-to-k8s","isActive":false,"evenSimpler":true}]},{"uuid":"a4fd9e6a-b547-4293-85d7-2e979f4d6693","title":"Advanced working with Spark jobs","data":"Spark jobs can be submitted to the cluster in a variety of ways:\n\nFor Spark applications that have no dependencies, it is sufficient to pass the application code in the job manifest. This approach is shown in the example of calculating the number π.\nFor Spark applications that require additional artifacts for their operation, you must manually add the required artifacts to the Cloud Storage bucket and edit the job manifest. This approach will be shown below.\n\nAs an example, an application that executes an SQL query against a ClickHouse deployed as a Cloud Databases instance will be used.\n\nPreparatory steps\n\nPrepare the environment for working with Python in any convenient way:\n\n   Using VK Cloud\n   By yourself\n\n\n   Create a JupyterHub instance in your VK Cloud project. It already contains the configured Python 3.x and pip, which you can work with from the JupyterHub notebook.\n\n\n   Install Python 3.x and pip.\n   If necessary, set up a virtual environment for Python.\n\n   For example, you can use conda or perform these steps manually.\n\n\nInstall the Cloud ML Platform library for Python.\n\n   JupyterHub\n   pip\n\n\n   Connect to the JupyterHub instance.\n   In the JupyterHub notebook, create and execute a cell with the following contents:\n\n            %pip install https://mlplatform.hb.ru-msk.vkcs.cloud/mlplatform_client.tar.gz\n\n\n   Execute the command:\n\n      pip install https://mlplatform.hb.ru-msk.vkcs.cloud/mlplatform_client.tar.gz\n\n\n   The up-to-date version of the library is always available at the link provided.\n\nCreate an access token with the Administrator role. This token is required to work with the library.\n\n\n   For simplicity, the token value is specified in the Python script examples in plain text.\n\n   When working in a production environment, do not place the token in scripts in plain text. Use environment variables, vaults for secrets, or other tools to work with sensitive data.\n\n\nCreate a Cloud Spark cluster.\n\n   Select the cluster parameters at your discretion.\n\nRun the script to get information about Spark clusters in the project:\n\n      from mlplatform_client import MLPlatform\n\n   REFRESH_TOKEN = ''\n\n   mlp = MLPlatform(REFRESH_TOKEN)\n   print(mlp.get_clusters())\n\n   Details about the clusters will be displayed.\n\nFind and write down the ID of the created cluster (contained in the id field).\n\nCreate a ClickHouse instance that:\n\n   uses the newest version available;\n   uses the Single configuration;\n   has an external IP address and is accessible from the Internet;\n   has a configured user user_spark;\n   is configured to work with the new db_spark database.\n\n   Make a note of the user password, you will need it later.\n\nFind out the IP address assigned to the created ClickHouse instance:\n\n   Go to your VK Cloud account.\n   Select the project where the database instance is located.\n   Go to Databases → Database instances.\n   Click the name of the instance and then click the Information tab.\n   Make a note of the external IP address of the instance.\n\n1. Create a file with the Spark application code\n\nCreate a query-clickhouse.py file with the following contents:\n\nimport os\nfrom pyspark.sql import SparkSession\n\nCLICKHOUSE_IP = os.environ.get('CH_IP')\nCLICKHOUSE_PORT = 8123\nCLICKHOUSE_DB = 'db_spark'\nCLICKHOUSE_USER = os.environ.get('CH_USER')\nCLICKHOUSE_USER_PASSWORD = os.environ.get('CH_PASSWORD')\nCLICKHOUSE_JDBC_DRIVER = os.environ.get('CH_DRIVER_JAR')\n\nspark = SparkSession \\\n    .builder \\\n    .appName('PythonSparkClickHouse') \\\n    .config('spark.driver.extraClassPath', f'./{CLICKHOUSE_JDBC_DRIVER}') \\\n    .getOrCreate()\n\nprint(\"Reading ClickHouse table\")\n\ndata_frame = spark.read.format('jdbc') \\\n    .option('url', f'jdbc:clickhouse://{CLICKHOUSE_IP}:{CLICKHOUSE_PORT}/{CLICKHOUSE_DB}') \\\n    .option('user', CLICKHOUSE_USER) \\\n    .option('password', CLICKHOUSE_USER_PASSWORD) \\\n    .option('query', 'SELECT * FROM system.build_options') \\\n    .option('driver', 'com.clickhouse.jdbc.ClickHouseDriver') \\\n    .load()\n\ndata_frame.show();\nprint(\"Finished reading ClickHouse table\")\n\nspark.stop()\n\nThis is the code for a Spark application that connects to a ClickHouse instance and fetches ClickHouse build information using an SQL query.\n\nTo simplify the use of this application, some connection parameters are set using environment variables:\n\nThe connection to ClickHouse is made to the CH_IP IP address using a driver that is stored in the CH_DRIVER_JAR JAR file. The driver file will be uploaded later.\nAuthentication in ClickHouse is performed using the CH_USER and CH_PASSWORD credentials. This approach helps to avoid placing sensitive data directly in the code.\n\nAll mentioned environment variables will be set up later when submitting a Spark job to the cluster.\n\n2. Upload the necessary files to the bucket\n\nFind out the name of the Cloud Storage bucket used by the cluster:\n\n   Run the script to get information about Spark clusters in the project:\n\n            from mlplatform_client import MLPlatform\n\n      REFRESH_TOKEN = ''\n\n      mlp = MLPlatform(REFRESH_TOKEN)\n      print(mlp.get_clusters())\n\n      Details about the clusters will be displayed.\n\n   Find out the information you need:\n\n      cluster identifier (contained in the id field);\n      Cloud Storage bucket name (contained in the s3_bucket_name field).\n\nUpload files to the spark-files directory of this bucket:\n\n   the query-clickhouse.py file with the Spark application code;\n   the clickhouse-jdbc-0.5.0-shaded.jar file with the JDBC driver for ClickHouse.\n\n     The link contains a shaded version of the JAR file, which includes all the necessary dependencies for the driver. This makes it easier to use the driver with the Spark application and to work with JAR files when submitting a Spark job.\n\n     If necessary, download another version of the driver from the Maven repository. In this case, in the script below, replace the driver version with the correct one.\n\n\n   When uploading files, do not change the default ACL selected (private).\n\n\nWhen submitting a Spark job, the paths to the files uploaded to the bucket will be specified. The Cloud Spark cluster already has access to objects from this bucket, no additional configuration is needed.\n\n3. Send the Spark job to the cluster\n\nCreate a clickhouse-secret.yaml file with the following contents:\n\n      apiVersion: v1\n   kind: Secret\n   metadata:\n     name: clickhouse-secret\n   stringData:\n     CH_USER: user_spark\n     CH_PASSWORD:\n\n   This file describes a Kubernetes secret that stores user credentials for connecting to a ClickHouse instance.\n\nSubmit the job to the cluster by executing the script:\n\n      from mlplatform_client import MLPlatform\n\n   REFRESH_TOKEN = ''\n   CLUSTER_ID = ''\n   BUCKET_NAME = ''\n   CLICKHOUSE_IP = ''\n\n   CLICKHOUSE_JDBC_DRIVER = 'clickhouse-jdbc-0.5.0-shaded.jar'\n   JOB_NAME = 'clickhouse-spark-job'\n\n   mlp = MLPlatform(refresh_token=REFRESH_TOKEN)\n\n   spark_job_manifest = mlp.get_default_manifest(cluster_id=CLUSTER_ID, job_name=JOB_NAME)\n\n   spark_job_manifest.main_app_file = f's3a://{BUCKET_NAME}/spark-files/query-clickhouse.py'\n   spark_job_manifest.add_jars([f's3a://{BUCKET_NAME}/spark-files/{CLICKHOUSE_JDBC_DRIVER}'])\n\n   spark_job_manifest.add_driver_env(\n       [\n           {'name': 'CH_IP', 'value': CLICKHOUSE_IP},\n           {'name': 'CH_DRIVER_JAR', 'value': CLICKHOUSE_JDBC_DRIVER}\n       ])\n\n   spark_job_manifest.add_executor_env(\n       [\n           {'name': 'CH_IP', 'value': CLICKHOUSE_IP},\n           {'name': 'CH_DRIVER_JAR', 'value': CLICKHOUSE_JDBC_DRIVER}\n       ])\n\n   mlp.create_secret_from_yaml(cluster_id=CLUSTER_ID, secret_yaml_path='clickhouse-secret.yaml')\n   spark_job_manifest.add_driver_env_from([{\"secretRef\": {\"name\": \"clickhouse-secret\"}}])\n   spark_job_manifest.add_executor_env_from([{\"secretRef\": {\"name\": \"clickhouse-secret\"}}])\n\n   spark_job_info = mlp.spark_submit_job(cluster_id=CLUSTER_ID, manifest=spark_job_manifest)\n   print(spark_job_info)\n\n   Information about the submitted task will be displayed, for example:\n\n      Job: clickhouse-spark-job, status: SUBMITTED, created_at: ...\n\n   The files required to run the Spark application were previously uploaded into the bucket used by the cluster. The default manifest for a Spark job does not contain information about where the required files are located. Therefore, the missing information has been added to the manifest:\n\n   The path to the Spark application executable file in spark_job_manifest.main_app_file;\n   A list of paths to the JAR files needed by the application (in this particular case, only one JDBC driver file is needed). The spark_job_manifest.add_jars() function is used to add the paths to these files.\n\n   Also, the values of required environment variables are added to the manifest. The variables CH_USER and CH_PASSWORD are used for authentication in ClickHouse, so their values are extracted from the Kubernetes clickhouse-secret secret. This secret was previously created from the clickhouse-secret.yaml file.\n\n4. Track the status of the Spark job\n\nMake sure that the result of executing the SQL query to ClickHouse appears in the job logs. If the result does not appear, run the script to retrieve the logs again: intermediate job logs may be displayed at the time when the job has not yet completed.\n\n      from mlplatform_client import MLPlatform\n\n   REFRESH_TOKEN = ''\n   CLUSTER_ID = ''\n   JOB_NAME = 'clickhouse-spark-job'\n\n   mlp = MLPlatform(REFRESH_TOKEN)\n\n   logs = mlp.spark_job_logs(CLUSTER_ID, JOB_NAME)\n   print(logs)\n\n   Example of partial output when the job completes successfully\n\n      ...\n   Reading ClickHouse table\n   ...\n\n   +--------------------+--------------------+\n   |                name|               value|\n   +--------------------+--------------------+\n   |        VERSION_FULL| ClickHouse 22.8.4.7|\n   |    VERSION_DESCRIBE|    v22.8.4.7-stable|\n   |     VERSION_INTEGER|            22008004|\n   |              SYSTEM|               Linux|\n   |     VERSION_GITHASH|baad27bcd2f8f9406...|\n   |    VERSION_REVISION|               54465|\n   |        VERSION_DATE|                    |\n   |          BUILD_TYPE|      RelWithDebInfo|\n   |    SYSTEM_PROCESSOR|              x86_64|\n   |       CMAKE_VERSION|              3.16.3|\n   |          C_COMPILER|   /usr/bin/clang-14|\n   |  C_COMPILER_VERSION|              14.0.6|\n   |        CXX_COMPILER| /usr/bin/clang++-14|\n   |CXX_COMPILER_VERSION|              14.0.6|\n   |             C_FLAGS| --gcc-toolchain=...|\n   |           CXX_FLAGS| --gcc-toolchain=...|\n   |          LINK_FLAGS| --gcc-toolchain=...|\n   |BUILD_COMPILE_DEF...|HAS_RESERVED_IDEN...|\n   |              STATIC|                  ON|\n   |USE_EMBEDDED_COMP...|                   1|\n   +--------------------+--------------------+\n   only showing top 20 rows\n\n   Finished reading ClickHouse table\n   ...\n\n\n(Optional) Get information about events in the cluster. Such information allows you to find out the current status of the cluster and jobs, for example, when investigating issues.\n\n      from mlplatform_client import MLPlatform\n\n   REFRESH_TOKEN = ''\n   CLUSTER_ID = ''\n\n   mlp = MLPlatform(REFRESH_TOKEN)\n\n   events = mlp.spark_events(CLUSTER_ID)\n   print(events)\n\nDelete unused resources\n\nIf you no longer need the created resources, delete them:\n\nDelete the Spark cluster.\nDelete the Docker registry for this Spark cluster.\nDelete objects from the bucket and the bucket itself that was used by this cluster.\nDelete the access token.\nDelete the ClickHouse instance.\n","url":"/en/ml/spark-to-k8s/use-cases/submit-advanced-job-clickhouse","description":"How to create a job with a non-standard manifest on the example of working with ClickHouse.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud Spark","link":"/en/ml/spark-to-k8s","isActive":false,"evenSimpler":true}]},{"uuid":"24d1bc66-ccad-4328-9a0c-838b402d43fa","title":"About service","data":"Cloud Voice is a service that allows you to recognize and synthesize speech. With it, you can voice the content on the site, analyze customer calls, or add voice control to the application.\n\n\n\nTo start using the service, go to the [Cloud Voice] page(https://msk.cloud.vk.com/app/services/machinelearning/voice/methods/). You must be logged in to do this.\n\nTo use the service, you need to get an access token. To learn how to get an access token, read here.\n\nSee also\n\nGetting an access token\n\nSpeech recognition\n\nSpeech synthesis\n","url":"/en/ml/cloud-voice/concepts/about","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud Voice","link":"/en/ml/cloud-voice","isActive":false,"evenSimpler":true}]},{"uuid":"f11242ce-ab53-4ea9-9700-f500a0b4b590","title":"Getting an access token","data":"The token is issued for:\n\nrecognition of audio files;\nrecognition of streaming audio;\nspeech synthesis.\n\nThere are two ways to get a token:\n\nOAuth is the preferred scenario as it implements a refresh token mechanism.\n\nAn example of a request to generate a token:\n\ncurl -X POST --location 'https://mcs.mail.ru/auth/oauth/v1/token' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"client_id\":\"mcs5719255887.voice.6Wg9D18XPZN28DWy4wEba\",\n\"client_secret\": \"1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ\",\n\"grant_type\":\"client_credentials\"\n}'\n\nAnswer example:\n\n{\n  \"refresh_token\": \"ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\",\n  \"access_token\": \"0987654321ABCDEFGHIJKLMNOPQRSTUVWXYZ\",\n  \"expired_in\": \"3600\",\n  \"scope\": {\n    \"tts\": 1,\n    \"asr_short\": 1,\n    \"asr_stream\": 1\n  }\n}\n\nIn response from the server, you will receive 2 tokens:\n\naccess_token** — access token (becomes invalid in an hour);\nrefresh_token** — token for refreshing the access token.\n\nExample of a token refresh request:\n\ncurl -X POST --location 'https://mcs.mail.ru/auth/oauth/v1/token' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"client_id\":\"mcs5719255887.voice.6Wg9D18XPZN28DWy4wEba\",\n\"refresh_token\":\"ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\",\n\"grant_type\":\"refresh_token\"\n}'\n\nAnswer example:\n\n{\n  \"refresh_token\": \"ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\",\n  \"access_token\": \"23wcTiUeaa334Uv6TH3StRGiZBMCyyo1Hzgqqa3n\",\n  \"expired_in\": \"3600\",\n  \"scope\": {\n    \"tts\": 1,\n    \"asr_short\": 1,\n    \"asr_stream\": 1\n  }\n}\n\nA project can have up to 25 active refresh_token at a time, each of them can have 25 access_token associated at any given time.\n\n\n\nThe received refresh_tokens need to be saved so that they can be used in the future.\n\n\n\nService Token is a less preferred scenario. The token is issued once and can work without time limit. The only risk of using such a token is that when the token is selected or compromised, an attacker may be able to use the service in very large volumes, which will entail unexpected expenses. When the funds on the personal account are exhausted, the service will be suspended.\n\nTo get a service token, click Machine Learning → Voice API → Add Service Token.\n\nSelect the required checkboxes and click \"Create\".\n\nThe received access token must be sent in all HTTP requests in the Bearer Token format.\n\nSee also\n\nSpeech recognition\n\nSpeech synthesis\n","url":"/en/ml/cloud-voice/service-management/get-voice-token","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud Voice","link":"/en/ml/cloud-voice","isActive":false,"evenSimpler":true}]},{"uuid":"b7004cb9-462c-40cf-a471-1163246ec17c","title":"Speech recognition","data":"There are two types of speech recognition available:\n\nrecognition of audio files;\nrecognition of streaming audio.\n\nRecognize audio files\n\nTo recognize speech from an audio file, send the audio file in the body of the POST request to https://voice.mcs.mail.ru/asr, specifying the correct Content -Type in header.\n\nRequest example:\n\ncurl -L --request POST 'https://voice.mcs.mail.ru/asr'\n--header 'Content-Type: audio/ogg; codecs=opus'\n--header 'Authorization: Bearer xxxxxxxxxx'\n--data-binary '@/Users/User/tts.ogg'\n\nAnswer example:\n\n{\n\t\"qid\": \"0ac6294a351d42ad859404ecd349e4b9\",\n\t\"result\": {\n\t\t\"texts\": [\n\t\t\t{\n\t\t\t\t\"text\": \"hello alice\",\n\t\t\t\t\"confidence\": 1.0,\n\t\t\t\t\"punctuated_text\": \"Hello, Alice.\"\n\t\t\t}\n\t\t],\n\t\t\"phrase_id\": \"20220921-1515-4d75-92b4-24b6c101ba6a\"\n\t}\n}\n\nSupported audio formats\n\n| container | Codec | content-type          |\n| --------- | ----- | --------------------- |\n| WAV       | —     | audio/wave            |\n| ogg       | Opus  | audio/ogg codecs=opus |\n\nRestrictions\n\n| Restriction                | Meaning |\n| -------------------------- | ------- |\n| Maximum audio file size    | 20 Mb   |\n| Maximum audio duration     | 5 min   |\n| Maximum number of channels | 1       |\n\nError codes\n\n| Code | Status | Description                                   |\n| ---- | ------ | --------------------------------------------- |\n| 4009 | 400    | Audio size too big                            |\n| 4033 | 400    | Unknown audio format                          |\n| 4034 | 400    | Audio is corrupted or in an unexpected format |\n| 4043 | 400    | Too long audio                                |\n| 4044 | 400    | Unsupported number of audio channels          |\n| 4045 | 400    | Unsupported audio sample rate                 |\n| 4048 | 400    | Invalid token                                 |\n| 4049 | 400    | Inactive project VK Cloud                        |\n\nRecognize streaming audio\n\nTo recognize a chunk (small piece of speech), you need to send a request to create a task. After that, it will be possible to send chunks and receive the final result.\n\nRequest to create a task\n\nIn order to create a task, it is enough to send a POST request to https://voice.mcs.mail.ru/asr_stream/create_task with an authorization header with access_token, the response will be task_id, task_token.\n\nRequest example:\n\ncurl --request POST \\\n  --url https://voice.mcs.mail.ru/asr_stream/create_task\n  --header 'Authorization: Bearer access_tokenxxxxxxxx'\n\nAnswer example:\n\n{\n  \"qid\": \"61b5cf067c494b4a9a0b87a3c43e37ef\",\n  \"result\": {\n    \"task_id\": \"05ad987e-ceee-4186-acdb-956148b91692\",\n    \"task_token\": \"040b2fcfc3d9b9806b691070e873125dfc0450a8251887ba91b19be08eb3951c\"\n  }\n}\n\nRequest to send a chunk\n\nA chunk is an audio fragment of the selected format, respectively, headers must be present in each chunk.\n\nTo send a chunk, all you need to do is:\n\nsend a POST request to https://voice.mcs.mail.ru/asr_stream/add_chunk, passing in the Authorization-task_token header;\npass task_id and chunk_num in GET parameters (numbering starts from 0);\nspecify the correct Content-Type in the request header.\na chunk is sent in the request body, which is an array of bytes in wav or ogg format.\n\nThe response will be the result of chunk recognition.\n\nRequest example:\n\ncurl --request POST \\\n  --url 'https://voice.mcs.mail.ru/asr_stream/add_chunk?task_id=xxxxx&chunk_num=2' \\\n  --header 'Authorization: Bearer task_tokenxxxxxxxx' \\\n  --header 'Content-Type: audio/wave' \\\n  --data 'xxxxxxxxxx'\n\nAnswer example:\n\n{\n  \"qid\": \"4d44cb0eb81f4e7f84a7997ec4f2f3c4\",\n  \"result\": {\n    \"text\": \"hello marusya\"\n  }\n}\n\n\n\nThe interval between sending chunks should not exceed 5 seconds, after that the task goes into done status and it will be impossible to continue sending chunks.\n\nChunks themselves must be sent sequentially and synchronously.\n\n\n\n\n\nFor the last chunk, you need to pass a GET-parameter with the value last=1.\n\nSupported audio formats\n\n| Container | Codek | Content type          |\n| --------- | ----- | --------------------- |\n| WAV       | —     | audio/wave            |\n| ogg       | Opus  | audio/ogg codecs=opus |\n\nRestrictions\n\n| Restriction                | Meaning |\n| -------------------------- | ------- |\n| Maximum chunk size         | 32100 B |\n| Maximum chunk duration     | 1 s     |\n| Maximum number of channels | 1       |\n| Minimum number of chunks   | 5       |\n\n\n\nThe recommended chunk length is 0.08 seconds.\n\nRequest to get the end result of the task\n\nAt any time after sending chunks, you can get the result, for this you need to send a GET request to https://voice.mcs.mail.ru/asr_stream/get_result, passing Authorization-task_token in the header, task_id in the GET parameters.\n\nThe response will include the recognition result with the current status of the task.\n\nRequest example:\n\ncurl --request GET \\\n  --url 'https://voice.mcs.mail.ru/asr_stream/get_result?task_id=xxxxx' \\\n  --header 'Authorization: Bearer task_tokenxxxxxxxx' \\\n\nAnswer example:\n\n{\n  \"qid\": \"517e5ba9f4a9465c9d73778bedac0808\",\n  \"result\": {\n    \"text\": \"hello marusya hello marusya\",\n    \"status\": \"done\"\n  }\n}\n","url":"/en/ml/cloud-voice/service-management/speech-recognition","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud Voice","link":"/en/ml/cloud-voice","isActive":false,"evenSimpler":true}]},{"uuid":"6efeac71-0667-448e-b932-2e039e3322ae","title":"Speech synthesis","data":"The speech synthesis function allows you to voice strings of text. To synthesize speech, use a POST or GET request to https://voice.mcs.mail.ru/tts.\n\nThe Request\n\n\n\nGET\nPOST\n\n\n\nTo synthesize speech using a GET request, send the text in the text parameter.\n\nRequest example:\n\ncurl -L --request GET \\\n    --url 'https://voice.mcs.mail.ru/tts?text=' \\\n    --output\n    --header 'Authorization: Bearer '\n\n\n\n\nTo synthesize speech using a POST request, send the text in the body of the request.\n\nRequest example:\n\ncurl -L --request POST \\\n  --url 'https://voice.mcs.mail.ru/tts?encoder=mp3' \\\n  --header 'Authorization: Bearer ' \\\n  --header 'Content-Type: text/plain' \\\n  --data '' \\\n  --output\n\nRequest parameters:\n\n| Parameter | Description | Transmission method | Accepted values |\n| --- | --- | --- | --- |\n| model_name | Name of the voice model | URL |katherine (or katherine-hifigan) — by default; maria (or maria-serious); pavel (or pavel-hifigan) |\n| encoder | Encoder type | URL | pcm — by default; mp3; opus |\n| tempo | Speech rate | URL | 0.75 to 1.75 |\n| text | Text for voiceover (GET request)| URL | Up to 2000 symbols in UTF-8 |\n| ` | Text for voiceover (POST request) | Option data`| Up to 2000 symbols in UTF-8|\n| ` | File name with extention | Option output` | — |\n| `| Access token for authorization. Read more about authorization in the section Getting an access token | Option header`| — |\n\nRestrictions\n\nThe text passed in the request must be encoded in UTF-8 and contain no more than 20,000 characters.\n\nResponse\n\nThe response contains an audio file of the selected format. By default, a PCM file is created.\n\nErro codes\n\n| Code  | State | Description                   |\n| ---- | ------ | -------------------------- |\n| 4048 | 400    | Invalid token       |\n| 4049 | 400    | Inactive VK Cloud project   |\n| 4051 | 14001  | Invalid text format |\n\nSee also\n\nAbout Cloud Voice\n\nGetting an access token\n\nSpeech recognition\n","url":"/en/ml/cloud-voice/service-management/text-to-speech","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud Voice","link":"/en/ml/cloud-voice","isActive":false,"evenSimpler":true}]},{"uuid":"fcc6c0b6-6369-4070-8490-627ffaa5c0cd","title":"Improving the quality of pronunciation","data":"Before processing text into TTS format, we recommend:\n\nRemove special characters and Latin characters. Cloud Voice is not always able to correctly pronounce the phrase in Latin.\nNumerals and abbreviations are best written in text. For example, \"Sokolnikov Square - 5 sq. km, and Monaco - about 2\". It would be more correct to issue it like this: “Sokolnikov Square is five square kilometers, and Monaco is about two.”\nPut in quotation marks the letters that you need to pronounce separately. For example: \"City with the letter \"O\"\". Add vowels to consonants: “Be”, “Ve”, “Ge”.\nAvoid complex sentences with an abundance of turns. It is better to divide the sentence into several simple ones.\n\nAccuracy of pronunciation\n\nTo improve the accuracy of text recognition, you can use special characters:\n| Sign | What does |\n| --- | --- |\n|  | stress. Placed before a vowel. | | ^ | Intonation. Placed before and after the word. | | . | Pause. Placed after the word. | | { and } | Transcription. Placed before and after the word. Example: {Mercedes}{Mercedes\\eu} |\n\nAccent\n\nSometimes there may be an error in emphasis. For example, \"Once Lev Nikolayevich Tolstoy challenged Ivan Sergeevich Turgenev to a duel.\" In such a case, we recommend using \"\\`\".\n\nGet:\n\n\"Once Lev Nikolayevich Tolst\\`oy challenged Ivan Sergeevich Turgenev to a duel.\"\n\nThere may also be an error with accents in words with \"ё\". It is important not to miss this letter when writing the text, otherwise instead of \"Blinded\" you get \"Blinded\".\n\nIntonation\n\nIf you want to emphasize a particular word, use \"^\".\n\n| Was                                                    | Became                                                   |\n| ------------------------------------------------------ | -------------------------------------------------------- |\n| Russia has its own Rome.\" Emphasis on the word \"Rome\". | \"Russia has ^its own^ Rome.\" Emphasis on the word \"own\". |\n\nPause\n\nIf you want to add pauses to your speech, you can separate the sentence with a period.\n\n| Was                                                         | Became                                                 |\n| ----------------------------------------------------------- | ------------------------------------------------------ |\n| \"Who was there just not: lions, tigers, giraffes, monkeys.\" | “Who was not there. Lions. Tigers. Giraffes. Monkeys\". |\n\nIf Cloud Voice does not pause between sentences, then this is solved by a simple line break. Example:\n“And you can also control the music with your voice.\nBy the way! Completely forgot\".\n\nTranscription\n\nSometimes synthesis reads words incorrectly. For example, in a word, the letters “O” and “G” are emphasized too strongly, so that a “Vologda dialect” appears. To make the word sound more familiar, it has to be written with “kavota” errors. Another example is Peter (city). Cloud Voice pronounces it as \"Peter\". To fix this, we write a transcription with an intentional error: \"P'itir\".\n\nSince in the application, in addition to the voice, there is also a text answer - errors are unacceptable.\n\nIn this case, we write both versions of the word. Moreover, the word is first written as it should look in the text, and then as it should be written for speech synthesis. And enclose them in curly brackets. It turns out: I saw {someone}{kavot} strange.\nWe do the same with foreign words that synthesis can read incorrectly: which is better {Mercedes}{Merced\\es} or {BMW}{Beemv\\e}?\n\nProblem cases\n\nBasically, problems can arise when the Latin alphabet is used. For example, black reads like \"black\" and is pronounced quickly, which together gives consonance with another word. You can correct this error using the symbol - (hyphen): \"b-lek\".\n\nUnforeseen situations can also arise with Russian words. For example, “Happy New Year! Your Yul'a and Marusya. There is an accent here, but even if it is present, “Yula” is pronounced. You can fix it in the way described above.\n\nDifficulties may arise with reading a foreign combination of letters th: theater, feather, the etc. Such words are pronounced with a Russian accent. To make speech closer to the pronunciation of native speakers, use the transcription through the letters \"f\", \"v\" and \"d\":\n\ntheatre → **fie-ta -feather → fevar\nthe* (national) → *te** (national)\n","url":"/en/ml/cloud-voice/service-management/pronunciation-quality","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud Voice","link":"/en/ml/cloud-voice","isActive":false,"evenSimpler":true}]},{"uuid":"5183fc57-8c72-43dc-bea0-a9d75537aa45","title":"Tariffication","data":"| Service                                 | Cost                       |\n| --------------------------------------- | -------------------------- |\n| Speech synthesis                        | 1 ruble / 1000 characters  |\n| Speech recognition from audio files     | 0.50 rubles / minute       |\n| Speech recognition from streaming audio | 0.50 rubles / minute       |\n","url":"/en/ml/cloud-voice/tariffication","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Cloud Voice","link":"/en/ml/cloud-voice","isActive":false,"evenSimpler":true}]},{"uuid":"f0cb9be0-7082-4c84-bffb-b747a678a371","title":"General description","data":"Vision is a face, object, process recognition technology based on machine learning and artificial neural networks. Vision will automate and improve the accuracy of complex visual inspections of varying complexity.\n\n98% accuracy of face detection among a million.\nTensorRT on inference, <10ms on photo on GPU.\n314 scene recognition classes, 25,000 object classes.\n\nVision technologies are available through APIs that are constantly being expanded. A list of them is available in this help center. Using the API, you can solve such cases as:\n\nRecognition of persons;\nDetermination of scenes and objects;\nClassification of documents;\nRecognition of the fields of documents;\nРаспознавание знаменитостей;\nText recognition on images;\nDetection of cars and their numbers;\nDetermination of car numbers;\nIncreased resolution;\nIdentification of defects in production.\n","url":"/en/ml/vision/concepts/about","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"6992fbb0-e323-40dd-8798-2d3cf57c0a73","title":"Limits","data":"General restrictions\n\nThe service supports for each account:\n\nMore than 100,000,000 transactions per month.\nBase of faces up to 75,000 per 1 namespace.\n\nImage processing\n\nMaximum number of images per request: 100.\nThe maximum size of each image should not exceed 15 MB.\nWhen sending multiple images in one request, their total size should not exceed 30 MB.\nMinimum image resolution: 5x5.\nMaximum image resolution: 3840x2160 (4K).\nAcceptable image formats: jpeg, png, tiff.\n","url":"/en/ml/vision/concepts/vision-limits","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"cf5fe676-6fa6-44cf-8ae9-3d9f36245521","title":"Authorization","data":"To authorize the client on the VK Cloud platform, a token is used, which is passed in the request parameter:\n\ncurl -k -v \"https://smarty.mail.ru/api/v1/objects/detect?oauth\\_provider=\"mcs&oauth\\_token=qh9sdcsX4iuKGFa1sNhhcyBQiJtWrX5TewjPkPf867ad53oFd\" -F file\\_0=@examples/car\\_number.jpg - F meta='{\"mode\":\\[\"object\"\\],\"images\":\\[ {\"name\":\"file\\_0\"}\\]}'\nAuthorization parameters:\n\n| Parameter | Description |\n|---------------|-------------------------------- ----------------- |\n| oauth_provider | mcs |\n| oauth_token | qh9sdcsX4iuKGFa1sNhhcyBQiJtWrX5TewjPkPf867ad53oFd |\n\noauth_provider\n\nauthorization server. In Vision, authorization is available through VK Cloud and OAUTH.MAIL.RU.\n\noauth_provider=mcs** — authorization available to all mcs clients that have Machine Learning -> Vision API enabled.\noauth_provider=mr** — authorization via oauth.mail.ru, available only for internal projects of the mail.ru company, you can learn more about it at https://o2.mail.ru/docs/.\n\noauth_token\n\nToken for client authorization on the VK Cloud platform.\n\nGetting a token in the VK Cloud authorization system\n\nAs part of authorization through mcs, 2 types of token are supported:\n\nservice_token;\naccess_token.\n\nservice_token\n\nThe token that is the easiest to generate, it has no restrictions on the lifetime and the number of tokens of this type.\n\nThe token is generated on the user's page in personal account.\n\naccess_token\n\nTo obtain this token, the OAuth 2.0 protocol is used.\n\nTo get the first access_token, you need to send a request to the authorization server (see below) with the mcs client ID (client_id) and secret key (client_secret) from personal account.\n\nIn response, 2 tokens will be received from the server:\n\naccess token (access_token);\na token for updating a “rotten” access token (refresh_token).\n\nThe first access_token is reusable and short-lived, it is used for authorization in an image recognition request. The second one, refresh_token, is used to refresh the access token. refresh_token has two properties that are inverse to access tokens: they are long-lived, but not reusable.\n\nIn total, 25 refresh_tokens are available to the client (/auth/oauth/v1/token with grant_type=\"client_credentials\").\n\nFor every refresh_token, the client can get 25 access_tokens ( /auth/oauth/v1/token c\" grant_type=\"refresh_token).\n\nThe usage scheme for tokens is as follows:\n\nThe user logs in to the service by passing the identifier and secret key to the server. In response, it receives 2 tokens and a lifetime.\nApplication saves tokens and uses access token for subsequent image recognition requests.\nWhen the access token lifetime comes to an end, requests for image recognition will stop passing:\n\n  \"status\":401,\"body\":\"authorization failed, provider: mcs, token: vMA3Pjyno6tvCdo8MeDQ8xYT(...), reason: CONDITION/UNAUTHORIZED, Access Token invalid\"\n\nYou will need to refresh the access token with a refresh token.\n\nGetting the first token\n\nWe need to send a request to the authorization server with the client ID and secret key:\n\ncurl -X POST --location 'https://mcs.mail.ru/auth/oauth/v1/token' \\\n   --header 'Content-Type: application/json' \\\n   --data '{\n   \"client_id\":\"\",\n   \"client_secret\": \"\",\n   \"grant_type\":\"client_credentials\"\n   }'\n\nQuery parameters\n\n| Parameter | Description |\n|--------------|---------------------------------- ------------- |\n| client_id | mcs1017666666.ml.vision.f7kk1rmajnhfy |\n| client_secret | 5FYLyJoex37xw45TJShx6dGifnouhdsOIndbsyg78ejnbs |\n| grant_type | client_credentials |\n\nclient_id\n\nThe client ID on the mcs platform is taken from the [personal account] page (https://msk.cloud.vk.com/app/services/machinelearning/vision/).\n\nclient_secret\n\nThe secret key is taken from the [personal account] page (https://msk.cloud.vk.com/app/services/machinelearning/vision/).\n\ngrant_type\n\nType of token to generate:\n\n\"grant_type\":\"client_credentials\"** — generation of access_token and refresh_token by client_secret (only for the first time).\n\"grant_type\":\"refresh_token\"** — generation of access_token via refresh_token to update access token.\n\nResponse to request\n\n{\n  \"refresh_token\": \"Q9fdfT49CZ19rfohBC4y8Du6PE89989898hghgh\",\n  \"access_token\": \"vMA3Pjyno6tvCdo8MeDQ8xfgbibiubr9r\",\n  \"expired_in\": \"3600\",\n  scope: {\n    \"objects\": 1,\n    video: 1\n    \"persons\": 1\n  }\n}\n\n| Parameter | Description |\n|--------------|---------------------------------- -------------------------------------- |\n| access_token | Access token for client authorization in image and video recognition requests |\n| refresh_token | Token to generate access_token when the old one expires |\n| expired_in | Lifetime of the generated access_token in seconds |\n\nRefresh token\n\nTo generate access_token via refresh_token, send a request to the authorization server:\n\ncurl -X POST --location 'https://mcs.mail.ru/auth/oauth/v1/token' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"client_id\":\"\",\n\"refresh_token\":\"\",\n\"grant_type\":\"refresh_token\"\n}'\n\nResponse to request\n\n{\n  \"refresh_token\": \"Q9fdfT49CZ19rfohBC4y8Du6PE89989898hghgh\",\n  \"access_token\": \"vMA3Pjyno6tvCdo8MeDQ8xfgbibiubr9r\",\n  \"expired_in\": \"3600\",\n  scope: {\n    \"objects\": 1,\n    video: 1\n    \"persons\": 1\n  }\n}\n\n| Parameter | Description |\n|--------------|------------------------------------------------ |\n| access_token |Access token for client authorization in image and video recognition requests |\n| refresh_token |Token to generate an access_token when the old one expires|\n| expired_in |The lifetime of the generated access_token in seconds |\n","url":"/en/ml/vision/quick-start/auth-vision","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"fcfe8b2e-1e80-44aa-b02a-23c2e6fa2c1c","title":"Creation and connection to the service","data":"The machine learning service is enabled by default in VK Cloud projects. You can connect to the service in two different ways, described below.\n\nAPI Endpoint\n\nAPI Endpoints are \"entry points\", the URL that connects to the management service. There are two for machine learning:\n\nVision for image recognition https://smarty.mail.ru/\nVision for video recognition https://smarty.mail.ru/\n\nAccess via client ID and secret key\n\nTo control access through the client ID and secret key, the Vision API page displays the required information. The client ID has already been generated and cannot be replaced, and the key can be reissued with the corresponding button.\n\nAccess via service token\n\nTo generate a service token, click the button and select the required scope:\n\nClick the Add Service Token button.\nSelect the required scope.\nClick Create.\n\nThe token will be generated and displayed on the page.\n","url":"/en/ml/vision/quick-start/start-vision","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"809f0eb5-52d7-4776-97f3-0b6e047b5af2","title":"Face recognition","data":"HOST: https://smarty.mail.ru\n\nFour API methods are used for face recognition:\n\nset (/api/v1/persons/set);\nrecognize (/api/v1/persons/recognize);\ndelete (/api/v1/persons/delete);\ntruncate (/api/v1/persons/truncate).\n\nLet's consider each of them in more detail.\n\nSet\n\nThis method allows you to establish a link between a given photo and a specific person_id.\n\nRequest\n\nAuthorization data is passed in the query string:\n\n| Parameter | Type | Meaning |\n| ------------- | ------ | ------------------------------------ |\n| oauth_token | string | OAuth2 access token (required non-empty)|\n| oauth_provider | string | OAuth2 provider (required non-empty) |\n\nSupported OAuth2 providers:\n\n| Provider | oauth_provider value | Getting a token |\n| -------- | ---------------------- | ------------------------------------ |\n| VK Cloud | mcs | See in article|\n\nRequest parameters are passed in JSON format in the request body with name=\"meta\":\n\n| Parameter | Type         | Meaning                                                  |\n| --------- | ------------- | ------------------------------------------------------ |\n| space | string | Filenames to match files in request and response (required non-empty) |\n| images | []image_meta | ID matched to the person in the photo (required non-empty)|\n\nThe space parameter is used to avoid intersections by person. So person1 from space 0 and person1 from space 1 are different. For applications that solve different problems, it makes sense to use different values ​​of space.\n\nA client can have up to 10 different spaces. Space values ​​range from 0 to 9. If the limit is exceeded, an error will be returned.\n\nimage_meta parameters:\n\n| Parameter | Type | Meaning |\n| --------- | ------ | -------- |\n| name | string | Filenames to match files in request and response (required non-empty) |\n| person_id | int | ID associated with the person in the photo (required non-empty) |\n\n\n\nThe method is subject to restrictions\n\nRequest example\n\ncurl -X 'POST' \"https://smarty.mail.ru/api/v1/persons/set?oauth_token=&oauth_provider=mcs\"      \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_set_ok.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"images\": [\n    {\n      \"name\": \"file\",\n      \"person_id\": 1\n    }\n  ]\n}'\n\nResponse\n\n| Parameter | Type | Meaning |\n| -------- | -------- | ------------------------------------------------------- |\n| status | int | 200 in case of successful communication with the Vision|\n| body | response | Response body |\n\nresponse parameters:\n\n| Parameter | Type | Meaning |\n| -------- | -------- | -------------------------------- |\n| objects | [] object | array of responses for each file |\n\nobject parameters:\n\n| Parameter | Type | Meaning |\n| -------- | ------ | ----------------------------------------------------- |\n| status | enum | Execution result |\n| error | string | Text description of the error (optional) |\n| name | string | File name to match files in request and response |\n\nstatus parameters:\n\n| Parameter | Meaning |\n|--------- |---------------------------------------------- |\n| 0 | Successfully |\n| 1 | Array of found document types per page |\n| 2 | Temporary error |\n\nAnswer example\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\"\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nAdditional examples\n\n\n  Field validation error (mismatch of file names with the form)\n\nRequest example (any image is used):\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/set?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_set_ok.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"images\": [\n    {\n      \"name\": \"file_10\",\n      \"person_id\": 12\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"could not get image by name file_10: http: no such file\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\n\n\n\n  The image does not contain a person's face\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/set?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_set_error_no_face.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"images\": [\n    {\n      \"name\": \"file\",\n      \"person_id\": 12\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 1,\n        \"error\": \"face set required only one face per image\",\n        \"name\": \"file\"\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\n\n\n\n  Image with multiple faces\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/set?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_set_error_many_people.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"images\": [\n    {\n      \"name\": \"file\",\n      \"person_id\": 12\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 1,\n        \"error\": \"face set required only one face per image\",\n        \"name\": \"file\"\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\n\n\n\n  Empty image\n\nAs an example, you can use any empty file with the JPG extension.\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/set?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@empty.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"images\": [\n    {\n      \"name\": \"file\",\n      \"person_id\": 12\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"empty image\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nRecognize\n\nThis method allows you to recognize a person from a given photo. If no match is found, a new person will be added.\n\nRequest\n\nAuthorizationThis data is passed in the query string:\n\n| Parameter | Type | Meaning |\n| ------------- | ------- | -------------------------------------------------- |\n| oauth_token | string | OAuth2 access token (required non-empty) |\n| oauth_provider | string | OAuth2 provider (required non-empty) |\n\nSupported OAuth2 providers:\n\n| Provider | oauth_provider value | Getting a token |\n| -------- | ---------------------- | ------------------------------------ |\n| VK Cloud | mcs | See in article|\n\nRequest parameters are passed in JSON format in the request body with name=\"meta\":\n\n| Parameter | Type | Default | Meaning |\n| ---------- | ------------ | ------------ | --------------------- |\n| space | string | \\-- | Numeric identifier used to avoid person intersections (required non-empty) |\n| create_new | bool | false | Whether to add a new person if no matches were found |\n| images | []image_meta | \\-- | Transferred image metadata (required non-empty) |\n\nFor a description of the space parameter, see the section of the Set method.\n\nimage_meta parameters:\n\n| Parameter | Type | Meaning |\n| -------- | ------ | ------------- |\n| name | string | Filenames to match files in request and response (required non-empty) |\n\nImages are passed in the body of the request, the values ​​of the name field must match those passed in images.\n\n\n\nThe method is subject to restrictions\n\nRequest example\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_recognize_ok_person_in_db.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"create_new\": false,\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse\n\n| Parameter | Type | Meaning |\n| ---------| -------- | ---------------------------------------------------------|\n| status | int | 200 in case of successful interaction with the Vision servers |\n| body | response | Response body |\n\nresponse parameters:\n\n| Parameter | Type | Meaning |\n| --------------- | -------- | -------------------------------- |\n| objects | [] object | Array of responses for each file |\n| aliases_changed | bool | Have aliases changed |\n\nobject parameters:\n\n| Parameter | Type | Meaning |\n| ---------------- | -------- | ------- |\n| status | enum | Execution result |\n| error | string | Text description of the error (optional) |\n| name | string | File name to match files in request and response |\n| persons | []person | List of persons found in the photo |\n\nstatus parameters:\n\n| Parameter | Meaning |\n| ------------ | -------------------- |\n| 0 | successfully |\n| 1 | permanent error |\n| 2 | temporary error |\n\nperson parameters:\n\n| Parameter | Type | Meaning |\n| ------------ | -------- | ----------------------------------------- |\n| tag | string | Found person ID |\n| coordinate | []int | Found face coordinates [left x, top y, right x, bottom y] |\n| aliases | []string | Array of similar persons (optional) |\n| confidence | float | Degree of confidence of the face detector that the found image is a face (from 0 to 1) |\n| similarity | float | The degree of similarity of the found face with the person in the database |\n| awesomeness | float | Conditional \"coolness\" of the photo (from 0 to 1) |\n\nFor the second model only:\n\n| Parameter | Type | Meaning |\n|------------ | ------- | -------------------------------------------------- |\n| sex | string | Person's gender [\"female\", \"male\"] |\n| age | float | Person's age |\n| emotions | string | Person's emotions: \"Neutral\", \"Happiness\", \"Sadness\", \"Surprise\", \"Fear\", \"Disgust\", \"Anger\", \"Contempt\" |\n| valence | float | The level of approval by a person of the situation in which he is [-1;1] |\n| arousal | float | Level of human involvement [-1 - sleepy, inactive person; 1 - active person] |\n\nThe value of tag may equal undefined if the value of create_new in the request was false and no corresponding person was found in the database for the provided image.\n\nAnswer example\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"persons\": [\n          {\n            \"tag\": \"person1\",\n            \"coord\": [\n              567,\n              376,\n              992,\n              931\n            ],\n            \"confidence\": 0.99917,\n            \"awesomeness\": 0.4894,\n            \"similarity\": 0.9721,\n            \"sex\": \"male\",\n            \"emotion\": \"Neutral\",\n            \"age\": 34,\n            \"valence\": -0.3236,\n            \"arousal\": 0.185,\n            \"frontality\": 0.8921,\n            \"visibility\": 0.9985\n          }\n        ]\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nAdditional examples\n\n\n  The face in the image is not in the database and create_new=true\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_recognize_ok_create_new.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"create_new\": true,\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"persons\": [\n          {\n            \"tag\": \"person2\",\n            \"coord\": [\n              842,\n              242,\n              1340,\n              908\n            ],\n            \"confidence\": 0.99957,\n            \"awesomeness\": 0.6065,\n            \"similarity\": 1,\n            \"sex\": \"female\",\n            \"emotion\": \"Happiness\",\n            \"age\": 28,\n            \"valence\": 0.6829,\n            \"arousal\": 0.0757,\n            \"frontality\": 0.9857,\n            \"visibility\": 0.9989\n          }\n        ]\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\n\n\n\n  There is no face in the image\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_set_error_no_face.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"create_new\": false,\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\"\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\n\n\n\n  The face in the image is not in the database and create_new=false\n\nIt is assumed that the face from the image was not added to the database using the /api/v1/persons/set method.\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_recognize_error_no_face_in_db.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"create_new\": false,\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"persons\": [\n          {\n            \"tag\": \"undefined\",\n            \"coord\": [\n              349,\n              45,\n              543,\n              308\n            ],\n            \"confidence\": 0.99977,\n            \"awesomeness\": 0.5002,\n            \"similarity\": 1,\n            \"sex\": \"female\",\n            \"emotion\": \"Surprise\",\n            \"age\": 31,\n            \"valence\": -0.1527,\n            \"arousal\": 0.3299,\n            \"frontality\": 0.8228,\n            \"visibility\": 0.997\n          }\n        ]\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\n\n\n\n  JSON generation error\n\nAs an example, you can use any empty file with the JPG extension.\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_recognize_ok_create_new.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"create_new\": false,\n  \"images\": [\n    {\n      \"name\": \"file1\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"could not get image by name file1: http: no such file\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\n\n\n\n  Invalid image\n\nAs an example, you can use any empty file with the JPG extension.\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@empty.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"create_new\": false,\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"empty image\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nDelete\n\nThis method allows you to remove the association between a photo and person_id.\n\nRequest\n\nAuthorization data is passed in the query string:\n\n| Parameter | Type | Meaning |\n| ------------- | ------ | ---------------------------------------- |\n| oauth_token | string | OAuth2 access token (required non-empty) |\n| oauth_provider | string | OAuth2 provider (required non-empty) |\n\nSupported OAuth2 providers:\n\n| Provider | oauth_provider value | Getting a token |\n| -------- | ---------------------- | ------------------------------------ |\n| VK Cloud | mcs | See in article|\n\nRequest parameters are passed in JSON format in the request body with name=\"meta\":\n\n| Parameter | Type | Meaning |\n| -------- | ------------ | -------------------------------------------------------- |\n| space | string | numeric identifier used to avoid person collisions (required non-empty) |\n| images | []image_meta | metadata of transferred images (required non-empty) |\n\nFor a description of the space parameter, see the section of the Set method.\n\nimage_meta parameters:\n\n|Parameter | Type | Meaning |\n|---------- | ------ | -------- |\n| name | string | Filenames to match files in request and response (required non-empty) |\n| person_id | int | ID associated with the person in the photo (required non-empty) |\n\nImages are passed in the body of the request, the values ​​of the name field must match those passed in images.\n\n\n\nThe method is subject to restrictions\n\nRequest example\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/delete?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"images\": [\n    {\n      \"name\": \"aaa\",\n      \"person_id\": 1\n    }\n  ]\n}'\n\nResponse\n\n| Parameter | Type | Meaning |\n| -------- | -------- | ------------------------------------------------------ |\n| status | int | 200 in case of successful interaction with the Vision servers |\n| body | response | Response body |\n\nresponse parameters:\n\n| Parameter | Type | Meaning |\n| -------- | -------- | -------------------------------- |\n| objects | [] object | Array of responses for each file |\n\nobject parameters:\n\n| Parameter | Type | Meaning |\n| -------- | ------ | ----------------------------------------------------- |\n| status | enum | Execution result |\n| error | string | Text description of the error (optional) |\n| name | string | File name to match files in request and response |\n\nstatus parameters:\n\n| Parameter | Meaning |\n| -------- | -------------------- |\n| 0 | Successfully |\n| 1 | Permanent error |\n| 2 | Temporary error |\n\nAnswer example\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"aaa\"\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nAdditional examples\n\n\n  JSON validation error (no person_id)\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/delete?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_recognize_error_no_face_in_db.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"space\": \"5\",\n  \"images\": [\n    {\n      \"name\": \"aaa\",\n      \"person_id\": 1\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"no person_id has been provided\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nTruncate\n\nThis method allows you to completely clear the space.\n\nRequest\n\nAuthorization data is passed in the query string:\n\n| Parameter | Type | Meaning |\n| ------------- | ------ | ---------------------------------------- |\n| oauth_token | string | OAuth2 access token (required non-empty) |\n| oauth_provider | string | OAuth2 provider (required non-empty) |\n\nSupported OAuth2 providers:\n\n| Provider | oauth_provider value | Getting a token |\n| -------- | ---------------------- | ------------------------------------ |\n| VK Cloud | mcs | See in article|\n\nRequest parameters are passed in JSON format in the request body with name=\"meta\":\n\n| Parameter | Type | Meaning |\n| -------- | ------ | -------- |\n| space | string | Numeric identifier used to avoid person collisions (required non-empty)|\n\nFor a description of the space parameter, see the section of the Set method.\n\nThis request does not require the transfer of images.\n\nRequest example\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/truncate?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'meta={\n  \"space\": \"5\"\n}'\n\nResponse\n\n| Parameter | Type | Meaning |\n| -------- | -------- | -------------------------------------------------------- |\n| status | int | 200 in case of successful interaction with the Vision servers |\n| body | response | Response body |\n\nAnswer example\n\n{\n  \"status\": 200,\n  \"body\": {},\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nAdditional examples\n\n\n  JSON validation error (no space field)\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/persons/truncate?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'meta={\n  \"azaza\": \"5\"\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"wrong space param : strconv.Atoi: parsing \\\"\\\": invalid syntax\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\n\n","url":"/en/ml/vision/service-management/face-recognition","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"41bc6c1a-d825-42b2-b8e9-7a5deb74082a","title":"Recognition of types of documents","data":"This method allows you to determine whether a photograph is a document and the possible type of document.\n\nHOST: https://smarty.mail.ru\n\nENDPOINT: /api/v1/docs/detect\n\nRequest\n\nAuthorization data is passed in the query string:\n\n| Parameter | Type | Meaning |\n| ---------------- |-------- | ------------------------------------------- |\n| oauth_token | string | OAuth2 access token (required non-empty) |\n| oauth_provider | string | OAuth2 provider (required non-empty) |\n\nSupported OAuth2 providers:\n\n| Provider | oauth_provider value | Getting a token |\n| ----------- | ------------------------- | ------------------- |\n| mail.ru | mcs | See article |\n\nRequest parameters are passed in JSON format in the request body with name=\"meta\":\n\n| Parameter | Type | Meaning |\n| ---------- | -------------- | ----------------------------------------------------- |\n| images | []image_meta | Metadata of transmitted images (required non-empty) |\n\nimage_meta parameters:\n\n| Parameter | Type | Meaning |\n| ---------- | -------- | ---------------------------------------------- |\n| name | string | Filenames to match files in request and response (required non-empty) |\n\nImages are passed in the body of the request, the values ​​of the name field must match those passed in images.\n\n\n\nThe method is subject to restrictions\n\nRequest example\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/docs/detect?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@docs_recognize_ok.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse\n\n| Parameter | Type | Meaning |\n| ---------- | ---------- | ---------------------------------------------------- |\n| status | int | 200 in case of successful interaction with the Vision servers |\n| body | string | Response body |\n\nresponse parameters:\n\n| Parameter | Type | Meaning |\n| ---------- | -------- | ------------------------------------------- |\n| status | enum | Execution result) |\n| error | string | Text description of the error (optional) |\n| name | string | File name to match files in request and response |\n| pages | []page | List of objects (marks) found on the image |\n\nstatus parameters:\n\n| Parameter | Meaning |\n| -------- | ---------------------------------------------------- |\n| 0 | Successfully |\n| 1 | Array of found document types per page |\n| 2 | Page number |\n\npage parameters:\n\n| Parameter | Type | Meaning |\n| ---------- | ------- | ----------------------------------------------- |\n| index | int | Page number |\n| docs | []doc | Array of found document types per page |\n\ndoc parameters:\n\n| Parameter | Meaning |\n| ---------- | ------------------------------------------------------------ |\n| eng | Type (name) of the document in English |\n| rus | Type (name) of the document in Russian |\n| sample | Degree of confidence that this type of document is in the image |\n\nFor each object (picture) there can be several types, with varying degrees of certainty. The \"Document\" label determines whether the image is a document and with what probability. The following types of documents are currently supported:\n\n| Eng label | Russian label |\n| -------------------- | -------------------- |\n| Akt | Act |\n| Akt_sverky | Reconciliation act |\n| diploma | Diploma |\n| doc | Document |\n| dogovor | Contract |\n| Doverennost | Power of attorney |\n| Inn| TIN |\n| logo | Logo |\n| Passport | Passport |\n| price_list | Price list |\n| Prikaz | Order |\n| protocol | Protocol |\n| Pts | PTS |\n| Registration_ts | Vehicle registration |\n| Requisite | Requisites |\n| resume | Resume |\n| Schet | Account |\n| certificate | Certificate |\n| snils | Snils |\n| Information | Help |\n| Testimony | Testimony |\n| Table | Report card |\n| charter | Charter |\n| Voditelskye_prava | Driving license |\n| Vypiska | Extract |\n| foreign passport | Passport |\n| Application | Application |\n\nResponse example\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"status\": 0,\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"pages\": [\n          {\n            \"index\": 0,\n            \"docs\": [\n              {\n                \"eng\": \"Pasport\",\n                \"rus\": \"Паспорт\",\n                \"probability\": 0.475\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nAdditional example\n\nDriver's license recognition\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/docs/detect?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@docs_detect_ok_prava.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"status\": 0,\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"pages\": [\n          {\n            \"index\": 0,\n            \"docs\": [\n              {\n                \"eng\": \"Voditelskye_prava\",\n                \"rus\": \"Водительские права\",\n                \"probability\": 0.8387\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nThere is no document in the image\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/docs/detect?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_set_error_no_face.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"status\": 0,\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"pages\": [\n          {\n            \"index\": 0,\n            \"docs\": []\n          }\n        ]\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nError in JSON generation (name mismatch in meta and image)\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/docs/detect?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@docs_detect_ok_prava.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"images\": [\n    {\n      \"name\": \"file1\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"could not get image by name file1: http: no such file\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n","url":"/en/ml/vision/service-management/vision-api-doctypes","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"477cbf32-5880-47d9-8146-e8d5f46c6fae","title":"Object recognition","data":"This method allows you to find various objects in the photo.\n\nHOST: https://smarty.mail.ru\n\nENDPOINT: /api/v1/objects/detect\n\nRequest\n\nAuthorization data is passed in the query string:\n\n| Parameter | Type | Meaning |\n| ------------- | ------ | ---------------------------------------- |\n| oauth_token | string | OAuth2 access token (required non-empty) |\n| oauth_provider | string | OAuth2 provider (required non-empty) |\n\nSupported OAuth2 providers:\n\n| Provider | oauth_provider value | Getting a token |\n| --------- | ---------------------- | ------------------------------------------------ |\n| VK Cloud | mcs | See in article |\n\nRequest parameters are passed in JSON format in the request body with name=\"meta\":\n\n| Parameter | Type | Meaning |\n| -------- | ------------ | ------------------------------ |\n| mode | []string | Types of objects to be searched for in the passed images (required non-empty) |\n| images | []image_meta | Transferred image metadata (required non-empty) |\n\nPossible values ​​for mode:\n\n| Parameter | Type |\n| ----------- | ---------------------------------- |\n| object  | Search objects in the image |\n| object2 | Search for objects in the image (v2 model version — recognizes objects belonging to more classes) |\n| scene | Search on scene image |\n| car_number | Search on the image for car numbers |\n| multiobject | Search the image for multi-objects — objects and the entire set of boxes of all found objects |\n| pedestrian | Search for people in the image (more precisely determines the set of boxes of all people in the image) |\n\nmode may contain one or more modes. For example:\n\n\"mode\":[\"object\"]\n\nThe method is subject to restrictions\n\nRequest example\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/objects/detect?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@objects_detect_ok_people_in_theatre.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": [\n    \"scene\",\n    \"multiobject\",\n    \"pedestrian\"\n  ],\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse\n\n| Parameter | Type | Meaning |\n| ------------- | -------- | -------------------------------------------------------- |\n| status | int | 200 in case of successful interaction with the Vision servers |\n| body | response | Response body |\n\nresponse parameters:\n\n| Parameter | Type | Meaning |\n| ------------------ | -------- | ------------------------ |\n| scene labels | [] object | Array of responses for each scene file (may be missing) |\n| object labels | [] object | Array of responses for each file with objects (may be missing) |\n| car_number_labels | [] object | Array of responses for each file with machine numbers (may be missing) |\n| multiobject labels | [] object | Array of responses for each file with multi-objects (may be missing) |\n| pedestrian labels | [] object | Array of responses for each file with people (may be missing) |\n\nobject parameters:\n\n| Parameter | Type | Meaning |\n| ---------------- | -------- | -------------------------------------------------- |\n| status | enum | Execution result |\n| error | string | Text description of the error (optional) |\n| name | string | File name to match files in request and response |\n| labels | [] label | Listobjects (marks) found on the image |\n| count_by_density | int | The number of people in the frame, calculated using the density map (only for mode=\"pedestrian\") |\n\nstatus parameters:\n\n| Parameter | Meaning |\n| ------------ | -------------------- |\n| 0 | Successfully |\n| 1 | Permanent error |\n| 2 | Temporary error |\n\nlabel parameters:\n\n| Parameter | Meaning |\n| ------------- | --------------------------------------------------------------- |\n| eng | Label (name) for the found object in English |\n| rus | Label (name) of the found object in Russian |\n| eng_categories | List of categories (each category includes many tags) in English (optional) |\n| eng_categories | List of categories (each category includes many tags) in Russian (optional) |\n| sample | Degree of confidence that this object is in the image |\n| coordinate | Found object coordinates (optional) |\n| types_prob | An array of license plate type probabilities. currently the following types are supported: \"rus\" — all types of Russian numbers; \"cis\" — numbers of the CIS (except individual and military Ukrainian ones); \"eu\" — one-story plates of Europe (optional, only for car_number mode). |\n\nResponse example\n\n\n  JSON response\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"multiobject_labels\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"labels\": [\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.9586,\n            \"coord\": [\n              84,\n              309,\n              148,\n              404\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.9102,\n            \"coord\": [\n              130,\n              325,\n              238,\n              428\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.8765,\n            \"coord\": [\n              208,\n              293,\n              258,\n              353\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.8186,\n            \"coord\": [\n              257,\n              297,\n              322,\n              393\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.7686,\n            \"coord\": [\n              62,\n              295,\n              106,\n              361\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.7274,\n            \"coord\": [\n              0,\n              284,\n              44,\n              360\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.632,\n            \"coord\": [\n              163,\n              294,\n              211,\n              363\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.6232,\n            \"coord\": [\n              432,\n              270,\n              589,\n              385\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.477,\n            \"coord\": [\n              202,\n              338,\n              304,\n              426\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.4546,\n            \"coord\": [\n              407,\n              291,\n              499,\n              368\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.4356,\n            \"coord\": [\n              190,\n              277,\n              219,\n              330\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.4347,\n            \"coord\": [\n              328,\n              282,\n              375,\n              334\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.4345,\n            \"coord\": [\n              246,\n              278,\n              274,\n              328\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.3994,\n            \"coord\": [\n              441,\n              270,\n              566,\n              336\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.3912,\n            \"coord\": [\n              40,\n              282,\n              74,\n              334\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.3674,\n            \"coord\": [\n              360,\n              272,\n              389,\n              319\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.3108,\n            \"coord\": [\n              498,\n              268,\n              606,\n              333\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.3014,\n            \"coord\": [\n              305,\n              269,\n              338,\n              318\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.271,\n            \"coord\": [\n              266,\n              264,\n              287,\n              301\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.2642,\n            \"coord\": [\n              364,\n              328,\n              445,\n              425\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.2412,\n            \"coord\": [\n              112,\n              274,\n              138,\n              307\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.2347,\n            \"coord\": [\n              131,\n              276,\n              167,\n              335\n            ]\n          },\n          {\n            \"eng\": \"Person\",\n            \"rus\": \"Человек\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.2133,\n            \"coord\": [\n              478,\n              277,\n              584,\n              359\n            ]\n          },\n          {\n            \"eng\": \"Chair\",\n            \"rus\": \"Стул\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.5267,\n            \"coord\": [\n              424,\n              386,\n              471,\n              427\n            ]\n          },\n          {\n            \"eng\": \"Chair\",\n            \"rus\": \"Стул\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.2696,\n            \"coord\": [\n              332,\n              340,\n              370,\n              369\n            ]\n          },\n          {\n            \"eng\": \"Chair\",\n            \"rus\": \"Стул\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.2405,\n            \"coord\": [\n              0,\n              370,\n              83,\n              428\n            ]\n          },\n          {\n            \"eng\": \"Backpack\",\n            \"rus\": \"Рюкзак\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.2856,\n            \"coord\": [\n              204,\n              348,\n              304,\n              428\n            ]\n          }\n        ]\n      }\n    ],\n    \"scene_labels\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"labels\": [\n          {\n            \"eng\": \"Auditorium\",\n            \"rus\": \"Зрительный зал\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.4926\n          },\n          {\n            \"eng\": \"Movie Theater\",\n            \"rus\": \"Кинотеатр\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.315\n          }\n        ]\n      }\n    ],\n    \"pedestrian_labels\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"labels\": [\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.9754,\n            \"coord\": [\n              81,\n              309,\n              147,\n              426\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.9312,\n            \"coord\": [\n              328,\n              280,\n              383,\n              352\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.9297,\n            \"coord\": [\n              133,\n              320,\n              278,\n              431\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.9257,\n            \"coord\": [\n              65,\n              292,\n              107,\n              355\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.9101,\n            \"coord\": [\n              208,\n              287,\n              268,\n              361\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.9076,\n            \"coord\": [\n              1,\n              287,\n              47,\n              356\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.9046,\n            \"coord\": [\n              159,\n              294,\n              214,\n              375\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.8955,\n            \"coord\": [\n              303,\n              273,\n              337,\n              320\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.888,\n            \"coord\": [\n              149,\n              306,\n              234,\n              404\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.8847,\n            \"coord\": [\n              255,\n              304,\n              339,\n              400\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.8834,\n            \"coord\": [\n              520,\n              260,\n              600,\n              337\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.8825,\n            \"coord\": [\n              30,\n              216,\n              50,\n              240\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.877,\n            \"coord\": [\n              244,\n              277,\n              274,\n              328\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.8759,\n            \"coord\": [\n              1,\n              335,\n              101,\n              433\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.8744,\n            \"coord\": [\n              436,\n              281,\n              548,\n              358\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.8659,\n            \"coord\": [\n              73,\n              257,\n              102,\n              294\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.8598,\n            \"coord\": [\n              423,\n              288,\n              608,\n              424\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.8592,\n            \"coord\": [\n              308,\n              278,\n              362,\n              355\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.856,\n            \"coord\": [\n              183,\n              300,\n              267,\n              398\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.8553,\n            \"coord\": [\n              124,\n              274,\n              165,\n              343\n            ]\n          },\n          {\n            \"eng\": \"Pedestrian\",\n            \"rus\": \"Человек\",\n            \"prob\": 0.8507,\n            \"coord\": [\n              356,\n              270,\n              392,\n              323\n            ]\n          }\n        ],\n        \"count_by_density\": 157\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nAdditional examples\n\nSearch for objects in an image with plants\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/objects/detect?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_set_error_no_face.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": [\n    \"object\",\n    \"scene\"\n  ],\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\n\n  Response example\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"object_labels\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"labels\": [\n          {\n            \"eng\": \"Close-up\",\n            \"rus\": \"Крупный план\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.4843,\n            \"coord\": [\n              165,\n              0,\n              834,\n              477\n            ]\n          },\n          {\n            \"eng\": \"Macro Photography\",\n            \"rus\": \"Макросъемка\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.5021,\n            \"coord\": [\n              165,\n              0,\n              834,\n              477\n            ]\n          },\n          {\n            \"eng\": \"Plant\",\n            \"rus\": \"Растение\",\n            \"eng_categories\": [\n              \"Plants\"\n            ],\n            \"rus_categories\": [\n              \"Растения\"\n            ],\n            \"prob\": 0.827,\n            \"coord\": [\n              165,\n              0,\n              834,\n              668\n            ]\n          },\n          {\n            \"eng\": \"Leaf\",\n            \"rus\": \"Листок\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.6623,\n            \"coord\": [\n              165,\n              0,\n              834,\n              573\n            ]\n          }\n        ]\n      }\n    ],\n    \"scene_labels\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"labels\": [\n          {\n            \"eng\": \"Rice Paddy\",\n            \"rus\": \"Рисовое поле\",\n            \"eng_categories\": [],\n            \"rus_categories\": [],\n            \"prob\": 0.6255\n          }\n        ]\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nInvalid JSON or image (no valid mode)\n\nRequest example (invalid JSON):\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/objects/detect?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@objects_detect_ok_people_in_theatre.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": [\n    \"sceneaaaa\",\n    \"multiobjet\"\n\n  ],\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nRequest example (invalid image):\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/objects/detect?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@empty.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": [\n    \"scene\",\n    \"multiobjeсt\"\n\n  ],\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"empty image\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n","url":"/en/ml/vision/service-management/vision-api-obj","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"9244167b-cfc1-4f28-bdd1-785533c65795","title":"Text recognition in documents","data":"This method allows you to recognize text in an image.\n\nHOST: https://smarty.mail.ru\n\nENDPOINT: /api/v1/text/recognize\n\nRequest\n\nAuthorization data is passed in the query string:\n\n| Parameter | Type | Meaning |\n| ---------------- | ------ | ---------------------------------------- |\n| oauth_token | string | OAuth2 access token (required non-empty) |\n| oauth_provider | string | OAuth2 provider (required non-empty) |\n\nSupported OAuth2 providers:\n\n| Provider | oauth_provider value | Getting a token | Projects |\n| --------- | ---------------------- | ------------------ | --------------------- |\n| VK Cloud | mcs | See in the article |\n\nRequest parameters are passed in JSON format in the request body with name=\"meta\":\n\n| Parameter | Type | Meaning |\n| -------- | ------------ | -------------------------------------------------------- |\n| images | []image_meta | Transferred image metadata (required non-empty) |\n| mode | string | Flag parameter: whether to issue a detailed answer, if \"detailed\", then detailed (the coordinates of the bounding box of the text and confidence are added to the answer), (optional) |\n\nimage_meta parameters:\n\n| Parameter | Type | Meaning |\n| ------------ | ------- | ------------- |\n| name | string | filenames to match files in request and response (required non-empty) |\n\nImages are passed in the body of the request, the values ​​of the name field must match those passed in images.\n\n\n\nThe method is subject to restrictions\n\nRequest example\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/text/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@docs_recognize_not_doc.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse\n\n| Parameter | Type | Meaning |\n| ------------ | -------- | --------------------------------------------------------- |\n| status | int | 200 in case of successful interaction with the Vision servers |\n| body | response | Response body |\n\nresponse parameters:\n\n| Parameter | Type | Meaning |\n| -------- | ---------- | -------------------------------- |\n| objects | [] object | Array of responses for each file |\n\nobject parameters:\n\n| Parameter | Type | Meaning |\n| ---------- | ------ | ----------------------------------------------------- |\n| status | enum | Execution result |\n| error | string | Text description of the error (optional) |\n| name | string | File name to match files in request and response |\n| text | string | Recognized text |\n| results | []line | If \"mode\":\"detailed\" is set - an array of answer strings for the page (text, bounding box, confidence) |\n\nstatus parameters:\n\n| Parameter | Meaning |\n| ------------ | -------------------- |\n| 0 | Successfully |\n| 1 | Permanent error |\n| 2 | Temporary error |\n\nline parameters:\n\n| Parameter | Type | Meaning |\n| ------------ | --------- | ----------------------------------------- |\n| line_prob | float32 | Line recognition confidence |\n| line_coord | []float32 | Line coordinates - x1,y1, x2, y2 - upper left and lower right points of the enclosing rectangle |\n| words | []word | Array of recognized response words in a string |\n\nword parameters:\n\n| Parameter | Type | Meaning |\n| ----------- | --------- | --------------------------------------------------------- |\n| sample | float32 | Word recognition confidence |\n| coordinate | []float32 | Word coordinates - x1,y1, x2, y2 - upper left and lower right points of the enclosing rectangle|\n| text | string | Array of recognized response words in a string |\n| lang_prob | float32 | Language recognition confidence |\n| language | string | Eng/rus/unknown. Unknown when does not contain letters of the alphabet |\n\nResponse example\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"text\": \"Когда пришла весна,\\nнаступили теплые дни. Там,\\nгде раньше лежал снег,\\nвесело бегут ручьи. Куда\\nни взглянешь, всюду\\nрасцветают подснежники.\\nЕсли посмотреть на\\nвесеннее небо, то можно\\nувидеть стаи птиц,\\nлетящих с юга. Когда\\nпросыпается природа от\\nзимнего сна, лес\\nнаполняется весенней\"\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nAdditional examples\n\nmode=detailed\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/text/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@docs_recognize_not_doc.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": \"detailed\",\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\n\n  Response example\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"results\": [\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  16,\n                  6,\n                  157,\n                  60\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"Когда\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  159,\n                  8,\n                  341,\n                  60\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"пришла\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  344,\n                  8,\n                  500,\n                  58\n                ],\n                \"prob\": 0.9997,\n                \"text\": \"весна,\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.9997,\n            \"line_coord\": [\n              16,\n              6,\n              500,\n              58\n            ]\n          },\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  17,\n                  70,\n                  252,\n                  122\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"наступили\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  254,\n                  69,\n                  428,\n                  124\n                ],\n                \"prob\": 0.9994,\n                \"text\": \"теплые\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  429,\n                  71,\n                  535,\n                  123\n                ],\n                \"prob\": 0.9873,\n                \"text\": \"дни.\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  535,\n                  72,\n                  643,\n                  122\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"Там,\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.9868,\n            \"line_coord\": [\n              17,\n              69,\n              643,\n              122\n            ]\n          },\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  17,\n                  135,\n                  102,\n                  187\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"где\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  105,\n                  133,\n                  284,\n                  185\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"раньше\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  285,\n                  133,\n                  440,\n                  187\n                ],\n                \"prob\": 0.9995,\n                \"text\": \"лежал\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  440,\n                  133,\n                  568,\n                  187\n                ],\n                \"prob\": 0.9893,\n                \"text\": \"снег,\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.9886,\n            \"line_coord\": [\n              17,\n              133,\n              568,\n              185\n            ]\n          },\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  17,\n                  198,\n                  187,\n                  250\n                ],\n                \"prob\": 0.9983,\n                \"text\": \"весело\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  188,\n                  198,\n                  320,\n                  250\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"бегут\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  322,\n                  199,\n                  474,\n                  251\n                ],\n                \"prob\": 0.9943,\n                \"text\": \"ручьи.\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  474,\n                  198,\n                  591,\n                  250\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"Куда\",\n                \"lang_prob\": 0.9994,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.9926,\n            \"line_coord\": [\n              17,\n              198,\n              591,\n              250\n            ]\n          },\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  17,\n                  263,\n                  79,\n                  313\n                ],\n                \"prob\": 0.999,\n                \"text\": \"ни\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  81,\n                  260,\n                  349,\n                  315\n                ],\n                \"prob\": 0.9979,\n                \"text\": \"взглянешь,\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  350,\n                  262,\n                  502,\n                  314\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"всюду\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.9969,\n            \"line_coord\": [\n              17,\n              260,\n              502,\n              313\n            ]\n          },\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  17,\n                  325,\n                  288,\n                  379\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"расцветают\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  288,\n                  326,\n                  612,\n                  377\n                ],\n                \"prob\": 0.9648,\n                \"text\": \"подснежники.\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.9646,\n            \"line_coord\": [\n              17,\n              325,\n              612,\n              377\n            ]\n          },\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  18,\n                  389,\n                  131,\n                  443\n                ],\n                \"prob\": 0.9829,\n                \"text\": \"Если\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  133,\n                  389,\n                  401,\n                  443\n                ],\n                \"prob\": 0.9994,\n                \"text\": \"посмотреть\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  403,\n                  391,\n                  464,\n                  441\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"на\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.9824,\n            \"line_coord\": [\n              18,\n              389,\n              464,\n              441\n            ]\n          },\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  16,\n                  453,\n                  240,\n                  507\n                ],\n                \"prob\": 0.9975,\n                \"text\": \"весеннее\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  242,\n                  455,\n                  375,\n                  507\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"небо,\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  375,\n                  455,\n                  433,\n                  505\n                ],\n                \"prob\": 0.9596,\n                \"text\": \"то\",\n                \"lang_prob\": 0.9463,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  436,\n                  453,\n                  595,\n                  507\n                ],\n                \"prob\": 0.9954,\n                \"text\": \"можно\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.9527,\n            \"line_coord\": [\n              16,\n              453,\n              595,\n              505\n            ]\n          },\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  16,\n                  519,\n                  203,\n                  571\n                ],\n                \"prob\": 0.9991,\n                \"text\": \"увидеть\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  217,\n                  520,\n                  323,\n                  570\n                ],\n                \"prob\": 0.6227,\n                \"text\": \"стаи\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  327,\n                  518,\n                  450,\n                  572\n                ],\n                \"prob\": 0.9975,\n                \"text\": \"птиц,\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.6207,\n            \"line_coord\": [\n              16,\n              518,\n              450,\n              570\n            ]\n          },\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  17,\n                  583,\n                  218,\n                  635\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"летящих\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  216,\n                  582,\n                  253,\n                  636\n                ],\n                \"prob\": 0.9991,\n                \"text\": \"с\",\n                \"lang_prob\": 0.9825,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  255,\n                  584,\n                  360,\n                  634\n                ],\n                \"prob\": 0.999,\n                \"text\": \"юга.\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  361,\n                  582,\n                  503,\n                  636\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"Когда\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.9979,\n            \"line_coord\": [\n              17,\n              582,\n              503,\n              634\n            ]\n          },\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  16,\n                  646,\n                  317,\n                  700\n                ],\n                \"prob\": 0.9993,\n                \"text\": \"просыпается\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  318,\n                  646,\n                  520,\n                  700\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"природа\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  522,\n                  647,\n                  581,\n                  699\n                ],\n                \"prob\": 0.9994,\n                \"text\": \"от\",\n                \"lang_prob\": 0.9997,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.9984,\n            \"line_coord\": [\n              16,\n              646,\n              581,\n              699\n            ]\n          },\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  16,\n                  711,\n                  211,\n                  763\n                ],\n                \"prob\": 0.9995,\n                \"text\": \"зимнего\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  214,\n                  711,\n                  313,\n                  764\n                ],\n                \"prob\": 0.9995,\n                \"text\": \"сна,\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  313,\n                  712,\n                  404,\n                  762\n                ],\n                \"prob\": 0.9998,\n                \"text\": \"лес\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.9991,\n            \"line_coord\": [\n              16,\n              711,\n              404,\n              762\n            ]\n          },\n          {\n            \"words\": [\n              {\n                \"coord\": [\n                  15,\n                  773,\n                  310,\n                  828\n                ],\n                \"prob\": 0.6855,\n                \"text\": \"наполняется\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              },\n              {\n                \"coord\": [\n                  313,\n                  775,\n                  538,\n                  826\n                ],\n                \"prob\": 0.9932,\n                \"text\": \"весенней\",\n                \"lang_prob\": 0.9998,\n                \"lang\": \"rus\"\n              }\n            ],\n            \"line_prob\": 0.6808,\n            \"line_coord\": [\n              15,\n              773,\n              538,\n              826\n            ]\n          }\n        ]\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nThere is no text on the image\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/text/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_set_error_no_face.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": \"detailed\",\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\"\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nEmpty image\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/text/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@empty.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": \"detailed\",\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"empty image\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nInvalid JSON\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/text/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_set_ok.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": \"detailed\",\n  \"images\": [\n    {\n      \"name\": \"file1\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"could not get image by name file1: http: no such file\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n","url":"/en/ml/vision/service-management/vision-api-txt","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"6a8f8a26-ae07-4da4-8948-126a7acb0344","title":"Image processing methods","data":"The improve method is used to enhance photos.\n\nRequest\n\nAuthorization data is passed in the query string:\n\n| Parameter | Type | Meaning |\n| ------------- | ------- | -------------------------------------------------- |\n| oauth_token | string | OAuth2 access token (required non-empty) |\n| oauth_provider | string | OAuth2 provider (required non-empty) |\n\nSupported OAuth2 providers:\n\n| Provider | oauth_provider value | Getting a token |\n| ------------- | --------------------- | -------------------------------------------------- |\n| VK Cloud | mcs | See in the article |\n\nRequest parameters are passed in JSON format in the request body with name=\"meta\":\n\n| Parameter | Type | Meaning |\n| ------------ | ------------ | ------------------------------------ |\n| mode | []string | Types of objects to be searched for in the passed images (required non-empty) |\n| images | []image_meta | Transferred image metadata (required non-empty) |\n| rfactor | int | Resolution increase factor, can be either 2 or 4 (required non-empty for resolution mode) |\n| ftype | string | Image type, \"art\" or \"photo\" (required non-empty for resolution mode) |\n\nPossible values ​​for mode:\n\n| Parameter | Meaning |\n| ------------ | ------------------------ |\n| improve | Photo recovery |\n| resolution | Resolution increase |\n\nimage_meta parameters:\n\n| Parameter | Type | Meaning |\n| ------------ | ------- | -------------------------------------------------- |\n| name | string | Filenames to match files in request and response (required non-empty) |\n\nImages are passed in the body of the request, the values ​​of the name field must match those passed in images.\n\n\n\nThe method is subject to restrictions\n\nRequest example\n\ncurl -X 'POST'   'https://smarty.mail.ru/api/v1/photo/improve?oauth_token=&oauth_provider=mcs'   -H 'accept: application/json'   -H 'Content-Type: multipart/form-data'   -F 'file=@photo_imrove_improve_ok.jpg;type=image/jpeg'   -F 'meta={\n  \"mode\": [\n    \"improve\"\n  ],\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse\n\n| Parameter | Type | Meaning |\n| ------------ | ------- | ------------------------------------------------------- |\n| status | int | 200 on success, otherwise the error description will be in body |\n| body | string | Response body |\n\nresponse parameters:\n\n| Parameter | Type | Meaning |\n| ------------ | -------------------- | ---------------------------------- |\n| improve | []improve_object | Array of responses for improve mode |\n| resolution | []resolution_object | Array of responses for resolution mode |\n\nimprove_object parameters:\n\n| Parameter | Type | Meaning |\n| ------------------ | ------- | ---------------------------------------- |\n| status | enum | Execution result |\n| error | string | Text description of the error (optional) |\n| name | string | File name to match files in request and response |\n| improved | string | Jpeg picture of a photograph with defects corrected (base64). The field may be absent or empty if, according to the algorithm, there is no point in restoring the photo (it is already good) |\n| colorized_improved | string | Jpeg picture of a photograph with defects corrected and color restored (base64). The field may be absent or empty if, according to the algorithm, the photo does not make sense to restore and paint over |\n| colored | string | Jpeg picture of a photo with restored color (base64) |\n| bw | bool | True - the algorithm considers that it was given a black-and-white photo as input, false - algoritm believes that he was given a color photo at the entrance |\n\nresolution_object parameters:\n\n| Parameter | Type | Meaning |\n| ------------ | ------- | ----------------------------------------------------------- |\n| status | enum | Execution result |\n| error | string | Text description of the error (optional) |\n| name | string | File name to match files in request and response |\n| resolved | string | Jpeg picture of a photo with increased resolution (base64) |\n\nstatus parameters:\n\n| Parameter | Meaning |\n| ------------ | -------------------- |\n| 0 | Successfully |\n| 1 | Permanent error |\n| 2 | Temporary error |\n\nResponse example\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"improve\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"improved\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgrdN6OW4fD17xGW9rNFH51rZO6fc/wBJl/eVinwtrWeShPc10C/8gt6wK8CGx3H/2Q==\",\n        \"colorized_improved\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/8AXKOs6NWdR22KdVvT+ugy3tZoo/OtbJ3T7n+ky/vKxT4W1rPJQnua6Bf+QW9YFelTWhnY/9k=\",\n        \"colorized\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/XQZb2s0UfnWtk7p9z/SZf3lYp8La1nkoT3NdAv/ILesCvUpK6IP/Z\",\n        \"bw\": true\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nAdditional examples\n\nIncreasing the image resolution\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/photo/improve?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@photo_imrove_resolution_ok.jpeg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": [\n    \"resolution\"\n  ],\n  \"rfactor\": 2,\n  \"rtype\": \"photo\",\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"resolution\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"resolved\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/6a+9FFZSSUj0MPqf/9k=\"\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nIncorrect rfactor\n\nRequest example:\n\ncurl -X 'POST'   'https://smarty.mail.ru/api/v1/photo/improve?oauth_token=&oauth_provider=mcs'   -H 'accept: application/json'   -H 'Content-Type: multipart/form-data'   -F 'file=@photo_imrove_resolution_ok.jpeg;type=image/jpeg'   -F 'meta={\n  \"mode\": [\n    \"resolution\"\n  ],\n  \"rfactor\": 1010,\n  \"rtype\": \"photo\",\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"rfactor must be 2 or 4\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nInvalid image\n\nRequest example:\n\ncurl -X 'POST'   'https://smarty.mail.ru/api/v1/photo/improve?oauth_token=&oauth_provider=mcs'   -H 'accept: application/json'   -H 'Content-Type: multipart/form-data'   -F 'file=@empty.jpg;type=image/jpeg'   -F 'meta={\n  \"mode\": [\n    \"resolution\"\n  ],\n  \"rfactor\": 2,\n  \"rtype\": \"photo\",\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"empty image\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nInvalid meta parameter\n\nRequest example:\n\ncurl -X 'POST'   'https://smarty.mail.ru/api/v1/photo/improve?oauth_token=&oauth_provider=mcs'   -H 'accept: application/json'   -H 'Content-Type: multipart/form-data'   -F 'file=@photo_imrove_resolution_ok.jpeg;type=image/jpeg'   -F 'meta={\n  \"mode\": [\n    \"resolution\"\n  ],\n  \"rfactor\": 2,\n  \"rtype\": \"photo\",\n  \"images\": [\n    {\n      \"name\": \"file1\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"could not get image by name file1: http: no such file\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n","url":"/en/ml/vision/service-management/vision-image-api","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"e317a657-2b44-4cc2-80fb-810e2ef21ece","title":"Document recognition","data":"This method allows you to recognize, for example, the fields of a passport in a photo. Let's take a look at its usage in more detail below.\n\nHOST: https://smarty.mail.ru\n\nENDPOINT: /api/v1/docs/recognize\n\nRequest\n\nAuthorization data is passed in the query string:\n\n| Parameter | Type | Meaning |\n| ------------- | ------ | ------------------------------------ |\n| oauth_token | string | OAuth2 access token (required non-empty)|\n| oauth_provider | string | OAuth2 provider (required non-empty) |\n\nSupported OAuth2 providers:\n\n| Provider | oauth_provider value | Getting a token |\n| -------- | ---------------------- | ------------------------------------ |\n| VK Cloud | mcs | See in article|\n\nRequest parameters are passed in JSON format in the request body with name=\"meta\":\n\n| Parameter | Type         | Meaning                                                  |\n| --------- | ------------- | ------------------------------------------------------ |\n| images | []image_meta | ID matched to the person in the photo (required non-empty)|\n\nimage_meta parameters:\n\n| Parameter | Type | Meaning |\n| --------- | ------ | -------- |\n| name | string | Filenames to match files in request and response (required non-empty) |\n\nImages are passed in the body of the request, the values ​​of the name field must match those passed in images.\n\n\n\nThe method is subject to restrictions\n\nRequest example\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/docs/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@docs_recognize_ok.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"labels\": {\n          \"birthday\": [\n            \"10.04.1990\"\n          ],\n          \"birthplace\": [\n            \"ГОР.\",\n            \"МОСКВА\"\n          ],\n          \"code_of_issue\": [\n            \"459-653\"\n          ],\n          \"date_of_issue\": [\n            \"11.11.1995\"\n          ],\n          \"first_name\": [\n            \"ФОМА\"\n          ],\n          \"last_name\": [\n            \"КИНЯЕВ\"\n          ],\n          \"middle_name\": [\n            \"СЕМЕНОВИЧ\"\n          ],\n          \"number\": [\n            \"233675\"\n          ],\n          \"place_of_issue\": [\n            \"ГОРОДА\",\n            \"МОСКВЫ\",\n            \"ОДИНЦОВСКОГО\",\n            \"РАЙОНА\",\n            \"ОТДЕЛОМ\",\n            \"ВНУТРЕННИХ\",\n            \"ДЕЛ\"\n          ],\n          \"series_number\": [\n            \"560Р\"\n          ],\n          \"sex\": [\n            \"МУЖ.\"\n          ]\n        }\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nAdditional examples\n\nThe fields in the image are not recognized\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/docs/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@docs_recognize_not_doc.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"labels\": {}\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nInvalid image\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/docs/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@empty.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"could not get image by name file1: http: no such file\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nIncorrect JSON (name mismatch in meta and image)\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/docs/recognize?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_set_ok.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"images\": [\n    {\n      \"name\": \"file1\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"could not get image by name file1: http: no such file\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n","url":"/en/ml/vision/service-management/docs-recognition","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"b68d8f4f-d74a-4b9f-972f-08e26782d630","title":"License plate recognition","data":"License plate recognition is a special case of using the detect method — the method allows you to find various objects in a photo.\n\nThis method allows you to find various objects in the photo.\n\nRequest\n\nAuthorization data is passed in the query string:\n\n| Parameter | Type | Meaning |\n| ------------- | ------- | ---------------------------------------- |\n| oauth_token | string | OAuth2 access token (required non-empty) |\n| oauth_provider | string | OAuth2 provider (required non-empty) |\n\nSupported OAuth2 providers:\n\n| Provider | oauth_provider value | Getting a token |\n| ---------- | ---------------------- | ------------------------------------ |\n| mail.ru | mr | See in the article |\n| VK Cloud | mcs | See in the article (all VK Cloud clients) |\n\nRequest parameters are passed in JSON format in the request body with name=\"meta\":\n\n| Parameter | Type | Meaning |\n| ------------ | ------------ | --------------------- |\n| mode | []string | Types of objects to be searched for in the passed images (required non-empty) |\n| images | []image_meta | Transferred image metadata (required non-empty) |\n\nPossible values ​​for mode:\n\n| Parameter | Meaning |\n|-------------|-------------------------------------------|\n| object | Search objects in the image |\n| scene | Search on scene image |\n| car_number | Search on the image for car numbers |\n| multiobject | Search on the image for multi-objects - objects and the whole set of boxes of all found objects |\n| pedestrian | Search for people in the image (more precisely determines the set of boxes of all people in the image) |\n\nmode may contain one or more modes. For example:\n\n\"mode\":[\"object\"]\n\nThe method is subject to restrictions.\n\nRequest example\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/objects/detect?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@objects_detect_ok_car_number.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": [\n    \"car_number\"\n\n  ],\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse\n\n| Parameter | Type | Meaning |\n| ------------ | ------- | ------------------------------------------------------- |\n| status | int | 200 on success, otherwise the error description will be in body |\n| body | string | Response body |\n\nresponse parameters:\n\n| Parameter | Type | Meaning |\n| ------------------ | -------- | ---------------------- |\n| scene labels | [] object | Array of responses for each scene file (may be missing) |\n| object labels | [] object | Array of responses for each file with objects (may be missing) |\n| car_number_labels | [] object | Array of responses for each file with machine numbers (may be missing) |\n| multiobject labels | [] object | Array of responses for each file with multi-objects (may be missing) |\n| pedestrian labels | [] object | Array of responses for each file with people (may be missing) |\n\nobject parameters:\n\n| Parameter | Type | Meaning |\n| ---------| -------- | ----------------------------------------------------- |\n| status | enum | Execution result |\n| error | string | Text description of the error (optional) |\n| name | string | File name to match files in request and response |\n| labels | [] label | List of objects (marks) found on the image |\n| count_by_density | int | The number of people in the frame, calculated using the density map (only for mode=\"pedestrian\") |\n\nstatus parameters:\n\n| Parameter | Meaning |\n| ------------ | -------------------- |\n| 0 | Successfully |\n| 1 | Permanent error |\n| 2 | Temporary error |\n\nlabel parameters:\n\n| Parameter | Meaning |\n| ------------- | ------------------------------------------------------------------------ |\n| eng | Label (name) for the found object in English |\n| rus | Label (name) of the found object in Russian |\n| eng_categories | List of categories (each category includes many tags) in English (optional) |\n| eng_categories | List of categories (each category includes many tags) in Russian (optional) |\n| sample | Degree of confidence that this object is in the image |\n| coordinate | Found object coordinates (optional) |\n| types_prob | An array of license plate type probabilities. currently the following types are supported: \"rus\" - all types of Russian license plates, \"cis\" - CIS license plates (except individual and Ukrainian military ones), \"eu\" - one-storey European license plates (optional, only for car_number mode) |\n\nResponse example\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"car_number_labels\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"labels\": [\n          {\n            \"eng\": \"MA77K0S\",\n            \"rus\": \"\",\n            \"prob\": 0.7194,\n            \"coord\": [\n              346,\n              111,\n              356,\n              115\n            ],\n            \"types_prob\": [\n              {\n                \"type\": \"ru\",\n                \"prob\": 0.3256\n              },\n              {\n                \"type\": \"cis\",\n                \"prob\": 0.9272\n              },\n              {\n                \"type\": \"eu\",\n                \"prob\": 0.5094\n              }\n            ]\n          },\n          {\n            \"eng\": \"K777\",\n            \"rus\": \"\",\n            \"prob\": 0.8366,\n            \"coord\": [\n              323,\n              109,\n              331,\n              117\n            ],\n            \"types_prob\": [\n              {\n                \"type\": \"ru\",\n                \"prob\": 0.0054\n              },\n              {\n                \"type\": \"cis\",\n                \"prob\": 0.3624\n              },\n              {\n                \"type\": \"eu\",\n                \"prob\": 0.8705\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nAdditional examples\n\nThe car number is not in the image\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/objects/detect?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_set_error_no_face.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": [\n    \"car_number\"\n  ],\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"car_number_labels\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\"\n      }\n    ]\n  },\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nEmpty image\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/objects/detect?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@empty.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": [\n    \"car_number\"\n\n  ],\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"empty image\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n\nInvalid JSON (mismatch of the file name with the form)\n\nRequest example:\n\ncurl -X 'POST' \\\n  'https://smarty.mail.ru/api/v1/objects/detect?oauth_token=&oauth_provider=mcs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@persons_set_ok.jpg;type=image/jpeg' \\\n  -F 'meta={\n  \"mode\": [\n    \"car_number\"\n\n  ],\n  \"images\": [\n    {\n      \"name\": \"file1\"\n    }\n  ]\n}'\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"could not get image by name file1: http: no such file\",\n  \"htmlencoded\": false,\n  \"last_modified\": 0\n}\n","url":"/en/ml/vision/service-management/plate-recognition","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"5dfd74b3-21f6-4c48-a9c5-8e34414613cc","title":"Text recognition in photos","data":"The method allows you to recognize:\n\ntext in an photos taken on the street (scene_text).\nhandwritten text.\n\nHOST: https://smarty.mail.ru\n\nENDPOINT: /api/v1/scene_text/recognize\n\nRequest\n\nAuthorization data is passed in the query string:\n\n| Parameter      | Type   | Description                              |\n| -------------- | ------ | ---------------------------------------- |\n| oauth_token    | string | OAuth2 access token (required non-empty) |\n| oauth_provider | string | OAuth2 provider (required non-empty)     |\n\nSupported OAuth2 providers:\n\n| Provider | oauth_provider value | Getting a token                                       |\n| -------- | -------------------- | ----------------------------------------------------- |\n| VK Cloud | mcs                  | See in the article |\n\nRequest parameters are passed in JSON format in the request body:\n\n| Parameter      | Type   | Required | Description                                               |\n| -------------- | ------ | ---------| --------------------------------------------------------- |\n| file           | string | Yes      | An array of files. The file names should be different     |\n| meta           | object | Yes      | Request body                                              |\n\nThe remaining parameters are passed to name=\"meta\":\n\n| Parameter      | Type   | Required | Description                                               |\n| -------------- | ------ | ---------| --------------------------------------------------------- |\n| images        | []image_meta  | Yes      | Metadata of transmitted images                    |\n| lang          | string          | No       | The expected language of the text in the photo: rus — Russian, eng — English. When specifying a parameter, the recognition accuracy increases |\n\nParameters of image_meta:\n\n| Parameter      | Type   | Required | Description                                               |\n| -------------- | ------ | ---------| --------------------------------------------------------- |\n|   name         | string | Yes      | File names for matching files in the request and response |\n\n\n\nThe method is subject to restrictions.\n\nRequest example\n\ncurl -X 'POST' \\\ncurl -X POST \"https://smarty.mail.ru/api/v1/scene_text/recognize?oauth_token=&oauth_provider=mcs\" \\\n -H \"Accept: application/json\" \\\n -H \"Content-Type: multipart/form-data\" \\\n -F \"file=@image3.jpg\" \\\n -F \"meta={\n  \"images\": [\n    {\n      \"name\": \"file\",\n      \"lang\": \"eng\"\n    }\n  ]\n}\"\n\nResponse\n\n| Parameter     | Type     | Description                                              |\n| ------------- | -------- | -------------------------------------------------------- |\n| status        | int      | The status code of the completed operation               |\n| body          | object   | Response body                                            |\n| objects       | array    | Array of results for each file                           |\n\nPossible status values:\n\n200 — successful interaction with Vision servers. For all other statuses, the error description is given in body.\n400 — invalid request: check the syntax of the entered data.\n403 — access denied: update the access token or choose another provider.\n500 — internal server error.\n\nParameters of objects:\n\n| Parameter     | Type     | Required | Description                                                 |\n| ------------- | -------- |--------- | ----------------------------------------------------------- |\n| status        | int      | Yes      | The status code of the completed operation: 0 — successful, 1 — permanent error, 2 — temporary error |\n| name          | string   | Yes      | The file name for matching files in the request and response |\n| words         | array    | Yes      | Array of recognized words in a string                        |\n\nParameters of words:\n\n| Parameter     | Type     | Required | Description                                                 |\n| ------------- | -------- |--------- | ----------------------------------------------------------- |\n| prob          | float    | Yes      | String Recognition Confidence                                |\n| coord         | int64| Yes      | Coordinates of the word — [[x1, y1], [x2, y2], [x3, y3], [x4, y4]] |\n| text          | string   | Yes      | Recognized response word                                     |\n\nResponse example\n\n{\n  \"status\": 200,\n  \"body\": {\n    \"objects\": [\n      {\n        \"status\": 0,\n        \"name\": \"file\",\n        \"words\": [\n          {\n            \"coord\": [\n              [\n                314,\n                395\n              ],\n              [\n                453,\n                395\n              ],\n              [\n                453,\n                433\n              ],\n              [\n                314,\n                433\n              ]\n            ],\n            \"prob\": 0.9947941769563452,\n            \"text\": \"SAMSUNG\"\n          }\n        ]\n      }\n    ]\n  }\n}\n\nAdditional examples\n\n\n    There is no text on the image\n\nRequest example:\n\ncurl -X POST \"https://smarty.mail.ru/api/v1/scene_text/recognize?oauth_token=&oauth_provider=mcs\" \\\n -H \"Accept: application/json\" \\\n -H \"Content-Type: multipart/form-data\" \\\n -F \"file=@image.jpg\" \\\n -F \"meta={\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}\"\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"empty image\"\n}\n\n\n\n\n    Invalid JSON\n\nRequest example:\n\ncurl -X POST \"https://smarty.mail.ru/api/v1/scene_text/recognize?oauth_token=&oauth_provider=mcs\" \\\n -H \"Accept: application/json\" \\\n -H \"Content-Type: multipart/form-data\" \\\n -F \"file=@image3.jpg\" \\\n -F \"meta={\n  \"images\": [\n    {\n      \"name\": \"file1\"\n    }\n  ]\n}\"\n\nResponse example:\n\n{\n  \"status\": 400,\n  \"body\": \"could not get image by name file1: http: no such file\"\n}\n\n\n","url":"/en/ml/vision/service-management/scene-text-recognition","description":"Definition of text in an image taken on the street (scene_text) and handwritten text.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"71566a39-1722-4366-80fe-92061240fed2","title":"Content recognition 18+","data":"This method allows you to find out if there is prohibited content (18+) in the photo.\n\nHOST: https://smarty.mail.ru\n\nENDPOINT: /api/v1/adult/detect\n\nRequest\n\nAuthorization data is passed in the query string:\n\n| Parameter      | Type   | Description                              |\n| -------------- | ------ | ---------------------------------------- |\n| oauth_token    | string | OAuth2 access token (required non-empty) |\n| oauth_provider | string | OAuth2 provider (required non-empty)     |\n\n\n\nObtaining an access token, as well as supported OAuth2 providers are given in the article Authorization.\n\n\n\nRequest parameters are passed in JSON format in the request body:\n\n| Parameter      | Type   | Required       | Description                                              |\n| -------------- | ------ | -------------- | -------------------------------------------------------- |\n| file           | string | Yes            | An array of files. The file names should be different    |\n| meta           | object | Yes            | Request body                                             |\n|  images        | array  | Yes            | Metadata of transmitted images                           |\n|   name         | string | Yes            | File names for matching files in the request and response|\n\n\n\nThe method is subject to restrictions.\n\nRequest example\n\ncurl -X POST \"https://smarty.mail.ru/api/v1/adult/detect?oauth_token=your_token&oauth_provider=mcs\" \\\n -H \"Accept: application/json\" \\\n -H \"Content-Type: multipart/form-data\" \\\n -F \"file=@lena_color.png\" \\\n -F \"meta={\n  \"images\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}\"\n\nResponse\n\n| Parameter     | Type     | Description                                              |\n| ------------- | -------- | -------------------------------------------------------- |\n| status        | int      | The status code of the completed operation               |\n| body          | object   | Response body                                            |\n| objects       | array    | Array of results for each file                           |\n\nstatus\n\nPossible status values:\n\n200 — successful interaction with Vision servers. For all other statuses, the error description is given in body.\n400 — invalid request: check the syntax of the entered data.\n403 — access denied: update the access token or choose another provider.\n500 — internal server error.\n\nobjects\n\n| Parameter     | Type     | Required | Description                                                 |\n| ------------- | -------- |--------------- | -------------------------------------------------------- |\n| status        | int      | Yes             | The status code of the completed operation: 0 — successful, 1 — permanent error, 2 — temporary error |\n| error         | string   | Yes             | Text description of the error                               |\n| name          | string   | Yes             | The file name for matching files in the request and response|\n| safe          | number   | Yes             | The degree of confidence that there is no content in the picture is 18+; the value in the segment [0;1] |\n\nResponse example\n\n{\n   \"status\": 200,\n   \"body\": {\n      \"objects\": [\n         {\n            \"status\": 0,\n            \"name\": \"file\",\n            \"safe\": 0.010846120305359364\n         }\n      ]\n   }\n}\n","url":"/en/ml/vision/service-management/nsfw-recognition","description":"Vision API methods for recognizing 18+ content in a photo","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"f13ceab2-fc1b-4026-9ab2-132e123f6bcc","title":"Example integration on Python","data":"Let's try to recognize the faces in the photo:\n\n\n\nDownload the file smarty.py.\n\n\npython examples/python/smarty.py\\\n-u \"https://smarty.mail.ru/api/v1/persons/recognize?oauth_provider=\"mcs&oauth_token=e50b000614a371ce99c01a80a4558d8ed93b313737363830\" \\\n-p examples/friends1.jpg \\\n--meta '{\"space\":\"1\", \"create_new\":false}' \\\n-v\n\nWe get the answer:\n\n\n{\n\"status\":200,\n\"body\":{\n\"objects\": [\n{\nstatus:0,\n\"name\":\"file_0\",\n\"persons\":[\n{\"tag\":\"undefined\",\"coord\":[102,30,184,134],\"confidence\":0.99999,\"awesomeness\":0.5025},\n{\"tag\":\"undefined\",\"coord\":[393,74,461,166],\"confidence\":0.99987,\"awesomeness\":0.548},\n{\"tag\":\"undefined\",\"coord\":[458,48,535,149],\"confidence\":0.99976,\"awesomeness\":0.4766},\n{\"tag\":\"undefined\",\"coord\":[273,45,352,147],\"confidence\":0.99963,\"awesomeness\":0.504},\n{\"tag\":\"undefined\",\"coord\":[525,81,600,184],\"confidence\":0.99954,\"awesomeness\":0.4849},\n{\"tag\":\"undefined\",\"coord\":[194,76,258,167],\"confidence\":0.9984,\"awesomeness\":0.5725}\n]\n}\n],\n\"aliases_changed\":false\n},\n\"htmlencoded\":false\n\"last_modified\":0\n}\n\nAs you can see, there were 6 faces in the photo, but they are all \"undefined\". What does \"undefined\" mean? It means that there are no recognized persons in the database yet.\n\nPay attention to the parameters \"space\":\"1\" and \"create_new\":false, their meaning will soon become clear from the examples.\n\nAdding a person to the database\n\nLet's try to add a person to the database. There is one person in this photo:\n\n\n\nWe will establish that this person has id=1 in the database. Let's do this with the following request and the \"person_id\":1 parameter.\n\npython examples/python/smarty.py\\\n-u \"https://smarty.mail.ru/api/v1/persons/set?oauth_provider=mcs&oauth_token=e50b000614a371ce99c01a80a4558d8ed93b313737363830\" \\\n-p examples/rachel-green.jpg \\\n--meta '{\"space\":\"1\", \"images\":[{\"person_id\":1}]}' \\\n-v\n\nIf the request is correct, the response will be:\n\n{\n\"status\":200,\n\"body\":{\n\"objects\":[\n{\"status\":0,\"name\":\"file_0\"}\n]\n},\n\"htmlencoded\":false\n\"last_modified\":0\n}\n\nNow let's try to search for faces again. Run the same query:\n\npython examples/python/smarty.py\\\n-u \"https://smarty.mail.ru/api/v1/persons/recognize?oauth_provider=mcs&oauth_token=e50b000614a371ce99c01a80a4558d8ed93b313737363830\" \\\n-p examples/friends1.jpg \\\n--meta '{\"space\":\"1\", \"create_new\":false}' \\\n-v\n\nThis time we get the answer:\n\n{\n\"status\":200,\n\"body\":{\n\"objects\": [\n{\nstatus:0,\n\"name\":\"file_0\",\n\"persons\":[\n{\"tag\":\"undefined\",\"coord\":[102,30,184,134],\"confidence\":0.99999,\"awesomeness\":0.5025},\n{\"tag\":\"undefined\",\"coord\":[393,74,461,166],\"confidence\":0.99987,\"awesomeness\":0.548},\n{\"tag\":\"undefined\",\"coord\":[458,48,535,149],\"confidence\":0.99976,\"awesomeness\":0.4766},\n{\"tag\":\"undefined\",\"coord\":[273,45,352,147],\"confidence\":0.99963,\"awesomeness\":0.504},\n{\"tag\":\"undefined\",\"coord\":[525,81,600,184],\"confidence\":0.99954,\"awesomeness\":0.4849},\n{\"tag\":\"person1\",\"coord\":[194,76,258,167],\"confidence\":0.9984,\"awesomeness\":0.5725}\n]\n}\n],\n\"aliases_changed\":false\n},\n\"htmlencoded\":false\n\"last_modified\":0\n}\n\nNow in the photo with six persons, we find the person we loaded with the persons/set method. We loaded a person with id=1 into the database, and now it is recognized:\n\n\"tag\":\"person1\"\n\ncreate_new parameter\n\nNow let's go back to the very beginning and imagine that we haven't loaded the personas into the database yet. Let's try to execute the query with \"create_new\":true:\n\npython examples/python/smarty.py\\\n-u \"https://smarty.mail.ru/api/v1/persons/recognize?oauth_provider=mcs&oauth_token=e50b000614a371ce99c01a80a4558d8ed93b313737363830\" \\\n-p examples/friends1.jpg \\\n--meta '{\"space\":\"1\", \"create_new\":true}' \\\n-v\n\nIn response:\n\n{\n\"status\":200,\n\"body\":{\n\"objects\": [\n{\nstatus:0,\n\"name\":\"file_0\",\n\"persons\":[\n{\"tag\":\"person1\",\"coord\":[102,30,184,134],\"confidence\":0.99999,\"awesomeness\":0.5025},\n{\"tag\":\"person2\",\"coord\":[393,74,461,166],\"confidence\":0.99987,\"awesomeness\":0.548},\n{\"tag\":\"person3\",\"coord\":[458,48,535,149],\"confidence\":0.99976,\"awesomeness\":0.4766},\n{\"tag\":\"person4\",\"coord\":[273,45,352,147],\"confidence\":0.99963,\"awesomeness\":0.504},\n{\"tag\":\"person5\",\"coord\":[525,81,600,184],\"confidence\":0.99954,\"awesomeness\":0.4849},\n{\"tag\":\"person6\",\"coord\":[194,76,258,167],\"confidence\":0.9984,\"awesomeness\":0.5725}\n]\n}\n],\n\"aliases_changed\":false\n},\n\"htmlencoded\":false\n\"last_modified\":0\n}\n\nIt can be seen that all found persons are automatically registered in the database, and they are assigned id. If Rachel is now searched using the persons/recognize method, she will now be recognized as \"person6\".\n\nspace parameter\n\nAllows you to create non-overlapping sets of persons. For example, in \"space\":\"1\" you can load (persons/set) all the characters of the series \"Friends\", and in \"space\":\"2\" - all the characters of the series \"Brigade\". Now you can create an automatic access system:\n\nin a Hollywood movie studio, look for (call persons/recognize) in \"space\":\"1\",\nin Moscow - in \"space\": \"2\".\n\nRemoving a person from the database\n\nIf you need to clean up the space (space) or some person was added to the database by mistake, you must use the request persons/delete.\n\npython examples/python/smarty.py\\\n-u \"https://smarty.mail.ru/api/v1/persons/delete?oauth_provider=mcs&oauth_token=e50b000614a371ce99c01a80a4558d8ed93b313737363830\" \\\n--meta '{\"space\":\"1\", \"images\":[{\"name\":\"myname\", \"person_id\":1}]}' \\\n-v\n\nIf the request is correct, the response will be:\n\n\n{\n\"status\":200,\nbody:\n{\n\"objects\":[{\"status\":0,\"name\":\"myname\"}]\n},\n\"htmlencoded\":false\n\"last_modified\":0\n}\n\nFor this request, it is not required to transfer a picture, only meta-information indicating the id of the person to be deleted is enough.\n\nCurrently, this request needs to be made using multipart/form\"-data, just like the previous requests.\n\nIn the future, it is planned to be able to make such requests (which do not require transferring a file to the server) through a regular application/json or even with a regular GET request.\n","url":"/en/ml/vision/how-to-guides/vision-py","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"0123e019-bba7-41ae-9385-a16b63d7002f","title":"Example integration on PHP","data":"For a detailed example, see the article \"Python integration example\". In this article, we will execute persons/recognize, persons/set, and persons/delete requests. This will require:\n\nsudo apt-get install php5-cli\nsudo apt-get install php5-curl\n\nRequest for recognition\n\nDownload the file smarty.php.\n\nphp examples/php/smarty.php\\\n\"https://smarty.mail.ru/api/v1/persons/recognize?oauth_provider=mr&oauth_token=\"e50b000614a371ce99c01a80a4558d8ed93b313737363830\" \\\nexamples/friends1.jpg \\\n'{\"space\":\"1\", \"images\":[{\"name\":\"examples/friends1.jpg\"}]}'\n\nAnswer:\n\n{\n\"status\":200,\n\"body\":{\n\"objects\":[{\nstatus:0,\n\"name\":\"examples/friends1.jpg\",\n\"persons\":[\n{\"tag\":\"undefined\",\"coord\":[102,30,184,134],\"confidence\":0.99999,\"awesomeness\":0.5025},\n{\"tag\":\"undefined\",\"coord\":[393,74,461,166],\"confidence\":0.99987,\"awesomeness\":0.548},\n{\"tag\":\"undefined\",\"coord\":[458,48,535,149],\"confidence\":0.99976,\"awesomeness\":0.4766},\n{\"tag\":\"undefined\",\"coord\":[273,45,352,147],\"confidence\":0.99963,\"awesomeness\":0.504},\n{\"tag\":\"undefined\",\"coord\":[525,81,600,184],\"confidence\":0.99954,\"awesomeness\":0.4849},\n{\"tag\":\"undefined\",\"coord\":[194,76,258,167],\"confidence\":0.9984,\"awesomeness\":0.5725}\n]}\n],\n\"aliases_changed\":false\n},\n\"htmlencoded\":false\n\"last_modified\":0\n}\n\nRequest to add to database\n\nphp examples/php/smarty.php\\\n\"https://smarty.mail.ru/api/v1/persons/set?oauth_provider=mr&oauth_token=\"e50b000614a371ce99c01a80a4558d8ed93b313737363830\" \\\nexamples/rachel-green.jpg \\\n'{\"space\":\"1\", \"images\":[{\"name\":\"examples/rachel-green.jpg\", \"person_id\":1}]}'\n\nIf the request is correct, the response will be:\n\n{\n\"status\":200,\n\"body\":{\n\"objects\":[\n{\"status\":0,\"name\":\"examples/rachel-green.jpg\"}\n]\n},\n\"htmlencoded\":false\n\"last_modified\":0\n}\n\nRequest to delete from database\n\nThis request does not require a file, so we pass an empty string instead of the path:\n\nphp examples/php/smarty.php\\\n\"https://smarty.mail.ru/api/v1/persons/delete?oauth_provider=mr&oauth_token=\"e50b000614a371ce99c01a80a4558d8ed93b313737363830\" \\\n\"\" \\\n'{\"space\":\"1\", \"images\":[{\"name\":\"examples/rachel-green.jpg\", \"person_id\":1}]}'\n\nIf the request is correct, the response will be:\n\n\n{\n\"status\":200,\nbody:\n{\n\"objects\":[{\"status\":0,\"name\":\"examples/rachel-green.jpg\"}]\n},\n\"htmlencoded\":false\n\"last_modified\":0\n}\n","url":"/en/ml/vision/how-to-guides/vision-php","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"b0993bdf-23a0-40e7-a16e-8f4b56ab6d48","title":"Using Curl","data":"one\\. License plates:\n\nRequest:\n\ncurl -k -v \"https://smarty.mail.ru/api/v1/objects/detect?oauth_provider=mcs&oauth_token=xxx\" \\\n   -F file_0=@examples/car_number1.jpg \\\n   -F file_1=@examples/x5.png \\\n   -F meta='{ \\\n                \"mode\":[\"car_number\"], \\\n                \"images\":[ \\\n                   { \\\n                     \"name\":\"file_1\" \\\n                   },\\\n                   { \\\n                      \"name\":\"file_0\" \\\n                   }]\\\n             }'\n\nAnswer:\n\n{\n  \"status\":200,\n  body:\n  {\n    \"car_number_labels\":[\n    {\n      status:0,\n      \"name\":\"file_0\",\n      labels:[\n         {\n           \"eng\":\"K200PT98\",\n           \"rus\":\"K200RT98\",\n           \"prob\":0.7168,\n           \"coord\":[24,70,220,117]\n         }]\n    },\n    {\n      status:0,\n      \"name\":\"file_1\",\n      labels:[\n        {\n          \"eng\":\"M505KC99\",\n          \"rus\":\"M505KS99\",\n          \"prob\":0.8434,\n         \"coord\":[955,427,1045,477]\n        }]\n    }]\n  },\n  \"htmlencoded\":false\n  \"last_modified\":0\n}\n\n2\\. Objects+scenes:\n\ncurl -k -v \"https://smarty.mail.ru/api/v1/objects/detect?oauth_provider=mcs&oauth_token=xxx\" -F file_0=@examples/car_number1.jpg -F file_1=@examples/x5.png -F meta='{\"mode\":[\"object\", \"scene\"],\"images\":[{\"name\":\"file_1\"}, {\"name\":\"file_0\"}]}'\n\n3\\. Persons:\n\ncurl -k -v \"https://smarty.mail.ru/api/v1/persons/recognize?oauth_provider=mcs&oauth_token=xxx\" -F file_0=@examples/friends1.jpg -F file_1=@examples/rachel-green .jpg -F meta='{\"images\":[{\"name\":\"file_1\"}, {\"name\":\"file_0\"}], \"space\":\"1\"}'\n","url":"/en/ml/vision/how-to-guides/vision-curl","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"06d9d61f-758a-45da-9f75-c947b3dec47f","title":"Release notes","data":"03.08.23\n\nAdded automatic text language detection for the scene_text/recognize method. Manual indication of the text language is available when adding the lang parameter to the request.\nExpanded the selection list of object detection models for the objects/detect method. The object2 option for the mode parameter has been added to the API.\n\nFor more details, see API specifications.\n\n15.06.23\n\nAdded handwriting recognition support for the scene_text/recognize method.\n\n06.04.23\n\nImplemented a text recognition method on an image taken on the street. Read more about the method in the article Text recognition in photos.\nUpdated the list of restrictions.\n\n16.12.22\n\nThe 18+ content recognition method is implemented, more about the method in the article Content recognition 18+.\n","url":"/en/ml/vision/changelog","description":"Release notes for Vision.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"d4a7f2a7-dac7-4acd-9d97-4e13535b4631","title":"Tariffication","data":"API Vision (photo)\n\n| Level | Transactions per month | Price per transaction |\n|---|---|---|\n| Standard | 1 - 1,000,000 | 0.06 ₽ |\n| Standard | 1,000,001 - 10,000,000 | 0.05 ₽ |\n| Standard | 10,000,001 - 100,000,000 | 0.04 ₽ |\n| Standard | from 100 000 001 | 0.03 ₽ |\n","url":"/en/ml/vision/tariffication","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Machine learning in the Cloud","link":"/en/ml","isActive":false,"evenSimpler":true},{"title":"Vision","link":"/en/ml/vision","isActive":false,"evenSimpler":true}]},{"uuid":"66721e11-9a49-45b8-9f69-5ba973d437c8","title":"Quick start","data":"Connect the service: if the service is in beta testing, send a request to technical support.\nCreate a VM from a public Linux image with the internet access. The example below uses the Ubuntu 22.04 image.\nConnect to the VM via SSH.\nInstall Fluent Bit version 2.1.9 on the VM:\n\n      curl https://cloudlogging.hb.ru-msk.vkcs.cloud/fluent-bit-install-scripts/install.sh | FLUENT_BIT_RELEASE_VERSION=2.1.9 sh\n\nGenerate credentials for connecting to the service.\nInstall and configure the vkcloudlogs-fluent-bit-plugin plugin.\n","url":"/en/monitoring-services/logging/quick-start","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Logging","link":"/en/monitoring-services/logging","isActive":false,"evenSimpler":true}]},{"uuid":"ac6cfa5b-10a1-4012-b7d5-6b26d7bfeff8","title":"About the service","data":"Service Description\n\nCloud Logging is a service for working with user application logs on VMs and in VK Cloud. It provides collecting, storing, reading, displaying and processing logs in a single interface in the cloud.\n\nThe service centralizes the logs of the entire product system in a single control panel, stores records and makes it convenient to search and work with them. It also helps to control the operation of applications and debug them if necessary.\n\nAdvantages\n\nReal-time data. You have access to the logs of all your applications or services in real time.\n\nConvenient search. In Cloud Logging, you can search, view and analyze logs using text search with additional filters.\n\nFlexible settings. You can customize the service for your tasks using the API for reading and collecting logs. For example, you can set the storage time for logs, set up roles for working with logs.\n\nAvailability and reliability. We will ensure the operability and fault tolerance of the service and guarantee its availability according to the SLA.\n\nSafety. In Cloud Logging, all data is securely protected: we store logs of each application group isolated from each other in the cloud. Tier III level data centers are located in Russia.\n","url":"/en/monitoring-services/logging/concepts/about","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Logging","link":"/en/monitoring-services/logging","isActive":false,"evenSimpler":true}]},{"uuid":"7f6e4bac-73e6-4caf-b207-ce71c78012ab","title":"Logging plugin","data":"Sending user application logs to the Cloud Logging service is enabled and configured via the vkcloudlogs-fluent-bit-plugin logging plugin.\nIt works with the Golang plugin interface provided by Fluent Bit.\n\n{heading(Plugin authorization parameters)[id=auth_parameters]}\n\n| Parameter | Required? | Description | Where to find |\n| -------- | ------------- | -------- | --------- |\n| auth_url | Yes | The Keystone service endpoint | The Auth URL parameter in your VK Cloud personal account |\n| project_id | Yes | An identifier of a VK Cloud project to which logs will be recorded | The Project ID parameter in your VK Cloud personal account.Example: a1b2c3d4e5f6g7h8i9a1b2c3d4e5f6g7.Do not confuse with Project Name of the form mcs1234567890 |\n| user_id | No | An identifier of a user on whose behalf logs will be recorded | Created in your VK Cloud personal account on the User credentials generation tab |\n| user_name | No | A login of a user on whose behalf logs will be recorded | The Username parameter in your VK Cloud personal account |\n| password | No | The password of the user specified in user_id or user_name | For user_id, the password is created in your VK Cloud personal account on the User credentials generation tab. For user_name, the password of your VK Cloud personal account is used |\n| key_file | No | A name of a JSON file that contains the user_id and password values |  |\n| internal | No | The parameter indicating whether the recording of technical service logs will be enabled: - true — enabled - false — disabledBy default: true |  |\n\nAcceptable options for specifying user credentials:\n\nuser_id and password;\nkey_file;\nuser_name and password (for example, your username and password to sign in to your VK Cloud personal account).\n\n\n\nAccounts generated for the Cloud Logging service have only the right to record logs. Therefore, authorization with their use is recommended as more secure.\n\nPlugin configuration parameters\n\n| Parameter | Required? | Description |\n| -------- | ------------- | -------- |\n| server_host_port | Yes | The Cloud Logging service address (cloudlogs.mcs.mail.ru:443) |\n| service_id | No | A service ID in the logging system: - databases — the Cloud Databases service - containers — the Cloud Containers service - bigdata — the Cloud Big Data service - vdi — the Cloud Desktop serviceIf not specified, the default value will be assigned.If necessary, create your own IDs through technical support or by yourself on the Other resources tab in the settings of the  Monitoring → Logging section |\n| group_id | No | A log group identifier. By default: a Tag parameter value assigned to an event in Fluent Bit |\n| group_id_key | No | A name of a parameter containing the log group identifier. Used if the group_id parameter is not specified |\n| stream_id | No | A log source identifier, for example: an instance ID (instance_id) or a VM ID (vm_id). By default: empty |\n| stream_id_key | No | A name of a parameter containing the log source identifier. Used if the stream_id parameter is not specified |\n| message_key | No | A name of a parameter containing a message that will be added to each log entry. By default: message |\n| level_key | No | A name of a parameter containing a logging level value. By default: level |\n| default_level | No | The logging level value. Used if there is no parameter with the name specified in level_key. By default: debug |\n| default_payload | No | A JSON string containing key/value pairs that will be added to the payload field of every log entry.Example: {\"tag\": \"example\", \"case\": 3}By default: an empty string |\n| tls_on | No | The parameter indicating whether TLS is enabled for the server_host_port address: - true — enabled - false — disabledBy default: true |\n| tls_verify | No | The parameter indicating whether the TLS certificate verification is enabled for the server_host_port address: - true — enabled - false — disabledBy default: true |\n\nExamples of setting up the plugin using its authorization and configuration parameters are given in the Plugin installation и Managing the logging agent sections.\n","url":"/en/monitoring-services/logging/concepts/logging-plugin","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Logging","link":"/en/monitoring-services/logging","isActive":false,"evenSimpler":true}]},{"uuid":"6694b5c1-0cf4-4005-8cdb-61877a9e813e","title":"The language of search queries","data":"The search language allows you to:\n\nsearch by one or more filter expressions,\napply logical and comparison operations.\n\nTo filter logs, use the filter parameters in the search field:\n\n| Parameter              | Format                                    | Description | Example |\n| ---------------------- | ----------------------------------------- | ------ | ------ |\n| message              | message: \"value\" | Search for records that have the specified values in their messages. It is a default parameter, you do not need to specify it in the request | message: \"Hello world!\" |\n| timestamp            | timestamp  \"value\" | Search for records sent in a given time interval | timestamp >= \"2022-04-10T00:00:00Z\" |\n| level                | level  value | Search for records with specified logging levels. Available logging levels — DEBUG, INFO, WARN, ERROR, CRITICAL | level = INFO |\n| payload              | payload: \"value\" | Search for records whose payload text contains specified values | payload: warning |\n| payload.field.search | payload.: \"value\" | Lexicographic search of records by payload elements. The payload prefix can be omitted if the root element of the tree does not match any of the parameters. You can check the presence of an element in the payload using the EXISTS operator, for example:payload.result EXISTS. Such a filter will output records that have a result element in their payload | payload.status: created |\n\nTo filter records by multiple values of the same parameter, list the values one by one.\n\n\n\nIf you do not specify an expression with fields in the search, then by default the search will be based on a substring in the message field.\n\nRequirements\n\nThe characters \", ' and ＼ in an expression are escaped using ＼.\nThe value does not need to be enclosed in quotes:\n\n   if it starts with a letter of the Latin alphabet and contains only letters of the Latin alphabet, numbers and underscores;\n   if it is an unsigned integer.\n\nAvailable values for timestamp: RFC3339.\n\nComparison operators\n\n| Operator | Decoding |\n| -------- | ----------- |\n| = | Equally |\n| `` | Not equal |\n| > | Greater than |\n| = | Greater than or equal to |\n| : | Contains |\n\nFilters with multiple conditions\n\nYou can combine several conditions in one filter using logical operators:\n\nAND\nOR\nNOT\n\nThe order of calculations\n\nUse parentheses to specify a specific order of calculations:\n\nparameter1: \"value1\" AND (parameter2 = \"value2\" OR parameter3 < \"value3\")\n","url":"/en/monitoring-services/logging/concepts/search-tools","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Logging","link":"/en/monitoring-services/logging","isActive":false,"evenSimpler":true}]},{"uuid":"57261c2d-bda2-4b5f-ab91-efddbf5f2615","title":"Generation of credentials","data":"The following credentials are used to record logs to the Cloud Logging service:\n\nuser_id — ID of the user on whose behalf logs will be recorded;\npassword — password of the specified user;\nproject_id — identifier of your VK Cloud project in OpenStack.\n\nTo generate the credentials for the service:\n\n\n\nPersonal account\n\n\n\nGo to your VK Cloud personal account.\nGo to Monitoring → Logging.\nClick the Settings button.\nGo to the User credential generation tab.\nClick the Generate button.\nSave the received credentials.\n\n\n\n","url":"/en/monitoring-services/logging/service-management/generate-userdata","description":"Generating a user ID and password to connect to the Cloud Logging service in your VK Cloud project.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Logging","link":"/en/monitoring-services/logging","isActive":false,"evenSimpler":true}]},{"uuid":"fd16f5be-7753-4a6b-99ed-975f209dd85e","title":"Plugin installation","data":"Install Fluent Bit if this has not been done before.\nInstall the vkcloudlogs-fluent-bit-plugin plugin.\n\n   CentOS 7.X, CentOS 8.X, AlmaLinux 9\n   Ubuntu 22.X, Astra Linux SE 1.7.2 «Орел»\n   AltLinux Server p10\n\n\n      sudo rpm -i https://cloudlogging.hb.ru-msk.vkcs.cloud/vkcloudlogs-fluent-bit-plugin/vkcloudlogs-fluent-bit-plugin-0.1.3-1.x86_64.rpm\n\n\n      curl -sSLo vkcloudlogs-fluent-bit-plugin_0.1.3_amd64.deb https://cloudlogging.hb.ru-msk.vkcs.cloud/vkcloudlogs-fluent-bit-plugin/vkcloudlogs-fluent-bit-plugin_0.1.3_amd64.deb\n   sudo dpkg -i vkcloudlogs-fluent-bit-plugin_0.1.3_amd64.deb\n\n\n      sudo apt-get install https://cloudlogging.hb.ru-msk.vkcs.cloud/vkcloudlogs-fluent-bit-plugin/vkcloudlogs-fluent-bit-plugin-0.1.3-1.x86_64.rpm\n\n\n   After installation, the vkcloudlogs-fluent-bit.service log collection agent will appear. It is disabled by default.\n\n\n   The vkcloudlogs-fluent-bit.service agent works with the following files:\n\n   /etc/vkcloudlogs-fluent-bit-plugin/vkcloudlogs-fluent-bit.conf — the main configuration file of the service with descriptions of sources and destination paths for logging. For more information, see the Logging plugin section and the Fluent Bit official documentation;\n   /etc/vkcloudlogs-fluent-bit-plugin/vkcloudlogs-fluent-bit-plugins.conf — the plugin connection configuration file.\n\n\nConfigure the plugin to send logs to the Cloud Logging service.\n\n   Find out the authorization parameters for your project.\n   Specify the connection parameters for the plugin in the vkcloudlogs-fluent-bit.conf file:\n\n      vkcloudlogs-fluent-bit.conf\n\n      This example configures data logging from ssh.service (section [INPUT]) to the Cloud Logging service (section [OUTPUT]).\n\n            [INPUT]\n         Name            systemd\n         Systemd_Filter  _SYSTEMD_UNIT=ssh.service\n         Lowercase       On\n         Read_From_Tail  On\n         Tag             system.*\n\n      [OUTPUT]\n         Name              vkcloudlogs\n         Match             system.*\n         auth_url\n         project_id\n         server_host_port\n         user_id\n         password\n\n\nActivate the vkcloudlogs-fluent-bit.service log collection agent by running the commands:\n\n      sudo systemctl enable vkcloudlogs-fluent-bit.service\n   sudo systemctl start vkcloudlogs-fluent-bit.service\n\nWait a few minutes for the data to accumulate.\nCheck the availability of logs in the Monitoring → Logging section of your VK Cloud personal account.\n","url":"/en/monitoring-services/logging/service-management/connect-plugin","description":"How to install and configure the vkcloudlogs-fluent-bit-plugin plugin to send logs to VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Logging","link":"/en/monitoring-services/logging","isActive":false,"evenSimpler":true}]},{"uuid":"3a21752e-98e0-42e9-846e-e0984d3a2077","title":"Managing the logging agent","data":"Turning the agent on and off\n\nConnect to the virtual machine.\nMake sure that you have the vkcloudlogs-fluent-bit-plugin plugin installed.\n\nTo turn the agent on, run the commands:\n\n  Linux\n\n\n    sudo systemctl enable vkcloudlogs-fluent-bit.service\n  sudo systemctl start vkcloudlogs-fluent-bit.service\n\n\nTo turn the agent off, run the commands:\n\n  Linux\n\n\n    sudo systemctl stop vkcloudlogs-fluent-bit.service\n  sudo systemctl disable vkcloudlogs-fluent-bit.service\n\n\n{heading(Configuring the agent to collect logs from additional text files)[id=configure_agent]}\n\nConnect to the virtual machine.\nMake sure that you have the vkcloudlogs-fluent-bit-plugin plugin installed.\nAdjust the vkcloudlogs-fluent-bit.conf file as needed:\n\n   in the [INPUT] section, specify the path to the source file in the Path parameter and the tag in the Tag parameter;\n   in the [OUTPUT] section, specify the same tag so that the records are sent to the logs.\n\n    Example of connecting the /var/log/auth.log file\n\n      [INPUT]\n      Name             tail\n      Path             /var/log/auth.log\n      Skip_Empty_Lines On\n      Tag              vkcloudlogs.tail.auth.log\n\n   [OUTPUT]\n      Name              vkcloudlogs\n      Match             vkcloudlogs.tail.*\n      auth_url          https://infra.mail.ru:35357/v3/\n      server_host_port  cloudlogs.mcs.mail.ru:443\n      user_id           user1\n      password          pwd12345\n      project_id        XXXX000XXXX00\n\n\n   To delete the log collection source, remove the corresponding entries from the vkcloudlogs-fluent-bit.conf file.\n\n\nRestart the agent:\n\n   Linux\n\n\n      sudo systemctl restart vkcloudlogs-fluent-bit.service\n\n\nRemoving the agent and plugin\n\nConnect to the virtual machine.\nMake sure that you have the vkcloudlogs-fluent-bit-plugin plugin installed.\nRun the command:\n\n   CentOS 7.X, CentOS 8.X, RedOS 7, AlmaLinux 9\n   Ubuntu 22.X, Astra Linux\n   AltLinux Server\n\n\n      sudo yum remove vkcloudlogs-fluent-bit-plugin.x86_64\n\n\n      sudo apt remove vkcloudlogs-fluent-bit-plugin\n\n\n      sudo apt-get remove vkcloudlogs-fluent-bit-plugin\n\n","url":"/en/monitoring-services/logging/service-management/manage-vkcloudlogs-plugin","description":"Managing the vkcloudlogs-fluent-bit.service agent on a VM in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Logging","link":"/en/monitoring-services/logging","isActive":false,"evenSimpler":true}]},{"uuid":"a5aad1eb-dd98-4aeb-9060-b92eda6fd99a","title":"Viewing logs","data":"To view the logs, use any method convenient for you:\n\nGo to the Monitoring → Logging section of your VK Cloud personal account.\nUse API methods.\n\nTo refine the search results, use the search query language. Examples of search expressions:\n\nSearch for messages containing internal error:\n\n    \"internal error\"\n\n  or\n\n    message: \"internal error\"\n\nSearch for messages containing error: \"service unavailable\":\n\n    message: \"error: \\\"service unavailable\\\"\"\n\nUsing time, logging level and payload:\n\n    level >= debug AND (timestamp  databases AND NOT message: hello AND payload.status EXISTS\n","url":"/en/monitoring-services/logging/service-management/view-logs","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Logging","link":"/en/monitoring-services/logging","isActive":false,"evenSimpler":true}]},{"uuid":"7b5ce2a1-69a7-4743-b404-eec9f0079bde","title":"About the service","data":"Alerting is a metric change notification service that collects cloud resources.\n\nYou can independently configure and receive notifications about the status of cloud resources. For example, about changing the state of your virtual machine or database and responding to the problem. This way, you can prevent potential crashes in your applications rather than learning about them after the fact.\n\nAlerting monitors changes in certain metrics using trigger rules and sends notifications through notification channels.\n\nTo start working with the service, you should register on the VK Cloud platform, go to your personal account and make sure that the monitoring service is active and the account balance is positive. It also requires that a virtual machine or database instance has been created. Then go to the \"Monitoring\" → \"Alerting\" section.\n","url":"/en/monitoring-services/alerting/about","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Alerting","link":"/en/monitoring-services/alerting","isActive":false,"evenSimpler":true}]},{"uuid":"f168675d-52e4-420e-a6a5-1a46bfbf06ae","title":"Create a notification channel","data":"To receive information about the occurrence of an incident, you must set the notification channel. The notification channel contains information about the recipient and sending methods.\n\n\n\nYou can specify multiple notification channels. One channel can be used multiple times in multiple triggers.\n\n\n\nTo create a channel, go to the \"Alerting\" page in the \"Monitoring\" section.\n\nClick the Notification Channels tab.\nClick the \"Add\" button.\nName the channel.\nSelect the notification type: phone or email.\nEnter the email or phone number of the recipient, depending on which type of notification you selected earlier.\nClick the Create Channel button.\n","url":"/en/monitoring-services/alerting/notification/notification-add","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Alerting","link":"/en/monitoring-services/alerting","isActive":false,"evenSimpler":true}]},{"uuid":"a433ebab-f86d-4b8c-bd72-5804ebb05ab3","title":"Editing a notification channel","data":"To edit the notification channel, go to the \"Alerting\" page in the \"Monitoring\" section.\n\nClick the Notification Channels tab.\nIn the notification channels table, select the notification channel to edit.\nClick \"...\" → \"Edit\".\nMake the necessary changes.\nClick the \"Save channel\" button.\n","url":"/en/monitoring-services/alerting/notification/notification-edit","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Alerting","link":"/en/monitoring-services/alerting","isActive":false,"evenSimpler":true}]},{"uuid":"cd0b257a-1b2d-4917-85bb-c45eee8975ee","title":"Editing a trigger","data":"To edit a trigger:\n\nGo to the \"Alerting\" page in the \"Monitoring\" section.\nIn the triggers table, select the trigger to edit.\nClick \"...\" → \"Edit\".\nEnter the necessary changes.\nClick the \"Save\" button.\n\n\n\nPlease note you can't change the resource type.\n\n\n","url":"/en/monitoring-services/alerting/triggers/trigger-edit","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Alerting","link":"/en/monitoring-services/alerting","isActive":false,"evenSimpler":true}]},{"uuid":"50192b5c-7e36-44d5-bfca-10284414836b","title":"Creating a trigger","data":"Trigger - defines a rule that applies to the metric value and notification condition.\n\nThe trigger has a comparison interval. If it is set, then the metric value must fall under the condition during this interval in order for an incident to be created.\n\nTo create a trigger:\n\nGo to the \"Alerting\" page in the \"Monitoring\" section.\nOn the \"Triggers\" tab, click the \"Add Trigger\" button.\nFill in all the necessary fields:\n   Give a name to the trigger.\n   Select the resource type: virtual machine or database.\n   Select the source and metric.\n   Set the trigger conditions.\n   Select the comparison interval.\nClick \"Next Step\". Here you need to select the notification channel.\nClick the \"+ Add Channel\" button.\nIn the window that appears, select an existing channel from the suggested list or create a new one:\n   Give it a name.\n   Select the type of notification: phone or email.\n   Enter the recipient's email or phone number.\n   Click the \"Add Channel\" button.\nThen click \"Create Trigger\".\n","url":"/en/monitoring-services/alerting/triggers/triggers-add","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Alerting","link":"/en/monitoring-services/alerting","isActive":false,"evenSimpler":true}]},{"uuid":"c5da9205-cc53-40af-b7df-d3c58d248c3c","title":"Trigger launch","data":"If a trigger fires more than 100 times within 24 hours, the trigger will automatically turn off. You will receive a notification email with recommendations.\n\nThere are several reasons why the trigger fires so often:\n\nThe trigger is not configured correctly. Read about how to configure it here.\nThere are problems that need to be fixed.\n\nWe recommend that you check the trigger settings and make sure that any issues are resolved. To run a trigger, click \"...\" in the trigger line -> \"Enable\".\n","url":"/en/monitoring-services/alerting/triggers/trigger-start","description":"Solving the problem of turning off the trigger and starting it.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Alerting","link":"/en/monitoring-services/alerting","isActive":false,"evenSimpler":true}]},{"uuid":"dcfa65ca-3849-43d7-8a3a-5d912f35c2d1","title":"Changing the status of an incident","data":"An incident is a fixation of the event of a metric value falling under the condition specified in the trigger. It contains information about the trigger, the time of the event, the value of the metric and all its labels.\n\nThere are three incident statuses:\n\nOpened — opened incident\nClosed — closed by user or automatically (for incidents with an interval)\nAcknowledged — marked by the user as accepted/processed\n\nTo change the incident status, go to the \"Alerting\" page in the \"Monitoring\" section.\n\nGo to the \"Incidents\" tab.\nIn the incident table, select the incident whose status you want to change.\nIn the \"Status\" column, select the new incident status:\n\nOpen → In progress\nOpen → Closed\nAt work → Private\n\nRe-creating incidents\n\nIf an incident has been created, but the metric value continues to fall under the condition in the trigger,\nthe incident is active and no new incidents are created.\n\nIf the metric value ceases to fall under the condition, then the incident ceases to be active. In the future, if the value starts falling under the condition again, a new incident will be created.\n\nFor triggers with a comparison interval, the value should stop falling under the condition\nduring the interval, and only then the incident ceases to be active.\n","url":"/en/monitoring-services/alerting/incidents","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Alerting","link":"/en/monitoring-services/alerting","isActive":false,"evenSimpler":true}]},{"uuid":"a4567bd4-4d10-458a-87bd-684e8f5f7b48","title":"Quick start","data":"С помощью Cloud Monitoring в личном кабинете VK Cloud будет создана виртуальная машина (ВМ) Linux настроен сбор метрик для нее с визуализацией в интерфейсе.\n\nБыстрый старт поможет вам начать работу с сервисом и познакомиться с его возможностями.\n\nПройдя все шаги быстрого старта, вы:\n\nСоздадите новую виртуальную машину (ВМ).\nПодключите мониторинг к ВМ.\nСоздадите для новый график.\n\nПодготовительные шаги\n\nПри создании ВМ активируйте пункт Включить мониторинг.\nДождитесь завершения создания ВМ.\nУзнайте ID созданной виртуальной машины.\nПодождите некоторое время, чтобы накопились данные для мониторинга.\n\n1. Создайте график\n\nПерейдите в раздел Мониторинг → Дашборды.\nНажмите кнопку Добавить график.\nНа шаге «Конфигурация графика» выберите:\n\n   Заголовок графика: укажите название графика, например, cpu-monitoring.\n   Тип ресурса: выберите вариант Виртуальная машина.\n   Тип диаграммы: выберите вариант Линейная.\n\nНажмите кнопку Следующий шаг.\nНа шаге «Параметры» выберите:\n\n   Название метрики: выберите cpu_usage.\n   Группировка: выберите поле vm_uuid.\n   Функция агрегации: среднее арифметическое.\n\nНажмите кнопку Создать график.\n\nПри корректной настройке на графике появятся значения для созданной ВМ.\n\n2. Проследите за сбором метрик\n\nПодождите некоторое время, чтобы накопились данные для мониторинга.\nОбновите страницу. Должны появиться данные за новый период.\n\nУдалите неиспользуемые ресурсы\n\nРаботающая ВМ потребляет вычислительные ресурсы. Если она вам больше не нужна:\n\nостановите ее, чтобы воспользоваться ею позже;\nудалите ее навсегда.\n","url":"/en/monitoring-services/monitoring/quick-start","description":"Пошаговая инструкция по созданию и подключению системы мониторинга, а также управлению версиями агентов.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Monitoring","link":"/en/monitoring-services/monitoring","isActive":false,"evenSimpler":true}]},{"uuid":"8b980499-86d8-4016-b222-7c3be4dba322","title":"Service architecture","data":"Концепции\n\nСервис Cloud Monitoring состоит из нескольких частей:\n\nМасштабируемый API по приему метрик\nРаспределенное хранилище метрик\nМасштабируемый API по чтению значений метрик и построению запроса\nАгенты для отправки стандартных метрик\nUI с возможностью строить дашборды и графики\nСервис централизованного обновления агентов мониторинга\n\nNamespaces\n\nКаждый сервис пишет данные в свой namespace. Стандартные имена Namespace, формируемые сервисами VK Cloud, выглядят как mcs/servicename. Если необходимо писать кастомные метрики, то название Namespace не должно начинаться с mcs/.\n\nТаблица стандартных Namespace\n\nНазвание сервисаНазвание NamespaceCloud Serversmcs/vmCloud Networksmcs/networkCloud Storagemcs/cloudstorageCloud Containersmcs/containersCloud Databasesmcs/databasesCloud Big Datamcs/bigdataSQSmcs/managedqueueMarketplacemcs/marketplaceCDNmcs/cdnArenadata DB as a Servicemcs/dwhCloud Monitoringmcs/monitoring\n\nМетки (Labels)\n\nВ Labels передается дополнительная метаинформация, которая может идентифицировать целевой ресурс, который мы мониторим. Т.е. В случае VM - название VM или id VM.\n\nДля каждой метрики, которая отправляется в хранилище можно указать произвольный набор пар “ключ-значение” (labels). Например, вы вместе со значением метрики cpu_total хотите передать имя хоста виртуальной машины и название availability zone, в которой она находится, тогда вам необходимо заполнить значения для меток ‘host’ и ‘availability_zone’.\n\nПереданные метки и их значения сохраняются в базу данных Cloud Monitoring. Значения одной и той же метрики автоматически агрегируются с точностью до периодов 1 минута, 5 минут, 1 час. Агрегирование происходит по всем полям labels, с которыми была сохранено значение метрики\n\nДалее ключи и значения меток можно использовать для построения запросов, которые фильтруют и группируют значения метрик. Например, можно построить такой запрос в формате ala PromQL:\n\nSUM BY(host) (cpu:Minimum{instance=\"\"server1\", app!=\"\"system\"})\n\nSUM BY(job) (cpu:Average{host=\"\"server1\", job!=\"\"system\"}[12h] offset 24h)\n\nАгрегация\n\nCloud Monitoring автоматически агрегирует значения метрик по интервалам 1 минута, 5 минут, 1 час\nСырые значения метрик автоматически удаляются после агрегации\nИз коробки поддерживаются следующие функции агрегации:\n    Минимум\n    Максимум\n    Среднее\nАгрегирование происходит по всем полям labels, с которыми была сохранено значение метрики\nАгрегированные метрики хранятся в течение 30 дней\nПользователь может получить значения агрегированных метрик с фильтрацией по необходимым значениям Labels\n\nЕдиницы измерения\n\nВ Cloud Monitoring есть стандартные единицы измерений, которые можно передавать и по которым агрегировать:\n\nSeconds | Microseconds | Milliseconds | Bytes | Kilobytes | Megabytes | Gigabytes | Terabytes | Bits | Kilobits | Megabits | Gigabits | Terabits | Percent | Count | Bytes/Second | Kilobytes/Second | Megabytes/Second | Gigabytes/Second | Terabytes/Second | Bits/Second | Kilobits/Second | Megabits/Second | Gigabits/Second | Terabits/Second | Count/Second | None\n\nЕсли не указана единица измерений, то подразумевается None.\n","url":"/en/monitoring-services/monitoring/concepts/architecture","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Monitoring","link":"/en/monitoring-services/monitoring","isActive":false,"evenSimpler":true}]},{"uuid":"2687400f-3cf7-4640-b1c0-813b974ca6f4","title":"Standard metrics","data":"По умолчанию стандартный агент мониторинга собирает следующую информацию из операционной системы:\n\nПотребление CPU (плагин cpu):\n\n  time_user (float)\n  time_system (float)\n  time_idle (float)\n  time_active (float)\n  time_nice (float)\n  time_iowait (float)\n  time_irq (float)\n  time_softirq (float)\n  time_steal (float)\n  time_guest (float)\n  time_guest_nice (float)\n  usage_user (float, percent)\n  usage_system (float, percent)\n  usage_idle (float, percent)\n  usage_active (float)\n  usage_nice (float, percent)\n  usage_iowait (float, percent)\n  usage_irq (float, percent)\n  usage_softirq (float, percent)\n  usage_steal (float, percent)\n  usage_guest (float, percent)\n  usage_guest_nice (float, percent)\n\nПотребление памяти (плагин mem):\n\n  active (integer, Darwin, FreeBSD, Linux, OpenBSD)\n  available (integer)\n  available_percent (float)\n  buffered (integer, FreeBSD, Linux)\n  cached (integer, FreeBSD, Linux, OpenBSD)\n  commit_limit (integer, Linux)\n  committed_as (integer, Linux)\n  dirty (integer, Linux)\n  free (integer, Darwin, FreeBSD, Linux, OpenBSD)\n  high_free (integer, Linux)\n  high_total (integer, Linux)\n  huge_pages_free (integer, Linux)\n  huge_page_size (integer, Linux)\n  huge_pages_total (integer, Linux)\n  inactive (integer, Darwin, FreeBSD, Linux, OpenBSD)\n  laundry (integer, FreeBSD)\n  low_free (integer, Linux)\n  low_total (integer, Linux)\n  mapped (integer, Linux)\n  page_tables (integer, Linux)\n  shared (integer, Linux)\n  slab (integer, Linux)\n  sreclaimable (integer, Linux)\n  sunreclaim (integer, Linux)\n  swap_cached (integer, Linux)\n  swap_free (integer, Linux)\n  swap_total (integer, Linux)\n  total (integer)\n  used (integer)\n  used_percent (float)\n  vmalloc_chunk (integer, Linux)\n  vmalloc_total (integer, Linux)\n  vmalloc_used (integer, Linux)\n  wired (integer, Darwin, FreeBSD, OpenBSD)\n  write_back (integer, Linux)\n  write_back_tmp (integer, Linux)\n\nСтатистика страниц ядра ОС (плагин kernel_vmstat):\n\n  nr_free_pages (integer, nr_free_pages)\n  nr_inactive_anon (integer, nr_inactive_anon)\n  nr_active_anon (integer, nr_active_anon)\n  nr_inactive_file (integer, nr_inactive_file)\n  nr_active_file (integer, nr_active_file)\n  nr_unevictable (integer, nr_unevictable)\n  nr_mlock (integer, nr_mlock)\n  nr_anon_pages (integer, nr_anon_pages)\n  nr_mapped (integer, nr_mapped)\n  nr_file_pages (integer, nr_file_pages)\n  nr_dirty (integer, nr_dirty)\n  nr_writeback (integer, nr_writeback)\n  nr_slab_reclaimable (integer, nr_slab_reclaimable)\n  nr_slab_unreclaimable (integer, nr_slab_unreclaimable)\n  nr_page_table_pages (integer, nr_page_table_pages)\n  nr_kernel_stack (integer, nr_kernel_stack)\n  nr_unstable (integer, nr_unstable)\n  nr_bounce (integer, nr_bounce)\n  nr_vmscan_write (integer, nr_vmscan_write)\n  nr_writeback_temp (integer, nr_writeback_temp)\n  nr_isolated_anon (integer, nr_isolated_anon)\n  nr_isolated_file (integer, nr_isolated_file)\n  nr_shmem (integer, nr_shmem)\n  numa_hit (integer, numa_hit)\n  numa_miss (integer, numa_miss)\n  numa_foreign (integer, numa_foreign)\n  numa_interleave (integer, numa_interleave)\n  numa_local (integer, numa_local)\n  numa_other (integer, numa_other)\n  nr_anon_transparent_hugepages (integer, nr_anon_transparent_hugepages)\n  pgpgin (integer, pgpgin)\n  pgpgout (integer, pgpgout)\n  pswpin (integer, pswpin)\n  pswpout (integer, pswpout)\n  pgalloc_dma (integer, pgalloc_dma)\n  pgalloc_dma32 (integer, pgalloc_dma32)\n  pgalloc_normal (integer, pgalloc_normal)\n  pgalloc_movable (integer, pgalloc_movable)\n  pgfree (integer, pgfree)\n  pgactivate (integer, pgactivate)\n  pgdeactivate (integer, pgdeactivate)\n  pgfault (integer, pgfault)\n  pgmajfault (integer, pgmajfault)\n  pgrefill_dma (integer, pgrefill_dma)\n  pgrefill_dma32 (integer, pgrefill_dma32)\n  pgrefill_normal (integer, pgrefill_normal)\n  pgrefill_movable (integer, pgrefill_movable)\n  pgsteal_dma (integer, pgsteal_dma)\n  pgsteal_dma32 (integer, pgsteal_dma32)\n  pgsteal_normal (integer, pgsteal_normal)\n  pgsteal_movable (integer, pgsteal_movable)\n  pgscan_kswapd_dma (integer, pgscan_kswapd_dma)\n  pgscan_kswapd_dma32 (integer, pgscan_kswapd_dma32)\n  pgscan_kswapd_normal (integer, pgscan_kswapd_normal)\n  pgscan_kswapd_movable (integer, pgscan_kswapd_movable)\n  pgscan_direct_dma (integer, pgscan_direct_dma)\n  pgscan_direct_dma32 (integer, pgscan_direct_dma32)\n  pgscan_direct_normal (integer, pgscan_direct_normal)\n  pgscan_direct_movable (integer, pgscan_direct_movable)\n  zone_reclaim_failed (integer, zone_reclaim_failed)\n  pginodesteal (integer, pginodesteal)\n  slabs_scanned (integer, slabs_scanned)\n  kswapd_steal (integer, kswapd_steal)\n  kswapd_inodesteal (integer, kswapd_inodesteal)\n  kswapd_low_wmark_hit_quickly (integer, kswapd_low_wmark_hit_quickly)\n  kswapd_high_wmark_hit_quickly (integer, kswapd_high_wmark_hit_quickly)\n  kswapd_skip_congestion_wait (integer, kswapd_skip_congestion_wait)\n  pageoutrun (integer, pageoutrun)\n  allocstall (integer, allocstall)\n  pgrotated (integer, pgrotated)\n  compact_blocks_moved (integer, compact_blocks_moved)\n  compact_pages_moved (integer, compact_pages_moved)\n  compact_pagemigrate_failed (integer, compact_pagemigrate_failed)\n  compact_stall (integer, compact_stall)\n  compact_fail (integer, compact_fail)\n  compact_success (integer, compact_success)\n  htlb_buddy_alloc_success (integer, htlb_buddy_alloc_success)\n  htlb_buddy_alloc_fail (integer, htlb_buddy_alloc_fail)\n  unevictable_pgs_culled (integer, unevictable_pgs_culled)\n  unevictable_pgs_scanned (integer, unevictable_pgs_scanned)\n  unevictable_pgs_rescued (integer, unevictable_pgs_rescued)\n  unevictable_pgs_mlocked (integer, unevictable_pgs_mlocked)\n  unevictable_pgs_munlocked (integer, unevictable_pgs_munlocked)\n  unevictable_pgs_cleared (integer, unevictable_pgs_cleared)\n  unevictable_pgs_stranded (integer, unevictable_pgs_stranded)\n  unevictable_pgs_mlockfreed (integer, unevictable_pgs_mlockfreed)\n  thp_fault_alloc (integer, thp_fault_alloc)\n  thp_fault_fallback (integer, thp_fault_fallback)\n  thp_collapse_alloc (integer, thp_collapse_alloc)\n  thp_collapse_alloc_failed (integer, thp_collapse_alloc_failed)\n  thp_split (integer, thp_split)\n\nИнформация о диске (плагин disk):\n\n  free (integer, bytes)\n  total (integer, bytes)\n  used (integer, bytes)\n  used_percent (float, percent)\n  inodes_free (integer, files)\n  inodes_total (integer, files)\n  inodes_used (integer, files)\n\nИнформация о дисковом IO (плагин diskio):\n\n  reads (integer, counter)\n  writes (integer, counter)\n  read_bytes (integer, counter, bytes)\n  write_bytes (integer, counter, bytes)\n  read_time (integer, counter, milliseconds)\n  write_time (integer, counter, milliseconds)\n  io_time (integer, counter, milliseconds)\n  weighted_io_time (integer, counter, milliseconds)\n  iops_in_progress (integer, gauge)\n  merged_reads (integer, counter)\n  merged_writes (integer, counter)\n\nИнформация о работе сетевой подсистемы внутри ОС (плагин net):\n\n  bytes_sent — the total number of bytes sent by the interface;\n  bytes_recv — the total number of bytes received by the interface;\n  packets_sent — the total number of packets sent by the interface;\n  packets_recv — the total number of packets received by the interface;\n  err_in — the total number of receive errors detected by the interface;\n  err_out — the total number of transmit errors detected by the interface;\n  drop_in — the total number of received packets dropped by the interface;\n  drop_out — the total number of transmitted packets dropped by the interface.\n\nИнформация из NetStat:\n\n  tcp_established\n  tcp_syn_sent\n  tcp_syn_recv\n  tcp_fin_wait1\n  tcp_fin_wait2\n  tcp_time_wait\n  tcp_close\n  tcp_close_wait\n  tcp_last_ack\n  tcp_listen\n  tcp_closing\n  tcp_none\n","url":"/en/monitoring-services/monitoring/concepts/mon-metrics","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Monitoring","link":"/en/monitoring-services/monitoring","isActive":false,"evenSimpler":true}]},{"uuid":"ee308a06-5eea-4d67-b01a-a1e1790e913e","title":"Functions and groupings of metrics","data":"Cloud Monitoring предоставляет API для получения значения метрик. Реализована поддержка подмножества языка PromQL для работы с метриками на основе запросов.\n\nМожно составлять и выполнять запрос на чтение метрик со следующим синтаксисом:\n\nНазвание метрики и функция агрегации\n\n  cpu:Minimum\n\nФильтрация по значению Label:\n\n  \\= — метки, которые точно соответствуют предоставленной строке;\n  != — метки, не совпадающие с указанной строкой;\n  \\=~ — метки, регулярное выражение которых соответствует указанной строке;\n  !~ — метки, которые не соответствуют регулярному выражению указанной строке.\n\nГруппировки:\n\n  SUM — сумма по размерностям;\n  MIN — минимум по размерностям;\n  MAX — максимум по размерностям;\n  AVG — среднее по размерностям;\n  STDDEV — стандартное отклонение совокупности по размерностям;\n  STDVAR — стандартная дисперсия генеральной совокупности по измерениям;\n  COUNT — количество элементов в векторе;\n  QUANTILE — φ-квантиль (0 ≤ φ ≤ 1) по измерениям.\n\nПримеры запросов:\n\nSUM BY(host) (cpu:Minimum{instance=\"server1\", app!=\"system\"})\n\nSUM BY(job) (cpu:Average{host=\"server1\", job!=\"system\"}[12h] offset 24h)\n","url":"/en/monitoring-services/monitoring/concepts/mon-read-metrics","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Monitoring","link":"/en/monitoring-services/monitoring","isActive":false,"evenSimpler":true}]},{"uuid":"01bc5adc-1356-46c6-bd50-5368cadf5cc1","title":"Installing monitoring in a new VM","data":"The article will tell you how to install monitoring in a new virtual machine.\n\nWhen creating a VM, activate the “Enable monitoring” item.\nWait for the VM to be created.\nGo to the “Monitoring” service.\nCreate a new dashboard.\n    Select a visualization type.\n    Select a standard service type/Namespace.\n        “Linux virtual machine”.\n    Select a metric from the list.\n    Select a unit of measure.\n        For standard metrics, we have already set the required unit of measure.\n    Select a filter condition for this metric.\n        It is possible to select several filtering conditions.\n    Select the field by which you want to group the received metrics data.\n        For example, if you select the “host” grouping field, then graphs for different virtual machines will be displayed as separate lines.\n    Select an aggregation function.\n        Average.\n        Minimum.\n        Max.\n    You can add multiple metrics to 1 chart.\n        If the metrics have different units of measurement, then the units of measurement will not be displayed on the chart.\n    To work with the dashboard, you can select a different date interval and different data aggregation intervals (from 1 minute to 30 days).\n","url":"/en/monitoring-services/monitoring/service-management/mon-setup-new","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Monitoring","link":"/en/monitoring-services/monitoring","isActive":false,"evenSimpler":true}]},{"uuid":"10c24b18-afb6-4975-bde7-db9005bac771","title":"Installation in existing VMs","data":"To install monitoring to an existing VM:\n\nGo to your VK Cloud personal account.\nGo to Cloud computing → Virtual machines.\nOpen the VM page by clicking on its name in the list.\nOn the General information tab, under the main VM information, go to the Configuring monitoring tab.\nCopy the command that appears.\nConnect to the virtual machine.\nOn the connected VM, run the command copied in step 5.\n\nThe monitoring agent will be installed and will start every time the VM starts.\n\nIf the Configuring monitoring tab is missing:\n\nMake sure two-factor authentication is enabled and API access is activated.\nGet an access token X-Subject-Token.\nGet the Project ID, a special identifier for working with services. Example: a1b2c3d4e5f6g7h8i9a1b2c3d4e5f6g7.\nGet the virtual machine ID.\nConnect to the virtual machine.\nRun the command:\n\n   Linux\n   Windows (powershell)\n\n\n      curl \\\n   -H \"content-type: application/json\" \\\n   -H \"X-Auth-Token: \" \\\n   -d '{\"instance_id\":\"\", \"capabilities\":[\"telegraf\"], \"os_type\":\"linux\"}' \\\n   -X POST  https://mcs.mail.ru/infra/templater/v2/project//link\n\n\n   Place the request body parameters into the params variable:\n\n            $params = @{'instance_id' = ''; 'os_type' = 'windows'; 'capabilities' = @('telegraf')} | convertto-json\n\n   Check the variable is created using echo $params. Expected output:\n\n            {\n         \"instance_id\": \"\",\n         \"capabilities\":  [\n            \"telegraf\"\n         ],\n         \"os_type\": \"windows\"\n      }\n\n   Run the command:\n\n            [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12; `\n      Invoke-WebRequest -Method Post -ContentType application/json `\n      -Uri https://mcs.mail.ru/infra/templater/v2/project//link `\n      -Headers @{'X-Auth-Token' = ''} `\n      -Body $params | Select-Object -Expand Content\n\n\nExecute the command from the contents of the response script parameter.\n\n   Example for Linux\n   Example for Windows (powershell)\n\n\n      sudo curl -s -H 'content-type: application/json' -X POST https://mcs.mail.ru/infra/templater/v2/project//link/XXXXUm5Yb33LJ7otcPnWSUXXXXXXXXXX/instance/ | sudo bash\n\n\n      [Net.SecurityProtocolType]::Tls12; Invoke-WebRequest -Method 'POST' -Headers @{'Content-Type' = 'application/json'} -Uri https://mcs.mail.ru/infra/templater/v2/project//link/XXXXUm5Yb33LJ7otcPnWSUXXXXXXXXXX/instance/ | iex\n\n\nMetrics for this VM can be found by the label host with the value hostname of the VM.\n\n\n","url":"/en/monitoring-services/monitoring/service-management/mon-setup-current","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Monitoring services","link":"/en/monitoring-services","isActive":false,"evenSimpler":true},{"title":"Cloud Monitoring","link":"/en/monitoring-services/monitoring","isActive":false,"evenSimpler":true}]},{"uuid":"f9633ee8-8112-449b-be16-befbccfbac4c","title":"About the service","data":"Marketplace is a VK Cloud subsystem that provides customers with access to third-party services of various kinds:\n\nprovided by the vendor;\nopen source solutions;\nself-developed services.\n\nAll details on infrastructure deployment and software or creating a tenant (account) remain on the VK Cloud side. The user receives a prepared instance of the service (an instance of the service), which, if necessary, can be customized to their needs. All deployed instances of the service are charged and managed within the VK Cloud project. The Marketplace helps you launch services faster and simplifies their support.\n\n\n\nBy using the services from the app store, you agree to the user Agreement.\n\nTypes of services\n\nVK Cloud supports two types of services:\n\nSaaS services — centrally installed multi-tenant products. The vendor deploys the service either on its own infrastructure, or on the infrastructure in its project in VK Cloud. The user is granted access to the service through a separate account (tenant). The service and its instances are managed on the VK Cloud side.\nImage-based services — a product that is deployed based on virtual machine images in the VK Cloud project. Additional infrastructure can be used to maintain the product: virtual networks, load balancers, DBaaS clusters, S3 object storage, backup. The service, its instances, and infrastructure are managed on the VK Cloud side.\n\nEach type of service is initialized in VK Cloud differently:\n\n\n\nSaaS\nImage-based\n\n\n\nThe user connects the service to the project.\nVK Cloud sends a request to the supplier to create a tenant account in his product.\nThe supplier registers a new account, sends the VK Cloud details.\nVK Cloud creates a service instance for management and sends the access details to the user.\n\n\n\n\nThe user connects the service to the project.\nVK Cloud creates the necessary infrastructure in the user's project.\nVK Cloud installs the service on the created infrastructure.\nVK Cloud creates a service instance for management and sends the access details to the user.\n\n\n\n\nKey differences for SaaS and image-based services:\n\n| Feature / Type | SaaS | Image-based |\n| ----- | --- | --- |\n| Infrastructure location | A separate account (tenant) on the side of the supplier company | Project in VK Cloud |\n| Management of the deployed infrastructure via VK Cloud | No | Yes |\n| Service instance monitoring | No | Yes |\n| Integration with VK Cloud services | No | Yes |\n| Tariffication | Product Usage | Product usage + deployed infrastructure in the project |\n| Changing the tariff plan | Yes | Yes |\n| Setting up tariff plan options | Yes | Yes |\n\nInformation for vendors\n\nVendors can add their software to VK Cloud by writing to marketplace@cloud.vk.com.\n","url":"/en/applications-and-services/marketplace/concepts/about","description":"Description of the capabilities of the Marketplace service in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"549f73f0-90ae-48d6-8cea-3976f56f3ff4","title":"Installing and configuring Hystax DR","data":"You can set up data recovery and IT infrastructure in case of unforeseen circumstances using the service Hystax Acura Disaster Recovery without suspending applications. Using the service allows you to optimize the indicators:\n\nRecovery Point Objective (RPO) — the time between the creation of the last backup and the moment of the accident. It can be configured individually for each resource.\nRecovery Time Objective (RTO) — the time between the response to an accident and the restoration of infrastructure. The service allows you to minimize this indicator.\n\n\n  How the service works\n\nAgents are installed on target VMs to synchronize with Hystax Acura.\nData replication is performed for synchronized VMs. If necessary, the VM backup schedule is configured.\nA disaster recovery plan is created: a description of the infrastructure and a set of instructions for recreating a business application in the VK Cloud. It is possible to create several plans. To reduce the RPO value, disaster recovery plans must be kept up to date.\nA backup infrastructure is created based on the plan.\nIn case of emergency situations, the backup infrastructure takes over the entire load.\n\n\n\nAfter completing all the steps of a instruction, you will:\n\nDeploy the Cloud Disaster Recovery service in VK Cloud.\nInstall the agent on the VM and perform data replication for it.\nCreate a backup infrastructure using a backup recovery plan.\nRestore the reserved infrastructure in VK Cloud.\n\nBy using the Hystax Acura Disaster Recovery service, you agree to the license agreements of Marketplace and Hystax Acura Disaster Recovery.\n\nPreparatory steps\n\nRegister at VK Cloud.\nConfigure two-factor authentication (2FA) for the account on whose behalf the restored infrastructure will be deployed.\nCreate a VM for which recovery will be applied. As part of the quick start, the Ubuntu-DR VM with the Ubuntu 18.04 operating system will be used.\nConnect Hystax Acura Disaster Recovery service.\n\n   Wait for the installation to complete — a link with a username and password will be sent to your email. The service will be deployed at https://dr.mcs-cloud.ru (Hystax Acura personal account).\n\n2. Make data replication\n\nSign in to Hystax Acura personal account using the received username and password.\nClick the Install replication agents button.\nIn the “Agent selection” step, select Linux and click Next.\nAt the “Agent settings” step, specify the parameters:\n\n   Machines group: Default.\n   Select target Linux distribution: Debian/Ubuntu (.deb package).\n   Snapshot driver deployment type: DKMS.\n\nClick the Next button.\nInstall the agent on the target VM, following the instructions for the Ubuntu distribution.\n\n\n   You can install migration agents on a group of VMs with different operating systems via Ansible.\n\n     Ansible manifest for installing agents\n\n      hosts: all\n     vars:\n       ansible_ssh_pipelining: true\n\n     tasks:\n       name: Generate URL rpm\n         set_fact:\n           download_url: \"https://{{ acura_host }}/linux_agent/{{ customer_id }}?dist_type=rpm&platform=x64\"\n           remote_path: /tmp/hlragent.rpm\n         when: ansible_os_family == \"RedHat\"\n\n       name: Generate URL deb\n         set_fact:\n           download_url: \"https://{{ acura_host }}/linux_agent/{{ customer_id }}?dist_type=deb&platform=x64\"\n           remote_path: /tmp/hlragent.deb\n         when: ansible_os_family == \"Debian\"\n\n       name: Download agent\n         get_url:\n           url: \"{{ download_url }}\"\n           dest: \"{{ remote_path }}\"\n           mode: 0644\n           validate_certs: no\n           timeout: 300\n         become: yes\n\n       name: Install Hystax Linux Replication Agent from rpm package\n         yum:\n           name: \"{{ remote_path }}\"\n           state: present\n         become: yes\n         when: ansible_os_family == \"RedHat\"\n\n       name: Install Hystax Linux Replication Agent from deb package\n         apt:\n           deb: \"{{ remote_path }}\"\n           state: present\n         become: yes\n         when: ansible_os_family == \"Debian\"\n\n       name: Remove package file\n         file:\n           path: \"{{ remote_path }}\"\n           state: absent\n         become: yes\n\n\n   After installing the VM agent, Ubuntu-DR will appear on the main page of Hystax Acura personal account with Unprotected status.\n\nExpand the Ubuntu-DR VM menu in the Machines Groups list and select the Edit Replication schedule option. In the window that opens, specify the parameters:\n\n   Use custom Replication schedule settings: select an option.\n   Volume availability zone: MS1.\n   Volume type: CEPH-HDD.\n\n\n   For more information about replication and creating a backup schedule, see Hystax official documentation.\n\n\nClick the Save button.\nOpen the Ubuntu-DR VM menu and select the Start Protection option.\nWait for the operation to complete — the VM status will change to Protected.\n\n3. Create a disaster recovery plan\n\nClick the Create DR plan button.\nIn the Name field, specify the name of the plan DR-plan.\nGo to the Expert tab and click Generate DR plan from all machines button.\n\n   A JSON file with the Ubuntu-DR VM will be generated.\n\nAdjust the plan according to the requirements for VM recovery after failures:\n\n    In the subnet_id parameter, specify the network ID for the Ubuntu-DR VM.\n    In the flavor parameter, specify the name of the VM template, specify it using the openstack flavor list command.\n\n    Detailed description of the parameters in the official documentation Hystax Acura.\n\n    Example of a disaster recovery plan\n\n    This plan describes two VMs and the subnet in which the migrated VMs will be deployed.\n\n        {\n      \"subnets\": {\n        \"subnet_0\": {\n          \"name\": \"subnet_0\",\n          \"cidr\": \"10.0.1.0/24\",\n          \"subnet_id\": \"2aebd081-44a8-480f-xxxx-yyyyyyyyyyyy\"\n        }\n      },\n      \"devices\": {\n        \"ubuntu01\": {\n          \"id\": \"ec09a435-3389-d19f-4cf4-zzzzzzzzzzz\",\n          \"security_groups\": [\n            \"default_all\"\n          ],\n          \"availability_zone\": \"MS1\",\n          \"rank\": 0,\n          \"flavor\": \"Standard-4-8-80\",\n          \"ports\": [\n            {\n              \"name\": \"port_0\",\n              \"ip\": \"10.0.1.23\",\n              \"floating_ip\": true,\n              \"subnet\": \"subnet_0\"\n            }\n          ]\n        },\n        \"centos01\": {\n          \"id\": \"a40d5ef3-e244-dab5-9df0-aaaaaaaaaaaa\",\n          \"security_groups\": [\n            \"default_all\"\n          ],\n          \"availability_zone\": \"DP1\",\n          \"rank\": 0,\n          \"flavor\": \"Standard-4-8-80\",\n          \"ports\": [\n            {\n              \"name\": \"port_0\",\n              \"ip\": \"10.0.1.27\",\n              \"floating_ip\": true,\n              \"subnet\": \"subnet_0\"\n            }\n          ]\n        }\n      }\n    }\n\n\nClick the Save button.\n\n4. Run the plan\n\nGo to Recover.\nSelect the DR-plan plan and click Next button.\nIn the Cloud Site Name field, specify the value VK-Cloud-infra and click Run Recover button.\n\n   The creation of a backup infrastructure will begin.\n\n   If the process completes successfully, VK-Cloud-infra will appear in the Cloud Site block with Active status.\n   If the process ended with errors, restart it:\n\n     In the Cloud Sites block, click the Delete button for the process that failed and confirm the deletion.\n     In the DR plans block, for the DR-plan plan, click the Edit button.\n     Make the necessary changes (in basic or expert mode).\n     Click the Save button.\n     Restart the plan.\n\n5. Restore infrastructure in VK Cloud\n\nGo to Failback.\nIn the “Select target cloud type” step, select the OPENSTACK option and click Next button.\nAt the “Select target environment” step, select the New OpenStack option and specify the parameters:\n\n   Cloud name: VK Cloud.\n   Keystone API endpoint: Keystone value from endpoints list, https://infra.mail.ru:35357/v3/ for VK Cloud.\n   User domain: value User Domain Name of project settings.\n   Username: user name with API access and a role not lower than the Project administrator.\n   Password: the user's password.\n   Target project domain: value Project Domain ID of project settings.\n   Target project ID: value Project ID of project settings\n   Hystax Service Network: The UUID of the network to which the VM will be deployed.\n   Floating IP Network: external network ext-net.\n\nClick the Next button.\nAt the “Select resource” step, select VK-Cloud-infra from the list From Cloud Site.\nClick the Next button.\nAt the “Failback settings” step, specify the name of the structure being restored.\nClick the Start Failback button.\n\n   The infrastructure in VK Cloud will be brought to the state corresponding to 'VK-Cloud-infra'.\n\n6. Check the functionality of the restored VM\n\nFind the restored VM in VK Cloud (_cloud_agent), perform arbitrary operations on it.\n\nDelete unused resources\n\nRunning VMs consume computing resources. If you don't need them anymore:\n\nDelete Ubuntu-DR VM.\nDelete the backup infrastructure VK-Cloud-infra via Hystax Acura personal account.\nDelete floating IP address if it was created during recovery.\n","url":"/en/applications-and-services/marketplace/initial-configuration/dr-quick-start","description":"Deployment the Hystax Acura Disaster Recovery service, creating a plan and launching a disaster recovery.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"727496dd-00d4-43f0-b0fd-944e841281c8","title":"Migration using Hystax Migration","data":"You can migrate your resources to VK Cloud using the Hystax Acura Migration service without suspending applications. You can transfer resources from both virtual and physical platforms.\n\n\n  Where can I transfer data from?\n\nSupported platforms: VK Cloud, Yandex Cloud, CROC Cloud, SberCloud, Basis.Cloud, OpenStack, VMware, Amazon Web Services, Google Cloud Platform, Microsoft Azure, Oracle Cloud, Alibaba Cloud, Hyper-V, as well as physical machines.\n\nSupported Applications: SAP, Microsoft Active Directory, PostgreSQL, Oracle, NGINX, Red Hat Jboss Enterprise, IBM WebSphere, Apache, VMware vSphere, MySQL, MongoDB, Hadoop, Spark.\n\nSupported operating systems: Windows, RHEL, CentOS, Debian, Ubuntu, Astra Linux, AltLinux, Red OS.\n\n\n\nThis instruction will help you migrate your resources to VK Cloud using the Hystax Acura Migration service using the example of the Ubuntu-MR VM with the Ubuntu 18.04 operating system.\n\nBy using the Hystax Acura Migration service, you agree to the license agreements of Marketplace and Hystax Acura Migration.\n\n1. Preparatory steps\n\nRegister at VK Cloud.\nConfigure two-factor authentication (2FA) for the account on whose behalf the restored infrastructure will be deployed.\nCreate a VM for which recovery will be applied. As part of the quick start, the Ubuntu-DR VM with the Ubuntu 18.04 operating system will be used.\nConnect Hystax Acura Migration service.\n\n   Wait for the installation to complete — a link with a username and password will be sent to your email. The service will be deployed at https://migration.mcs-cloud.ru (Hystax Acura personal account).\n\n2. Make data replication\n\nSign in to Hystax Acura personal account using the received username and password.\nClick the Install replication agents button.\nIn the “Agent selection” step, select Linux and click Next.\nAt the “Agent settings” step, specify the parameters:\n\n   Machines group: Default.\n   Select target Linux distribution: Debian/Ubuntu (.deb package).\n   Snapshot driver deployment type: DKMS.\n\nClick the Next button.\nInstall the agent on the target VM, following the instructions for the Ubuntu distribution.\n\n\n   You can install migration agents on a group of VMs with different operating systems via Ansible.\n\n     Ansible manifest for installing agents\n\n      hosts: all\n     vars:\n       ansible_ssh_pipelining: true\n\n     tasks:\n       name: Generate URL rpm\n         set_fact:\n           download_url: \"https://{{ acura_host }}/linux_agent/{{ customer_id }}?dist_type=rpm&platform=x64\"\n           remote_path: /tmp/hlragent.rpm\n         when: ansible_os_family == \"RedHat\"\n\n       name: Generate URL deb\n         set_fact:\n           download_url: \"https://{{ acura_host }}/linux_agent/{{ customer_id }}?dist_type=deb&platform=x64\"\n           remote_path: /tmp/hlragent.deb\n         when: ansible_os_family == \"Debian\"\n\n       name: Download agent\n         get_url:\n           url: \"{{ download_url }}\"\n           dest: \"{{ remote_path }}\"\n           mode: 0644\n           validate_certs: no\n           timeout: 300\n         become: yes\n\n       name: Install Hystax Linux Replication Agent from rpm package\n         yum:\n           name: \"{{ remote_path }}\"\n           state: present\n         become: yes\n         when: ansible_os_family == \"RedHat\"\n\n       name: Install Hystax Linux Replication Agent from deb package\n         apt:\n           deb: \"{{ remote_path }}\"\n           state: present\n         become: yes\n         when: ansible_os_family == \"Debian\"\n\n       name: Remove package file\n         file:\n           path: \"{{ remote_path }}\"\n           state: absent\n         become: yes\n\n\n   After installing the VM agent, Ubuntu-DR will appear on the main page of Hystax Acura personal account with Discovered status.\n\nExpand the Ubuntu-DR VM menu in the Machines Groups list and select the Start Replication option.\nWait for the operation to complete — the VM status will change to Synced.\n\n3. Create a migration plan\n\nClick the Create Migration plan button.\nIn the Name field, specify the name of the plan MR-plan.\nGo to the Expert tab and click Generate Migration Plan from all machines button.\n\n   A JSON file with the Ubuntu-MR VM will be generated.\n\nAdjust the plan according to the requirements for VM recovery after failures:\n\n    In the subnet_id parameter, specify the network ID for the Ubuntu-DR VM.\n    In the flavor parameter, specify the name of the VM template, specify it using the openstack flavor list command.\n\n    Detailed description of the parameters in the official documentation Hystax Acura.\n\n    Example of a single VM migration plan\n\n    This plan describes one VM and the subnet in which the migrated VM will be deployed.\n\n        {\n        \"subnets\": {\n            \"subnet_0\": {\n                \"name\": \"subnet_0\",\n                \"cidr\": \"10.0.1.0/24\",\n                \"subnet_id\": \"2aebd081-44a8-480f-xxxx-yyyyyyyyyyyy\"\n            }\n        },\n        \"devices\": {\n            \"ubuntu01\": {\n                \"id\": \"ec09a435-3389-d19f-4cf4-zzzzzzzzzzz\",\n                \"security_groups\": [\n                    \"default_all\"\n                ],\n                \"availability_zone\": \"MS1\",\n                \"rank\": 0,\n                \"flavor\": \"Standard-4-8-80\",\n                \"ports\": [\n                    {\n                        \"name\": \"port_0\",\n                        \"ip\": \"10.0.1.23\",\n                        \"floating_ip\": true,\n                        \"subnet\": \"subnet_0\"\n                    }\n                ]\n            }\n        }\n    }\n\n\nClick the Save button.\n\n4. Run the plan\n\nGo to Migrate.\nSelect the MR-plan plan and click Next button.\nIn the Cloud Site Name field, specify the value VK-Cloud-infra and click Run migration button.\n\n   The migration process will begin.\n\n   If the process completes successfully, VK-Cloud-infra will appear in the Cloud Site block with Active status.\n   If the process ended with errors, restart it:\n\n     In the Cloud Sites block, click the Delete button for the process that failed and confirm the deletion.\n     In the Migration plans block, for the MR-plan plan, click the Edit button.\n     Make the necessary changes (in basic or expert mode).\n     Click the Save button.\n     Restart the plan.\n\n5. Check the functionality of the added VM\n\nFind the added VM in VK Cloud (_cloud_agent), perform arbitrary operations on it.\n\nDelete unused resources\n\nRunning VMs consume computing resources. If you don't need them anymore:\n\nDelete Ubuntu-MR VM, added to Hystax Acura personal account.\nDelete (detach) the backup infrastructure VK-Cloud-infra via Hystax Acura personal account.\nDelete floating IP address if it was created during migration.\n","url":"/en/applications-and-services/marketplace/initial-configuration/migrate-hystax","description":"Steps to migrate the client's infrastructure to the target VK Cloud project using Hystax Acura Migration.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"75df804b-853a-4947-ae4e-5f8096f60506","title":"Infrastructure protection with UserGate NGFW","data":"You can configure a firewall to protect your infrastructure from network attacks using the UserGate NGFW service. Its features:\n\nprotection against attacks;\ntraffic management;\nusers authentication;\nblocking malicious content when browsing external Internet resources.\n\n\n    Recommended server specifications for the service\n\n\n\nTo create a VM that has more than 24 CPUs, contact technical support.\n\n\n\nThis instruction will help you deploy the UserGate NGFW service on a VM in VK Cloud, connect to the UserGate NGFW console and add a new network through the service console.\n\nBy using the UserGate NGFW service, you agree to the license agreements of Marketplace and UserGate.\n\n\n\nUserGate NGFW is provided according to the BYOL (Bring Your Own License) model: purchase a license yourself to use the service.\n\nPreparatory steps\n\nRegister at VK Cloud.\nCreate a network with the Internet access, if one has not already been created.\nIn the settings of the subnet where the VM with the deployed service will be located, disable the Private DNS option.\nDeploy the UserGate NGFW service:\n\n   Select the previously created network with Internet access and subnet.\n   Choose other parameters at your discretion.\n\n   After the installation is complete, you will receive a one-time link to your login and password by email. Write them down. The service will be deployed at an address like https://:8001 (the UserGate console).\n\n(Optional) Configure an intermediate server (jump host) for the service VM to increase operational security.\n\n1. Add a network to the service\n\n\n\nBy default, UserGate NGFW is created with a single network to connect to the MGMT port.\n\n\n\nGo to the UserGate console directly via the IP of the virtual machine or through the intermediate server.\nAt the language selection step, specify English.\nAt the time zone selection step, specify Europe/Moscow.\nRead and accept the terms of the license agreement.\nAt the step of installing the first node, specify the login and password received after deploying the service.\nClick the Start button.\n\n   The UserGate console dashboard will open.\n\nGo to Settings → Network → Interfaces. Make sure that there is one network adapter in the Current node group (the network selected during the service deployment stage).\nGo to your personal account at VK Cloud.\nGo to Cloud Servers → Virtual.\nOpen the page of the service VM (usually usergate) by clicking on its name in the list.\nGo to the Networks tab.\nConnect the required network to the VM.\nReboot the VM using VK Cloud or via the UserGate console.\n\n2. Check for the added network\n\nGo to the UserGate console directly via the IP of the virtual machine or through the intermediate server.\nGo to Settings → Network → Interfaces.\nMake sure that the new network adapter has appeared in the Current node group. It will be inactive.\n\nFor advanced service configuration, use the official UserGate NGFW manual.\n\n\n\nRecommendations for working with the service:\n\nWhen configuring the network interface, use static addressing: the IP address must match the address assigned to the VM port. You can see the address on the Networks tab of the VM page.\nIf you are connecting to the external network, manually configure the IP addresses in the UserGate console. Use the network details from the section Cloud networks → Networks.\n\nDelete unused resources\n\nThe running service infrastructure consumes computing resources. If you no longer need it:\n\nDelete the UserGate NGFW service instance.\nDelete the network used for the service.\nDelete the floating IP address created during service deployment.\n","url":"/en/applications-and-services/marketplace/initial-configuration/usergate-start","description":"Deployment of the UserGate NGFW service in a VK Cloud project, connecting a network to the service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"6c17dbdd-c35c-4253-8a71-3aa2f79034db","title":"Nexus 3 deployment","data":"You can deploy repositories of libraries and artifacts used in modular development using the Nexus 3 service. This instruction will help you deploy the Nexus 3 service on a VM in VK Cloud, sign in to the service console and create a new user.\n\nBy using the Nexus 3 service, you agree to the license agreements of the Marketplace and Sonatype services.\n\nTo deploy the Nexus service in a project:\n\nRegister at VK Cloud.\nCreate a network with Internet access, if one has not been created earlier.\nIn the settings of the subnet where the VM with the deployed service will be located, disable the Private DNS option.\nDeploy Nexus 3 service:\n\n   Select the previously created network with Internet access and subnet. The external IP address will be assigned automatically.\n   Choose the other parameters at your discretion.\n\n   After the installation is complete, you will receive a one-time link to the Nexus 3 console (nexus_url) and a password (password). The service will be deployed at the address like https://nexus-.xaas.msk.vkcs.cloud.\n\nGo to the Nexus 3 console.\nClick the Sign in button.\nIn the window that opens, enter the username admin and the password admin_password received by mail.\n(Optional) Go through the checklist from the official documentation.\n\n\n\nFor the extended configuration of the service, use the official Sonatype Nexus Repository 3 instructions.\n\n\n","url":"/en/applications-and-services/marketplace/initial-configuration/nexus-3-start","description":"Steps to deploy the Sonatype Nexus 3 service to the target VK Cloud project using the Marketplace.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"3ae4cfae-d23c-468f-85b4-0ded5aa6aba6","title":"Grafana deployment","data":"You can visualize resource monitoring data using the Grafana service.\n\nThis instruction will help you deploy the Grafana 10 service on a VM in VK Cloud, sign in to the service console and create a new user.\n\nBy using the Grafana service, you agree to the license agreements of the services Marketplace and Grafana Labs.\n\nTo deploy the Grafana service in a project:\n\nRegister at VK Cloud.\nCreate a network, if one has not been created earlier.\nIn the settings of the subnet where the VM with the deployed service will be located, disable the Private DNS option.\nDeploy Grafana service:\n\n   Как будет размещена Grafana: specify the external access type to Grafana to have access to the service via an external IP address.\n   Резервное копирование: select the no option to not copy the service data. With the yes option, data for the last 7 days is saved to the Cloud Storage.\n   Сеть: select the previously created network and subnet.\n\n   Specify the other parameters at your discretion.\n\n   Wait for the installation to complete and a one-time link will be sent to your email with a URL and password. The service will be deployed at the address https://grafana-.xaas.msk.vkcs.cloud (Grafana console).\n\nUse the grafana_url link from the email to go to the Grafana console.\nClick the Sign in button.\nIn the window appears, enter the username admin and the password admin.\nEnter a new password.\n(Optional) Create a dashboard according to the official documentation.\n\n\n\nFor advanced configuration of the service, use the official Grafana instructions.\n\n\n","url":"/en/applications-and-services/marketplace/initial-configuration/grafana-start","description":"Steps to deploy the Grafana 10 service to the target VK Cloud project using the Marketplace.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"88d700e5-4c3d-4a99-a5ee-ac43a5b4afcb","title":"Redash 10.1 deployment","data":"You can visualize data from various sources (databases, Google Sheets), as well as build interactive reports using the Redash service. This instruction will help you deploy the Redash 10.1.0 service on a VM in VK Cloud, sign in to the service console and create a new user.\n\nBy using the Redash 10.1.0, you agree to the license agreements of the Marketplace and Redash services.\n\nTo deploy the Redash 10 service in a project:\n\nRegister at VK Cloud.\nCreate a network with Internet access, if one has not been created earlier.\nIn the settings of the subnet where the VM with the deployed service will be located, disable the Private DNS option.\nDeploy Redash 10 service:\n\n   Select the previously created network with Internet access and subnet. The external IP address will be assigned automatically.\n   Choose the other parameters at your discretion.\n\n   After the installation is completed, a one-time link with access will be sent to your email.\n\nFollow the link from the email.\nSave the data to access Redash, including redash_url — has the https://redash-.xaas.msk.vkcs.cloud format.\n\n\n   If you have not saved the access data, generate new ones.\n\n\nGo to the Redash console using the link from redash_url.\nIn the administrator registration data window that opens, and click the Setup button.\n(Optional) Go through the getting started guide. If necessary, review the official Redash documentation.\n","url":"/en/applications-and-services/marketplace/initial-configuration/redash-start","description":"Steps to deploy the Redash 10.1.0 service to the target VK Cloud project using the Marketplace. Deploying the Redash service for data visualization tasks.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"540d3b48-8433-4b50-a91e-6e6306560fb5","title":"VictoriaMetrics deployment","data":"You can collect, store and analyze metrics in the time series database using the VictoriaMetrics service.\n\nThe instructions will help you deploy the VictoriaMetrics service (using the example of version 1.93.9) on a VM in VK Cloud and configure the collection of metrics.\n\nBy using VictoriaMetrics, you agree to the Marketplace and VictoriaMetrics license agreements.\n\nTo deploy the VictoriaMetrics service in a project:\n\nRegister at VK Cloud.\nCreate a network, if one has not been created earlier.\nIn the settings of the subnet where one or more servers with the deployed service will be located, disable the Private DNS option.\nDeploy service in the project by choosing the appropriate tariff plan (Single, Cluster Mini, Cluster Maxi):\n\n    Learn more about configurations for tariff plans\n\n   Single\n   Cluster Mini\n   Cluster Maxi\n\n\n   One server responsible for receiving, storing and processing metrics. The service is deployed on a single VM, supports vertical scaling (increasing CPU and RAM).\n\n\n   A cluster of several nodes with components:\n\n   vminsert — receiving metrics in various formats;\n   vmselect — executing queries for metrics stored within vmstorage;\n   vmstorage — storing metrics on disk.\n\n   Additionally, you can configure vmagent on any node, the executable file is included in the delivery.\n\n   An instance of the service is deployed on a given number of nodes, each node includes all three components. All nodes in the cluster are equivalent. Configuration type and the disk size is set to be the same for all nodes in the cluster. Supports vertical (increasing CPU and RAM) and horizontal (adding nodes) scaling.\n\n\n   A cluster of several nodes with components:\n\n   vminsert — receiving metrics in various formats;\n   vmselect — executing queries for metrics stored within vmstorage;\n   vmstorage — storing metrics on disk.\n\n   Additionally, you can configure vmagent on any node, the executable file is included in the delivery.\n\n   An instance of the service is deployed on a specified number of nodes, each node includes only one of the components. Configuration type and the disk size is set individually for each node of the cluster. Supports vertical (increasing CPU and RAM) and horizontal (adding nodes) scaling.\n\n   Single\n   Cluster Mini\n   Cluster Maxi\n\n\n   At the “Настройки VictoriaMetrics” step:\n\n      Резервное копирование: select the no option to not save data to the object storage Cloud Storage. With the yes option, the data for the last 7 days will be copied.\n      Сколько хранить все метрики: specify the storage time of the metrics with the appropriate suffix: h (hour), d (day), w (week), y (year). If you do not specify a suffix, months are used as the unit of measurement. The minimum value is 24h (1d), the default is 12 (12 months).\n      Параметры дедупликаци: specify the frequency of deleting the same metrics, use the suffixes ms, s, m, h. A metric is a collection of the metric itself and its metadata. For example, the metrics cpu{host=hostname1} and cpu{host=hostname2} are considered different. The default value is 1ms.\n\n   Click the Next step button.\n   At the “Параметры сервера” step:\n\n      Сеть: select the previously created network and subnet.\n      Availability zone: select which of the data centers the VM will be running in.\n      Type of virtual machine: select the preset VM configuration. For more information, see Cloud Servers service overview.\n      For the system disk and data disk:\n\n        Disk size: specify the required VM disk size in gigabytes.\n        Disk Type: select one of the values — HDD, SSD or High-IOPS SSD. For more information, see Cloud Servers service overview.\n\n   Click the Next step button.\n   At the “Confirmation” step, review the calculated cost of the service and click Connect the tariff.\n\n\n   At the “Настройки VictoriaMetrics” step:\n\n      Резервное копирование: select the no option to not save data to the object storage Cloud Storage. With the yes option, the data for the last 7 days will be copied.\n      Replication factor: specify the number of copies of metrics that will be recorded in vmstorage on different VMs.\n      Сколько хранить все метрики: specify the storage time of the metrics with the appropriate suffix: h (hour), d (day), w (week), y (year). If you do not specify a suffix, months are used as the unit of measurement. The minimum value is 24h (1d), the default is 12 (12 months).\n      Параметры дедупликаци: specify the frequency of deleting the same metrics, use the suffixes ms, s, m, h. A metric is a collection of the metric itself and its metadata. For example, the metrics cpu{host=hostname1} and cpu{host=hostname2} are considered different. The default value is 1ms.\n\n   Click the Next step button.\n   At the “Параметры серверов” step:\n\n      Количество серверов: specify the number of VMs to deploy in the cluster.\n      Сеть: select the previously created network and subnet.\n      Availability zone: select which of the data centers the VM will be running in.\n      Type of virtual machine: select the preset VM configuration. For more information, see Cloud Servers service overview.\n      For the system disk and data disk:\n\n        Disk size: specify the required VM disk size in gigabytes.\n        Disk Type: select one of the values — HDD, SSD or High-IOPS SSD. For more information, see Cloud Servers service overview.\n\n   Click the Next step button.\n   At the “Confirmation” step, review the calculated cost of the service and click Connect the tariff.\n\n\n   На шаге «Настройки Кластера»:\n\n      Резервное копирование: select the no option to not save data to the object storage Cloud Storage. With the yes option, the data for the last 7 days will be copied.\n      Replication factor: specify the number of copies of metrics that will be recorded in vmstorage on different VMs.\n      Сколько хранить все метрики: specify the storage time of the metrics with the appropriate suffix: h (hour), d (day), w (week), y (year). If you do not specify a suffix, months are used as the unit of measurement. The minimum value is 24h (1d), the default is 12 (12 months).\n      Параметры дедупликаци: specify the frequency of deleting the same metrics, use the suffixes ms, s, m, h. A metric is a collection of the metric itself and its metadata. For example, the metrics cpu{host=hostname1} and cpu{host=hostname2} are considered different. The default value is 1ms.\n\n   Click the Next step button.\n   At the “Общие параметры” step:\n\n      Сеть: select the previously created network and subnet.\n      Availability zone: select which of the data centers the VM will be running in.\n      Disk size: specify the required VM disk size in gigabytes.\n      Disk Type: select one of the values — HDD, SSD or High-IOPS SSD. For more information, see Cloud Servers service overview.\n\n   Click the Next step button.\n   At the “Параметры компонентов” step:\n\n      For each of the vmselect, vminsert and vmstorage components, specify the number of deployed VMs in the cluster and the type of virtual machine.\n      For the data disk for vmstorage:\n\n        Disk size: specify the required VM disk size in gigabytes.\n        Disk Type: select one of the values — HDD, SSD or High-IOPS SSD. For more information, see Cloud Servers service overview.\n\n   At the “Confirmation” step, review the calculated cost of the service and click Connect the tariff.\n\n\n   After the installation is completed, a one-time link with access will be sent to your email.\n\nFollow the link from the email.\nSave the data to access VictoriaMetrics.\n\n\n   If you have not saved the access data, generate new one.\n\n\n(Optional) Configure the collection of metrics depending on the selected configuration:\n\n   Single: use the instructions from the official documentation.\n   Cluster Mini and Cluster Maxi: use the vmagent utility.\n\n   For the extended configuration of the service, use the official instructions VictoriaMetrics.\n","url":"/en/applications-and-services/marketplace/initial-configuration/victoriametrics-start","description":"Deployment of the VictoriaMetrics service in the VK Cloud project in a single or cluster configuration.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"216f93f5-8ffc-4f06-9182-3efa8158dc42","title":"OpenVPN deployment","data":"You can manage access to the cloud resources using the OpenVPN. This guide will help you deploy the OpenVPN service on a virtual machine (VM) in VK Cloud, access the service console, and create a new user.\n\nBy using the OpenVPN service, you agree to the service agreements of Marketplace and OpenVPN.\n\nTo deploy the OpenVPN service in the project::\n\nRegister at VK Cloud.\nCreate a network with Internet access, if one has not been created earlier.\nIn the settings of the subnet where the VM with the deployed service will be located, disable the Private DNS option.\nDeploy the OpenVPN service:\n\n   In the “Настройки Кластера” step, specify VM settings:\n\n      Сеть: select the previously created network with Internet access and subnet. The external IP address will be assigned automatically.\n      Choose other VM parameters at your discretion.\n\n   Click the Next Step button.\n   In the “Настройки OpenVPN” step, specify network connectivity settings:\n\n      Подсеть клиентских адресов: specify the subnet address where the service will assign addresses for user devices. Ensure that this address space does not overlap with the address space being configured for access.\n      Адрес DNS: the DNS server address that OpenVPN will use to resolve domain names to IP addresses. It can be a local or external server if it is accessible from the subnet.\n      Активировать full-tunneling: if this option is enabled, all user traffic will be routed through the VPN. This will increase the server load if large data streams are being transmitted. Disable the option if you only need to route user traffic through the VPN to access the subnet.\n      Адреса подсетей: specify the subnet addresses, separated by commas, that need to be accessed. Example: 10.0.0.0/24,10.0.10.0/24. The subnets specified in this field will be accessible from the subnet where the service was deployed. The deployment subnet is specified by default in the service, so there is no need to add it here. This parameter is ignored if the Активировать full-tunneling option is enabled.\n\n\n      Check the router settings in the subnets: network connectivity has to be set between subnets in the Адреса подсетей field.\n\n\n   Click the Next Step button.\n   In the “Confirmation” step, review the estimated cost of the service and click the Connect the Tariff button.\n\n   After the installation is complete, an email with a one-time link to a page will be sent, which contains:\n\n   configuration for the first user;\n   access key to the VM;\n   external and internal IP addresses of the VM.\n\nConnect to the service VM via SSH. To do this, run the command in the terminal:\n\n      ssh -i  ubuntu@\n\n   Here:\n\n   `` — access key obtained from the one-time link.\n   `` — external IP address of the VM obtained from the one-time link.\n\nGet root user privileges, run the sudo bash command.\nCreate a new user. Run the command:\n\n      /home/ubuntu/create_client.sh\n\n   After the script successfully completes, a configuration file for the new user will appear in the /etc/openvpn/server path.\n(Optional) Block the user. Run the command:\n\n      /home/ubuntu/revoke_client.sh\n\n   After the script successfully completes, the crl.pem certificate will be updated, and the OpenVPN service will be restarted. Access for the `` user will be revoked.\n\n   The user configuration file will not be deleted from the /etc/openvpn/server directory. When trying to create a user with the same name, the script will run, but nothing will happen as the user already exists. To restore access for the user, create a configuration file with a new name.\n\n\n\nFor advanced service configuration, use the official OpenVPN instruction.\n\n\n","url":"/en/applications-and-services/marketplace/initial-configuration/openvpn-start","description":"Deployment of the service providing secure access to VK Cloud project infrastructure, adding and blocking users.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"06de4ba5-2b22-49ab-9fff-bbbbcaf0423f","title":"Connecting the service","data":"Go to VK Cloud personal account.\nGo to App store.\nOn the card of the required service, click Details.\nRead the description of the service. When installing the image-based service, make sure that there are enough quotas in the project.\nClick the Connect.\nFollow the steps of the tariff plan configuration wizard.\n\n\n   The steps may vary for different services.\n\n\nConfirm the connection of the service.\n\n   Wait for the installation to complete — a one-time link with the details for working with the service will be sent to the mail. Save them before you start working.\n\n\n\nInstructions for getting started with the services are given in the section Configuring marketplace services.\n\n\n","url":"/en/applications-and-services/marketplace/service-management/pr-instance-add","description":"Instructions for connecting the service via the Marketplace in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"e8c30e16-478b-4fde-9d38-36c580d4b841","title":"Managing a service instance","data":"Viewing information about a service instance\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to App store → My services.\nOpen the service instance page by clicking on its name in the list.\n\n\n\n\nPossible service instance statuses:\n\nРазворачивается** — a service instance in the process of deployment.\nАктивный** — the service instance has been created or updated, ready for use.\nОшибка создания** — failed to create a service instance due to an error.\nОбновляется** — an instance of the service in the process of updating the tariff plan or options.\nОшибка обновления** — failed to update the service instance due to an error.\nУдален** — the service instance has been deleted.\nПриостанавливается** — the service instance is in the process of deactivation due to insufficient amount to pay for the use of the service.\nОшибка приостановки** — the service instance could not be deactivated due to an error.\nНеактивный** — the service instance has been deactivated. The deactivated instance of the service is unavailable for use.\nВозобновляется** — service instance in the process of activation.\n\nActivating a service instance\n\nActivation will be required:\n\nwhen tariffication is prepaid;\nafter replenishment of the project account, if the service instance was deactivated due to lack of funds.\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to App store → My services.\nOpen the service instance page by clicking on its name in the list.\nGo to Service properties tab.\nClick the Activate button.\n\nEditing the tariff plan\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to App store → My services.\nOpen the service instance page by clicking on its name in the list.\nGo to Tariff plans tab.\nClick the Set up a tariff button.\nSpecify the new values of the tariff options and click Connect the tariff button.\n\nChanging the tariff plan\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to App store → My services.\nOpen the service instance page by clicking on its name in the list.\nGo to Tariff plans tab.\nClick the Choose a tariff button.\nSpecify the new tariff plan and click Connect the tariff button.\n\n\n\n\nThe update of the service instance will begin, the process may take a long time. If the update has completed with the status Ошибка обновления, updating access to a service instance.\n\nUpdating access to a service instance\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to App store → My services.\nOpen the service instance page by clicking on its name in the list.\nGo to the Service properties tab.\nClick the Update Access button.\nConfirm the update of the access parameters.\n\n   An email to update access to the service instance will be sent to the user's email.\n\nRecreating or re-updating a service instance\n\nThe operation is available if the service instance is in the status Ошибка создания or Ошибка обновления.\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to App store → My services.\nOpen the service instance page by clicking on its name in the list.\nGo to the Service properties tab.\nClick the Retry button.\n\nDeleting a service instance\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nGo to App store → My services.\nOpen the service instance page by clicking on its name in the list.\nGo to the Service properties tab.\nClick the Delete button.\nConfirm the deletion.\n\n   The service instance will be deleted, but will remain in the general list.\n\n\n\n","url":"/en/applications-and-services/marketplace/service-management/pr-instance-manage","description":"Operations for managing deployed SaaS and image-based service instances in Marketplace VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"3a2a66f7-907b-4d3f-9bbb-d2fa5fcde780","title":"Tariffication","data":"About tariffication\n\nThe cost of using the service instance depends on the selected tariff plan and options. It is reflected in the service configuration wizard and consists of two parts:\n\nThe cost of using the service, it is reflected in the Tariff plans tab.\nFor image-based services — the cost of the infrastructure, it is reflected in the section Details.\n\nThe total cost will be displayed at the last step of connecting the service.\n\nThere are two types of tariff plans available for charging services:\n\n\n\nPrepaid\nPostpaid\n\n\n\nThe cost is fixed.\nPayment is debited once in the reporting period on the date of activation of the service.\n\nIf there are not enough funds when debiting, the instance status changes:\n\nIf a free promo tariff is available for an instance, the instance is forcibly transferred to it.\nIf a free promo tariff is not available for an instance, the instance is blocked for use until funds are replenished.\n\n\n\n\nThe cost is calculated based on the amount of service resources actually used (for example, the amount of storage used).\nThe frequency and day of debiting depend on the specific service.\n\nIf there are not enough funds when debiting, the instance status changes:\n\nSoft Limit. The beginning of the period is considered the first unsuccessful attempt to write off funds. The maximum duration of the period is up to 10 days, the balance is checked daily. If there were no successful write-offs during this period, the Hard Limit period begins.\nHard Limit. Compulsory cancellation of all debts on the service. The instance of the service is blocked until the full payment of the debt for 10 days:\n\n   If the balance remains positive after debiting, the period is reset to zero, a notification is received about the need for manual activation for the service instance.\n   If there are not enough funds, after the expiration of the period, the service instance is deleted without the possibility of recovery.\n\n\n\n\n\n\nFor information on how to pay for the use of the service instance, see the section Billing.\n\nCharged\n\nFor SaaS and image-based services:\n\nApplication of paid tariff plan options. Tariff option — a specific parameter of the tariff plan.\nVirtual project resources used for backup or recovery.\nOther services provided within the service (the final set depends on the vendor's service).\n\nFor image-based services, VK Cloud virtual resources deployed to maintain the service infrastructure are charged: VMs, disks, networks, routers. The full list of required resources is available when connecting the service instance.\n\nNot charged\n\nUse of the personal account of any of the vendor's services.\nPerforming basic operations with the service instance: restart, delete.\nNetwork traffic to maintain infrastructure.\nUse of the service when choosing a promotional tariff.\n","url":"/en/applications-and-services/marketplace/tariffication","description":"Features of billing services and applications in the Marketplace VK Cloud, cost calculation.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"a40ba425-ac76-4fad-a3ea-1afafa79d7a0","title":"FAQ","data":"\n\n\nWhat is Marketplace?\n\nMarketplace is a VK Cloud subsystem that provides customers with access to third—party services of various kinds, for more information, see the article About the service.\n\n\n\n\n\nWhat services and applications are available for connection?\n\nYou can view the current list of services and applications in the VK Cloud personal account in the section App Store.\n\n\n\n\n\nWhat is the difference between SaaS and image-based services?\n\nThe services differ in architecture and pricing, the full list of differences is in the article About the service.\n\n\n\n\n\nHow to connect a new service?\n\nUse the instructions to connect the service instance.\n\n\n\n\n\nWhat should I do if the service did not start when connecting?\n\nCheck if there are enough quotas in the project to deploy the service. Free up resources or increase quotas through technical support.\nWith prepaid payment system: check if there are enough funds in the project account.\nUpdate service instance.\n\n\n\n\n\nHow are SaaS and image-based services charged?\n\nThe set of charged resources differs for different types of services, for more information, see the article Tariffication.\n\n\n\n\n\nWhat happens to the service instance if no payment was made?\n\nIf no payment was made with the postpaid method, the service will be deleted, for more information, see the article Tariffication.\n\n\n\n\n\nHow do I add a new service to the Marketplace?\n\nWrite to marketplace@cloud.vk.com with information about the service you are going to add to the Marketplace.\n\n\n","url":"/en/applications-and-services/marketplace/faq","description":"Answers to frequently asked questions about the Vk Cloud Marketplace service.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"Marketplace","link":"/en/applications-and-services/marketplace","isActive":false,"evenSimpler":true}]},{"uuid":"bb21c270-567d-44f5-82cc-1f336d3857a6","title":"About service","data":"With the help of ready-made infrastructure solutions, VK Cloud, in partnership with 1C Firm, provides an opportunity to quickly connect the entire range of cloud solutions for your business: from cloud infrastructure to end applications that support the business.\n\n1C: GRM (\"Ready Workplace\") is a convenient and intuitive interface through which you can connect and deploy the infrastructure in a couple of clicks, as well as receive qualified support.\n\nDescription\n\nService \"1C: Ready Workplace\" allows you to rent popular software products from 1C:\n\n1c accounting\n1C: Salary and Human Resources\n1C: Management of our company\n\nThe service is a remote user access to selected 1C programs deployed in the cloud infrastructure through a browser.\n\nBenefits\n\nWorking with a ready-made 1C application has advantages over deploying your own server and setting up software:\n\nFast access.** The work is carried out through a web browser, where the application is accessed from any computer connected to the Internet.\nFree trial period** . The ability to test the service and evaluate its functionality.\nFlexible payment.** Payment on a monthly basis, according to the rental model, allows you to significantly save the budget, where all costs are transparent and can be regulated.\nImprovements on demand.** There is no need to hire IT specialists to configure and maintain the application. You can get advice or order a revision by sending an email to support.1c.grm@mcs.mail.ru.\nSecurity** . The application infrastructure is based on the VK Cloud fault-tolerant services platform, where virtual machines provide reliable storage and processing of 1C applications.\n\nWho benefits from 1C: timing\n\nRenting a cloud solution based on 1C products and VK Cloud services is suitable for:\n\nPrivate accountants\nOutsourced accounting companies\nFor entrepreneurs\nSmall business\n\nIt becomes easy to combine the work of the company's employees and the accountant in a single information space. There is no need to set up a complex exchange of peripheral bases and purchase several user licenses. You can remotely access the information you need from a tablet or smartphone.\n\nThe cloud service reduces the initial IT costs, because as your business expands, you can connect additional working licenses.\n\nUse cases\n\nDepending on the number of required licenses, the application can be created in file or client-server mode:\n\nFile mode** : recommended for infobases in which up to 5 users are supposed to work simultaneously (simultaneous connection sessions). For the file infobase, up to 30 GB of total data volume is provided, including 5 GB for the working infobase and 25 GB for all its backups.\nClient-server mode** : this type of operation is recommended for \"heavy\" infobases, as well as for infobases in which the simultaneous operation of 6 to 50 users (connection sessions). For a client-server database, no more than 60 GB of total data is provided, including 10 GB for a working infobase and 50 GB for its backups.\n\nNote\n\nIf the total amount of data is exceeded, send an email to support.1c.grm@mcs.mail.ru to increase the volume. The operation is charged separately and regulated by the 1C Firm's pricing policy. The current rates for additional space can be checked with technical support.\n\nLimitations\n\nSimultaneous work with 1C: Timing applications is limited to 5 users who can connect remotely.\n\nFor 1C: GRM applications, there are physical limitations of database tables, the total volume of which should not exceed 12GB of data, and one table cannot be more than 4GB. If this volume is exceeded, VK Cloud cannot guarantee the reliability and fault tolerance of the 1C: GRM application service.\n\nBackup\n\nThe 1C GRM service can back up the application and its data, providing the ability to restore configuration and data upon request:\n\nAutomatic backups are performed at night with a frequency of 1 day on weekdays.\nManual backup is performed upon sending an email to support.1c.grm@mcs.mail.ru.\n\nThe maximum storage depth for backups is 7 units, provided that the total database size does not exceed the quota for storing backups.\n\nSupport\n\nThe 1C: GRM service is supported by technical experts under an agreement with a partner of 1C Firm.\n\n1C: GRM support conditions include an extended list of features:\n\nConsultation on the operation of 1C software products\nConsultation on the operation of the infrastructure of the 1C: GRM solution\nSolving application problems\nRecommendations for optimal interaction with the service\nUpdating application versions and configurations\nApplication customization for individual needs\nDisabling subscription\n\nTo contact technical support, send an email to support.1c.grm@mcs.mail.ru.\n\nSupport is provided on weekdays, from 09:00 to 18:00 Moscow time (UTC + 3). The first response time for requests for application errors is 2 hours, for other requests - 1 business day.\n","url":"/en/applications-and-services/1cgrm/about","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"1C Workplace","link":"/en/applications-and-services/1cgrm","isActive":false,"evenSimpler":true}]},{"uuid":"2303fdfd-1ff3-4c0b-9b2f-4a801f1c35c7","title":"Connecting the service","data":"To activate the 1C: GRM service, you must have a registered account and an active project. If necessary, you should confirm the account and activate the phone number, after which the VK Cloud panel will inform you about the availability of creating an application:\n\n\n\nAdding a 1C application consists of several stages, which are accompanied by the application creation wizard.\n\nUser\n\nThe user of the application is a representative of a legal entity who registers the organization in the application.\n\nNote\n\nUser account data is not entered into the 1C application database and the organization must be registered independently after starting the application.\n\nTo create it, at the first step, select the appropriate item \"Create a new user\":\n\nUser creation\n\nIn the wizard for creating a new user, you need to fill in the card with his data:\n\nFull Name. Organization contact details\nPhone. Contact phone of the representative of the organization\nEmail. The current mailing address of the representative to which notifications will be sent\nINN. Taxpayer identification number\n\n\n\nAttention\n\nWhen registering a user, you should pay attention to the filled in data. Once a user has been created, it is impossible to change or delete his data.\n\nThe TIN field is subject to verification at the stage of creation using the 1C: Counterparty database. Upon completion of filling in the data of the application creation form, the TIN is verified. If it is filled in incorrectly, the application will not be created; to correct the value of the TIN field, go back to the first stage and specify the correct value.\n\nThe registered user will be sent a message to the specified email address containing data for logging in and administering the 1C application.\n\nLicenses\n\nRemote access to the application via the web interface is possible if there is a sufficient number of licenses that grant the user the right to access the created application.\n\n\n\nFor 1C applications, the maximum value of possible remote access licenses is provided:\n\nFor file mode: 1 to 5 units\nFor client-server mode: from 6 to 50 units\n\nLicenses are assigned automatically when a user is created in the application.\n\nYou can change the number of licenses in the application control panel: both up and down, but only within the selected mode. If you need to increase the number of licenses over 5 when using the file mode, send an email to support.1c.grm@mcs.mail.ru with a request to change the type.\n\nNote\n\nThe change in the number of licenses in the application card occurs instantly, but it may take up to 5 minutes for the changes in the application to take effect.\n\nApplication launch\n\nOnce the application has been successfully created, it will appear in the list of applications in the VK Cloud panel. The application card contains the data entered during creation. It is impossible to change or delete this data.\n\nAttention\n\nThe application creation process can take up to 15 minutes. At this time, a virtual infrastructure is created and the 1C application is deployed. During this period, activities with the application will be unavailable.\n\nEditing an application\n\nManaging installed applications is available in the section of your personal account in the context menu of each application.\n\n\n\nEditing is possible only for the number of licenses that provide remote access to users to the application:\n\nConnect to app\n\nRemote access to the application for its administration is carried out using user accounts created in the application. Each user created consumes one license assigned to the application in the VK Cloud panel.\n\nThe initial connection for setting up and administering the application can be performed using one license assigned to the User when creating the 1C: Timing application. Connection data is sent to the User's mailing address specified during registration.\n\nDisabling subscription\n\nTo disable your subscription to 1C: GRM, send an email to support.1c.grm@mcs.mail.ru.\n","url":"/en/applications-and-services/1cgrm/connect","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"1C Workplace","link":"/en/applications-and-services/1cgrm","isActive":false,"evenSimpler":true}]},{"uuid":"79a5eb51-2971-46d0-be58-e005ec266955","title":"Tariffication","data":"The cost\n\nFor all software products of the 1C: GRM service, the general principle of cost calculation operates, regardless of the type of application, consisting of 3 basic components:\n\nComponentCompositionSoftware product license60% of the costInfrastructure30% of the costSupport10% of the cost\n\nThe table shows the proportional ratio of the cost of the components of the 1C: timing service.\n\nThe calculation of the monthly cost of each component is made automatically in the personal account when the application is launched and depends on the type of application and the number of its users.\n\nThe base cost of applications is determined as follows:\n\napplicationThe costAccounting department39 ₽ per day for 1 licenseSalary and Personnel Management41 ₽ per day for 1 licenseManaging Our Firm40 ₽ per day for 1 license\n\n\n\nPayment is made on the basis of 30 calendar days in a month.\n\nTrial period\n\nFor all 1C software products: Accounting, Salary and Human Resources Management and Management of our company, a demo period is available, within which the service is provided without restrictions for a period of 30 days.\n\nTo get demo access to the application, just select it when creating. There is no withdrawal of funds during the demo access, and this service is not displayed in the \"Balance\" section of the VK Cloud personal account.\n\n\n\nDemo access is provided only for the first application of each type in the project. The trial period cannot be extended or stopped. The trial period starts when the application is created.\n\nBalance and documents\n\nBased on the result of using the 1C: GRM service, the project owner will be invoiced for payment according to the actual time and volume of service use.\n\nIf you need to deposit funds and make a prepayment for the service, send an email to support.1c.grm@mcs.mail.ru.\n\nAt the end of the billing period, the project owner will be sent closing documents, reflecting the period and volume of use of services.\n\nAll services provided within the 1C: GRM are not displayed in the general section Balance your VK Cloud personal account.\n","url":"/en/applications-and-services/1cgrm/tariffication","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"1C Workplace","link":"/en/applications-and-services/1cgrm","isActive":false,"evenSimpler":true}]},{"uuid":"f79ff022-d493-4f24-ae7d-92acc66224f7","title":"FAQ","data":"\n\nWhat questions can I contact technical support about?\n\nInstallation of updates.\nConsultations on operational issues related to the service.\nRecovery from backups.\n\nOther questions, including consultations on working in the program, are not covered by technical support.\n\n\n\n\n\nWhat do I need to run 1C: GRM?\n\nThe launch of 1C: GRM applications is carried out with an activated account and confirmed credentials.\n\n\n\n\n\nCan I use a thin client to connect?\n\nYes, to connect to 1C:GRM, you can use a thin client installed on a local computer.\n\n\n\n\n\nHow many applications can be created?\n\nVK Cloud does not limit the number of created 1C: GRM applications.\n\n\n\n\n\nHow many users can be connected?\n\nThe file use of the 1C application limits remote access to 5 users, client-server: from 6 to 50.\n\n\n\n\n\nWhat to do if I don't know a TIN?\n\nThe application user can be a representative of an existing organization. To find out the TIN of the organization, contact its chief accountant or CEO.\n\n\n\n\n\nDo I need to create a virtual machine myself or do you provide a ready-to-use one?\n\nThe entire infrastructure is deployed automatically. VK Cloud provides an HTTP link to a ready-to-use 1C database. You can access it via a browser or 1C thin client. Access at the infrastructure and operating system level is not provided.\n\n\n\n\n\nCan I use my own 1C licenses?\n\nNo, this option is not provided in the 1C:GRM service.\n\n\n\n\n\nI already use the 1C system. How do I transfer data to your service?\n\nIn your 1C system, use the \"Unload data from the service\" function.\nSend an email to support.1c.grm@mcs.mail.ru. Our specialists will help you migrate for free.\n\n\n\n\n\nIs it possible to unload the database stored in 1C:GRM? Can this be done on a schedule?\n\nYes, by default the backup database is created daily. To obtain such a copy or change the backup schedule, send an email to support.1c.grm@mcs.mail.ru.\n\n\n\n\n\nIs it possible to install configuration extensions in 1C?\n\nYes, it is possible.\n\n\n\n\n\nCan I use the configurator and modify the program myself?\n\nThis option is not provided.\n\n\n\n\n\nHow to unsubscribe from the 1C: GRM?\n\nTo disable your subscription to 1C: GRM, send an email to support.1c.grm@mcs.mail.ru.\n\n\n","url":"/en/applications-and-services/1cgrm/faq","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Applications and IT services","link":"/en/applications-and-services","isActive":false,"evenSimpler":true},{"title":"1C Workplace","link":"/en/applications-and-services/1cgrm","isActive":false,"evenSimpler":true}]},{"uuid":"72074075-db91-44c6-a4fa-08ea60d1b2a6","title":"API access control","data":"Activation of API access is necessary to work with VK Cloud using the public API and additional tools such as OpenStack CLI, Terraform, kubectl and others.\n\nActivate API access\n\nEnable two-factor authentication.\nClick on the username in the header of the page and enable 2FA in one of the ways:\n\n      Project settings\n      Account settings\n\n\n      In the drop-down menu, select the option Project settings.\n      On the API access tab click Activate API access.\n\n\n      In the drop-down menu, select the option Account settings.\n      Click on the Security tab.\n      In the API access section, click Activate API access.\n\n\nDeactivate API access\n\nEnable two-factor authentication if it has been disabled.\nClick on the user login in the upper right corner of VK Cloud personal account.\nFrom the drop-down menu, select the option Account settings.\nClick on the Security tab.\nIn the API access section, click Deactivate API access.\n","url":"/en/tools-for-using-services/api/rest-api/enable-api","description":"Enabling or disabling access to the VK Cloud API.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"API","link":"/en/tools-for-using-services/api","isActive":false,"evenSimpler":true}]},{"uuid":"449c7801-b300-44e3-9f11-3d3bd5e1b7d1","title":"Getting a Keystone access token","data":"A Keystone token is required to work with some components and resources of the VK Cloud platform via their API. The examples of such are the Virtual networks (Neutron) service and the backups component of the Cloud Servers service.\n\nPreparatory steps\n\nGo to your VK Cloud personal account.\nMake sure that two-factor authentication and API access are enabled.\nAt the top of your personal account page, select the project for which you need a token.\n\nToken generation\n\n\n\nA generated token is valid for one hour. All generated tokens remain valid for their lifetime.\n\n\n\nGet a token:\n\n\n\nPersonal account\nOpenStack CLI\ncURL\n\n\n\nOn the Project settings page of the personal account, open the API access tab.\n\n    A new token is generated automatically when you open the page. If the page remains open, the token is automatically regenerated once in an hour.\n\nIn the lower part of the page, click on the Copy icon next to the API access token parameter. The token will be copied to clipboard.\n\n    The token lifetime is shown when you hover your mouse over the Copy icon. If the token expires soon, use the Reissue button.\n\n\n\n\nMake sure you have the OpenStack client installed and authenticate yourself in the project.\n\nExecute the command:\n\n        openstack token issue -c id -f value\n\n    The token value will be output to the console.\n\n\n\n\nInstall the cURL utility, if not already installed.\n\nAuthenticate yourself in the project. The authentication procedure is the same for OpenStack client and for cURL utility.\n\nPerform the command for your operating system:\n\n    Linux\n    Windows (cmd)\n\n\n        curl -X POST \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"auth\": {\n            \"identity\": {\n                \"methods\": [\n                    \"password\"\n                ],\n                \"password\": {\n                    \"user\": {\n                        \"domain\": {\n                            \"id\": \"'$OS_USER_DOMAIN_NAME'\"\n                        },\n                        \"name\": \"'$OS_USERNAME'\",\n                        \"password\": \"'$OS_PASSWORD'\"\n                    }\n                }\n            },\n            \"scope\": {\n                \"project\": {\n                    \"id\": \"'$OS_PROJECT_ID'\",\n                    \"region\": \"'$OS_REGION_NAME'\"\n                }\n            }\n        }\n    }' \\\n    -i \"https://infra.mail.ru:35357/v3/auth/tokens\" | grep -i '^x-subject-token'| cut -d ':' -f 1,2\n\n\n        curl -X POST ^\n    -H \"Content-Type: application/json\" ^\n    -d \"{\\\"auth\\\": {\\\"identity\\\": {\\\"methods\\\": [\\\"password\\\"], \\\"password\\\": {\\\"user\\\": {\\\"domain\\\": {\\\"id\\\": \\\"%OS_USER_DOMAIN_NAME%\\\"}, \\\"name\\\": \\\"%OS_USERNAME%\\\",\\\"password\\\": \\\"%OS_PASSWORD%\\\"}}}, \\\"scope\\\": {\\\"project\\\": {\\\"id\\\": \\\"%OS_PROJECT_ID%\\\"}}}}\" ^\n    -i \"https://infra.mail.ru:35357/v3/auth/tokens\" | findstr /B x-subject-token | findstr x-subject-token\n\n\nThe token value will be output in the x-subject-token parameter.\n\n\nExamples of responses to a token generation request\n\n\n\nLinux\nWindows (cmd)\n\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 27038  100 26470  100   568  99259   2129 --:--:-- --:--:-- --:--:-- 99138\nx-subject-token: gAAAAABkirBWYerPg-2A_W0blpcg_qcmTck9K3cC1zf4JUnP3lnpq-bf3W_AXbMx8wDd7PNO704lf00QX3--BRvFB-UcI5IQq5GtVNVzkHoqem4Ocg_-fmRgCdtSSrKvw_KqjpxoksOi2EocauqogKJebeYgAoheSMEnrSz4G70OrTHwUmhI4z0\n\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   230    0     0  100   230      0    920 --:--:-- --:--:-- --:--:--   923FINDSTR: Слишком длинная строка 12.\nFINDSTR: Line 12 is too long.\n100 26700  100 26470  100   230  49114    426 --:--:-- --:--:-- --:--:-- 49628\nFINDSTR: Line 12 is too long.\nx-subject-token: gAAAAABkirQja1Lgr9psuyf6fC6e3Sy5WMYubpmwMNPXuT6APQkf-BPRRAySTBGP2h9Iq2U533pi13h_ZIHa0viga7HxmSsEeCZ_Fq1CEy0m75lmpDtZYd8SAazmjqbV5Kf4ygGnp77kPadkL0hAgC0b7vKjgNGoZ9bLZDBQmlEivNMlptyZKcQ\n\nUsage example for the token\n\nTask: to get a list of networks via the REST API (Neutron service).\n\nIn your personal account, find the endpoint for the Neutron service. In this example: https://infra.mail.ru:9696.\nGet the token and copy its value.\nExecute the command using the cURL utility:\n\n      curl https://infra.mail.ru:9696/v2.0/networks -H \"Accept: application/json\" -H \"X-Auth-Token: \"\n\n   An example of the result\n\n      {\n        \"networks\": [\n            {\n                \"ipv6_address_scope\": null,\n                \"dns_domain\": null,\n                \"revision_number\": 6,\n                \"port_security_enabled\": true,\n                \"id\": \"0e4d7c1e-ba20-0000-0000-7623648487a6\",\n                \"router:external\": false,\n                \"availability_zone_hints\": [],\n                \"availability_zones\": [\n                    \"nova\"\n                ],\n                \"ipv4_address_scope\": null,\n                \"shared\": false,\n                \"project_id\": \"b5b7ffd4ef0547e5b222f44500000000\",\n                \"status\": \"ACTIVE\",\n                \"subnets\": [\n                    \"5ab0164b-2528-0000-0000-b2a8d5e62661\"\n                ],\n                \"private_dns_domain\": \"mcs.local.\",\n                \"description\": \"\",\n                \"tags\": [],\n                \"updated_at\": \"2022-11-22T07:24:53Z\",\n                \"name\": \"demoNet2\",\n                \"admin_state_up\": true,\n                \"tenant_id\": \"b5b7ffd4ef0547e5b222f44500000000\",\n                \"created_at\": \"2022-11-22T07:24:51Z\",\n                \"mtu\": 1500,\n                \"sdn\": \"neutron\"\n            },\n        ]\n   }\n\n\nOther examples of token usage:\n\nviewing logs in the Cloud Logging service;\n\nworking with public DNS.\n","url":"/en/tools-for-using-services/api/rest-api/case-keystone-token","description":"Getting an access token in VK Cloud services","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"API","link":"/en/tools-for-using-services/api","isActive":false,"evenSimpler":true}]},{"uuid":"9e50195c-7ba1-4e88-9536-caa872a52df9","title":"API endpoints","data":"In your personal account, you can view the addresses of the available API endpoints. The list and addresses of endpoints differ between regions.\n\nTo work with VK Cloud via the API activate API access. Read more about working with services in API specification.\n\nViewing the list of endpoints\n\nThe list is available to all users, regardless of their role if services are activated in the project.\n\nTo see the list of endpoints for your region:\n\nGo to VK Cloud personal account.\nClick on the user name in the header of the page and select Project settings.\nGo to API endpoints tab.\n\nEndpoints of the Moscow region\n\nCloud Servers\n\n| Endpoint                            | Address                 | Purpose |\n|-------------------------------------|-------------------------|--|\n| Nova              | https://infra.mail.ru:8774/v2.1                     | Managing virtual machines |\n| Cinder            | https://public.infra.mail.ru:8776/v3/   | Managing disks and their snapshots |\n| Glance            | https://infra.mail.ru:9292                          | Managing VM images |\n| Karboii           | https://mcs.mail.ru/infra/karboii/v1                | Managing backups of VMs and database instances |\n| Manila            | https://public.infra.mail.ru:8786/v2/   | Managing file shares |\n\n\n\nThe Cloudlogs endpoint (https://mcs.mail.ru/cloudlogs/v1/logs) that is not shown in the personal account allows viewing VM logs.\n\n\n\nOther endpoints\n\nCloud Containers\n\n| Endpoint                            | Address                 | Purpose |\n|-------------------------------------|-------------------------|--|\n| Magnum            | https://infra.mail.ru:9511/v1                       | Managing Cloud Containers |\n| Magnum-addons     | https://mcs.mail.ru/infra/container/addons          | Managing Cloud Containers add-ons |\n\nCloud Networks\n\n| Endpoint                            | Address                 | Purpose |\n|-------------------------------------|-------------------------|--|\n| Neutron           | https://infra.mail.ru:9696                          | Managing all network infrastructure objects, except public DNS zones and load balancers |\n| Octavia           | https://public.infra.mail.ru:9876                   | Managing load balancers |\n| Publicdns         | https://mcs.mail.ru/public-dns                      | Managing public DNS zones |\n\nCloud Big Data\n\n| Endpoint                            | Address                 | Purpose |\n|-------------------------------------|-------------------------|--|\n| Sahara            | https://infra.mail.ru:8386/v1.1/        | Managing Cloud Big Data clusters |\n\nCloud Databases\n\n| Endpoint                            | Address                 | Purpose |\n|-------------------------------------|-------------------------|--|\n| Trove             | https://infra.mail.ru:8779/v1.0/        | Managing databases |\n\nCloud Storage\n\n| Endpoint                            | Address                          | Purpose |\n|-------------------------------------|----------------------------------|--|\n| S3 domain                 | https://hb.ru-msk.vkcs.cloud/    | Managing Cloud storage |\n\nCloud ML Platform\n\n| Endpoint                              | Address                     | Purpose |\n|---------------------------------------|---------------------------| -----------------|\n| Mlplatform | https://msk.cloud.vk.com/infra/mlplatform   | Managing Cloud ML Platform |\n\nAI API\n\n| Endpoint                      | Address                   | Purpose |\n|-------------------------------|---------------------------|--|\n| Vision for image and video recognition  | https://smarty.mail.ru/   | Objects recognition |\n\nEndpoints not associated with a service\n\n| Endpoint                            | Address                 | Purpose |\n|-------------------------------------|-------------------------|--|\n| Audit             | https://mcs.mail.ru/auditlogs/v1/       | Collecting the statistics of users' actions in the project |\n| Barbican          | https://public.infra.mail.ru:9311                   | A protected storage for secrets (SSH keys, Keystone tokens, TLS certificates) |\n| Keystone          | https://infra.mail.ru:35357/v3/                     | Managing project users, user authentication via tokens |\n| Quota-manager     | https://mcs.mail.ru/quota-manager                   | Viewing project quotas |\n| Heat              | https://infra.mail.ru:8004/v1/          | Cloud services orchestration (deprecated)\n\nGetting Project ID\n\nThe addresses of some endpoints contain the variable ``. When creating a request, replace this variable with the Project ID of the current project.\n\nThe Project ID does not match the project ID to VK Cloud personal account. This is another identifier that is used in configuration files:\n\nIn openrc.sh, as the value of the variable OS_PROJECT_ID. This file is required to work with additional tools such as the OpenStack CLI or cURL.\nIn vkcs_provider.tf, as the value of the project_id parameter. This file is required to work through Terraform.\n\nTo get the Project ID value for a project:\n\nGo to VK Cloud personal account.\nOpen the required project by selecting it from the list in the header of the personal account page.\nClick on the user name in the header of the page and select Project settings.\nGo to API access or Terraform tab.\n\n    The value is available on both tabs, under the headings Project ID and Project ID / tenant_id, respectively.\n\n\n    On the API Access tab, the Project ID value is displayed if API access is activated.\n\n","url":"/en/tools-for-using-services/api/rest-api/endpoints","description":"List of VK Cloud API endpoints for the current region.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"API","link":"/en/tools-for-using-services/api","isActive":false,"evenSimpler":true}]},{"uuid":"e4e9f881-50e0-4342-acee-9170162fe15e","title":"Installing jq","data":"To filter the output of data in JSON format, use the jq utility. It is applicable for API and OpenStack data received from the platform.\n\njq must be pre-installed on your system.\n\n\n  Example of unformatted JSON\n\n    {\"status\":200,\"body\":{\"object_labels\":[{\"status\":0,\"name\":\"file\",\"labels\":[{\"eng\":\"Close-up\",\"rus\":\"Крупный план\",\"eng_categories\":[],\"rus_categories\":[],\"prob\":0.4843,\"coord\":[165,0,834,477]},{\"eng\":\"Macro Photography\",\"rus\":\"Макросъемка\",\"eng_categories\":[],\"rus_categories\":[],\"prob\":0.5021,\"coord\":[165,0,834,477]},{\"eng\":\"Plant\",\"rus\":\"Растение\",\"eng_categories\":[\"Plants\"],\"rus_categories\":[\"Растения\"],\"prob\":0.827,\"coord\":[165,0,834,668]},{\"eng\":\"Leaf\",\"rus\":\"Листок\",\"eng_categories\":[],\"rus_categories\":[],\"prob\":0.6623,\"coord\":[165,0,834,573]}]}],\"scene_labels\":[{\"status\":0,\"name\":\"file\",\"labels\":[{\"eng\":\"Rice Paddy\",\"rus\":\"Рисовое поле\",\"eng_categories\":[],\"rus_categories\":[],\"prob\":0.6255}]}]},\"htmlencoded\":false,\"last_modified\":0}\n\n\n  Example of JSON using jq\n\n        {\n      \"status\": 200,\n      \"body\": {\n      \"object_labels\": [\n            {\n            \"status\": 0,\n            \"name\": \"file\",\n            \"labels\": [\n            {\n                  \"eng\": \"Close-up\",\n                  \"rus\": \"Крупный план\",\n                  \"eng_categories\": [],\n                  \"rus_categories\": [],\n                  \"prob\": 0.4843,\n                  \"coord\": [\n                  165,\n                  0,\n                  834,\n                  477\n                  ]\n            },\n            {\n                  \"eng\": \"Macro Photography\",\n                  \"rus\": \"Макросъемка\",\n                  \"eng_categories\": [],\n                  \"rus_categories\": [],\n                  \"prob\": 0.5021,\n                  \"coord\": [\n                  165,\n                  0,\n                  834,\n                  477\n                  ]\n            },\n            {\n                  \"eng\": \"Plant\",\n                  \"rus\": \"Растение\",\n                  \"eng_categories\": [\n                  \"Plants\"\n                  ],\n                  \"rus_categories\": [\n                  \"Растения\"\n                  ],\n                  \"prob\": 0.827,\n                  \"coord\": [\n                  165,\n                  0,\n                  834,\n                  668\n                  ]\n            },\n            {\n                  \"eng\": \"Leaf\",\n                  \"rus\": \"Листок\",\n                  \"eng_categories\": [],\n                  \"rus_categories\": [],\n                  \"prob\": 0.6623,\n                  \"coord\": [\n                  165,\n                  0,\n                  834,\n                  573\n                  ]\n            }\n            ]\n            }\n      ],\n      \"scene_labels\": [\n            {\n            \"status\": 0,\n            \"name\": \"file\",\n            \"labels\": [\n            {\n                  \"eng\": \"Rice Paddy\",\n                  \"rus\": \"Рисовое поле\",\n                  \"eng_categories\": [],\n                  \"rus_categories\": [],\n                  \"prob\": 0.6255\n            }\n            ]\n            }\n      ]\n      },\n      \"htmlencoded\": false,\n      \"last_modified\": 0\n      }\n\n\nInstallation\n\n\n\nDebian/Ubuntu\nRHEL/CentOS 7/8\nWindows\nMac OS\n\n\n\nsudo apt-get install jq\n\n\n\n\nyum install epel-release -y\nyum update -y\nyum install jq -y\n\n\n\n\nInstall package manager Chocolatey, if it is not already installed.\nOpen PowerShell and run the command:\n\n      choco install jq -y\n\n\nUse the package manager that is convenient for you:\n\nFink:\n\n    fink install jq\n\nHomebrew:\n\n    brew install jq\n\nMacPorts:\n\n    port install jq\n\n\nUsage example\n\nGet the list plans via Karboii API in formatted JSON.\n\nCommand:\n\ncurl -X GET -H \"Accept: application/json\" -H \"X-Auth-Token: \" \"https://mcs.mail.ru/infra/karboii/v1/${OS_PROJECT_ID}/plans\" | jq \".\"\n\n\n  The result of the command execution\n\n        {\n        \"plans\": [\n          {\n            \"full_day\": null,\n            \"id\": \"7d07648b-fa8a-XXXX-XXXX-b20f71171d1f\",\n            \"name\": \"Backup_plan_24.05.2023\",\n            \"project_id\": \"b5b7ffd4ef0547e5b222f44555dfXXXX\",\n            \"provider_id\": \"37997f75-0637-XXXX-XXXX-49ff2ff11fa5\",\n            \"resources\": [\n              {\n                \"id\": \"8f1ba150-905f-4ae4-XXXX-d18844b30d19\",\n                \"name\": \"CentOS_STD1-1\",\n                \"type\": \"OS::Nova::Server\"\n              }\n            ],\n            \"retention_type\": \"max_backups\",\n            \"status\": \"running\"\n          },\n          {\n            \"full_day\": null,\n            \"id\": \"3a92e8f1-05da-XXXX-XXXX-2a442bb33eb2\",\n            \"name\": \"ElenasBackup_plan_15.06.2023\",\n            \"project_id\": \"b5b7ffd4ef0547e5b222f44555dfXXXX\",\n            \"provider_id\": \"37997f75-0637-XXXX-XXXX-49ff2ff11fa5\",\n            \"resources\": [\n              {\n                \"id\": \"29f07bc3-e915-4141-XXXX-c4d2a716db7d\",\n                \"name\": \"MySQL-5897-1\",\n                \"type\": \"OS::Trove::Instance\"\n              }\n            ],\n            \"retention_type\": \"max_backups\",\n            \"status\": \"suspended\"\n          },\n          {\n            \"full_day\": null,\n            \"id\": \"2fb5ac1d-f69f-XXXX-XXXX-dd45a9b71444\",\n            \"name\": \"MongoDB-3479\",\n            \"project_id\": \"b5b7ffd4ef0547e5b222f44555dfXXXX\",\n            \"provider_id\": \"37997f75-0637-XXXX-XXXX-49ff2ff11fa5\",\n            \"resources\": [\n              {\n                \"id\": \"b24d0df0-3f2f-4245-XXXX-0481c1ef2376\",\n                \"name\": \"MongoDB-3479\",\n                \"type\": \"OS::Trove::Instance\"\n              }\n            ],\n            \"retention_type\": \"max_backups\",\n            \"status\": \"suspended\"\n          }\n        ]\n      }\n\n\nDetailed information about the syntax and use of the utility in the official documentation.\n\n\n","url":"/en/tools-for-using-services/api/rest-api/install-jq","description":"Installing the jq utility for automatic output of structured JSON in the console. It can be used to output OpenStack and API commands.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"API","link":"/en/tools-for-using-services/api","isActive":false,"evenSimpler":true}]},{"uuid":"350799e2-5faf-4714-a678-7fa97dce86d9","title":"Projects","data":"A project is a structural unit within the cloud that owns resources: virtual machines, databases, Kubernetes clusters and others. Grouping by projects is convenient when there are several different tasks, each of which requires its own service infrastructure with its own individual settings. In addition, the project organization of work helps:\n\ndelimit user access to resources and data;\ntrack resource consumption separately for each of the tasks.\n\nWhen registering a new account in VK Cloud, a project is automatically created in which the current user is registered as the owner. The project owner can create new projects and invite users to all his projects by assigning them roles. The same user can be a participant in several projects and have different roles in them.\n\nEach VK Cloud project has a separate balance which can be replenished using the card linked to the project or other payment methods.\n\nAutomatic freezing of the project\n\n\n\nAutomatic freezing is not used in projects in which services are provided on a post-payment basis.\n\n\n\nIf the balance of the project becomes negative, the project is frozen:\n\nservices are blocked: the user cannot create new objects and edit already created ones;\nrunning objects (for example, virtual machines) are stopped.\n\nSome of the objects remain readable, making it possible to transfer data from them.\n\n\n\nAfter the project is frozen, funds continue to be debited in it for data storage and the use of chargeable resources — for example, floating IP addresses. The balance of the project goes further into the negative. You can preserve the project to stop debiting funds.\n\n\n\nIf the balance remains negative, the objects are deleted beyond recovery:\n\nif there was no cash flow in the project for the entire period of existence — 3 days after freezing;\nif payment was made — 30 days after freezing.\n\nA number of services may have their own terms for freezing and deleting objects.\n\n\n  Example for the Object storage service\n\nWhen the negative balance reaches -1000 rubles, the service objects are frozen.\n\n    All the time while the objects are frozen, the service charges for data storage, while the objects have read access.\n\nIf the positive balance of the project is not restored within 30 days, the objects are blocked.\n\n    When objects are blocked, write-offs for them stop. There is no access to objects, including read access. 30 days after switching to this state, if the positive balance is not restored, the objects are deleted.\n\nFor more information about the service, see Object storage.\n\n\n\nAfter the positive balance is restored, the services in the project are automatically activated after a while. Objects that have been stopped must be started manually after defrosting.\n\nIn any state of the project balance, including after objects are deleted, all project participants have access to their personal account. The accounts of the participants and the project itself are not deleted.\n","url":"/en/tools-for-using-services/account/concepts/projects","description":"What is a project in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"4b933b3a-93fe-4cef-af82-4a42434c9923","title":"Personal account roles and their permissions","data":"When a user is invited to a project, he or she is assigned with a role. The role defines the scope of permissions available to the user when working with the personal account and with the cloud services.\n\nThe same user can be a member of several projects and have different roles in each of them. The same user can be assigned with several roles in one project; in this case, the permissions of all assigned roles are summarized.\n\nYou can view the list of project members and their roles on the Manage access page of the personal account.\n\nGeneral project management roles\n\nProject owner\n\nA user with the widest set of permissions.\n\nA Project owner is a user who created the project or for whom it was created automatically by the platform during the account registration.\n\nThere can be only one Project owner in a project. This role cannot be assigned to an existing user or to a member that is being invited.\n\nSuperadministrator\n\nA user with the same permissions as the Project owner, including linking a card and making payments.\n\nSuperadministrator is the only role, besides the Project owner, who can activate services in the project.\n\nProject administrator\n\nA user who can create and edit objects in all services.\n\nA Project administrator cannot:\n\nactivate services;\nmake payments (can only view the project balance);\ninvite members to the project.\n\nUser access administrator\n\nA role intended for working with project members on the access management page.\n\nA User access administrator can invite, delete project members, and change the roles assigned to them.\n\nThis role has no access to services and to project balance information.\n\nBilling administrator\n\nA role intended for project balance management.\n\nA billing administrator can:\n\nlink a card to the project, if not linked yet;\nmake a payment or configure balance auto-completion.\n\nThis role has no access to services and to the list of project members.\n\nViewer\n\nA user who has read access to all project information, including the members list, services, project balance, and expenses details.\n\nA viewer cannot create any objects or edit any settings, except the settings of their account.\n\nSpecialized roles\n\nEach of the roles below is intended for working with one of the platform services only. There roles have permissions:\n\nin their target service;\nin adjacent services, to an extent that is necessary for working with their target service.\n\nAll these roles have no access to the project members list and to project balance information.\n\nFor detailed information about the permissions of these roles, see section Roles and their permissions: the full matrix.\n\nAll operations available to specialized roles are also available to Project owner, Superadministrator, and Project administrator.\n\nVirtual machine administrator\n\nA user with this role can perform basic operations in the Cloud Servers service.\n\nHowever, this role has only read permissions for:\n\nbackup plans;\nfile storages.\n\nThis user can also create and edit rule groups (firewalls) in the Virtual networks service.\n\nNetwork administrator\n\nA user with this role can perform a full set of operations in the Virtual networks and DNS services.\n\nNetwork security administrator\n\nA user with this role has read access to the Virtual networks and DNS services.\n\nHe can create and edit only rule groups (firewalls).\n\nInternal network administrator\n\nA user with this role:\n\nhas read access to the Virtual networks and DNS services;\ncan create and edit virtual networks and subnets, routers;\ncan add floating IP addresses to the project.\n\nKubernetes administrator, operator, and auditor\n\nFor detailed information about the permissions of these roles, see section Roles for the Cloud Containers service and their permissions.\n\nRoles and their permissions: the full matrix\n\nPermissions are the privileges for viewing (R in the table below) or editing (RW) of certain objects. An editing permission includes the privilege of creating and deleting an object, and also includes the permission for viewing. A dash (—) indicates no access to a service or functionality.\n\nRoles for the Containers service and their permissions\n\nThe following user roles are intended for working with the Сontainers service:\n\nKubernetes administrator,\nKubernetes operator,\nKubernetes auditor.\n\nThe operations available to Kubernetes administrator are also available to Project owner, Superadministrator, and Project administrator.\n\nFor other roles these operations are unavailable.\n\nFor Kubernetes clusters of version 1.23 and later, the role of a Kubernetes administrator, operator, or auditor also defines the internal Kubernetes role (admin, edit, or view) assigned to the user.\n\n\n\nSome of the actions below are available only in the certain state of a cluster. For example, installing or deleting an add-on is possible only when a cluster is running.\n\n\n\n\n| Operation/Role                                       |  Kubernetesadministrator | Kubernetesoperator | Kubernetesauditor | Viewer |\n|------------------------------------------------------| ---------------------------  | ---------------------- | --------------------- | ------ |\n| Create a cluster                                     | &#10003; | —        | —        | — |\n| Delete a cluster                                     | &#10003; | —        | —        | — |\n| Start a cluster                                      | &#10003; | &#10003; | —        | — |\n| Stop a cluster                                       | &#10003; | &#10003; | —        | — |\n| Show information abouta cluster and node groups  | &#10003; | &#10003; | &#10003; | &#10003; |\n| Get kubeconfig                                       | &#10003; | &#10003; | &#10003; | — |\n| Get the secret to accessthe Kubernetes Dashboard | &#10003; | &#10003; | &#10003; | &#10003; |\n| Update version                                       | &#10003; | &#10003; | —        | — |\n| Change the virtual machine type                      | &#10003; | &#10003; | —        | — |\n| Change the size of the Prometheus disk               | &#10003; | &#10003; | —        | — |\n| Add a node group                                     | &#10003; | &#10003; | —        | — |\n| Delete a node group                                  | &#10003; | &#10003; | —        | — |\n| Change scaling settings                              | &#10003; | &#10003; | —        | — |\n| Change Labels and Taints                             | &#10003; | &#10003; | —        | — |\n| Install/delete an add-on                           | &#10003; | &#10003; | —        | — |\n\n","url":"/en/tools-for-using-services/account/concepts/rolesandpermissions","description":"Role model in VK Cloud personal account.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"7d9ca829-6fd1-4121-b3ae-5479ed770b65","title":"Quotas and limits","data":"Quotas\n\nQuotas are the restrictions applied to the project:\n\nfor the use of virtual computing resources of the platform;\nto create objects in specific services.\n\nFor example, the available quotas are checked when a new virtual machine is created. If the quotas allow you to create another virtual machine, and there are enough resources (vCPU, memory and disk space) to create it, the virtual machine is successfully created.\n\nQuotas are not shared between several projects of the same owner and are not inherited by one project from another. The list of quotas varies for different regions.\n\nAfter activating the services, basic quotas become available to the project. They have default values that apply to all new projects in the region.\n\nQuotas can be viewed:\n\nshort list — in the personal account\nextended list — via the OpenStack CLI\n\nThere are implicit quotas that cannot be seen in any way, they manifest themselves as error messages when trying to create a new object. An example of an implicit quota is quota for the number of projects, which can be created by a single user.\n\nQuotas are closely related to technical limits.\n\nQuotas and technical limits\n\nTechnical limits are the limitations of the platform due to the features of the VK Cloud architecture.\n\nSome of the limits are strict, they cannot be exceeded physically. For example, the limit on 4 GPUs per instance is related to the limitation of KVM technology, which does not allow connecting more than 4 video cards to one virtual machine.\n\nOther limits are not related to physical limitations and are based on the operational requirements of the services. For example, network restrictions are introduced to ensure optimal network performance and stable operation.\n\nThere are no corresponding quotas for some technical limits.\n\nQuotas can be increased within the technical limits by contacting technical support.\n\nNon-rigid limits can be exceeded — either also through technical support, or independently. However, exceeding these limits can negatively affect the stability of work.\n\nThe following is a list of quotas and limits for the Moscow region.\n\nCommon\n\n[cols=\"2,1,1,1,1\", options=\"header\"]\n|===\n| Parameter\n| Quota\n| Basic quota\n| Limit\n| Hard\n\n| The number of projects the user has\n| —\n| 5 pcs.\n| 100 pcs.\n| &#10003;\n\n| Number of administrators in the project\n| —\n| 50 people\n| 50 people\n| &#10003;\n\n| Number of SSH key pairs\n| key-pairs in CLI\n| 100 key pairs\n| not limited\n| —\n\n|===\n\nVirtual machines\n\nTotal number of VMs and vCPUs\n[cols=\"2,1,1,1,1,1\", options=\"header\"]\n|===\n| Parameter\n| Quota in PA\n| Quota in CLI\n| Basic quota\n| Limit\n| Hard\n\n| Number of instances in the project\n| Virtual machines\n| instances\n| 6 pcs.\n| 1000 pcs.\n| —\n\n| Number of vCPUs in the project\n| vCPU\n| cores\n| 9 pcs.\n| not limited\n|\n\n| Total amount of RAM in the project\n| RAM\n| ram\n| 10240 MB\n| not limited\n|\n\n|===\n\nThe limit on the number of instances in the project can be exceeded if the corresponding quota is increased upon request to technical support.\n\n\n\nNo more than 1000 virtual machines can be displayed in the personal account.\n\nLimits without quotas\n\n| Parameter                                       | Limit         | Hard |\n|-------------------------------------|-------|-----------------------|\n| Number of vCPUs per instance           | 32 pcs.  | —          |\n| Number of High-Freq vCPUs per instance | 24 pcs.      | — |\n| Number of GPUs per instance            | 4 pcs.       | &#10003; |\n| The amount of RAM per instance         | 1024 GB     | — |\n\nLimits on the number of processors (vCPU and High-Freq vCPU) and RAM are associated with the limitations of standard hypervisors. If you need more vCPUs or High-Freq vCPUs, contact technical support and order a dedicated hypervisor with the necessary characteristics.\n\nThe limit on 4 GPUs per instance is related to the limitation of KVM technology, which does not allow connecting more than 4 video cards to one virtual machine.\n\nCloud Containers\n\nThere are no quotas for the Cloud Containers service. The use of this service is limited only by technical limits.\n\n| Parameter                                         | Limit              | Hard |\n|--------------------------------------------------|--------------------|---------|\n| Number of nodes in the node group | 100 pcs.            | &#10003; |\n| Number of node groups in the cluster                | 50 pcs.             | — |\n| Number of pods at the node                  | 110 pcs.            | — |\n| The number of entities (ReplicaSet, StatefulSet, namespaces, etc.) in the cluster | 10000 pcs.          | — |\n| Length of the k8s cluster name and node group | 24 characters         | &#10003; |\n\nNon-rigid limits can be exceeded without contacting technical support. Recommendations for these and other parameters in official Kubernetes documentation. The limit on the number of nodes in a node group is not due to the requirements of Kubernetes, the limit is introduced at the VK Cloud level.\n\nDisks and images\n\nDisk capacity\n\n[cols=\"3,2,2,1,1\", options=\"header\"]\n|===\n| Parameter\n| Quota in PA\n| Quota in CLI\n| Basic quota\n| Limit\n\n| The total volume of disks in the project\n| Disk size\n| gigabytes\n| 200 GB\n| not limited\n\n| Total volume of High-IOPS SSD disks\n| High-IOPS SSD size\n| gigabytes_high-iops\n| 200 GB\n| not limited\n\n| Total volume of High-IOPS SSD disks in availability zone DP1\n| High-IOPS SSD size in the (DP1) area\n| gigabytes_dp1-high-iops\n| 200 GB\n| not limited\n\n| Total volume of High-IOPS SSD disks in availability zone MS1\n| High-IOPS SSD size in the Moscow (MS1) (MS1) area\n| gigabytes_ko1-high-iops\n| 200 GB\n| not limited\n\n|===\n\nQuotas for the total volume of disks in the project take into account all types of disks, including High-IOPS SSDs. Quotas for the total volume of High-IOPS SSD disks take into account disks created in both availability zones. If you request through technical support an increase in the quota included in another quota, the quota of a higher level will be increased proportionally.\n\nNumber of disks\n\n[cols=\"3,2,2,1,1\", options=\"header\"]\n|===\n| Parameter\n| Quota in PA\n| Quota in CLI\n| Basic quota\n| Limit\n\n| Number of disks in the project\n| Disks\n| volumes\n| 10 pcs.\n| not limited\n\n| Number of High-IOPS SSD disks in the project\n| High-IOPS SSD\n| volumes_high-iops\n| 10 pcs.\n| not limited\n\n| Number of High-IOPS SSD drives in availability zone DP1\n| High-IOPS SSD volumes in - (DP1) area\n| volumes_dp1-high-iops\n| 10 pcs.\n| not limited\n\n| Number of High-IOPS SSD drives in availability zone MS1\n| High-IOPS SSD volumes in Moscow (MS1) (MS1) area\n| volumes_ko1-high-iops\n| 10 pcs.\n| not limited\n\n|===\n\nQuotas for the total number of disks in the project take into account all types of disks, including High-IOPS SSDs. Quotas for the total number of High-IOPS SSD disks take into account disks created in both availability zones. If you request through technical support an increase in the quota included in another quota, the quota of a higher level will be increased proportionally.\n\nLimits without quotas\n\n| Parameter                  | Limit | Hard |\n|---------------------------|---------|---|\n| Number of disks per instance  | 25 pcs.| + |\n| Size of one HDD              | 50 TB | + |\n| Size of one SSD disk              | 10 TB | + |\n| Size of one High-IOPS SSD disk    | 2 TБ via VK Cloud personal account, 5 TB via OpenStack CLI | — |\n| Size of a single image                 | 500 GB | — |\n| Total volume of images                  | 2 TB | — |\n| Total number of disks snapshots         | 200 pcs. | — |\n\nThe limit on the number of disks per instance (25 pcs.) is associated with PCI bus limitations.\n\nThe limit on the size of one image (500 GB) is a hard limit for the Glance service. If you need to download a larger image, use the instruction.\n\nThe limit on the size of one SSD-HIGH-IOPS disk (5 TB) is due to operational requirements, it can be exceeded without contacting technical support. A disk created over the limit is no different in performance, but restoring or migrating such a disk will take considerable time and will involve risks.\n\nThe limit on the total volume of images (2 TB) can be exceeded by contacting technical support.\n\nThe limit on the number of disks snapshots (200 pcs.) can be exceeded by contacting technical support.\n\nFile storage\n\n[cols=\"3,2,1,1,1\", options=\"header\"]\n|===\n| Parameter\n| Quota in PA\n| Basic quota\n| Limit\n| Hard\n\n| Number of file storages in the project\n| NFS/CIFS file storage\n| 10 pcs.\n| not limited\n| —\n\n| The total volume of all file storages in the project\n| Size of NFS/CIFS file shares\n| 200 GB\n| not limited\n| —\n\n| The total volume of snapshots of file storages in the project\n| NFS/CIFS snapshot size\n| 200 GB\n| not limited\n| —\n\n| Number of file storage networks\n| NFS / CIFS file storage networks\n| 5 pcs.\n| not limited\n| —\n\n| Size of one file storage\n| —\n| —\n| 50 TB\n| &#10003;\n\n|===\n\nQuota for the number of file storage networks (NFS / CIFS file storage networks) is a quota for internal use.\n\nCloud Networks\n\nThe limits in this section are determined by the requirements of operation, they are used to ensure the stable operation of networks. It is not recommended to request quotas exceeding the corresponding limits via technical support.\n\nNetworks, subnets and IP addresses\n\n\n\nSprut\nNeutron\n\n\n\n[cols=\"3,2,2,1,1,1\", options=\"header\"]\n|===\n| Parameter\n| Quota in PA\n| Quota in CLI\n| Basic quota\n| Limit\n| Hard\n\n| Number of networks in the project\n| Sprut networks\n| networks\n| 10 pcs.\n| 20 pcs\n| —\n\n| Number of subnets in the project\n| Sprut subnets\n| subnets\n| 10 pcs.\n| 20 pcs\n| —\n\n| Number of floating IP addresses\n| Sprut floating IP addresses\n| floating-ips\n| 6 pcs.\n| 50 pcs.\n| —\n\n|===\n\n\n\n\n[cols=\"3,2,2,1,1,1\", options=\"header\"]\n|===\n| Parameter\n| Quota in PA\n| Quota in CLI\n| Basic quota\n| Limit\n| Hard\n\n| Number of networks in the project\n| Neutron networks\n| networks\n| 10 pcs.\n| 20 pcs\n| —\n\n| Number of subnets in the project\n| Neutron subnets\n| subnets\n| 10 pcs.\n| 20 pcs\n| —\n\n| Number of floating IP addresses\n| Neutron floating IP addresses\n| floating-ips\n| 6 pcs.\n| 50 pcs.\n| —\n\n|===\n\n\n\n\nEven if the quota for the total number of subnets in the project has been increased through technical support, it is not recommended to create more than 20 subnets in one network.\n\nOther parameters\n\n\n\nSprut\nNeutron\nOctavia\n\n\n\n[cols=\"3,2,2,1,1,1\", options=\"header\"]\n|===\n| Parameter\n| Quota in PA\n| Quota in CLI\n| Basic quota\n| Limit\n| Hard\n\n| Number of routers\n| Sprut routers\n| routers\n| 12 pcs.\n| 20 pcs.\n| —\n\n| Number of advanced routers\n| Advanced routers\n| dc_router\n| 3 шт.\n| 3 шт.\n| —\n\n| Number of security groups in the project\n| Sprut firewall groups\n| secgroups\n| 12 pcs.\n| not limited\n| —\n\n| Number of security rules in the project\n| Sprut firewall rules\n| secgroup-rules\n| 200 pcs.\n| not limited\n| —\n\n| Number of ports in the project\n| Sprut ports\n| ports\n| 120 pcs.\n| 500 pcs.\n| —\n\n| Number of external Neutron or Sprut ports in the project\n| Sprut external network ports\n| external_port\n| 5 pcs.\n| 6 pcs.\n| —\n\n|===\n\n\n\n\n[cols=\"3,2,2,1,1,1\", options=\"header\"]\n|===\n| Parameter\n| Quota in PA\n| Quota in CLI\n| Basic quota\n| Limit\n| Hard\n\n| Number of routers\n| Neutron routers\n| routers\n| 12 pcs.\n| 20 pcs.\n| —\n\n| Number of security groups in the project\n| Neutron firewall groups\n| secgroups\n| 12 pcs.\n| not limited\n| —\n\n| Number of security rules in the project\n| Neutron firewall rules\n| secgroup-rules\n| 200 pcs.\n| not limited\n| —\n\n| Number of ports in the project\n| Neutron ports\n| ports\n| 120 pcs.\n| 500 pcs.\n| —\n\n| Number of external Neutron or Sprut ports in the project\n| Neutron external network ports\n| external_port\n| 5 pcs.\n| 6 pcs.\n| —\n\n|===\n\n\n\n\n[cols=\"2,1,1,1,1,1\", options=\"header\"]\n|===\n| Parameter\n| Quota in PA\n| Quota in CLI\n| Basic quota\n| Limit\n| Hard\n\n| Number of load balancers\n| Octavia balancers\n| load_balancers\n| 12 pcs.\n| 20 pcs.\n| —\n\n| Number of balancing rules per load balancer\n| —\n| —\n| —\n| 10 pcs. via PA, 30 pcs. via CLI\n| —\n\n|===\n\n\n\n\nYou can exceed the limit on the number of balancing rules per load balancer (10 pcs.) without contacting technical support. To do this, create rules using the OpenStack CLI. For stable operation, it is not recommended to use more than 30 rules per load balancer.\n\nThe quota Neutron balancers, which is displayed in VK Cloud personal account, is not used.\n\nCloud Backup\n\n| Parameter                                     | Quota in CLI | Basic quota  | Limit | Hard |\n|-----------------------------------------------|---------------------------------|--------------------|--|--|\n| Number of backups                             | backups| 400 pcs. | not limited | — |\n| Number of snapshots                           | quota  snapshots in the extended list | 200 pcs. | not limited | — |\n| Size of one backup                  | — | — | 320 TB        | — |\n| Total volume of images                           | — | — | 2 TB          | — |\n\nThe limit on the backup size (320 TB) is due to the limit on the file size in the Object Storage service (S3), this service is used to store backups. This limit can be increased by contacting technical support.\n\nCloud Storage\n\nThere are no quotas for the Cloud Storage service. The use of these services is limited only by technical limits.\n\n[cols=\"3,2,4,1\", options=\"header\"]\n|===\n| Parameter\n| Limit\n| Comment\n| Hard\n\n| Number of accounts\n| 25 pcs.\n| —\n| &#10003;\n\n| Number of buckets\n| 100 pcs.\n| —\n| &#10003;\n\n| Number of objects in the bucket\n| not limited\n| —\n| —\n\n| Number of objects in the bucket\n| 32 GB, 320 TB\n| 32 GB for a regular file, 320 TB for a multipart\n| &#10003;\n\n| Size of one bucket\n| not limited\n| —\n| —\n\n| Rate limit: normal requests\n| 1000 request/sec\n| unlimited, maximum known value among all projects: 9000 requests/sec\n| —\n\n| Rate limit: listing requests\n| 250 request/sec\n| unlimited, maximum known value among all projects: 500 requests/sec\n| —\n\n|===\n\nThe rate limits can be increased by contacting technical support.\n","url":"/en/tools-for-using-services/account/concepts/quotasandlimits","description":"Listing quotas and technical restrictions that apply in VK Cloud projects.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"b44fdaa7-397b-44a3-8bd1-0329d8e42b0f","title":"Regions","data":"A region is a geographical area that unites availability zones. Each availability zone has one or more data processing centers (data centers) where cloud infrastructure objects are physically located.\n\nThe region is assigned to each project automatically and depends on the URL of the site where the project owner registered account.\n\nYou can create objects (for example, virtual machines) only in the availability zone of the region in which the project was created.\n\n\n\nAll regions have a single user base. It is not possible to register accounts with the same mail in different regions.\n\n\n\nThe following regions are available:\n\n|            | Moscow             | Kazakhstan              |\n|-------------------------------|--------------------|------------------------|\n| Site URL | https://cloud.vk.com | https://vkcloud.kz, https://kz.cloud.vk.com/ |\n| Availability zones             | MS1 (KO1), GZ1, ME1 | QAZ |\n| Project currency               | Rubles               | Tenge |\n\nProjects created in different regions have different:\n\na set of available services;\nset of quotas;\naddresses of VK Cloud API endpoints;\nthe name of the region in the configuration files openrc and Terraform.\n\n\n\nIt is not possible to combine virtual networks of projects from different regions using standard methods. Network connectivity between such projects can be configured using VPN tunnel.\n\n\n","url":"/en/tools-for-using-services/account/concepts/regions","description":"What is a region, what regions exist in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"daf2b052-025f-4e82-bdb7-5a5bc08a1a37","title":"Join a project by invitation","data":"An invitation email contains the identifier of the project to which you were invited and a list of roles assigned to you. If you are invited to several projects, a separate email arrives for each project.\n\nIf you don't see the invitation in your Inbox, check the Spam folder.\n\nAccept the invitation\n\nThe authorization procedure depends on whether an account with your email is registered in VK Cloud or not.\n\n\n\nThe account is not registered\nThe account is registered\n\n\n\nClick on the link provided in the invitation email.\n\nOn the page that opens, enter a new password and click Register.\n\n    The minimum password length is 8 characters. The password must contain:\n\n    uppercase and lowercase Latin letters;\n    at least one number or one special character ?, !, ~, @, #, $, %, ^, &, _, -, +, *, =, ;, :, ,, ., /, \\, |, `  `, [, ], {, }, (, )`.\n\n    The password must not contain repeating groups of characters, such as `aaa123123123`.\n\n\n\nIf the Enable two-factor authentication page opens, configure two-factor authentication to access your personal account.\n\n\n\n\n\n\nClick on the link provided in the invitation email.\n\nOn the invitation page that opens, click Accept.\n\nOn the authorization page, enter your email as login and your password.\n\nIf mandatory two-factor authentication is enabled in the project, you will be requested to authenticate yourself via a code or to configure two-factor authentication in your personal account.\n\nAfter logging in, switch to the project to which you were invited:\n\n    In the top bar, click on the arrow next to the current project identifier.\n\n    Select the target project identifier from the drop-down list.\n\nWhat's next?\n\nView the personal account features.\nConfigure your account settings.\n","url":"/en/tools-for-using-services/account/service-management/project-invitation","description":"Sign in to a project following a link in an invitation email.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"680fcbd9-ed6d-4b16-80d1-b8da82f54ac9","title":"Service activation","data":"Linking a phone number\n\n\n\nOne phone number can only be linked to one account.\n\nIf you fail to register a phone number at this stage, registration can be postponed with the Enter later button, then return to this stage when you go to any of the VK Cloud services of your personal account.\n\n\n\nAfter entering the number, an SMS will be sent to the specified phone, the confirmation code from which should be entered in the confirmation field.\n\nIn case of successful confirmation of the phone number, you will see a message about the possibility of activating services.\n\nNow you can fully use the account by clicking the Enable Services button.\n\n\n\nPhone number activation is available only for mobile operators operating in the territory of the Russian Federation and starting with +7.\n\nIf this message appears, then all data is confirmed and the services are ready for use.\n\nLinking a bank card\n\nIn order for your services to work correctly, link a bank card to the created account: specify the card details in the field of the same name. If there are enough funds on the account for the first debit, the card will be successfully linked.\n\n\n\nUp to 500 rubles can be debited as a starting amount. This amount is credited to the balance of your project.\n\n\n\n\n\nThe first (test) debit from the card cannot be refunded.\n\n\n\nYou cannot use a virtual card to activate services.\n","url":"/en/tools-for-using-services/account/service-management/activation","description":"Verify phone number to activate services.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"4f161add-0806-448f-b660-54f2f6ff6ee0","title":"Changing the profile","data":"Editing user profile\n\nGo to VK Cloud personal account.\nClick on the username in the header of the page and select Account settings. A page with account information opens.\nOn the User profile fill in the fields:\n    Name and surname: enter your first and last name;\n    Company: specify the name of the company where you work;\n    Job title: specify your position.\n\n\n    To change the phone number or email, contact technical support.\n\n\nClick the Save changes button.\n\nPassword change\n\nGo to VK Cloud personal account.\nClick on the username in the header of the page and select Account settings.\nOn the Security tab click the Change password button.\nFill in the fields:\n   Old password: enter your current account password;\n   New password: set a new password for your account;\n   Confirm password: enter the new password again.\nClick the Save button.\n","url":"/en/tools-for-using-services/account/service-management/account-manage/editinfo","description":"How to change the data in the user profile and password from the VK Cloud personal account.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"9a35a60a-87f1-4967-a61c-fe704739f69b","title":"2FA management","data":"You can enable two-factor authentication (2FA) in VK Cloud to enhance account security.\n\nAfter enabling 2FA, each time you sign in to your personal account, you will need to enter a one-time 6-digit code from the authentication application.\n\nEnabling 2FA\n\nInstall one of the applications for generating one-time codes:\n   Google Authenticator,\n   Duo.\nGo to VK Cloud personal account.\nClick on the username in the header of the page and go to the 2FA activation page in one of the ways:\n\n   Security\n   Account settings\n\n\n   Select Security from the drop-down list. A page with a QR code will open.\n\n\n   Select Account settings from the drop-down list.\n   Go to the Security tab.\n   Click the Enable security. A page with a QR code will open.\n\n\nScan the code using the selected application to generate one-time codes.\n\n\n   If the QR code is not read by the application, click on the link below the QR code to get the code for manual entry. Select manual input in the application, enter your username and the received code.\n\n\nEnter the code generated by the application in the field Code from application.\nEnter your account password in the field Account password.\nClick the Enable.\n\nTwo-factor authentication will be enabled for your account. A list of backup codes will be displayed with which you can sign in to your personal account if the authentication application is unavailable.\n\nDisabling 2FA\n\nGo to VK Cloud personal account.\nClick on the username in the header of the page and select Account settings.\nOn the Security click the Disable security.\nEnter the password from your account and the code from the authentication application.\nClick the Turn off button.\n","url":"/en/tools-for-using-services/account/service-management/account-manage/manage-2fa","description":"Enabling and disabling two-factor authentication in VK Cloud personal account.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"03eb6789-2dac-4d45-b4ed-cd769e480880","title":"Managing key pairs","data":"Key pairs are used to connect to a VM via SSH. The key pair consists of public and private keys: the public key is placed on the VM, the private key is stored by the user.\n\nViewing information about a key pair\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nClick on the user's name in the header of the page.\nSelect Key pairs from the drop-down list.\nClick on the name of the required key pair. Information about it will be displayed.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\nRun the command:\n\n      openstack keypair show\n\n\nTo display data only about the public key, add the --public-key option to the command.\n\nCreating a key pair\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nClick on the user's name in the header of the page.\nSelect Key pairs from the drop-down list.\nClick the Create key button.\nEnter the name of the key and click Create key.\n\n   The private key will be downloaded to the local device.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\nRun the command:\n\n      openstack keypair create\n\nSave the private key that appears on the screen to a file with the extension .pem.\n\nImporting an existing key\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nGo to VK Cloud personal account.\nClick on the user's name in the header of the page.\nSelect Key pairs from the drop-down list.\nClick the Import key button.\nIn the window that opens, fill in the fields:\n\n   Name of key: specify the name of the created key pair.\n   Public key: insert the contents of the ssh-rsa public key.\n\nClick the Import key button.\n\n\n\n\nUse the GitLab official documentation for local generation of a key pair.\nMake sure that OpenStack client is installed and authenticate to the project.\nRun the command:\n\n      openstack keypair create --public-key\n\n\nRecovery of the key pair\n\n\n\nThe private key cannot be restored! Create a new key pair and upload the public key to the VM.\n\n\n\nTo restore access to a Linux virtual machine via SSH using a key pair, use the instructions from the article VM management.\n\nDeleting a key pair\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nThis is a group operation: if necessary, you can delete several key pairs at once by selecting them using the checkboxes.\n\nGo to VK Cloud personal account.\nClick on the user's name in the header of the page.\nSelect Key pairs from the drop-down list.\nClick on the icon Trash in the line with the object being deleted.\nConfirm the deletion.\n\n\n\n\nMake sure that OpenStack client is installed and authenticate to the project.\nRun the command:\n\n      openstack keypair delete\n\n","url":"/en/tools-for-using-services/account/service-management/account-manage/keypairs","description":"Available operations with SSH keys on the VK Cloud platform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"478c33e8-fec6-43fa-9102-09ab9f7a7cca","title":"Creating a project","data":"You can create your own projects to VK Cloud. By creating a project, you become its owner.\n\n\n\nWithin the framework of basic quotas one user can create no more than 5 projects. This number can be increased within the existing limit upon request to technical support.\n\n\n\nTo create a project:\n\nGo to VK Cloud personal account.\n\nIn the header of your personal account, click on the name of the project.\n\n    A list of projects in which you are a member will open.\n\nAt the end of the list, click Create a new project.\n\n    If your phone was not linked to your account, link it.\n\nEnter the project name.\n\n    Name requirements:\n\n   length from 4 to 63 characters;\n   only lowercase Latin letters, numbers and special characters are allowed:\n      _, - – except for the last character;\n      ., +, @, # — except for the first and last characters.\n\nClick the Save button.\n\nAdditional configuration of the project\n\nActivate the services: click the Enable services button in the section of any service.\n\nLink the payment card if it is not already linked.\n\nTop up the project balance to start using the services.\n\n    Projects created by users have balance initially it is 0 rubles. Welcome bonuses are not awarded for such projects.\n\n   Learn more about the cost of services in the article on pricing.\n","url":"/en/tools-for-using-services/account/service-management/project-settings/create","description":"Description of the procedure for creating a project on the VK Cloud platform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"add6e526-5ddf-4a95-814d-3a080aaa9aee","title":"Project management","data":"Getting the Project ID\n\nEach project has a unique identifier (PID) of the form mcsNNNNNNNNNN, for example, mcs0123456789. It is assigned automatically and is used when contacting technical support.\n\nTo get the PID:\n\nGo to VK Cloud personal account.\nIn the header of your personal account, click on the name of the project.\n\n    A list of projects in which you are a member will open.\n\nTo copy the PID of the required project, hover over the project name and click on the icon Copy which will appear on the right.\n\nChanging the project name\n\nThe project name can be changed at any stage of the project life.\n\nGo to VK Cloud personal account.\nIn the header of your personal account, click on the name of the project.\n\n    A list of projects in which you are a member will open.\n\nHover the mouse cursor over the name of the required project and click on the icon Edit which will appear on the right.\n\nEnter a new name and click Save.\n\nYou can also rename the project through the account menu. Select Project settings and go to the General information tab.\n\nFor projects created automatically — for example, when registering an account — the project name by default matches its identifier.\n\n\n\nOnly users with the role of the project owner or superadministrator can change the project name.\n\nChange of the project owner\n\nA project can have only one owner. After appointing another user as the owner, you will switch with him roles.\n\n\n\nA change of ownership is possible in any state of the project balance.\n\n\n\nFollow the preparatory steps:\n\n    If necessary, untie the payment card from the project by contacting technical support.\n\n    Make sure that the user you want to make the owner:\n\n        Added to the list of members and accepted the invitation to the project.\n\n            In this case, it is displayed in the list with the activation status Yes.\n\n        (Optional) Has the role of superadministrator, project administrator or user administrator (IAM) in the project.\n\n            This will allow you to control the change of roles after you cease to be the owner.\n\n        Agree to accept the project.\n\n            The transfer of the project is possible only with the consent of the new owner.\n\nContact technical support on behalf of the current owner, provide the following information:\n\n    project id (PID);\n    the new owner's email address.\n\n(Optional) Check the role change in the list of project members.\n\n\n\nThe new owner may need to link a payment card, if the project does not have a linked card, and confirm phone number.\n\nProject conservation\n\nIf you want to suspend work on a project, mothball the project to stop debiting funds. If the project goes into negative territory, it will be frozen with subsequent deletion of objects and data.\n\nTo preserve the project:\n\nDetermine which data from the project you want to save:\n\n    Click on the user name in the header of the personal account page, select Balance and payments.\n    Expand the menu of each service to see the objects in which data is stored and their size.\n\nStop all the VMs that you want to save data from.\nTransfer or copy all the necessary data.\n\n    For example, create and export disk images of the virtual machines stopped in the previous step.\n\nDelete all objects that are charged or occupy disk space — floating IP addresses, disks, buckets, load balancers, and others.\n\n    The full list of such objects is also on the page Balance and payments.\n\n    Since the boot disks of virtual machines cannot be deleted separately, delete VMs.\n\n    Objects that do not consume resources — for example, networks and subnets — can be left in the project.\n\nYou will be able to reactivate the project after any period of time by uploading the saved data to it and restoring the infrastructure.\n\nTransferring objects between projects\n\nYou can transfer disks between projects. This allows you to transfer virtual machines between projects.\n\nDisk transfer is possible only within one region. If the projects are located in different regions, upload locally the disk image and download it to the new project.\n\nCurrently, transferring PaaS service objects between projects is not supported. For example, the virtual machine on which the database was deployed can be moved to another project only as a regular virtual machine. It is not possible to transfer such a virtual machine as a database instance or create a database instance with a disk transferred from another project.\n\nBy contacting technical support, it is possible to transfer objects:\n\nfloating IP addresses;\naddresses of the external networks.\n\nViewing project quotas\n\n\n\nPersonal account\nOpenStack CLI\n\n\n\nQuotas for the most frequently used resources are displayed on the main page of the personal account.\n\nTo see a more complete list of quotas and find out detailed information about each quota from the list:\n\nClick on the user name in the header of the personal account page, select from the drop-down list Quotas management. The quotas page opens.\n\n    The same page is available if you select Project settings and go to the Quotas tab.\n\nClick on the iconInformation to the right of the required quota. Information will be displayed about which objects and in which services the quota is spent.\n\nAn extended list of quotas for the project can be obtained through the OpenStack CLI. Information about all quotas and the possibilities of increasing them in the section Quotas and limits.\n\n\n\n\nTo get an extended list of quotas for a project:\n\nMake sure that OpenStack client is installed and authenticate to the project.\nRun the command:\n\n        openstack quota show\n\n    Example of command output for the Moscow region\n\n        +----------------------------+--------------------------------------------------+\n    | Field                      | Value                                            |\n    +----------------------------+--------------------------------------------------+\n    | backup-gigabytes           | -1                                               |\n    | backups                    | 400                                              |\n    | cores                      | 9                                                |\n    | fixed-ips                  | -1                                               |\n    | floating-ips               | 6                                                |\n    | gigabytes                  | 200                                              |\n    | gigabytes_ceph             | -1                                               |\n    | gigabytes_ceph-hdd         | -1                                               |\n    | gigabytes_ceph-ssd         | -1                                               |\n    | gigabytes_dev-ceph         | -1                                               |\n    | gigabytes_dp1              | -1                                               |\n    | gigabytes_dp1-high-iops    | 200                                              |\n    | gigabytes_dp1-local-ssd    | -1                                               |\n    | gigabytes_dp1-ssd          | -1                                               |\n    | gigabytes_ef-nvme          | -1                                               |\n    | gigabytes_high-iops        | 200                                              |\n    | gigabytes_ko1-high-iops    | 200                                              |\n    | gigabytes_ko1-local-ssd    | -1                                               |\n    | gigabytes_ko1-local-ssd-g2 | -1                                               |\n    | gigabytes_ko1-ssd          | -1                                               |\n    | gigabytes_local-ssd        | -1                                               |\n    | gigabytes_manila           | -1                                               |\n    | gigabytes_ms1              | -1                                               |\n    | gigabytes_octavia-hdd      | -1                                               |\n    | gigabytes_octavia-ssd      | -1                                               |\n    | gigabytes_perf-test        | -1                                               |\n    | gigabytes_ssd              | -1                                               |\n    | health_monitors            | -1                                               |\n    | injected-file-size         | 10240                                            |\n    | injected-files             | 5                                                |\n    | injected-path-size         | 255                                              |\n    | instances                  | 6                                                |\n    | key-pairs                  | 100                                              |\n    | l7_policies                | -1                                               |\n    | listeners                  | -1                                               |\n    | load_balancers             | 12                                               |\n    | location                   | ...                                              |\n    | networks                   | 10                                               |\n    | per-volume-gigabytes       | -1                                               |\n    | pools                      | 30                                               |\n    | ports                      | 120                                              |\n    | project                    | b5b7ffd4ef0547e5b222f44555dfXXXX                 |\n    | project_name               | mcsXXXXXXXXXX                                    |\n    | properties                 | 128                                              |\n    | ram                        | 10240                                            |\n    | rbac_policies              | 10                                               |\n    | routers                    | 12                                               |\n    | secgroup-rules             | 200                                              |\n    | secgroups                  | 12                                               |\n    | server-group-members       | 100                                              |\n    | server-groups              | 50                                               |\n    | snapshots                  | 200                                              |\n    | snapshots_ceph             | -1                                               |\n    | snapshots_ceph-hdd         | -1                                               |\n    | snapshots_ceph-ssd         | -1                                               |\n    | snapshots_dev-ceph         | -1                                               |\n    | snapshots_dp1              | -1                                               |\n    | snapshots_dp1-high-iops    | -1                                               |\n    | snapshots_dp1-local-ssd    | -1                                               |\n    | snapshots_dp1-ssd          | -1                                               |\n    | snapshots_ef-nvme          | -1                                               |\n    | snapshots_high-iops        | -1                                               |\n    | snapshots_ko1-high-iops    | -1                                               |\n    | snapshots_ko1-local-ssd    | -1                                               |\n    | snapshots_ko1-local-ssd-g2 | -1                                               |\n    | snapshots_ko1-ssd          | -1                                               |\n    | snapshots_local-ssd        | -1                                               |\n    | snapshots_manila           | -1                                               |\n    | snapshots_ms1              | -1                                               |\n    | snapshots_octavia-hdd      | -1                                               |\n    | snapshots_octavia-ssd      | -1                                               |\n    | snapshots_perf-test        | -1                                               |\n    | snapshots_ssd              | -1                                               |\n    | subnet_pools               | -1                                               |\n    | subnets                    | 10                                               |\n    | volumes                    | 10                                               |\n    | volumes_ceph               | -1                                               |\n    | volumes_ceph-hdd           | -1                                               |\n    | volumes_ceph-ssd           | -1                                               |\n    | volumes_dev-ceph           | -1                                               |\n    | volumes_dp1                | -1                                               |\n    | volumes_dp1-high-iops      | 10                                               |\n    | volumes_dp1-local-ssd      | -1                                               |\n    | volumes_dp1-ssd            | -1                                               |\n    | volumes_ef-nvme            | -1                                               |\n    | volumes_high-iops          | 10                                               |\n    | volumes_ko1-high-iops      | 10                                               |\n    | volumes_ko1-local-ssd      | -1                                               |\n    | volumes_ko1-local-ssd-g2   | -1                                               |\n    | volumes_ko1-ssd            | -1                                               |\n    | volumes_local-ssd          | -1                                               |\n    | volumes_manila             | -1                                               |\n    | volumes_ms1                | -1                                               |\n    | volumes_octavia-hdd        | -1                                               |\n    | volumes_octavia-ssd        | -1                                               |\n    | volumes_perf-test          | -1                                               |\n    | volumes_ssd                | -1                                               |\n    +----------------------------+--------------------------------------------------+\n\n\n    The value -1 for the quota means “unlimited”. To find out which resources the quotas from the list correspond to, read the section Quotas and limits.\n\n    You can output quotas separately for components nova and cinder:\n\n    nova\n    cinder\n\n\n        nova quota-show\n\n\n        cinder quota-show\n\n\n    To view quotas for other OpenStack components, install appropriate OpenStack client packages (manila, neutron and others) and use a package-specific command. To find out the syntax of the command, install the appropriate package and run  help quota-show, for example, manila help quota-show.\n\nIncreasing project quotas\n\nIf the project lacks quotas, contact technical support on behalf of the project owner and provide the information:\n\nproject id (PID) and region;\nwhich quotas need to be increased (for example, the number of CPUs, the available amount of RAM) and by how much.\n\n\n\nInformation about increasing quotas for GeekBrains program participants you can find in FAQ.\n\n{heading(Viewing project SDN)[id=sdn_view]}\n\nGo to your personal VK Cloud account.\nOpen the project settings with one of the following ways:\n\n    Click the project name in the header of the personal account page. Click more-icon for the project you need and select Project settings.\n\n    Click the user name in the header of the personal account page and select Project settings.\n\nGo to the Virtual networks tab.\n\nThere are the default and additional SDN of the project, if it is connected. To change the SDN type, contact technical support.\n\nDeleting a project\n\nDeleting a project is also possible with a negative balance.\n\nTo delete a project:\n\nMake sure that you have transferred all the necessary data from the project.\nContact technical support on behalf of the project owner, provide in the request the project ID (PID) and region of the project being deleted.\n\n(For legal entities) Specify for what period you need closing documents.\n\n    You will be contacted to confirm the deletion.\n\n\n\nAfter confirmation, the project and its data are permanently deleted.\n\n\n","url":"/en/tools-for-using-services/account/service-management/project-settings/manage","description":"Change of owner, increase of quotas, transfer of objects and other operations.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"82ae5d0c-1dbd-4198-9160-5f4189a4cb8e","title":"Access management","data":"Access control operations are available only to the following roles of personal account: owner, superadministrator and user Administrator (IAM). Viewing of project members is also available to the project administrator and viewer.\n\nViewing project members\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nClick on the user's name in the header of the page.\nFrom the drop-down list, select Manage access.\n\nLogin, assigned roles and status are displayed for each member (Yes — the user has joined the project, No — the user did not accept the invitation sent to him).\n\nViewing the roles involved in the project\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nClick on the user's name in the header of the page, from the drop-down list, select Manage access.\nOn the access management page, go to the tab Roles.\n\n    The project owner sees in the list all roles except his own, the superadministrator — everything except his own and the owner, the user administrator (IAM) — everything except his own, the owner and the superadministrator.\n\nClick on the role to see who it was assigned to.\n\n    The list shows all the members invited to the role, including those who did not accept the invitation.\n\nInclusion of mandatory 2FA in the project\n\nFor security reasons, you can make two-factor authentication (2FA) mandatory for project members:\n\nGo to VK Cloud personal account.\nEnable 2FA for your account, if it is not already enabled.\nClick on the user's name in the header of the page, from the drop-down list, select Manage access.\nOn the access management page, enable the option Mandatory two-factor authentication.\n\nThe option can be turned on and off at any stage of the project's life.\n\nInviting a new member to the project\n\nYou can invite both a user who already has an account in VK Cloud, and an unregistered user.\n\nIf the user enters the project by invitation, he does not need to confirm the phone number and link the card.\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nClick on the user's name in the header of the page, from the drop-down list, select Manage access.\nOn the access management page, click Add member.\nEnter the email address of the member and select from the list the role you want to assign to him.\n\n    The role of superadministrator can be assigned only by the project owner. The role of User Administrator (IAM) can be assigned only by the project owner or superadministrator. You cannot invite a member to the role of the project owner.\n\n    If a member is assigned multiple roles, their permissions are summed up.\n\nClick the Add user button.\n\n\n\n\nAn invitation will be sent to the specified email address, it is valid for 24 hours.\n\nAfter logging in to the project the new member's status in the column Activated will change to Yes.\n\n\n\nThe member's email is not editable, and the invitation cannot be resent to it. If you have entered an incorrect email address or the invitation has expired, delete the member and invite him again.\n\nChanging member role\n\nA project member cannot change the role for:\n\nthe owner of the project;\nmyself;\na member with the same role as himself.\n\nOnly the project owner can assign another role to the superadministrator.\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nClick on the user's name in the header of the page, from the drop-down list, select Manage access.\nClick   for the required project member and select Edit.\nIn the window that opens, specify the list of new roles in the field User role.\n\n    At least one role should remain in the list.\n\nClick the Save changes button.\n\nDeleting member\n\nThe member cannot delete:\n\nthe owner of the project;\nmyself;\na member with the same role as himself.\n\nOnly the project owner can delete the superadministrator.\n\n\n\nPersonal account\n\n\n\nThis is a group operation: if necessary, you can delete several members at once by selecting them using the checkboxes.\n\nGo to VK Cloud personal account.\nClick on the user's name in the header of the page, from the drop-down list, select Manage access.\nClick   for the required project member and select Delete.\nConfirm the deletion.\n\n\n\n","url":"/en/tools-for-using-services/account/service-management/project-settings/access-manage","description":"Adding and removing participants, viewing their roles, enabling mandatory 2FA in the project.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"c8bc0d3a-565e-4b5f-8557-37bd59e7fdbc","title":"Viewing notifications","data":"Notifications of the personal account inform about significant technical updates of the platform, for example, about new features of services or about support for new versions of software products.\n\nTo view the list of notifications in short and full versions:\n\nClick on the icon Notifications in the header of the personal account page.\n\n    A window opens with a list of recent notifications.\n\nTo see the full list, click All notifications.\n\n    The list will open on a separate page.\n\nDifferent action buttons are available for different types of notifications in the list, for example:\n\nMore**: view additional information on the update.\nTry out**: open the section of your personal account where you can connect the functionality.\nSubmit a request**: create a request for access to a tool or service.\n","url":"/en/tools-for-using-services/account/service-management/notifications-view","description":"Viewing information about technical updates of the platform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"1983f0c6-f588-4088-bbbe-2213de48628c","title":"Working with the action log","data":"The activity log stores the history of operations performed by VK Cloud components. The actions of the following components are saved (specified in the format for API requests):\n\nnova — controller of computing resources.\ncinder — working with VM disks.\nneutron — management of cloud virtual networks.\nglance — storing and working with images.\noctavia — managing load balancers.\ndbaas — creating and managing DB instances.\nmagnum — orchestration of K8s containers.\niam — managing users in the project (only available when contacting technical support).\n\nLog data can be useful both for internal analysis of incidents and when contacting technical support.\n\nDownloading the action log\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nClick on the user's name in the header of the page.\nSelect Action logger from the drop-down list.\nClick the Download report button.\n\nThe generated report will be uploaded with the extension .xlsx.\n\nViewing log\n\n\n\nPersonal account\nAPI\n\n\n\nGo to VK Cloud personal account.\nClick on the user's name in the header of the page.\nSelect Action logger from the drop-down list.\n(Optional) Specify the time range:\n\n   Click the Generate another request.\n   In the window that opens, select the target range manually or using the calendar.\n   Click the Show logs button.\n\nTo open detailed information about an individual record, click on the icon Info to the right of the entry.\n\n\n\n\nEnable two-factor authentication (2FA) for your account.\nGet the X-Auth-Token.\nFind out the address of the Audit endpoint.\nRun the request:\n\n      curl -X GET \"/logs\" -H \"X-Auth-Token: \"\n\n   More information about formulating a request in the section Examples of API requests to the action log.\n\n   Additional parameters can be specified in the request (header):\n\n   | Parameter | Format | Description |\n   | --- | --- | --- |\n   | from   | RFC3339 | The beginning of the time range |\n   | to     | RFC3339 | End of time range |\n   | source | string  | The source component of the operation |\n   | marker | string  | The token for requesting the next page, previously returned by the API. TTL of markers — 1 hour |\n   | limit  | integer | The number of records to return. If omitted, returns 100 records |\n\n\n\n\nEach action log record provides the information:\n\nevent_id — ID of the operation.\nuser_email — the mail of the user who performed the operation.\ntimestamp — date and time of the operation.\nsource — the component is the source of the operation.\naction — a brief description of the operation.\nsuccess — a sign of successful completion of the operation.\nmethod — REST method of the performed operation.\nuri — the path where the request was executed.\nrequest_body — request body (if any).\nresponse_body — response body (if any).\n\nExamples of API requests to the action log\n\nTo split the output of records to the console or file into lines, the requests use the jq utility.\n\n\n    Retrieving the latest log records\n\nTo get the last 2 records from the action log of the Magnum component, run the request:\n\ncurl -X GET \"https://mcs.mail.ru/auditlogs/v1/b5b7ffd4efXXXX/logs?\\\nsource=magnum&\\\nlimit=2&\\\nfrom=&\\\nto=\" \\\n-H \"X-Auth-Token: gAAAAABlXDFc8RTqKryFlXXX\" \\\n-H \"Content-Type: application/json\" | jq\n\nResponse example:\n\n{\n  \"logs\": [\n    {\n      \"action\": \"unknown\",\n      \"event_id\": \"4f6ed6e5-XXXX-dcc2279ba39d\",\n      \"method\": \"DELETE\",\n      \"request_body\": \"\",\n      \"request_id\": \"req-05134dd5-XXXX-18b29ea5552e\",\n      \"response_body\": \"\",\n      \"source\": \"magnum\",\n      \"success\": \"yes\",\n      \"timestamp\": \"2023-11-20T09:15:11Z\",\n      \"uri\": \"/infra/container/v1/nodegroups/XXXX-4eb4e8ec5de9\",\n      \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0\",\n      \"user_email\": \"XXXX@vk.team\",\n      \"user_id\": \"d98c90595998426f9c69746f02aXXXX\"\n    },\n    {\n      \"action\": \"unknown\",\n      \"event_id\": \"00a5def3-XXXX-f0884f24798b\",\n      \"method\": \"PATCH\",\n      \"request_body\": \"{\\\"delta\\\":-1}\",\n      \"request_id\": \"req-f697a08b-XXXX-e59c66306dd1\",\n      \"response_body\": \"{\\\"uuid\\\": \\\"31a092d7-XXXX\\\"}\",\n      \"source\": \"magnum\",\n      \"success\": \"yes\",\n      \"timestamp\": \"2023-11-20T09:08:18Z\",\n      \"uri\": \"/infra/container/v1/nodegroups/XXXX-4eb4e8ec5de9/actions/scale\",\n      \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0\",\n      \"user_email\": \"XXXX@vk.team\",\n      \"user_id\": \"d98c90595998426f9c69746f02aXXXX\"\n    }\n  ],\n  \"marker\": \"eyJ0bSI6MCwib2ZzIjo1LCJzcmMiOiJtYWdudW0iLCJXXXX\"\n}\n\n\n\n\n    Retrieving log records for a given period\n\nTo get the last 2 records from the action log of the Nova component for a given period, run the request:\n\ncurl -X GET \"https://mcs.mail.ru/auditlogs/v1/b5b7ffd4efXXXX/logs?\\\nsource=nova&\\\nlimit=2&\\\nfrom=2023-10-15T10:00:00.000Z&\\\nto=2023-11-15T16:43:00.477Z\" \\\n-H \"X-Auth-Token: gAAAAABlXEVTelmi_XXXX\" \\\n-H \"Content-Type: application/json\" | jq\n\nResponse example:\n\n{\n  \"logs\": [\n    {\n      \"action\": \"create-vm\",\n      \"event_id\": \"a2d05902-XXXX-60bce13de1f7\",\n      \"method\": \"POST\",\n      \"request_body\": \"{\\\"server\\\":{\\\"name\\\":\\\"BY-CentOS_prometheus\\\",\\\"key_name\\\":\\\"ADH-clusterXXXX\\\",XXXX}}\",\n      \"request_id\": \"req-1d76a3f3-XXXX-b695d066e606\",\n      \"response_body\": \"{\\\"server\\\": {\\\"security_groups\\\": [{\\\"name\\\": \\\"71d90a92-XXXX\\\"}, {\\\"name\\\": \\\"XXXX-aecb77b43bec\\\"}], XXXX}}\",\n      \"source\": \"nova\",\n      \"success\": \"yes\",\n      \"timestamp\": \"2023-11-15T12:16:26Z\",\n      \"uri\": \"/v2.1/servers\",\n      \"user_agent\": \"axios/1.4.0\",\n      \"user_email\": \"XXXX@vk.team\",\n      \"user_id\": \"5f48556ef89444dbab8fa82669dXXXX\"\n    },\n    {\n      \"action\": \"vm-action\",\n      \"event_id\": \"fc98d3d7-XXXX-c2c5fd8fe619\",\n      \"method\": \"POST\",\n      \"request_body\": \"{\\\"addFloatingIp\\\":{\\\"address\\\":\\\"XXXX\\\"}}\",\n      \"request_id\": \"req-f358678d-XXXX-311861a4ff77\",\n      \"response_body\": \"\",\n      \"source\": \"nova\",\n      \"success\": \"yes\",\n      \"timestamp\": \"2023-11-15T09:43:41Z\",\n      \"uri\": \"/v2.1/servers/c6be363f-f56c-XXXX/action\",\n      \"user_agent\": \"HashiCorp Terraform/1.4.0-dev XXXX gophercloud/2.0.0\",\n      \"user_id\": \"649a35d97fc64452b019a0809dXXXX\"\n    }\n  ],\n  \"marker\": \"eyJ0bSI6MTY5NzM2NDAwMCwib2ZzIjo1LCJXXXX\"\n}\n\n\n\n\n    Using the marker parameter\n\nUsing the marker parameter, a large request for log records can be divided into several partial requests. Action log records are arranged in reverse order of time, with the most recent at the beginning of the log. So the first partial request will return a bunch of the most recent records, the next one will return a bunch of earlier records, and so on.\n\nTo output all log records of the Nova component for a given period into files in portions of 10 records per file:\n\nRequest output of 10 recent log records for the specified period to the nova_part1.log file:\n\n      curl -X GET \"https://mcs.mail.ru/auditlogs/v1/b5b7ffd4efXXXX/logs?\\\n   source=nova&\\\n   limit=10&\\\n   from=2023-10-15T10:00:00.000Z&\\\n   to=2023-11-15T16:43:00.477Z\" \\\n   -H \"X-Auth-Token: gAAAAABlXDFc8RTqKryFlXXXX\" \\\n   -H \"Content-Type: application/json\" | jq > nova_part1.log\n\nExtract the value of the marker parameter from the nova_part1.log file:\n\n      cat nova_part1.log | grep marker\n\n   Response example:\n\n      \"marker\": \"eyJ0bSI6MTY5NzM2NDAwMCwib2ZzIjoxMCwidG8iOjE3MDAwNjY1ODAsXXXX\"\n\nRequest output of 10 earlier log records to the nova_part2.log file using the marker parameter value:\n\n      curl -X GET \"https://mcs.mail.ru/auditlogs/v1/b5b7ffd4efXXXX/logs?\\\n   source=nova&\\\n   marker=eyJ0bSI6MTY5NzM2NDAwMCwib2ZzIjoxMCwidG8iOjE3MDAwNjY1ODAsXXXX&\\\n   limit=10&\\\n   from=2023-10-15T10:00:00.000Z&\\\n   to=2023-11-15T16:43:00.477Z\" \\\n   -H \"X-Auth-Token: gAAAAABlXDFc8RTqKryFlXXXX\" \\\n   -H \"Content-Type: application/json\" | jq > nova_part2.log\n\nRepeat the previous request, changing only the file name (for example: nova_part3.log, nova_part4.log, ...), until you get all the log records for the specified period.\n\n\n","url":"/en/tools-for-using-services/account/service-management/actionslogs","description":"Description of working with the user activity log.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"f88d725e-fdbc-419d-8fad-64ca5cbdd75b","title":"Working with the actions log","data":"Information about events and errors that occurred during the operation of the service is stored in VK Cloud logs. This data is used by technical support when analyzing emerging incidents.\n\n\n\nIf there are failures in the running VM, provide technical support with VM message logs.\n\n\n\nTo download logs locally:\n\n\n\nPersonal account\n\n\n\nGo to VK Cloud personal account.\nClick on the user's name in the header of the page.\nFrom the drop-down list, select Application debug.\n\n\n   Detailed information about each event in JSON format is available when you click on it.\n\n\nConfigure the displayed entries:\n\n   Backend type: select a service from the drop-down list. Select the option Unknown backend if the required service is not in the list.\n   Show only 4хх and 5хх: enable the option if you want to display only events completed with errors of type 4XX or 5XX.\n\nClick Download log. A file of the form dpp-log-.json will be downloaded.\n\n\n\n","url":"/en/tools-for-using-services/account/service-management/debugging","description":"Viewing and preparing records with the help of log entries for logging metrics, events and errors when contacting technical support.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"d1c12c61-f441-4d38-b888-e4d615d88963","title":"Configuring Identity Federation","data":"You can connect your identity provider (IdP) to VK Cloud. This will allow your employees to access VK Cloud without entering their login and password, using their corporate authentication data from the IdP. This mode is called Identity Federation.\n\nTo work in federated mode, the IdP must support the SAML 2.0 standard.\n\nThe following steps will show you how to configure federated identity using the Active Directory Federation Services (AD FS) as an example.\n\n\n\nWhen changing your Active Directory domain, remove the existing federation and set up a new one.\n\nBefore you start\n\nSet up AD FS and create users and groups.\nExport the XML metadata file of your AD FS.\n\n1. Create identity federation in VK Cloud\n\nCreating an Identity Federation in VK Cloud is available only to a Project owner role user.\n\nGo to VK Cloud Account.\nGo to the Identity Federation section.\nGo to the Federations tab.\nClick Create.\nUpload XML file with your AD FS metadata and click Create.\nAfter creating the identity federation an XML file will be configured. The file contains data for setting up relying party trust. Download the file to upload it to your identity provider.\n\nThe following credentials will appear on the Federations tab:\n\nURL for federated users to sign in to VK Cloud. Example: https://cloud.vk.com/v1/federation/saml/54f0267b-31f6-XXXX-XXX-2a24c5f436fb/signin.\nFederation ID. Example: 54f0267b-31f6-XXXX-XXX-2a24c5f436fb.\n\n2. Set up AD FS\n\nCreate a relying party trust using federation metadata. Use the XML metadata file. Instructions here.\nConfigure the mapping between user attributes and AD FS outgoing claim types (Claims Mapping). Add the following rules:\n   Sending an authentication method claim: Send an Authentication Method Claim\n   Sending LDAP attributes as claims: Send LDAP Attributes as Claims\n   Sending group membership as a claim: Send Group Membership as a Claim\n\n3. Configure role and group mapping in VK Cloud\n\nConfiguring a relationship between AD FS groups and VK Cloud roles is available only for the following roles: Project owner, Superadministrator, and User access administrator. Viewing the relationships is also available for the Project administrator and Viewer roles.\n\nGo to VK Cloud Account.\nGo to the Manage access section.\nGo to the Groups tab.\nClick the name of the project, that you set up the Identity Federation for.\nAdd the groups that you use in Active Directory:\n\n   Click the Add button. If there are already created groups on the page, click the Add group button.\n   Configure the group:\n      Group name: specify the name of the Active Directory group that the user is a member of.\n      Permissions:\n         Select Project to associate the group and roles within the same project. You can associate the same group with different roles in different projects, which allows you to differentiate the level of federated user access to projects.\n         Select Domain to associate the group and roles across all projects. The Domain permission is only available to the project owner and provides the federated user with the same access level to all projects owned by the owner.\n      Group roles: select the VK Cloud roles, that correspond to your access matrix for the group being created.\n\n   Click the Add button.\n\n4. Check the possibility of logging in via federation\n\nEnter the URL for federated user login in the browser address bar. You will be redirected to the AD FS authentication page.\nEnter your corporate authentication credentials. After successful authorization, you will be redirected to the main VK Cloud personal account page.\nVerify that the automatically assigned VK Cloud user role matches the one selected during group configuration.\n\n\n\nActive Directory is not automatically synchronized with VK Cloud. After blocking a user in Active Directory, remove them from projects in the VK Cloud personal account.\n\n\n","url":"/en/tools-for-using-services/account/how-to-guides/add-fim-for-ad","description":"Configuring federated access to VK Cloud via Active Directory.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"973d9006-c8dd-4695-9f26-28a37fc3b227","title":"Checking the system status","data":"Through your personal account, you can monitor the status of VK Cloud services in real time, as well as get a history of resolved incidents and announcements about planned work.\n\nThe information is available on the page Статус работы сервисов.\n\nStatuses can be useful if you encounter service failures. For example, when creating a VM, an error occurred: the VM was created, but did not appear in the list of virtual machines.\n\n1. Preparatory steps\n\nGo to VK Cloud personal account.\nClick on the user name in the header of the page and select Application debug. The event log opens.\nMake sure that the log contains information about errors like 5XX. This is a sign of possible problems with the operation of the services.\n\n2. View the status of the services\n\nGo to VK Cloud personal account.\nClick on the user name in the header of the page and select State of system. The Статус работы сервисов page opens with information about service outages.\nCheck the status of the services:\n   If the services are stable, contact technical support.\n   If the services are working intermittently, then the problem is known, and the VK Cloud team is already looking for a solution. Click Подписаться на обновления to monitor the status of services in the official Telegram channel VK Cloud.\n\nWhen the incident is resolved, the service will resume, and information about the incident will appear on the Статус работы сервисов page.\n\n3. View information about the incident\n\nWait for the notification about the resumption of the service.\nSee the detailed information about the incident:\n   Go to VK Cloud personal account.\n   Click on the user name in the header of the page and select State of system.\n   Go to the История событий tab.\n   In the drop-down list, select Облачные вычисления.\n   Click on the name of the incident you want. Its description, status and chronology of events will appear.\n","url":"/en/tools-for-using-services/account/how-to-guides/systemstate","description":"Description of the process of obtaining information about the operation of user resources in real time.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"8a3f8d22-34bc-4785-985d-9426fa41a5ff","title":"Deleting an account","data":"If you no longer plan to use VK Cloud services, delete your account. Along with it, all projects in which you are the owner will be deleted.\n\n\n\nAfter deletion, it is impossible to restore the account.\n\n\n\nTo stop debiting funds for using the services, it is not necessary to delete the account. It is enough to preserve or delete projects that consume resources.\n\nTo delete an account:\n\nMake sure that you have transferred all the necessary data from projects and services.\n\n    If you need help with data migration, contact technical support.\n\nContact technical support on behalf of the project owner with a request to delete the account.\n\n    You will be contacted for confirmation. Deletion is possible only after correspondence with the user from his registered mail.\n\nIf necessary, perform refund.\n","url":"/en/tools-for-using-services/account/how-to-guides/account-delete","description":"Description of the process of deleting a VK Cloud user account.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"7153a6a3-19af-4a7d-906c-dd88aad569a8","title":"Creating an idea on the Ideas portal","data":"On the ideas portal you can offer ideas for improving the VK Cloud platform and vote for the introduction of new features.\n\n1. Preparatory steps\n\nGo to VK Cloud personal account.\nClick on the user name in the header of the page and select Ideas Portal.\n\n2. Formulate an idea\n\nClick the Предложить идею in the page header.\nIn the window that appears:\n\n   Что бы вы хотели видеть у нас?: briefly describe the essence of the proposed changes.\n   Прикрепить файл: add a diagram or an image of the expected result.\n   Подписаться на обновления: select the option to receive email notifications.\n\nClick the Предложить button.\n\n3. Follow the implementation of the idea\n\nGo to Ideas Portal.\n(Optional) Vote for the idea:\n\n   Go to the Голосование tab.\n   Find your idea, use sorting if necessary.\n   Click on the icon Голосовать to the right of your idea.\n\n   If you liked other ideas, vote for them in the same way.\n\nWait for the email notification about taking the idea to work:\n\n   Go to the Роадмап tab.\n   Find the proposed idea in the block В работе.\n\nWait for the email notification about the successful implementation of the idea.\nCheck the implementation of the idea:\n\n   Make sure that the idea appeared on the portal on the Готовые tab.\n   Go to  VK Cloud personal account and make sure the idea is implemented.\n","url":"/en/tools-for-using-services/account/how-to-guides/ideasportal","description":"Sending and viewing suggestions for improving the VK Cloud platform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"315854c0-1f37-4cd4-b572-edc1a4e8615a","title":"Tariffication","data":"The use of a personal account is not charged.\n\nPrices for services are given in price list and in project settings. To calculate the cost of the selected service configuration, use the calculator. To learn how to pay for the Platform's services, read the Billing section.\n","url":"/en/tools-for-using-services/account/tariffication","description":"Features of VK Cloud personal account billing.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"d1de9952-9fa7-411f-a846-cdf596729e0c","title":"FAQ","data":"\n\n\nHow to register in VK Cloud?\n\nGo to VK Cloud page and click Create a VK Cloud Account. Fill in all the required fields to start using the account. Read more in the article Registration in VK Cloud.\n\n\n\n\n\nHow do I register with a phone number from another country?\n\nContact technical support by email support@mcs.mail.ru with a request to enter the phone number manually.\n\n\n\nThrough the VK Cloud personal account, you can register only phone numbers that start with +7.\n\n\n\n\n\n\n\nWhat rights do the project participants have?\n\nThe role model of the project participants is described in the article Personal account roles and their permissions.\n\n\n\n\n\nHow do I add an additional participant to the project?\n\nThe procedure is described in the article Access management. When adding, specify the role, focusing on the role matrix.\n\n\n\n\n\nHow do I recover my account password?\n\nClick Forgot password? on the authorization page to your personal account.\nEnter your email in the field Email which was used during registration.\nPress the button Send password. An email with instructions for password recovery will be sent to the specified email address.\nFollow the link in the email about password recovery from the VK Cloud personal account.\nEnter a new password in the field of the same name.\nClick the Save button.\n\n\n\n\n\nHow do I change the phone number for an account?\n\nContact technical support, enter the data:\n\ncurrent phone number\nnew phone number\n\nThe number will be changed after successful confirmation of the user's identity.\n\n\n\n\n\nHow do I change the owner of the VK Cloud account?\n\nUse the instructions from the article Project management.\n\n\n\n\n\nCan a GeekBrains program participants request an increase in quotas on VK Cloud?\n\nYes, they can. To do this, contact the designated GeekBrains curator.\n\n\n\n\n\nHow do I delete an account?\n\nContact technical support, read more in the article Deleting an account.\n\n\n","url":"/en/tools-for-using-services/account/faq","description":"Answers to frequently asked questions about VK Cloud personal account.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Personal account","link":"/en/tools-for-using-services/account","isActive":false,"evenSimpler":true}]},{"uuid":"aa7db855-805c-400d-b71a-30e1112121d1","title":"OpenStack CLI","data":"The OpenStack Command line interface (OpenStack CLI) allows you to work with VK Cloud platform services via the console. To use the OpenStack CLI, install the OpenStack client and authenticate.\n\n1. Install the OpenStack client\n\n\n\nDebian, Ubuntu\nRHEL 8, CentOS 8, Fedora\nCentOS 7\nWindows\n\n\n\nsudo apt update\nsudo apt install python3-openstackclient\n\n\n\n\ndnf install https://www.rdoproject.org/repos/rdo-release.el8.rpm\ndnf update\ndnf install python3-openstackclient\n\n\n\n\nyum install https://rdoproject.org/repos/rdo-release.rpm\nyum upgrade\nyum install python-openstackclient\n\n\n\n\nThe instruction is written using the example of Python 3.10.11 and Microsoft C++ Build Tools 2022. For other versions of programs, the names and versions of components may differ.\n\nDownload and install Python3. It is recommended to use version 3.6 or 3.8.\nDownload and run Microsoft C++ Build Tools.\nGo to the Additional components tab, select from the list and install:\n\n   C++ CMake tools for Windows. When you select this component, the MSVC v143 - VS 2022 C++ x64/x86 build tools (latest) component will be automatically selected.\n   Windows 10 SDK.\n\nRun the command:\n\n      pip install -UI python-openstackclient\n\n\n2. (Optional) Install additional packages\n\nInstall packages to work with specific OpenStack services:\n\n      pip install python-client\n\n   Service names:\n\n   cinder – block storage API and extensions;\n   glance – images API;\n   heat – orchestration API;\n   neutron – network API;\n   nova – cloud computing (VM) and extensions API;\n   octavia — load balancer API;\n   sahara – Cloud Big Data processing API.\n\nInstall the Shared File System API client (Manila CLI) using the command:\n\n      pip install \"python-manilaclient==4.1.3\"\n\n3. Complete authentication\n\nGo to your VK Cloud personal account.\nMake sure that two-factor authentication and API access are enabled.\nSelect a project.\nOn the Project settings page, go to the API access tab.\nClick Download openrc version 3. The -openrc.sh file will be uploaded.\nSpecify the authentication credentials in the environment variables.\n\n   Linux\n   Windows (cmd)\n   Windows (PowerShell)\n\n\n   Run the script:\n\n            source -openrc.sh\n\n   Enter the password of the project user.\n\n\n   Copy the parameter values without quotes from the -openrc.sh file and run the commands:\n\n            set OS_INTERFACE=\n      set OS_IDENTITY_API_VERSION=\n      set OS_PROJECT_ID=\n      set OS_REGION_NAME=\n      set OS_USER_DOMAIN_NAME=\n      set OS_USERNAME=\n      set OS_AUTH_URL=\n\n   Specify the password by running the command:\n\n            set OS_PASSWORD=\n\n\n   Copy the data from the -openrc.sh file and run the commands:\n\n            $env:OS_INTERFACE = \"\"\n      $env:OS_IDENTITY_API_VERSION = \"\"\n      $env:OS_PROJECT_ID = \"\"\n      $env:OS_REGION_NAME = \"\"\n      $env:OS_USER_DOMAIN_NAME = \"\"\n      $env:OS_USERNAME = \"\"\n      $env:OS_AUTH_URL = \"\"\n\n   Specify the password by running the command:\n\n            $env:OS_PASSWORD = \"\"\n\n\n4. Check if the OpenStack CLI is ready to work\n\nCheck the availability of the OpenStack client:\n\n      openstack --version\n\n   If the OpenStack client is installed, its version will be displayed in the console output.\n\nMake sure that the environment variables match the project by running the command:\n\n   Linux\n   Windows (cmd)\n   Windows (PowerShell)\n\n\n      env | grep OS_\n\n\n      set | findstr OS_\n\n\n      gci env: | where name -like 'OS_*'\n\n\n   The environment variables must contain authentication credentials corresponding to the project.\n\nRun the command that uses the OpenStack client. For example:\n\n      openstack project list\n\n   The console output should display a list of available projects.\n\nExamples of OpenStack CLI commands\n\nView the list of available flavors:\n\n      openstack flavor list\n\n   Example of the command output\n\n      +--------------------------------------+-------------------+-------+------+-----------+-------+-----------+\n   | ID                                   | Name              |   RAM | Disk | Ephemeral | VCPUs | Is Public |\n   +--------------------------------------+-------------------+-------+------+-----------+-------+-----------+\n   | 00bbf595-aa67-XXXX-b566-92cbe8d00941 | STD2-16-32        | 32768 |    0 |         0 |    16 | True      |\n   | 03c66e24-b386-XXXX-91f8-36e898d7fa72 | STD3-1-2          |  2048 |    0 |         0 |     1 | True      |\n   | 04db9642-04fe-XXXX-89cb-c5a778be9ef3 | STD2-6-24         | 24576 |    0 |         0 |     6 | True      |\n   | 0c5d5d41-1317-XXXX-ab58-9c9e04da50d6 | STD2-4-12         | 12288 |    0 |         0 |     4 | True      |\n   | 17f80791-c0dd-XXXX-adaa-8c4a83fa0c51 | STD2-8-16         | 16384 |    0 |         0 |     8 | True      |\n   | 19ad4a49-5b3d-XXXX-a61d-b4b8b44c9842 | STD3-16-64        | 65536 |    0 |         0 |    16 | True      |\n   | 19dc16ec-6d6c-XXXX-af1a-ff5cbb056bed | STD3-6-12         | 12288 |    0 |         0 |     6 | True      |\n\n\nOutput information about a specific image:\n\n      openstack image show c6320138-035f-40d8-XXXX-e814edb2ce5f\n\n   Example of the command output (shortened)\n\n        +------------------+------------------------------------------------------+\n    | Field            | Value                                                |\n    +------------------+------------------------------------------------------+\n    | checksum         | 896ea37e28d82a548cedf1e0aa92XXXX                     |\n    | container_format | bare                                                 |\n    | created_at       | 2023-03-29T14:06:44Z                                 |\n    | disk_format      | raw                                                  |\n    | file             | /v2/images/c6320138-035f-40d8-XXXX-e814edb2ce5f/file |\n    | id               | c6320138-035f-40d8-XXXX-e814edb2ce5f                 |\n    | min_disk         | 0                                                    |\n    | min_ram          | 0                                                    |\n    | name             | Alt-Linux-P9-Starter-Kit                             |\n    | owner            | b5b7ffd4ef0547e5b222f44555dfXXXX                     |\n    | properties       | base_image_ref='1a8aa332-d8ef-4c40-XXXX-cade8b59aea3'|\n    | protected        | False                                                |\n    | schema           | /v2/schemas/image                                    |\n    | size             | 1653604352                                           |\n    | status           | active                                               |\n    | tags             |                                                      |\n    | updated_at       | 2023-03-29T14:08:15Z                                 |\n    | visibility       | private                                              |\n    +------------------+------------------------------------------------------+\n\n\nGet the list of availability zones:\n\n      openstack availability zone list\n\n   The example result of running the command\n\n        +-----------+-------------+\n    | Zone Name | Zone Status |\n    +-----------+-------------+\n    | MS1       | available   |\n    | GZ1       | available   |\n    | ME1       | available   |\n    +-----------+-------------+\n\n","url":"/en/tools-for-using-services/cli/openstack-cli","description":"Using the OpenStack CLI to manage resources on the VK Cloud platform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"CLI","link":"/en/tools-for-using-services/cli","isActive":false,"evenSimpler":true}]},{"uuid":"f8a5e198-6044-4223-acd1-ff924eefe3d2","title":"Quick start","data":"Preparatory steps\n\nInstall Terraform from the official VK Cloud mirror.\nOpen your VK CLoud personal account.\n\nEnable two-factor authentication and API access, if not enabled yet.\n\nClick on your login at the top of the page, select Project settings from the drop-down menu.\n\nGo to the Terraform tab. Download the main Terraform configuration and the Terraform mirror configuration files by clicking the corresponding buttons.\n\n    Files named vkcs_provider.tf and terraform.rc will be downloaded.\n\nPerform the following actions with the files:\n\n    Windows\n    Other OS\n\n\n    Paste %APPDATA% into the address bar of Windows Explorer and copy the terraform.rc file to the directory that opens.\n\n    Copy the vkcs_provider.tf file into the working directory from which you are going to work with the platform.\n\n        Typically a separate working directory is created for each VK Cloud project.\n\n\n    Rename the Terraform mirror configuration file from terraform.rc to .terraformrc.\n    Copy the .terraformrc file to the user's home directory root.\n    Copy the vkcs_provider.tf file into the working directory from which you are going to work with the platform.\n\n        Typically a separate working directory is created for each VK Cloud project.\n\n\n    You can create both configuration files yourself. You can also edit the downloaded files — for example, add other Terraform providers. The contents of the files are described in section Configuration files.\n\n\nTerraform initialization\n\nIn the directory from which you are going to work with the project, run the command:\n\nterraform init\n\nThis will create supplementary files needed for Terraform to work.\n\nCreating resources via Terraform\n\nCreate a configuration of resources in the working directory — for example, a configuration for creating a virtual machine.\n\nRun the command:\n\n        terraform apply\n\n    In the terminal window, type yes to confirm the operation.\n\nWait until the operation is complete.\n\nThe created resources will be available in your personal account.\n\nUpdate Terraform\n\nIf the provider version in the vkcs_provider.tf file is lower than the one needed to create the resource, an error will occur when checking the configuration. To avoid this, update your provider:\n\nOpen the vkcs_provider.tf file and edit the version as you need, for example: version = \"~> 0.6.0\".\nRun the command in the terminal:\n\n    terraform init --upgrade\n","url":"/en/tools-for-using-services/terraform/quick-start","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"2085f47a-a039-47e5-9f33-1e3fa3f62e0a","title":"Creating a VM","data":"Using Terraform you can create virtual machines. As an example, a VM will be created that is accessible from an external network. To access the VM, a pair of SSH keys already existing in the project will be used.\n\nTwo options for VM configuration will be considered: without additional settings and with an additional disk connected.\n\nPreparatory steps\n\nReview the available resources and quotas for the region where you plan to create a VM. Different regions may have different quotas configured.\n\n    If you want to increase quotas, contact technical support.\n\nInstall Terraform and configure the environment if it is not already done.\n\n    Place the provider settings in the Terraform configuration file provider.tf.\n\nMake sure the OpenStack client is installed and then authenticate with the project.\n\nCreate a Terraform configuration file variables.tf with variables:\n\n      variable \"image_flavor\" {\n     type = string\n     default = \"Ubuntu-22.04-202208\"\n   }\n\n   variable \"compute_flavor\" {\n     type = string\n     default = \"STD2-2-4\"\n   }\n\n   variable \"key_pair_name\" {\n     type = string\n     default = \"keypair-terraform\"\n   }\n\n   variable \"availability_zone_name\" {\n     type = string\n     default = \"MS1\"\n   }\n\n   This file declares the following variables:\n\n   image_flavor: the name of the virtual machine image;\n   compute_flavor: the name of the virtual machine configuration template;\n   key_pair_name: the name of the key pair that will be used to connect to the virtual machine via SSH;\n   availability_zone_name: the name of the availability zone where the virtual machine will be hosted.\n\n   If necessary, adjust the values of the variables. First find out their acceptable values:\n\n   image_flavor\n   compute_flavor\n   key_pair_name\n   availability_zone_name\n\n\n   Using OpenStack CLI:\n\n      openstack image list\n\n\n   Using OpenStack CLI:\n\n      openstack flavor list\n\n\n   Use one of the methods:\n\n    Via personal account:\n\n      Go to your VK Cloud personal account.\n\n      Click on your username in the page header.\n\n      Select Key pairs from the drop-down list.\n\n        The Key pairs tab of the Account information page will open.\n\n        Key pair names appear under the Key name heading.\n\n    Using OpenStack CLI:\n\n      Run the command:\n\n                openstack keypair list\n\n      Copy the required key pair name from the list.\n\n\n   In the section about availability zones.\n\n\n1. Create a file describing the basic network infrastructure\n\nCreate a network.tf file.\n\n    The file describes the resources of the virtual network in which the VM will operate.\n\nMake sure that the following resources are present in network.tf:\n\n   vkcs_networking_network,\n   vkcs_networking_subnet,\n   vkcs_networking_router,\n   vkcs_networking_router_interface.\n\n    There is no need to configure additional security groups.\n\nThe resources arguments are described in the Terraform provider documentation.\n\n2. Create a file with the VM description\n\nCreate a main.tf file.\n\nDepending on the required configuration option (a VM without additional settings or a VM with an additional disk), place the contents of one of the tabs below into the file.\n\n\n\nVM without additional settings\nVM with additional disk\n\n\n\nThe file describes:\n\nVM parameters;\nresources assigned to the VM that are necessary for access from an external network:\n  a floating IP address;\n  a SSH key pair that will be used for access;\n  security groups to which the VM must be added: default and ssh (both groups are configured in VK Cloud by default).\n\ndata \"vkcs_compute_flavor\" \"compute\" {\n  name = var.compute_flavor\n}\n\ndata \"vkcs_images_image\" \"compute\" {\n  name = var.image_flavor\n}\n\nresource \"vkcs_compute_instance\" \"compute\" {\n  name                    = \"compute-instance\"\n  flavor_id               = data.vkcs_compute_flavor.compute.id\n  key_pair                = var.key_pair_name\n  security_groups         = [\"default\",\"ssh\"]\n  availability_zone       = var.availability_zone_name\n\n  block_device {\n    uuid                  = data.vkcs_images_image.compute.id\n    source_type           = \"image\"\n    destination_type      = \"volume\"\n    volume_type           = \"ceph-ssd\"\n    volume_size           = 8\n    boot_index            = 0\n    delete_on_termination = true\n  }\n\n  network {\n    uuid = vkcs_networking_network.network.id\n  }\n\n  depends_on = [\n    vkcs_networking_network.network,\n    vkcs_networking_subnet.subnetwork\n  ]\n}\n\nresource \"vkcs_networking_floatingip\" \"fip\" {\n  pool = data.vkcs_networking_network.extnet.name\n}\n\nresource \"vkcs_compute_floatingip_associate\" \"fip\" {\n  floating_ip = vkcs_networking_floatingip.fip.address\n  instance_id = vkcs_compute_instance.compute.id\n}\n\noutput \"instance_fip\" {\n  value = vkcs_networking_floatingip.fip.address\n}\n\n\n\nThe file describes:\n\nVM parameters;\nresources assigned to the VM that are necessary for access from an external network:\n  a floating IP address;\n  a SSH key pair that will be used for access;\n  security groups to which the VM must be added: default and ssh (both groups are configured in VK Cloud by default);\nan additional 50 GB block device;\na synthetic resource vkcs_compute_volume_attach for connecting the block device to the VM.\n\ndata \"vkcs_compute_flavor\" \"compute\" {\n  name = var.compute_flavor\n}\n\ndata \"vkcs_images_image\" \"compute\" {\n  name = var.image_flavor\n}\n\nresource \"vkcs_compute_instance\" \"compute\" {\n  name                    = \"compute-instance\"\n  flavor_id               = data.vkcs_compute_flavor.compute.id\n  key_pair                = var.key_pair_name\n  security_groups         = [\"default\",\"ssh\"]\n  availability_zone       = var.availability_zone_name\n\n  block_device {\n    uuid                  = data.vkcs_images_image.compute.id\n    source_type           = \"image\"\n    destination_type      = \"volume\"\n    volume_type           = \"ceph-ssd\"\n    volume_size           = 8\n    boot_index            = 0\n    delete_on_termination = true\n  }\n\n  network {\n    uuid = vkcs_networking_network.network.id\n  }\n\n  depends_on = [\n    vkcs_networking_network.network,\n    vkcs_networking_subnet.subnetwork\n  ]\n}\n\nresource \"vkcs_blockstorage_volume\" \"compute-volume\" {\n  name                  = \"myVolume\"\n  description           = \"Additional volume for my app\"\n  size                  = 50\n  availability_zone     = var.availability_zone_name\n  volume_type           = \"ceph-ssd\"\n}\n\nresource \"vkcs_compute_volume_attach\" \"compute-volume-attached\" {\n  instance_id = vkcs_compute_instance.compute.id\n  volume_id   = vkcs_blockstorage_volume.compute-volume.id\n  }\n\nresource \"vkcs_networking_floatingip\" \"fip\" {\n  pool = data.vkcs_networking_network.extnet.name\n}\n\nresource \"vkcs_compute_floatingip_associate\" \"fip\" {\n  floating_ip = vkcs_networking_floatingip.fip.address\n  instance_id = vkcs_compute_instance.compute.id\n}\n\noutput \"instance_fip\" {\n  value = vkcs_networking_floatingip.fip.address\n}\n\n\nThe resources arguments are described in the Terraform provider documentation.\n\n3. Create resources using Terraform\n\nPlace the Terraform configuration files provider.tf, variables.tf, network.tf and main.tf in the same directory.\n\nGo to this directory.\n\nRun the command:\n\n        terraform init\n\nRun the command:\n\n        terraform apply\n\n    When prompted for confirmation, enter yes.\n\nWait until the operation completes.\n\nOnce resource creation is complete, the Terraform instance_fip output will show the floating IP address assigned to the VM.\n\n4. Check if the example works\n\nConnect via SSH to the compute-instance virtual machine.\n\nTo connect use:\n\nthe IP address from the instance_fip output;\nthe private SSH key from the keypair-terraform key pair.\n\nIf the example worked successfully, the console will show typical Ubuntu output:\n\nWelcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.15.0-46-generic x86_64)\n\nDocumentation: https://help.ubuntu.com\nManagement: https://landscape.canonical.com\nSupport: https://ubuntu.com/advantage\n\nSystem information as of Wed May 10 18:05:44 UTC 2023\n\nSystem load: 0.0078125 Processes: 98\nUsage of /: 35.2% of 7.42GB Users logged in: 0\nMemory usage: 9% IPv4 address for ens3: 192.168.199.20\nSwap usage: 0%\n...\n\nDelete unused resources\n\nSome of the objects created in this scenario consume resources. If you no longer need them, delete them:\n\nGo to the directory with the Terraform configuration files.\n\nRun the command:\n\n        terraform destroy\n\n    When prompted for confirmation, enter yes.\n\nWait until the operation completes.\n","url":"/en/tools-for-using-services/terraform/how-to-guides/iaas/create","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"2ffd7c4d-09b0-4260-81bb-2e3d7cc5be95","title":"Creating NFS","data":"\n\nMake sure you installed and configured Terraform.\n\n\n\nTo create an NFS, create a file nfs.tf, which will describe the configuration of the NFS being created. This example creates an NFS and grants read/write access from two IP addresses. Add the text from the example below and correct the settings for your NFS.\n\nCreate a virtual network for NFS\n\nWhen creating an NFS, you must specify a network and subnet on which this resource will be created. You can create a network and subnet according to instruction and specify them in the vkcs_networking_network and vkcs_networking_subnet resources in the example below.\n\nIf you use a network and subnet created in another way, specify them as data source vkcs_networking_network and vkcs_networking_subnet instead of specifying the corresponding resources.\n\nCreate an NFS\n\nTo create an NFS, you need the following objects:\n\nResources:\n\n  vkcs_networking_network: a network on which the NFS will be created. In the example below, a network named sfs is created.\n  vkcs_networking_subnet: a subnet of the network. In the example: sfs.\n  vkcs_sharedfilesystem_sharenetwork: use this resource to set up a shared network. The shared network stores information that NFS servers can use when creating the NFS. The following arguments are supported:\n\n    name: a name for the shared network. Changing this argument updates the name of the existing shared network.\n    neutron_net_id: the UUID of the Neutron network when setting up or updating the shared network. Changing this argument updates the existing shared network if it is not being used by shared resources.\n    neutron_subnet_id: the UUID of the Neutron subnet when setting up or updating the shared network. Changing this argument updates the existing shared network if it is not being used by shared resources.\n\n  vkcs_sharedfilesystem_share: use this resource to set up a share. The following arguments are supported:\n\n    name: the share name. Changing this argument updates the name of the existing share.\n    description: a human-readable description of the share. Changing this argument updates the description of the existing share.\n    share_proto: the sharing protocol, it can be NFS, CIFS, CEPHFS, GLUSTERFS, HDFS or MAPRFS. Changing this argument creates a new share.\n    share_type: the share type. If you omit this argument, the default share type is used.\n    size: the share size, in gigabytes. The requested share size cannot exceed the allowed quota in GB. Changing this argument changes the size of the existing share.\n    share_network_id: the ID of the network with the NFS server.\n\n  vkcs_sharedfilesystem_share_access: use this resource to manage share lists. The following arguments are supported:\n\n    share_id: the UUID of the share that you have been granted access to.\n    access_type: the access rule type. Can be one of the following: ip, user, cert, or cephx.\n    access_to: the value specifying access. This can be either an IP address or a username verified by the configured Cloud Networks security service.\n    access_level: the level of access to the shared resource. Can be either rw for read-write access or ro for read-only access.\n\nExample of the nfs.tf file:\n\n\nresource \"vkcs_networking_network\" \"sfs\" {\n      name = \"network\"\n    }\n\nresource \"vkcs_networking_subnet\" \"sfs\" {\n  name = \"subnet\"\n  cidr = \"192.168.199.0/24\"\n  network_id = \"${vkcs_networking_network.sfs.id}\"\n}\n\nresource \"vkcs_sharedfilesystem_sharenetwork\" \"sharenetwork\" {\n  name                = \"test_sharenetwork\"\n  neutron_net_id      = \"${vkcs_networking_network.sfs.id}\"\n  neutron_subnet_id   = \"${vkcs_networking_subnet.sfs.id}\"\n}\n\nresource \"vkcs_sharedfilesystem_share\" \"share\" {\n  name             = \"nfs_share\"\n  description      = \"test share description\"\n  share_proto      = \"NFS\"\n  share_type       = \"default_share_type\"\n  size             = 1\n  share_network_id = \"${vkcs_sharedfilesystem_sharenetwork.sharenetwork.id}\"\n}\n\nresource \"vkcs_sharedfilesystem_share_access\" \"share_access_1\" {\n  share_id     = \"${vkcs_sharedfilesystem_share.share.id}\"\n  access_type  = \"ip\"\n  access_to    = \"192.168.199.10\"\n  access_level = \"rw\"\n}\n\nresource \"vkcs_sharedfilesystem_share_access\" \"share_access_2\" {\n  share_id     = \"${vkcs_sharedfilesystem_share.share.id}\"\n  access_type  = \"ip\"\n  access_to    = \"192.168.199.11\"\n  access_level = \"rw\"\n}\n\nApply changes\n\nAdd the text of the example to the nfs.tf file and run the following commands:\n\nterraform init\nterraform apply\n","url":"/en/tools-for-using-services/terraform/how-to-guides/iaas/nfs","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"15fefef0-d0d1-40f0-aac4-f8dc8dc5619c","title":"Creating key pairs","data":"\n\nMake sure you installed and configured Terraform.\n\n\n\nTo create a key pair, create a keypair.tf file that describes its configuration. Add the text from the example below and correct the settings for your infrastructure. This example describes how to create a key pair with the name test-keypair and display the public and private keys using the output operator.\n\nCreate a key pair\n\n\n\nThe private key generated by this resource will be stored unencrypted in your Terraform state file. The use of this resource for production deployments is not recommended. Instead, create a private key file outside of Terraform and distribute it securely to the system where Terraform will run.\n\n\n\nTo create a key pair, you need the vkcs_compute_keypair resource.\n\nresource \"vkcs_compute_keypair\" \"keypair\" {\n  name = \"test-keypair\"\n}\n\noutput \"public_key\" {\n  value = vkcs_compute_keypair.keypair.public_key\n}\n\noutput \"private_key\" {\n  value = vkcs_compute_keypair.keypair.private_key\n  sensitive = true\n}\n\nHere, name is a unique name of the key pair (a required argument). Changing this argument will create a new key pair.\n\nApply changes\n\nAdd the text of the example to the keypair.tf file and run the commands:\n\nterraform init\nterraform apply\n","url":"/en/tools-for-using-services/terraform/how-to-guides/iaas/keypair","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"e065e289-daa0-4d2c-8d53-9d7e3d1500d6","title":"Managing disks and their snapshots","data":"\n\nMake sure you installed and configured Terraform.\n\nCreate a disk volume\n\nTo create a disk volume, create a volume.tf file that describes the configuration of the volume to be created. Add the text from the example below and correct the settings for your volume. This example describes how to create a 1 GB disk volume of type ceph-ssd in the GZ1 availability zone.\n\nTo create a disk volume, you need the vkcs_blockstorage_volume resource that provides a block storage volume. You can create, modify, and delete block storage volumes.\n\nresource \"vkcs_blockstorage_volume\" \"volume\" {\n  name = \"volume\"\n  description = \"test volume\"\n  metadata = {\n    foo = \"bar\"\n  }\n  size = 1\n  availability_zone = \"GZ1\"\n  volume_type = \"ceph-ssd\"\n}\n\nHere:\n\nname: the volume name.\ndescription: the volume description.\nmetadata: the key value map for the volume metadata.\nsize: (required) the volume size in gigabytes.\navailability_zone: (required) the name of the availability zone of the data centers storing the volume.\nvolume_type: (required) the volume type.\n\nApply changes\n\nAdd the text of the example to the volume.tf file and run the following commands:\n\nterraform init\nterraform apply\n\nCreate а snapshot\n\nTo create a snapshot of the state of a disk volume, create a file snapshot.tf that describes the configuration of the snapshot to be created. Add the text from the examples below and adjust the settings for your infrastructure. This example describes how to create a snapshot of a disk volume with the resource ID vkcs_blockstorage_volume.volume.id.\n\nTo create a snapshot you need the vkcs_blockstorage_snapshot resource that a block storage snapshot. You can create, modify, and delete a snapshot of a block storage volume.\n\nresource \"vkcs_blockstorage_snapshot\" \"snapshot\" {\n  volume_id = \"${vkcs_blockstorage_volume.volume.id}\"\n  name = \"snapshot\"\n  description = \"test snapshot\"\n  metadata = {\n    foo = \"bar\"\n  }\n}\n\nHere:\n\nvolume_id: (required) the ID of the volume for which the snapshot to be created.\nname: the snapshot name.\ndescription: the snapshot description.\nmetadata: the key value map for the volume metadata.\n\nApply changes\n\nAdd the text of the example to the snapshot.tf file and run the following commands:\n\nterraform init\nterraform apply\n","url":"/en/tools-for-using-services/terraform/how-to-guides/iaas/volumes","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"c14f9f6e-6b12-4ae8-871f-994740237ea2","title":"Creating a DB instance","data":"The article provides examples of creating database instances with various settings using Terraform.\n\nPreparatory steps\n\nCheck out the available resources and quotas for the region in which you plan to create a DB instance. Different quotas can be set up for different regions.\n\n   If you want to increase quotas, contact technical support.\n\nInstall Terraform and configure the environment if it is not already done.\n\n    Put the provider settings in the Terraform configuration file provider.tf.\n\n1. Create a file with a description of the DB instance\n\nIn all the examples below, DB instances are created with the following properties:\n\nRegion: by default (the same as the project region).\nAvailability zone: by default (for the Moscow region — GZ1).\nDBMS type and version: MySQL 8.0.\nConfiguration of the DB instance: Single.\nExternal IP address: assigned.\n\n\n\nWhen creating MySQL, PostgreSQL, Postgres Pro Enterprise, Postgres Pro Enterprise 1C database instances in Cluster configuration a service load balancer will be created.\n\nUsing a load balancer is charged.\n\n\n\nSelect one of the cluster creation examples and create a Terraform configuration file main.tf with the appropriate content:\n\n\n\nWithout additional settings\nWith extension\nWith DB and user\n\n\n\nThe flavor for the instance VM is set via the db-instance-flavor variable.\n\nvariable \"db-instance-flavor\" {\n  type    = string\n  default = \"STD3-2-6\"\n}\n\ndata \"vkcs_compute_flavor\" \"db\" {\n  name = var.db-instance-flavor\n}\n\nresource \"vkcs_networking_network\" \"db\" {\n  name           = \"db-net\"\n  admin_state_up = true\n}\n\nresource \"vkcs_networking_subnet\" \"db-subnetwork\" {\n  name            = \"db-subnet\"\n  network_id      = vkcs_networking_network.db.id\n  cidr            = \"10.100.0.0/16\"\n  dns_nameservers = [\"8.8.8.8\", \"8.8.4.4\"]\n}\n\ndata \"vkcs_networking_network\" \"extnet\" {\n  name = \"internet\"\n}\n\nresource \"vkcs_networking_router\" \"db-router\" {\n  name                = \"db-router\"\n  admin_state_up      = true\n  external_network_id = data.vkcs_networking_network.extnet.id\n}\n\nresource \"vkcs_networking_router_interface\" \"db\" {\n  router_id = vkcs_networking_router.db-router.id\n  subnet_id = vkcs_networking_subnet.db-subnetwork.id\n}\n\nresource \"vkcs_db_instance\" \"db-instance\" {\n  name        = \"db-instance\"\n  datastore {\n    type    = \"mysql\"\n    version = \"8.0\"\n  }\n  floating_ip_enabled = true\n  flavor_id   = data.vkcs_compute_flavor.db.id\n  size        = 8\n  volume_type = \"ceph-ssd\"\n  disk_autoexpand {\n    autoexpand    = true\n    max_disk_size = 1000\n  }\n  network {\n    uuid = vkcs_networking_network.db.id\n  }\n}\n\n\n\n\nThe capabilities setting of the vkcs_db_instance resource is responsible for adding extensions.\n\nvariable \"db-instance-flavor\" {\n  type    = string\n  default = \"STD3-2-6\"\n}\n\ndata \"vkcs_compute_flavor\" \"db\" {\n  name = var.db-instance-flavor\n}\n\nresource \"vkcs_networking_network\" \"db\" {\n  name           = \"db-net\"\n  admin_state_up = true\n}\n\nresource \"vkcs_networking_subnet\" \"db-subnetwork\" {\n  name            = \"db-subnet\"\n  network_id      = vkcs_networking_network.db.id\n  cidr            = \"10.100.0.0/16\"\n  dns_nameservers = [\"8.8.8.8\", \"8.8.4.4\"]\n}\n\ndata \"vkcs_networking_network\" \"extnet\" {\n  name = \"ext-net\"\n}\n\nresource \"vkcs_networking_router\" \"db-router\" {\n  name                = \"db-router\"\n  admin_state_up      = true\n  external_network_id = data.vkcs_networking_network.extnet.id\n}\n\nresource \"vkcs_networking_router_interface\" \"db\" {\n  router_id = vkcs_networking_router.db-router.id\n  subnet_id = vkcs_networking_subnet.db-subnetwork.id\n}\n\nresource \"vkcs_db_instance\" \"db-instance\" {\n  name        = \"db-instance\"\n  datastore {\n    type    = \"mysql\"\n    version = \"8.0\"\n  }\n  floating_ip_enabled = true\n  flavor_id   = data.vkcs_compute_flavor.db.id\n  size        = 8\n  volume_type = \"ceph-ssd\"\n  disk_autoexpand {\n    autoexpand    = true\n    max_disk_size = 1000\n  }\n  network {\n    uuid = vkcs_networking_network.db.id\n  }\n  capabilities {\n    name = \"node_exporter\"\n  }\n    settings = {\n      \"listen_port\" : \"9100\"\n  }\n}\n\n\n\n\nThe vkcs_db_database and vkcs_db_user resources are responsible for adding databases and users, respectively.\n\nThe DB user's password is sensitive data. For safe work with it, use a variable, for example: db_user_password. Add the sensitive = true parameter to this variable to hide its output in the console:\n\nvariable \"db_user_password\" {\n  type      = string\n  sensitive = true\n}\n\n\n  How do I set a value for a variable with the sensitive = true parameter?\n\nUse one of the ways to set a password:\n\nCreate a file secret.tfvars, write db_user_password=\"YOUR_DB_PASSWORD\" into it and pass it as an argument to the terraform apply command:\n\n      terraform apply -var-file=\"secret.tfvars\"\n\n    Confidential data will be stored separately from the configuration.\n\nIn the shell session from which you plan to work with Terraform, set the value via the environment variable with the prefix TF_VAR:\n\n   Linux, macOS\n   Windows (cmd)\n   Windows (PowerShell)\n\n\n      export TF_VAR_db_user_password=YOUR_DB_PASSWORD\n\n\n        set TF_VAR_db_user_password=YOUR_DB_PASSWORD\n\n\n        $Env:TF_VAR_db_user_password = \"YOUR_DB_PASSWORD\"\n\n\n  When the configuration is applied, the password value is automatically loaded from the environment.\n\n\n\nIf you do not set the password value, Terraform will require you to enter it in the console when applying the configuration.\n\n\n\n\n\nA configuration file with the addition of a database and a user:\n\nvariable \"db_user_password\" {\n  type      = string\n  sensitive = true\n}\n\nvariable \"db-instance-flavor\" {\n  type    = string\n  default = \"STD3-2-6\"\n}\n\ndata \"vkcs_compute_flavor\" \"db\" {\n  name = var.db-instance-flavor\n}\n\nresource \"vkcs_networking_network\" \"db\" {\n  name           = \"db-net\"\n  admin_state_up = true\n}\n\nresource \"vkcs_networking_subnet\" \"db-subnetwork\" {\n  name            = \"db-subnet\"\n  network_id      = vkcs_networking_network.db.id\n  cidr            = \"10.100.0.0/16\"\n  dns_nameservers = [\"8.8.8.8\", \"8.8.4.4\"]\n}\n\ndata \"vkcs_networking_network\" \"extnet\" {\n  name = \"ext-net\"\n}\n\nresource \"vkcs_networking_router\" \"db-router\" {\n  name                = \"db-router\"\n  admin_state_up      = true\n  external_network_id = data.vkcs_networking_network.extnet.id\n}\n\nresource \"vkcs_networking_router_interface\" \"db\" {\n  router_id = vkcs_networking_router.db-router.id\n  subnet_id = vkcs_networking_subnet.db-subnetwork.id\n}\n\nresource \"vkcs_db_instance\" \"db-instance\" {\n  name        = \"db-instance\"\n  datastore {\n    type    = \"mysql\"\n    version = \"8.0\"\n  }\n  floating_ip_enabled = true\n  flavor_id   = data.vkcs_compute_flavor.db.id\n  size        = 8\n  volume_type = \"ceph-ssd\"\n  disk_autoexpand {\n    autoexpand    = true\n    max_disk_size = 1000\n  }\n  network {\n    uuid = vkcs_networking_network.db.id\n  }\n}\n\nresource \"vkcs_db_database\" \"db-database\" {\n  name        = \"testdb\"\n  dbms_id = vkcs_db_instance.db-instance.id\n  charset     = \"utf8\"\n}\n\nresource \"vkcs_db_user\" \"db-user\" {\n  name        = \"testuser\"\n  password    = var.db_user_password\n  dbms_id = vkcs_db_instance.db-instance.id\n  databases   = [vkcs_db_database.db-database.name]\n}\n\n2. Create resources using Terraform\n\nPut the Terraform configuration files terraform.rc, vkcs_provider.tf, main.tf and secret.tfvars (if created) in the same directory.\nGo to this directory.\nRun the command:\n\n        terraform init\n\n    Wait for Terraform initialization to complete.\n\nRun the command:\n\n        terraform apply\n\n    When prompted for confirmation, print yes.\n\nWait for the operation to complete.\n\n3. Check the configuration application\n\nGo to your VK Cloud personal account, Databases → Databases instances. Make sure that all the objects described in the configuration have been created successfully:\n\nThe DB instance.\nThe external IP address for the instance — is displayed on the instance page.\nThe Prometheus Node exporter extension (if installed) — is displayed on the instance page on the Extensions tab.\nDB on the instance (if created) — is displayed on the instance page on the List of databases tab.\nDB user (if created) — is displayed on the instance page on the Users tab.\n\nDelete unused resources\n\nSome objects created in this scenario consume resources. If you no longer need them, delete them:\n\nGo to the directory with the Terraform configuration files.\nRun the command:\n\n        terraform destroy\n\n    When prompted for confirmation, print yes.\n\nWait for the operation to complete.\n","url":"/en/tools-for-using-services/terraform/how-to-guides/dbaas/create-db-instance","description":"How to create a DB instance using Terraform in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"5c8c4163-6f78-4bab-ae4d-383dc6c00a47","title":"Creating Kubernetes cluster","data":"The examples of creating different Kubernetes clusters using Terraform are provided below. The cluster creation procedure is described in details in the Creating a cluster with Terraform section.\n\nBefore creating a cluster\n\nCheck out the available resources and quotas for the region in which you plan to create a cluster. Different quotas can be set up for different regions.\n\n   If you want to increase quotas, contact technical support.\n\nLearn about the features of using Terraform with the Cloud Containers service.\n\nInstall Terraform and configure the environment if it is not already done.\n\n   Place the provider settings in the provider.tf Terraform configuration file.\n\n1. Create a file describing the cluster network infrastructure\n\nCreate the network.tf Terraform configuration file which describes the cluster network infrastructure:\n\ndata \"vkcs_networking_network\" \"extnet\" {\n  name = \"internet\"\n}\n\nresource \"vkcs_networking_network\" \"k8s\" {\n  name           = \"k8s-net\"\n  admin_state_up = true\n}\n\nresource \"vkcs_networking_subnet\" \"k8s\" {\n  name            = \"k8s-subnet\"\n  network_id      = vkcs_networking_network.k8s.id\n  cidr            = \"192.168.199.0/24\"\n  dns_nameservers = [\"8.8.8.8\", \"8.8.4.4\"]\n}\n\nresource \"vkcs_networking_router\" \"k8s\" {\n  name                = \"k8s-router\"\n  admin_state_up      = true\n  external_network_id = data.vkcs_networking_network.extnet.id\n}\n\nresource \"vkcs_networking_router_interface\" \"k8s\" {\n  router_id = vkcs_networking_router.k8s.id\n  subnet_id = vkcs_networking_subnet.k8s.id\n}\n\n2. Create a file describing the Kubernetes cluster\n\nIn the examples below, clusters with the following configuration are created:\n\nThe Moscow region, the GZ1 availability zone.\nKubernetes version: 1.25.\n\nOne STD3-4-8 master node.\nTwo STD2-2-4 worker nodes:\n\n  with two labels configured:\n\n    env:test\n    disktype:ssd\n\n  with two taints configured:\n\n    taintkey1:taintvalue1: PreferNoSchedule\n    taintkey2:taintvalue2: PreferNoSchedule\n\nExternal public IP address assigned to each cluster.\n\nSelect one of the cluster creation examples and create the main.tf Terraform configuration file with the necessary content:\n\n\n\nWith no advanced settings\nWith redefining pod subnet\nWith installing Docker Registry\n\n\n\ndata \"vkcs_compute_flavor\" \"k8s-master-flavor\" {\n    name = \"STD3-4-8\"\n}\n\ndata \"vkcs_compute_flavor\" \"k8s-node-group-flavor\" {\n name = \"STD2-2-4\"\n}\n\ndata \"vkcs_kubernetes_clustertemplate\" \"k8s-template\" {\n    version = \"1.25\"\n}\n\nresource \"vkcs_kubernetes_cluster\" \"k8s-cluster\" {\n\n  depends_on = [\n    vkcs_networking_router_interface.k8s,\n  ]\n\n  name                = \"k8s-cluster-tf\"\n  cluster_template_id = data.vkcs_kubernetes_clustertemplate.k8s-template.id\n  master_flavor       = data.vkcs_compute_flavor.k8s-master-flavor.id\n  master_count        = 1\n  network_id          = vkcs_networking_network.k8s.id\n  subnet_id           = vkcs_networking_subnet.k8s.id\n  availability_zone   = \"GZ1\"\n\n  floating_ip_enabled = true\n\n}\n\nresource \"vkcs_kubernetes_node_group\" \"k8s-node-group\" {\n  name = \"k8s-node-group\"\n  cluster_id = vkcs_kubernetes_cluster.k8s-cluster.id\n  flavor_id = data.vkcs_compute_flavor.k8s-node-group-flavor.id\n\n  node_count = 2\n\n\n  labels {\n        key = \"env\"\n        value = \"test\"\n    }\n\n  labels {\n        key = \"disktype\"\n        value = \"ssd\"\n    }\n\n  taints {\n        key = \"taintkey1\"\n        value = \"taintvalue1\"\n        effect = \"PreferNoSchedule\"\n    }\n\n  taints {\n        key = \"taintkey2\"\n        value = \"taintvalue2\"\n        effect = \"PreferNoSchedule\"\n    }\n}\n\n\n\n\nThe labels = { calico_ipv4pool } setting of the vkcs_kubernetes_cluster resource is used to redefine the pod subnet.\n\ndata \"vkcs_compute_flavor\" \"k8s-master-flavor\" {\n    name = \"STD3-4-8\"\n}\n\ndata \"vkcs_compute_flavor\" \"k8s-node-group-flavor\" {\n name = \"STD2-2-4\"\n}\n\ndata \"vkcs_kubernetes_clustertemplate\" \"k8s-template\" {\n    version = \"1.25\"\n}\n\nresource \"vkcs_kubernetes_cluster\" \"k8s-cluster\" {\n\n  depends_on = [\n    vkcs_networking_router_interface.k8s,\n  ]\n\n  name                = \"k8s-cluster-tf\"\n  cluster_template_id = data.vkcs_kubernetes_clustertemplate.k8s-template.id\n  master_flavor       = data.vkcs_compute_flavor.k8s-master-flavor.id\n  master_count        = 1\n  network_id          = vkcs_networking_network.k8s.id\n  subnet_id           = vkcs_networking_subnet.k8s.id\n  availability_zone   = \"GZ1\"\n\n  floating_ip_enabled = true\n\n  labels = {\n    calico_ipv4pool   = \"10.222.0.0/16\"\n  }\n\n}\n\nresource \"vkcs_kubernetes_node_group\" \"k8s-node-group\" {\n  name = \"k8s-node-group\"\n  cluster_id = vkcs_kubernetes_cluster.k8s-cluster.id\n  flavor_id = data.vkcs_compute_flavor.k8s-node-group-flavor.id\n\n  node_count = 2\n  autoscaling_enabled = false\n  min_nodes = 1\n  max_nodes = 3\n\n  labels {\n        key = \"env\"\n        value = \"test\"\n    }\n\n  labels {\n        key = \"disktype\"\n        value = \"ssd\"\n    }\n\n  taints {\n        key = \"taintkey1\"\n        value = \"taintvalue1\"\n        effect = \"PreferNoSchedule\"\n    }\n\n  taints {\n        key = \"taintkey2\"\n        value = \"taintvalue2\"\n        effect = \"PreferNoSchedule\"\n    }\n}\n\n\n\n\nThe labels = { docker_registry_enabled } setting of the vkcs_kubernetes_cluster resource is used to install the service.\n\ndata \"vkcs_compute_flavor\" \"k8s-master-flavor\" {\n    name = \"STD3-4-8\"\n}\n\ndata \"vkcs_compute_flavor\" \"k8s-node-group-flavor\" {\n name = \"STD2-2-4\"\n}\n\ndata \"vkcs_kubernetes_clustertemplate\" \"k8s-template\" {\n    version = \"1.25\"\n}\n\nresource \"vkcs_kubernetes_cluster\" \"k8s-cluster\" {\n\n  depends_on = [\n    vkcs_networking_router_interface.k8s,\n  ]\n\n  name                = \"k8s-cluster-tf\"\n  cluster_template_id = data.vkcs_kubernetes_clustertemplate.k8s-template.id\n  master_flavor       = data.vkcs_compute_flavor.k8s-master-flavor.id\n  master_count        = 1\n  network_id          = vkcs_networking_network.k8s.id\n  subnet_id           = vkcs_networking_subnet.k8s.id\n  availability_zone   = \"GZ1\"\n\n  floating_ip_enabled = true\n\n  labels = {\n    docker_registry_enabled = true\n  }\n\n}\n\nresource \"vkcs_kubernetes_node_group\" \"k8s-node-group\" {\n  name = \"k8s-node-group\"\n  cluster_id = vkcs_kubernetes_cluster.k8s-cluster.id\n  flavor_id = data.vkcs_compute_flavor.k8s-node-group-flavor.id\n\n  node_count = 2\n  autoscaling_enabled = false\n  min_nodes = 1\n  max_nodes = 3\n\n  labels {\n        key = \"env\"\n        value = \"test\"\n    }\n\n  labels {\n        key = \"disktype\"\n        value = \"ssd\"\n    }\n\n  taints {\n        key = \"taintkey1\"\n        value = \"taintvalue1\"\n        effect = \"PreferNoSchedule\"\n    }\n\n  taints {\n        key = \"taintkey2\"\n        value = \"taintvalue2\"\n        effect = \"PreferNoSchedule\"\n    }\n}\n\n3. Create the necessary resources using Terraform\n\nPlace the created Terraform configuration files provider.tf, network.tf and main.tf in one directory.\nGo to this directory.\nRun the command:\n\n      terraform init\n\nRun the command:\n\n      terraform apply\n\n   When prompted for confirmation, enter yes.\n\nWait for the operation to complete.\n\nDelete unused resources\n\nIf you no longer need the resources created with Terraform, delete them:\n\nSwitch to the directory that contains the Terraform configuration files.\n\nRun the command:\n\n      terraform destroy\n\n   When prompted for confirmation, enter yes.\n\nWait for the operation to complete.\n","url":"/en/tools-for-using-services/terraform/how-to-guides/k8s/create","description":"Example of creating a Kubernetes cluster using Terraform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"e7464702-ab4e-4c53-9288-9e3a8b4ef4d7","title":"Creating a JupyterHub instance","data":"This article provides examples of creating a JupyterHub instance using Terraform.\n\nWhen creating the instance the following was used:\n\nresource vkcs_mlplatform_jupyterhub\nresource vkcs_networking_network\ndata source vkcs_compute_flavor\ndata source vkcs_networking_network\n\nBefore creating an instance\n\nReview the available resources and quotas for the region where you want to create the instance. Different regions may have different quotas configured.\n\n   To increase your quotas, please contact technical support.\n\nInstall Terraform and configure the environment if it is not already done.\n\nTo go through this case, you need a provider version 0.6.0 or higher. Make sure that the provider version in the vkcs_provider.tf file is not lower. If the provider version is lower, update the provider.\n\n1. Create a JupyterHub instance manifest file\n\nIn the example, the instance is created with the following configuration:\n\ndefault region, availability zone — GZ1\nboot disk type — SSD\nusername — admin, password — Password!\ndata disk type — SSD, sizes — 60 and 70 GB\n\nCreate a Terraform configuration file named main.tf with the content:\n\nresource \"vkcs_mlplatform_jupyterhub\" \"jupyterhub\" {\n  name              = \"tf-example\"\n  admin_name        = \"admin\"\n  admin_password    = \"Password!\"\n  flavor_id         = data.vkcs_compute_flavor.basic.id\n  availability_zone = \"GZ1\"\n  boot_volume = {\n    volume_type = \"ceph-ssd\"\n  }\n  data_volumes = [\n    {\n      size        = 60\n      volume_type = \"ceph-ssd\"\n    },\n    {\n      size        = 70\n      volume_type = \"ceph-ssd\"\n    }\n  ]\n  networks = [\n    {\n      network_id = data.vkcs_networking_network.default.id\n    },\n  ]\n}\n\nHere:\n\nadmin_password — the JupyterHub instance administrator password.\n\n    Password requirements\n\n  Must contain at least 8 characters.\n  Must contain uppercase and lowercase Latin letters.\n  Must contain at least one digit and a special character ?, !, ~, @, #, $, %, ^, &, _, -, +, *, =, ;, :, ,, ., `, |, [, ], {, }, (, )`.\n\nflavor_id — a VM type ID. You can specify the ID in the manifest or get it from the data source.\n\n    Examples\n\n  flavor_id = data.vkcs_compute_flavor.basic.id: the ID is taken from the vkcs_compute_flavor data source, which will be generated further.\n  flavor_id = \"aee06bce-xxxx-xxxx-xxxx-ec4210cc6bac\": the ID is taken using OpenStack CLI.\n\n\nnetwork_id — an ID of the network where the instance will be hosted. The instance can be hosted on an existing network or a new one. You can specify the ID in the manifest or get it from the data source or resource.\n\n    Examples\n\n  network_id = vkcs_networking_network.default.id: the instance will be hosted on a new network, which will be created by the vkcs_networking_network resource. The resource will be generated further.\n  network_id = data.vkcs_networking_network.default.id: the instance will be hosted on an existing network. Its ID is taken from the vkcs_networking_network data source. The source will be generated further.\n  network_id = \"bb76507d-yyyy-yyyy-yyyy-2bca1a4c4cfc\": the instance will be hosted on an existing network. Its ID is taken from the list of networks in your VK Cloud personal account or via the Openstack CLI.\n\n\n2. (Optional) Create a file describing the data source for the VM type\n\nCreate a Terraform configuration file named flavor.tf to describe the VM type:\n\ndata \"vkcs_compute_flavor\" \"basic\" {\n  name = \"STD2-4-4\"\n}\n\nThe specified VM type will be used to create a JupyterHub instance in your Terraform project.\n\n3. (Optional) Create a file describing the network infrastructure for the instance\n\nCreate a Terraform configuration file named network.tf with the network infrastructure description:\n\n\n\nExisting network\nNew network\n\n\n\nExample data source for a network existing in your project:\n\ndata \"vkcs_networking_network\" \"default\" {\n  name = \"default\"\n  sdn = \"neutron\"\n}\n\nCreate networks\nresource \"vkcs_networking_network\" \"app\" {\n  name        = \"app-tf-example\"\n  description = \"Application network\"\n  sdn = \"neutron\"\n}\n\nresource \"vkcs_networking_subnet\" \"app\" {\n  name       = \"app-tf-example\"\n  network_id = vkcs_networking_network.app.id\n  cidr       = \"192.168.199.0/24\"\n}\n\nGet external network with Inernet access\ndata \"vkcs_networking_network\" \"extnet\" {\n  name = \"ext-net\"\n}\n\nCreate a router to connect netwoks\nresource \"vkcs_networking_router\" \"router\" {\n  name = \"router-tf-example\"\nConnect router to Internet\n  external_network_id = data.vkcs_networking_network.extnet.id\n}\n\nConnect networks to the router\nresource \"vkcs_networking_router_interface\" \"app\" {\n  router_id = vkcs_networking_router.router.id\n  subnet_id = vkcs_networking_subnet.app.id\n}\n\n4. Create the necessary resources using Terraform\n\nPut the Terraform configuration files in one directory:\n\n   vkcs_provider.tf\n   main.tf\n   flavor.tf (if created)\n   network.tf(if created)\n\nOpen this directory.\nMake sure that the configuration files are correct and contain the required changes:\n\n      terraform validate && terraform plan\n\nApply the changes:\n\n      terraform apply\n\n   Enter yes to confirm.\n\nWait for the operation to complete.\n\n5. Check configuration application\n\nVerify that the JupyterHub instance was successfully created:\n\nGo to your VK Cloud personal account.\nGo to ML Platform → Instances. Make sure your JupyterHub instance is created and active.\n\nDelete unused resources\n\nIf you no longer need the Terraform resources, delete them:\n\nOpen the directory that contains the Terraform configuration files.\nRun the command:\n\n      terraform destroy\n\n   Enter yes to confirm.\n\nWait for the operation to complete.\n","url":"/en/tools-for-using-services/terraform/how-to-guides/mlplatform/jupyterhub","description":"How to create a JupyterHub instance using Terraform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"5dfc3087-4fb6-4912-8c0e-7eac46234c7d","title":"Creating an MLflow instance","data":"This article provides examples of creating an MLflow instance using Terraform.\n\nWhen creating the instance the following was used:\n\nresource vkcs_mlplatform_mlflow\nresource vkcs_mlplatform_jupyterhub\nresource vkcs_networking_network\ndata source vkcs_compute_flavor\ndata source vkcs_networking_network\n\nBefore creating an instance\n\nReview the available resources and quotas for the region where you want to create the instance. Different regions may have different quotas configured.\n\n   To increase your quotas, please contact technical support.\n\nInstall Terraform and configure the environment if it is not already done.\n\nTo go through this case, you need a provider version 0.6.0 or higher. Make sure that the provider version in the vkcs_provider.tf file is not lower. If the provider version is lower, update the provider.\n\n1. Create an MLflow instance manifest file\n\nIn the example, the instance is created with the following configuration:\n\ndefault region, availability zone — GZ1\ndisk type — SSD, size — 50 GB\ndata disk type — SSD, size — 60 GB\n\nCreate a Terraform configuration file named main.tf with the content:\n\nresource \"vkcs_mlplatform_mlflow\" \"mlflow\" {\n  name              = \"tf-example\"\n  flavor_id         = data.vkcs_compute_flavor.basic.id\n  jh_instance_id    = vkcs_mlplatform_jupyterhub.jupyterhub.id\n  demo_mode         = true\n  availability_zone = \"GZ1\"\n  boot_volume = {\n    size        = 50\n    volume_type = \"ceph-ssd\"\n  }\n  data_volumes = [\n    {\n      size        = 60\n      volume_type = \"ceph-ssd\"\n    },\n  ]\n  networks = [\n    {\n      network_id = vkcs_networking_network.default.id\n    },\n  ]\n}\n\nHere:\n\nflavor_id — a VM type ID. You can specify the ID in the manifest or get it from the data source.\n\n    Examples\n\n  flavor_id = data.vkcs_compute_flavor.basic.id: the ID is taken from the vkcs_compute_flavor data source, which will be generated further.\n  flavor_id = \"aee06bce-xxxx-xxxx-xxxx-ec4210cc6bac\": the ID is taken using OpenStack CLI.\n\n\njh_instance_id — a JupyterHub instance ID for collaboration. You can use an existing instance or create a new one.\n\n    Examples\n\n  jh_instance_id = vkcs_mlplatform_jupyterhub.jupyterhub.id: a new JupyterHub instance will be created. The ID will be taken after creating the vkcs_mlplatform_jupyterhub resource. The resource will be generated further.\n  jh_instance_id = \"a57e9e91-yyyy-yyyy-yyyy-fedc7ac78c33\": the ID of an existing instance can be taken from JupyterHub instance details in your VK Cloud personal account.\n\n\ndemo_mode — use true to store all data on the instance VM, use false to connect an S3 bucket with a Postgres database for storing data.\n\nnetwork_id — an ID of the network where the instance will be hosted. The instance can be hosted on an existing network or a new one. You can specify the ID in the manifest or get it from the data source or resource.\n\n    Examples\n\n  network_id = vkcs_networking_network.default.id: the instance will be hosted on a new network, which will be created by the vkcs_networking_network resource. The resource will be generated further.\n  network_id = data.vkcs_networking_network.default.id: the instance will be hosted on an existing network. Its ID is taken from the vkcs_networking_network data source. The source will be generated further.\n  network_id = \"bb76507d-yyyy-yyyy-yyyy-2bca1a4c4cfc\": the instance will be hosted on an existing network. Its ID is taken from the list of networks in your VK Cloud personal account or via the Openstack CLI.\n\n\n2. (Optional) Create a file describing the data source for the VM type\n\nCreate a Terraform configuration file named flavor.tf to describe the VM type:\n\ndata \"vkcs_compute_flavor\" \"basic\" {\n  name = \"STD2-4-4\"\n}\n\nThe specified VM type will be used to create an MLflow instance in your Terraform project.\n\n3. (Optional) Create a file describing the synchronized JupyterHub instance\n\nCreate a Terraform configuration file named jh_instance.tf to describe the JupyterHub instance.\n\n4. (Optional) Create a file describing the network infrastructure for the instance\n\nCreate a Terraform configuration file named network.tf with the network infrastructure description:\n\n\n\nExisting network\nNew network\n\n\n\nThe data source example for a network existing in a project:\n\ndata \"vkcs_networking_network\" \"default\" {\n  name = \"default\"\n  sdn = \"neutron\"\n}\n\nCreate networks\nresource \"vkcs_networking_network\" \"app\" {\n  name        = \"app-tf-example\"\n  description = \"Application network\"\n  sdn = \"neutron\"\n}\n\nresource \"vkcs_networking_subnet\" \"app\" {\n  name       = \"app-tf-example\"\n  network_id = vkcs_networking_network.app.id\n  cidr       = \"192.168.199.0/24\"\n}\n\nGet external network with Inernet access\ndata \"vkcs_networking_network\" \"extnet\" {\n  name = \"ext-net\"\n}\n\nCreate a router to connect netwoks\nresource \"vkcs_networking_router\" \"router\" {\n  name = \"router-tf-example\"\nConnect router to Internet\n  external_network_id = data.vkcs_networking_network.extnet.id\n}\n\nConnect networks to the router\nresource \"vkcs_networking_router_interface\" \"app\" {\n  router_id = vkcs_networking_router.router.id\n  subnet_id = vkcs_networking_subnet.app.id\n}\n\n5. Create the necessary resources using Terraform\n\nPut the Terraform configuration files in one directory:\n\n   vkcs_provider.tf\n   main.tf\n   flavor.tf (if created)\n   jh_instance.tf(if created)\n   network.tf(if created)\n\nOpen this directory.\nMake sure that the configuration files are correct and contain the required changes:\n\n      terraform validate && terraform plan\n\nApply the changes:\n\n      terraform apply\n\n   Enter yes to confirm.\n\nWait for the operation to complete.\n\n6. Check configuration application\n\nVerify that the MLflow instance was successfully created:\n\nGo to your VK Cloud personal account.\nGo to ML Platform → Instances. Make sure your MLflow instance is created and active.\n\nDelete unused resources\n\nIf you no longer need the Terraform resources, delete them:\n\nOpen the directory that contains the Terraform configuration files.\nRun the command:\n\n      terraform destroy\n\n   Enter yes to confirm.\n\nWait for the operation to complete.\n","url":"/en/tools-for-using-services/terraform/how-to-guides/mlplatform/mlflow","description":"How to create an MLflow instance using Terraform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"465263b5-7ca8-4be1-8897-e3e22f48ad32","title":"Creating an MLflow Deploy instance","data":"This article provides examples of creating an MLflow deploy-instance using Terraform.\n\nWhen creating the instance the following was used:\n\nresource vkcs_mlplatform_mlflow_deploy\nresource vkcs_mlplatform_mlflow\nresource vkcs_networking_network\ndata source vkcs_compute_flavor\ndata source vkcs_networking_network\n\nBefore creating an instance\n\nReview the available resources and quotas for the region where you want to create the instance. Different regions may have different quotas configured.\n\n   To increase your quotas, please contact technical support.\n\nInstall Terraform and configure the environment if it is not already done.\n\nTo go through this case, you need a provider version 0.6.0 or higher. Make sure that the provider version in the vkcs_provider.tf file is not lower. If the provider version is lower, update the provider.\n\n1. Create an MLflow Deploy instance manifest file\n\nIn the example, the instance is created with the following configuration:\n\ndefault region, availability zone — GZ1\nboot disk type — SSD, size — 50 GB\ndata disk type — SSD, size — 60 GB\n\nCreate a Terraform configuration file named main.tf with the content:\n\nresource \"vkcs_mlplatform_mlflow_deploy\" \"deploy\" {\n  name               = \"tf-example\"\n  flavor_id          = data.vkcs_compute_flavor.basic.id\n  mlflow_instance_id = vkcs_mlplatform_mlflow.mlflow.id\n  availability_zone  = \"GZ1\"\n  boot_volume = {\n    size        = 50\n    volume_type = \"ceph-ssd\"\n  }\n\n  data_volumes = [\n    {\n      size        = 60\n      volume_type = \"ceph-ssd\"\n    },\n  ]\n  networks = [\n    {\n      network_id = vkcs_networking_network.app.id\n    },\n  ]\n}\n\nHere:\n\nflavor_id — a VM type ID. You can specify the ID in the manifest or get it from the data source.\n\n    Examples\n\n  flavor_id = data.vkcs_compute_flavor.basic.id: the ID is taken from the vkcs_compute_flavor data source, which will be generated further.\n  flavor_id = \"aee06bce-xxxx-xxxx-xxxx-ec4210cc6bac\": the ID is taken using OpenStack CLI.\n\n\nmlflow_instance_id — an MLflow instance ID for collaboration. You can use an existing instance or create a new one.\n\n    Examples\n\n  mlflow_instance_id = vkcs_mlplatform_mlflow.mlflow.id: a new MLflow instance will be created. The ID will be taken after creating the vkcs_mlplatform_mlflow resource. The resource will be generated further.\n  mlflow_instance_id = \"a57e9e91-yyyy-yyyy-yyyy-fedc7ac78c33\": the ID of an existing instance can be taken from MLflow instance details in your VK Cloud personal account.\n\n\nnetwork_id — an ID of the network where the instance will be hosted. The instance can be hosted on an existing network or a new one. You can specify the ID in the manifest or get it from the data source or resource.\n\n    Examples\n\n  network_id = vkcs_networking_network.default.id: the instance will be hosted on a new network, which will be created by the vkcs_networking_network resource. The resource will be generated further.\n  network_id = data.vkcs_networking_network.default.id: the instance will be hosted on an existing network. Its ID is taken from the vkcs_networking_network data source. The source will be generated further.\n  network_id = \"bb76507d-yyyy-yyyy-yyyy-2bca1a4c4cfc\": the instance will be hosted on an existing network. Its ID is taken from the list of networks in your VK Cloud personal account or via the Openstack CLI.\n\n\n2. (Optional) Create a file describing the data source for the VM type\n\nCreate a Terraform configuration file named flavor.tf to describe the VM type:\n\ndata \"vkcs_compute_flavor\" \"basic\" {\n  name = \"STD2-4-4\"\n}\n\nThe specified VM type will be used to create an MLflow Deploy instance in your Terraform project.\n\n3. (Optional) Create a file describing the synchronized MLflow instance\n\nCreate a Terraform configuration file named mlflow_instance.tf to describe the MLflow instance.\n\n4. (Optional) Create a file describing the network infrastructure for the instance\n\nCreate a Terraform configuration file named network.tf with the network infrastructure description:\n\n\n\nExisting network\nNew network\n\n\n\nThe data source example for a network existing in a project:\n\ndata \"vkcs_networking_network\" \"default\" {\n  name = \"default\"\n  sdn = \"neutron\"\n}\n\nCreate networks\nresource \"vkcs_networking_network\" \"app\" {\n  name        = \"app-tf-example\"\n  description = \"Application network\"\n  sdn = \"neutron\"\n}\n\nresource \"vkcs_networking_subnet\" \"app\" {\n  name       = \"app-tf-example\"\n  network_id = vkcs_networking_network.app.id\n  cidr       = \"192.168.199.0/24\"\n}\n\nGet external network with Inernet access\ndata \"vkcs_networking_network\" \"extnet\" {\n  name = \"ext-net\"\n}\n\nCreate a router to connect netwoks\nresource \"vkcs_networking_router\" \"router\" {\n  name = \"router-tf-example\"\nConnect router to Internet\n  external_network_id = data.vkcs_networking_network.extnet.id\n}\n\nConnect networks to the router\nresource \"vkcs_networking_router_interface\" \"app\" {\n  router_id = vkcs_networking_router.router.id\n  subnet_id = vkcs_networking_subnet.app.id\n}\n\n5. Create the necessary resources using Terraform\n\nPut the Terraform configuration files in one directory:\n\n   vkcs_provider.tf\n   main.tf\n   flavor.tf (if created)\n   mlflow_instance.tf(if created)\n   jh_instance.tf(if created)\n   network.tf(if created)\n\nOpen this directory.\nMake sure that the configuration files are correct and contain the required changes:\n\n      terraform validate && terraform plan\n\nApply the changes:\n\n      terraform apply\n\n   Enter yes to confirm.\n\nWait for the operation to complete.\n\n6. Check configuration application\n\nVerify that the MLflow Deploy instance was successfully created:\n\nGo to your VK Cloud personal account.\nGo to ML Platform → Instances. Make sure your MLflow Deploy instance is created and active.\n\nDelete unused resources\n\nIf you no longer need the Terraform resources, delete them:\n\nOpen the directory that contains the Terraform configuration files.\nRun the command:\n\n      terraform destroy\n\n   Enter yes to confirm.\n\nWait for the operation to complete.\n","url":"/en/tools-for-using-services/terraform/how-to-guides/mlplatform/deploymlflow","description":"How to create an MLflow Deploy instance using Terraform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"0705e6dd-fb0b-4bee-ac41-0da12b11f7ab","title":"Creating networks and changing security groups","data":"\n\nMake sure you installed and configured Terraform.\n\n\n\nTo create a network or security group, create a network.tf file, which will describe the configuration of the created entities. Add the text from the example below and correct the settings for your networks and security groups.\n\nTo create a network and security groups, you need the following objects:\n\n    Resources:\n\n       vkcs_networking_network: a network to which changes will be made.\n       vkcs_networking_subnet: a subnet from the network. In the example: subnetwork.\n       vkcs_networking_router: a router for an external network and interaction with the outside world. In the example: router.\n       vkcs_networking_router_interface: connect the router to the internal network.\n       vkcs_networking_secgroup: a security group to which access rules will be added.\n       vkcs_networking_secgroup_rule: a rule for the security group. In the example, an access is opened to the network from any IP address on ports 22 and 3389.\n       vkcs_networking_port: create a network port resource inside VK Cloud.\n       vkcs_networking_port_secgroup_associate: bind a port to the security group.\n\n    Data sources:\n\n       vkcs_networking_network: an external network for obtaining a floating IP address.\n\n        data \"vkcs_networking_network\" \"extnet\" {\n       name=\"internet\"\n    }\n\n    resource \"vkcs_networking_network\" \"network\" {\n       name=\"net\"\n    }\n\n    resource \"vkcs_networking_subnet\" \"subnetwork\" {\n       name=\"subnet_1\"\n       network_id = vkcs_networking_network.network.id\n       cidr=\"192.168.199.0/24\"\n    }\n\n    resource \"vkcs_networking_router\" \"router\" {\n       name=\"router\"\n       admin_state_up = true\n       external_network_id = data.vkcs_networking_network.extnet.id\n    }\n\n    resource \"vkcs_networking_router_interface\" \"db\" {\n       router_id = vkcs_networking_router.router.id\n       subnet_id = vkcs_networking_subnet.subnetwork.id\n    }\n\n\n    resource \"vkcs_networking_secgroup\" \"secgroup\" {\n       name=\"security_group\"\n       description = \"terraform security group\"\n    }\n\n    resource \"vkcs_networking_secgroup_rule\" \"secgroup_rule_1\" {\n       direction = \"ingress\"\n       ethertype=\"IPv4\"\n       port_range_max = 22\n       port_range_min = 22\n       protocol=\"tcp\"\n       remote_ip_prefix = \"0.0.0.0/0\"\n       security_group_id = vkcs_networking_secgroup.secgroup.id\n       description = \"secgroup_rule_1\"\n    }\n\n    resource \"vkcs_networking_secgroup_rule\" \"secgroup_rule_2\" {\n       direction = \"ingress\"\n       ethertype=\"IPv4\"\n       port_range_max = 3389\n       port_range_min = 3389\n       remote_ip_prefix = \"0.0.0.0/0\"\n       protocol=\"tcp\"\n       security_group_id = vkcs_networking_secgroup.secgroup.id\n    }\n\n    resource \"vkcs_networking_port\" \"port\" {\n       name=\"port_1\"\n       admin_state_up = \"true\"\n       network_id = vkcs_networking_network.network.id\n\n       fixed_ip {\n       subnet_id = vkcs_networking_subnet.subnetwork.id\n       ip_address = \"192.168.199.23\"\n       }\n    }\n\n    resource \"vkcs_networking_port_secgroup_associate\" \"port\" {\n       port_id = vkcs_networking_port.port.id\n       enforce = \"false\"\n       security_group_ids = [\n       vkcs_networking_secgroup.secgroup.id,\n       ]\n    }\n\nAdd the example to the network.tf file and run the following commands:\n\n      terraform init\n         terraform apply\n","url":"/en/tools-for-using-services/terraform/how-to-guides/vnet/network","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"f84c78b0-333d-403b-8998-8f5cd404ce9d","title":"Creating a VPN connection","data":"\n\nMake sure you installed and configured Terraform.\n\n\n\nTo create a VPN connection, create a file vpn.tf, which describes the configuration of the connection to be created. Add the text from the examples below and correct the arguments values for your connection.\n\nCreate a virtual network\n\nTo create a VPN connection, you need a virtual network with a router. If you already have a network and a router, skip this section and go to Creating a VPN connection.\n\nCreate a network with the following objects:\n\nResources:\n\n  vkcs_networking_network: a network where a VM will be created. In the example below, the network named extnet is created.\n  vkcs_networking_subnet: a subnet from the network. In the example: subnet.\n  vkcs_networking_router: a router for an external network and interaction with the outside world. In the example: router.\n  vkcs_networking_router_interface: connects the router to the internal network.\n\nData sources:\n\n  vkcs_networking_network: an external network for obtaining a floating IP address.\n\ndata \"vkcs_networking_network\" \"extnet\" {\n   name=\"internet\"\n}\n\nresource \"vkcs_networking_network\" \"network\" {\n   name=\"vpnaas_network\"\n}\n\nresource \"vkcs_networking_subnet\" \"subnet\" {\n  network_id = \"${vkcs_networking_network.network.id}\"\n  cidr=\"192.168.199.0/24\"\n}\n\nresource \"vkcs_networking_router\" \"router\" {\n   name=\"router\"\n   external_network_id = data.vkcs_networking_network.extnet.id\n}\n\nresource \"vkcs_networking_router_interface\" \"router_interface\" {\n  router_id = \"${vkcs_networking_router.router.id}\"\n  subnet_id = \"${vkcs_networking_subnet.subnet.id}\"\n}\n\nCreate a VPN connection\n\nvkcs_vpnaas_service: manages the VPN service inside VK Cloud. The resource supports a router_id argument, which is the router ID. Changing the value of this argument creates a new service. If you need to use an existing router, specify its ID (data.vkcs_networking_router.router.id) using the data source:\n\ndata \"vkcs_networking_router\" \"router\" {\n   name=\"router_1\"\n}\n\nvkcs_vpnaas_ipsec_policy: controls the IPSec policy of the resource inside VK Cloud. The resource supports a name argument, which is the name of the policy to be created. Changing the value of this argument changes the name of the existing policy.\n\nvkcs_vpnaas_ike_policy: controls the IKE policy of the resource inside VK Cloud. The resource supports a name argument, which is the name of the policy to be created. Changing the value of this argument changes the name of the existing policy.\n\nvkcs_vpnaas_endpoint_group: manages the \"endpoint group\" resource inside VK Cloud. The following arguments are supported:\n\n  type: the type of endpoints in the group. The following types are acceptable: subnet, cidr, network, router, or vlan. Changing the value of this argument creates a new group.\n  endpoints: the list of endpoints of the same type included in the endpoint group. The type of the list items is determined by the type argument. Changing the value of the endpoints argument creates a new group.\n\nvkcs_vpnaas_site_connection: manages the site IPSec connection resource inside VK Cloud. The following arguments are supported:\n\n  name: the connection name. Changing the value of this argument changes the name of the existing connection.\n  ikepolicy_id: the ID of the IKE policy. Changing the value of this argument creates a new connection.\n  ipsecpolicy_id: the ID of the IPsec policy. Changing the value of this argument creates a new connection.\n  vpnservice_id: the VPN service ID. Changing the value of this argument creates a new connection.\n  psk: the public key. Any value of type string is acceptable.\n  peer_address: the public IPv4 or IPv6 address of the peer gateway, or FQDN.\n  peer_id: the peer router ID for authentication. The acceptable type values are IPv4 address, IPv6 address, e-mail, key ID, FQDN. Typically, the value of this argument is the same as the value of the peer_address argument. Changing the value of the peer_id argument changes the policy of the existing connection.\n  local_ep_group_id: the ID of the endpoint group, which includes the private subnets of the local connection. Requires the peer_ep_group_id argument to be specified unless backward compatibility mode is enabled, where the peer_cidrs values are already provided with the subnet_id value of the VPN service. Changing the value of this paraargumentmeter changes the existing connection.\n  peer_ep_group_id: the ID of the endpoint group, which includes the private CIDR addresses of the peer connection in the format /. Requires local_ep_group_id to be specified unless backward compatibility mode is enabled, where the peer_cidrs values are already provided with the subnet_id value of the VPN service.\n  dpd: the settings dictionary for the Dead Peer Detection (DPD) protocol. The following arguments are supported:\n\n    action: the DPD action. Possible values: clear, hold, restart, disabled, restart-by-peer. Default value: hold.\n    timeout: the DPD time-out in seconds. Must be a positive integer, which is greater than the interval argument value. Default value: 120.\n    interval: the DPD interval in seconds. Must be a positive integer. Default value: 30.\n\n  depends_on: the VPN connection will start after creating the specified resources.\n\nresource \"vkcs_vpnaas_service\" \"service\" {\n   router_id = \"${vkcs_networking_router.router.id}\"\n}\n\nresource \"vkcs_vpnaas_ipsec_policy\" \"policy_1\" {\n   name = \"ipsec-policy\"\n}\n\nresource \"vkcs_vpnaas_ike_policy\" \"policy_2\" {\n   name = \"ike-policy\"\n}\n\nresource \"vkcs_vpnaas_endpoint_group\" \"group_1\" {\n\ttype = \"cidr\"\n\tendpoints = [\"10.0.0.24/24\", \"10.0.0.25/24\"]\n}\nresource \"vkcs_vpnaas_endpoint_group\" \"group_2\" {\n\ttype = \"subnet\"\n\tendpoints = [ \"${vkcs_networking_subnet.subnet.id}\" ]\n}\n\nresource \"vkcs_vpnaas_site_connection\" \"connection\" {\n\tname = \"connection\"\n\tikepolicy_id = \"${vkcs_vpnaas_ike_policy.policy_2.id}\"\n\tipsecpolicy_id = \"${vkcs_vpnaas_ipsec_policy.policy_1.id}\"\n\tvpnservice_id = \"${vkcs_vpnaas_service.service.id}\"\n\tpsk = \"secret\"\n\tpeer_address = \"192.168.10.1\"\n\tpeer_id = \"192.168.10.1\"\n\tlocal_ep_group_id = \"${vkcs_vpnaas_endpoint_group.group_2.id}\"\n\tpeer_ep_group_id = \"${vkcs_vpnaas_endpoint_group.group_1.id}\"\n\tdpd {\n\t\taction   = \"restart\"\n\t\ttimeout  = 42\n\t\tinterval = 21\n\t}\n\tdepends_on = [\"vkcs_networking_router_interface.router_interface\"]\n}\n\nAdd both parts of the example to the vpn.tf file and run the following commands:\n\nterraform init\nterraform apply\n","url":"/en/tools-for-using-services/terraform/how-to-guides/vnet/vpn","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"39e5018f-e040-49ad-af48-0adc3c4e20a2","title":"Creating and configuring load balancers","data":"\n\nMake sure you installed and configured Terraform.\n\n\n\nTo create a load balancer, create a lb.tf file that describes the configuration of the load balancer to be created. Add the text from the example below and correct the arguments values for your load balancer. The example describes the load balancer setup for two VMs, the traffic on which will be distributed by the created load balancer using the ROUND_ROBIN method.\n\nCreate a network\n\nTo create a network for a load balancer, you need the following objects:\n\nResources:\n\n  vkcs_networking_network: a network where a VM will be created. In the example below, a network named lb is created.\n  vkcs_networking_subnet: a subnet from the network. In the example: lb.\n\nresource \"vkcs_networking_network\" \"lb\" {\n   name=\"network\"\n}\n\nresource \"vkcs_networking_subnet\" \"lb\" {\n   name=\"subnet\"\n   cidr=\"192.168.199.0/24\"\n   network_id = \"${vkcs_networking_network.lb.id}\"\n}\n\nCreate a load balancer\n\nTo create a load balancer, you need the following objects:\n\nData sources:\n\n  vkcs_images_image: an installation image for the instance to be created.\n  vkcs_compute_flavor: a flavor (CPU, RAM, Disk) of the VM. You can see it in the VM creation wizard in your VK Cloud personal account.\n\nResources:\n\n  vkcs_compute_instance: manages the resource of the virtual machine instance. Traffic on these VMs will be distributed by the load balancer to be created.\n\n\n     All arguments, including the instance administrator password, will be stored in their original state in plain text. Learn more about sensitive data.\n\n\n     The resource supports the following arguments:\n\n    name: the VM name.\n    flavor_id: the VM flavor used during its creation.\n    security_groups: the list of security group names assigned to this VM.\n    image_id: the OS image used to create this VM.\n    network: the network connected when creating this VM.\n    depends_on: this VM will start after creating the resources specified by this argument.\n\n  vkcs_lb_loadbalancer: manages the load balancer resource in VK Cloud. The following arguments are supported:\n\n    name: the human readable name for the load balancer. Doesn't have to be unique.\n    vip_subnet_id: the subnet where the load balancer address will be allocated. You can only create load balancers on networks that are allowed by the policy (for example, networks that are owned by you or networks that are public). Changing this argument creates a new load balancer.\n    availability_zone: availability_zone, where the balancer will be placed. To optimize and speed up the balancer connection to VMs, place them in the same zone. You can get the list of availability zones in your VK Cloud personal account or via OpenStack CLI.\n    tags: the list of simple strings assigned to the load balancer.\n\n  vkcs_lb_listener: manages the listener resource in VK Cloud. The following arguments are supported:\n\n    name: the human readable name for the listener. Doesn't have to be unique.\n    protocol: the protocol of the listener, it can be TCP, HTTP, HTTPS, TERMINATED_HTTPS, UDP. Changing this argument creates a new listener.\n    protocol_port: the port on which client traffic is listened. Changing this argument creates a new listener.\n    loadbalancer_id: the load balancer on which this listener should be provided. Changing this argument creates a new listener.\n\n  vkcs_lb_pool: manages the pool resource in VK Cloud. The following arguments are supported:\n\n    name: the human-readable name of the pool.\n    protocol: the protocol of the pool. Can be one of the following:TCP, HTTP, HTTPS, PROXY or UDP. Changing this argument creates a new pool.\n    lb_method: the load balancing algorithm for distributing traffic between pool members. It must be one of the following: ROUND_ROBIN, LEAST_CONNECTIONS, SOURCE_IP, or SOURCE_IP_PORT.\n    listener_id: the listener to which the pool members will be associated. Changing this argument creates a new pool. Note: You must specify either loadbalancer_id or listener_id.\n\n  vkcs_lb_member: manages the member resource in VK Cloud. The following arguments are supported:\n\n    address: the IP address of the member to which it will receive traffic from the load balancer. Changing this argument creates a new member.\n    protocol_port: the port on which client traffic is listened. Changing this argument creates a new member.\n    pool_id: the ID of the pool this member will be assigned to. Changing this argument creates a new member.\n    subnet_id: the subnet where the member can be accessed. Changing this argument creates a new member.\n    weight: a positive integer value indicating the relative portion of the traffic that this member must receive from the pool. For example, a member with the weight of 10 gets five times more traffic than a member with the weight of 2. The default value is 1.\n\n\ndata \"vkcs_images_image\" \"compute\" {\n    name=\"Ubuntu-18.04-STD2\"\n}\n\ndata \"vkcs_compute_flavor\" \"compute\" {\n   name=\"STD2-2-4\"\n}\n\nresource \"vkcs_compute_instance\" \"compute_1\" {\n   name=\"compute-instance-1\"\n   flavor_id = data.vkcs_compute_flavor.compute.id\n   security_groups = [\"default\"]\n   image_id = data.vkcs_images_image.compute.id\n\n   network {\n     uuid = vkcs_networking_network.lb.id\n     fixed_ip_v4 = \"192.168.199.110\"\n   }\n\n   depends_on = [\n     vkcs_networking_network.lb,\n     vkcs_networking_subnet.lb\n   ]\n}\n\nresource \"vkcs_compute_instance\" \"compute_2\" {\n   name=\"compute-instance-2\"\n   flavor_id = data.vkcs_compute_flavor.compute.id\n   security_groups = [\"default\"]\n   image_id = data.vkcs_images_image.compute.id\n\n   network {\n     uuid = vkcs_networking_network.lb.id\n     fixed_ip_v4 = \"192.168.199.111\"\n   }\n\n   depends_on = [\n     vkcs_networking_network.lb,\n     vkcs_networking_subnet.lb\n   ]\n}\n\nresource \"vkcs_lb_loadbalancer\" \"loadbalancer\" {\n   name=\"loadbalancer\"\n   vip_subnet_id = \"${vkcs_networking_subnet.lb.id}\"\n   availability_zone = \"GZ1\"\n   tags = [\"tag1\"]\n}\n\nresource \"vkcs_lb_listener\" \"listener\" {\n   name=\"listener\"\n   protocol=\"HTTP\"\n   protocol_port = 8080\n   loadbalancer_id = \"${vkcs_lb_loadbalancer.loadbalancer.id}\"\n}\n\nresource \"vkcs_lb_pool\" \"pool\" {\n   name=\"pool\"\n   protocol=\"HTTP\"\n   lb_method = \"ROUND_ROBIN\"\n   listener_id = \"${vkcs_lb_listener.listener.id}\"\n}\n\nresource \"vkcs_lb_member\" \"member_1\" {\n   address = \"192.168.199.110\"\n   protocol_port = 8080\n   pool_id = \"${vkcs_lb_pool.pool.id}\"\n   subnet_id = \"${vkcs_networking_subnet.lb.id}\"\n   weight = 0\n}\n\nresource \"vkcs_lb_member\" \"member_2\" {\n   address = \"192.168.199.111\"\n   protocol_port = 8080\n   pool_id = \"${vkcs_lb_pool.pool.id}\"\n   subnet_id = \"${vkcs_networking_subnet.lb.id}\"\n}\n\nApply changes\n\nAdd both parts of the example to the lb.tf file and run the following commands:\n\nterraform init\nterraform apply\n","url":"/en/tools-for-using-services/terraform/how-to-guides/vnet/lb","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"ae7b13c4-c7a3-4b36-a5ec-66b889d19769","title":"Routing traffic between networks","data":"The following is an example of configuring routing between two networks using Terraform.\n\nThe infrastructure of the example:\n\nIn the example, two networks are created: private common-private and public common-public ones. Each network consists of one subnet.\nRouters with interfaces in the corresponding subnets are created for each network.\nThe public network router is configured to access the external network. Therefore, floating IP addresses can be assigned to objects in this network.\nEach network has a port that corresponds to its only subnet. A common-router virtual machine is connected to these ports, acting as a router between the two networks.\nThe routers have the configured static routes that lead to another subnet through the virtual machine port.\nA virtual machine is created in each subnet. The virtual machine on the public network has a floating IP address.\n\n  These machines are used to test the routing settings between networks: a successful ping between them will indicate the correct configuration.\n\nExample Infrastructure Scheme\n\nBefore you start\n\nCheck out the available resources and quotas for the region in which you plan to create a cluster. Different quotas can be set up for different regions.\n\n   If you want to increase quotas, contact technical support.\n\nMake sure that OpenStack client is installed and authenticate to the project.\n\nInstall Terraform and configure the environment if it is not already done.\n\n   Put the provider settings in the Terraform configuration file provider.tf.\n\nCreate a Terraform configuration file variables.tf with variables:\n\n      variable \"image_flavor\" {\n     type = string\n     default = \"Ubuntu-22.04-202208\"\n   }\n\n   variable \"compute_flavor\" {\n     type = string\n     default = \"STD2-2-4\"\n   }\n\n   variable \"key_pair_name\" {\n     type = string\n     default = \"default\"\n   }\n\n   variable \"availability_zone_name\" {\n     type = string\n     default = \"MS1\"\n   }\n\n   The following variables are declared in this file:\n\n   image_flavor: the name of the VM image;\n   compute_flavor: the name of the virtual machine configuration template;\n   key_pair_name: the name of the key pair that will be used to connect to the virtual machine via SSH;\n   availability_zone_name: the name of the availability zone where the VM will be hosted.\n\n   If necessary, adjust the values of the variables. First find out their acceptable values:\n\n   image_flavor\n   compute_flavor\n   key_pair_name\n   availability_zone_name\n\n\n   Using OpenStack CLI:\n\n      openstack image list\n\n\n   Using CLI:\n\n      openstack flavor list\n\n\n   Using CLI:\n\n      openstack keypair list\n\n\n   From the document on availability zones.\n\n\n1. Create files describing the basic network infrastructure\n\nCreate a Terraform configuration file common-public.tf. It describes:\n\n   a public network and subnet;\n   a router with access to an external internet network and an interface in a public subnet.\n\n      data \"vkcs_networking_network\" \"extnet\" {\n     name = \"internet\"\n   }\n\n   resource \"vkcs_networking_network\" \"common-public\" {\n     name = \"common-public-net\"\n   }\n\n   resource \"vkcs_networking_subnet\" \"common-public\" {\n     name            = \"common-public-subnet\"\n     network_id      = vkcs_networking_network.common-public.id\n     cidr            = \"192.168.200.0/24\"\n     dns_nameservers = [\"8.8.8.8\", \"8.8.4.4\"]\n   }\n\n   resource \"vkcs_networking_router\" \"common-public\" {\n     name                = \"common-public-router\"\n     admin_state_up      = true\n     external_network_id = data.vkcs_networking_network.extnet.id\n   }\n\n   resource \"vkcs_networking_router_interface\" \"common-public\" {\n     router_id = vkcs_networking_router.common-public.id\n     subnet_id = vkcs_networking_subnet.common-public.id\n   }\n\nCreate a Terraform configuration file common-private.tf. It describes:\n\n   a public network and subnet;\n   a router with an interface in a private subnet.\n\n      resource \"vkcs_networking_network\" \"common-private\" {\n     name = \"common-private-net\"\n   }\n\n   resource \"vkcs_networking_subnet\" \"common-private\" {\n     name            = \"common-private-subnet\"\n     network_id      = vkcs_networking_network.common-private.id\n     cidr            = \"192.168.199.0/24\"\n     dns_nameservers = [\"8.8.8.8\", \"8.8.4.4\"]\n   }\n\n   resource \"vkcs_networking_router\" \"common-private\" {\n     name           = \"common-private-router\"\n     admin_state_up = true\n   }\n\n   resource \"vkcs_networking_router_interface\" \"common-private\" {\n     router_id = vkcs_networking_router.common-private.id\n     subnet_id = vkcs_networking_subnet.common-private.id\n   }\n\n2. Create a file describing the infrastructure for the virtual machine as a router\n\nCreate a Terraform configuration file main.tf. It describes:\n\nPorts in the public and private subnets that the VM connects to.\nConfiguration of the virtual machine.\n\n  The configuration uses the user_data argument to run a command that enables routing using cloud-init:\n\n    sysctl -w net.ipv4.ip_forward=1\n\nStatic routes to the required networks via the ports of the VM.\nFloating IP address for connecting to the virtual machine via SSH.\n\ndata \"vkcs_compute_flavor\" \"router-compute\" {\n  name = var.compute_flavor\n}\n\ndata \"vkcs_images_image\" \"router-image\" {\n  name = var.image_flavor\n}\n\nresource \"vkcs_networking_port\" \"common-private\" {\n  name       = \"common-private\"\n  network_id = vkcs_networking_network.common-private.id\n  allowed_address_pairs {\n    ip_address = vkcs_networking_subnet.common-public.cidr\n  }\n  admin_state_up = \"true\"\n}\n\nresource \"vkcs_networking_port\" \"common-public\" {\n  name       = \"common-public\"\n  network_id = vkcs_networking_network.common-public.id\n  allowed_address_pairs {\n    ip_address = vkcs_networking_subnet.common-private.cidr\n  }\n  admin_state_up = \"true\"\n}\n\nresource \"vkcs_compute_instance\" \"common-router\" {\n  name              = \"common-router\"\n  flavor_id         = data.vkcs_compute_flavor.router-compute.id\n  security_groups   = [\"default\", \"ssh\"]\n  availability_zone = var.availability_zone_name\n  key_pair          = var.key_pair_name\n  config_drive      = true\n\n  user_data = <<EOF\n#cloud-config\nruncmd:\nsysctl -w net.ipv4.ip_forward=1\nEOF\n\n  block_device {\n    uuid                  = data.vkcs_images_image.router-image.id\n    source_type           = \"image\"\n    volume_size           = 10\n    volume_type           = \"ceph-ssd\"\n    boot_index            = 0\n    destination_type      = \"volume\"\n    delete_on_termination = true\n  }\n\n  network {\n    port = vkcs_networking_port.common-public.id\n  }\n\n  network {\n    port = vkcs_networking_port.common-private.id\n  }\n\n  depends_on = [\n    vkcs_networking_network.common-private,\n    vkcs_networking_subnet.common-private,\n    vkcs_networking_network.common-public,\n    vkcs_networking_subnet.common-public\n  ]\n}\n\nresource \"vkcs_networking_router_route\" \"common-public\" {\n  router_id        = vkcs_networking_router.common-public.id\n  destination_cidr = vkcs_networking_subnet.common-private.cidr\n  next_hop         = vkcs_networking_port.common-public.all_fixed_ips[0]\n  depends_on = [\n    vkcs_networking_router_interface.common-public,\n    vkcs_networking_port.common-public,\n    vkcs_compute_instance.common-router\n  ]\n}\n\nresource \"vkcs_networking_router_route\" \"common-private\" {\n  router_id        = vkcs_networking_router.common-private.id\n  destination_cidr = vkcs_networking_subnet.common-public.cidr\n  next_hop         = vkcs_networking_port.common-private.all_fixed_ips[0]\n  depends_on = [\n    vkcs_networking_router_interface.common-private,\n    vkcs_networking_port.common-private,\n    vkcs_compute_instance.common-router\n  ]\n}\n\nresource \"vkcs_networking_floatingip\" \"fip-router\" {\n  pool = data.vkcs_networking_network.extnet.name\n}\n\nresource \"vkcs_compute_floatingip_associate\" \"fip-router\" {\n  floating_ip = vkcs_networking_floatingip.fip-router.address\n  instance_id = vkcs_compute_instance.common-router.id\n}\n\n3. Create a file with a description of the test VMs\n\nCreate a Terraform configuration file test-vms.tf. It describes:\n\nVirtual machines:\n\n  common-instance-public in a public subnet with a floating address. You can connect to such a virtual machine via SSH from the Internet.\n  common-instance-private in a private subnet. You can connect to such a virtual machine via SSH from another virtual machine.\n\n  These virtual machines will be used for verifying the health of routing between two networks.\n\nTerraform (output) with VMs IP addresses.\n\ndata \"vkcs_compute_flavor\" \"vm-compute\" {\n  name = var.compute_flavor\n}\n\ndata \"vkcs_images_image\" \"vm-image\" {\n  name = var.image_flavor\n}\n\nresource \"vkcs_compute_instance\" \"common-private\" {\n  name              = \"common-instance-private\"\n  flavor_id         = data.vkcs_compute_flavor.vm-compute.id\n  security_groups   = [\"default\", \"ssh\"]\n  availability_zone = var.availability_zone_name\n  key_pair          = var.key_pair_name\n\n  block_device {\n    uuid                  = data.vkcs_images_image.vm-image.id\n    source_type           = \"image\"\n    volume_size           = 10\n    volume_type           = \"ceph-ssd\"\n    boot_index            = 0\n    destination_type      = \"volume\"\n    delete_on_termination = true\n  }\n\n  network {\n    uuid = vkcs_networking_network.common-private.id\n  }\n\n  depends_on = [\n    vkcs_networking_network.common-private,\n    vkcs_networking_subnet.common-private\n  ]\n}\n\nresource \"vkcs_compute_instance\" \"common-public\" {\n  name              = \"common-instance-public\"\n  flavor_id         = data.vkcs_compute_flavor.vm-compute.id\n  security_groups   = [\"default\", \"ssh\"]\n  availability_zone = var.availability_zone_name\n  key_pair          = var.key_pair_name\n\n  block_device {\n    uuid                  = data.vkcs_images_image.vm-image.id\n    source_type           = \"image\"\n    volume_size           = 10\n    volume_type           = \"ceph-ssd\"\n    boot_index            = 0\n    destination_type      = \"volume\"\n    delete_on_termination = true\n  }\n\n  network {\n    uuid = vkcs_networking_network.common-public.id\n  }\n\n  depends_on = [\n    vkcs_networking_network.common-public,\n    vkcs_networking_subnet.common-public\n  ]\n}\n\nresource \"vkcs_networking_floatingip\" \"fip\" {\n  pool = data.vkcs_networking_network.extnet.name\n}\n\nresource \"vkcs_compute_floatingip_associate\" \"fip\" {\n  floating_ip = vkcs_networking_floatingip.fip.address\n  instance_id = vkcs_compute_instance.common-public.id\n}\n\noutput \"common-instance-public-floating-ip\" {\n  description = \"The common-public VM floating IP address\"\n  value       = vkcs_networking_floatingip.fip.address\n}\n\noutput \"common-instance-public-ip\" {\n  description = \"The common-public VM IP address\"\n  value       = vkcs_compute_instance.common-public.network[0].fixed_ip_v4\n}\n\noutput \"common-instance-private-ip\" {\n  description = \"The common-private VM IP address\"\n  value       = vkcs_compute_instance.common-private.network[0].fixed_ip_v4\n}\n\n4. Create the necessary resources using Terraform\n\nPlace the created Terraform configuration files in one directory:\n\n   provider.tf;\n   variables.tf;\n   main.tf;\n   common-public.tf;\n   common-private.tf;\n   test-vms.tf.\n\nGo to this directory.\n\nRun the command:\n\n      terraform init\n\nRun the command:\n\n      terraform apply\n\n   When prompted for confirmation, enter yes.\n\nWait for the operation to complete.\n\n   After the resource creation is complete, the following Terraform outputs will be shown:\n\n   common-instance-private-ip: the IP address of the virtual machine in the private subnet.\n   common-instance-public-ip: the IP address of the virtual machine in the public subnet.\n   common-instance-public-floating-ip: floating IP address of the virtual machine in the public subnet.\n\n   Use these IP addresses when checking the health of the example.\n\n5. Check the performance of the example\n\nConnect via SSH to the virtual machine common-instance-public.\n\n   To connect, use:\n\n   IP address from the output common-instance-public-floating-ip.\n   SSH key with the name specified in the variable key_pair_name of the variables.tf file.\n\nPing the IP address from the output common-instance-private-ip:\n\n      ping 192.168.199.XXX\n\n   A successful ping indicates the correct operation of the example:\n\n      PING 192.168.199.XXX (192.168.199.XXX) 56(84) bytes of data.\n   64 bytes from 192.168.199.XXX: icmp_seq=1 ttl=63 time=1.82 ms\n   64 bytes from 192.168.199.XXX: icmp_seq=2 ttl=63 time=0.875 ms\n   64 bytes from 192.168.199.XXX: icmp_seq=3 ttl=63 time=0.903 ms\n   64 bytes from 192.168.199.XXX: icmp_seq=4 ttl=63 time=0.966 ms\n   ^C\n   --- 192.168.199.10 ping statistics ---\n   4 packets transmitted, 4 received, 0% packet loss, time 3004ms\n   rtt min/avg/max/mdev = 0.875/1.139/1.815/0.391 ms\n\nDelete unused resources\n\nIf you no longer need the resources created with Terraform, delete them:\n\nSwitch to the directory that contains the Terraform configuration files.\n\nRun the command:\n\n      terraform destroy\n\n   When prompted for confirmation, enter yes.\n\nWait for the operation to complete.\n","url":"/en/tools-for-using-services/terraform/how-to-guides/vnet/networks-routing","description":"How to set up traffic routing between networks using Terraform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"b1e0aefd-1931-493b-833b-5a020029a12b","title":"Creating a Spark cluster","data":"This article provides an example of creating a Spark cluster via Terraform.\n\nWhen creating the cluster the following was used:\n\nResources: vkcs_mlplatform_spark_k8s, vkcs_mlplatform_k8s_registry, vkcs_networking_network.\nData sources: vkcs_compute_flavor, vkcs_networking_network.\n\nBefore creating a cluster\n\nReview the available resources and quotas for the region where you want to create the cluster. Different regions may have different quotas configured.\n\n   To increase your quotas, please contact technical support.\n\nInstall Terraform and configure the environment if it is not already done.\n\nTo go through this case, you need a provider version 0.7.0 or higher. Make sure that the provider version in the vkcs_provider.tf file is not lower. If the provider version is lower, update the provider.\n\n1. Create a Spark cluster manifest file\n\nIn the example, the cluster is created with the following configuration:\n\ndefault region, availability zone — GZ1\nminimum worker-nodes — 2, maximum worker-nodes — 100\ncluster mode — DEV\nsleep mode after 120 minutes of inactivity\ndeleting the cluster after 1440 minutes of inactivity\n\nCreate a Terraform configuration file named main.tf with the content:\n\nlocals {\n  spark_configuration = {\n    \"spark.eventLog.dir\"     = \"s3a://spark-bucket\"\n    \"spark.eventLog.enabled\" = \"true\"\n  }\n  spark_environment_variables = {\n    \"ENV_VAR_1\" : \"env_var_1_value\"\n    \"ENV_VAR_2\" : \"env_var_2_value\"\n  }\n}\n\nresource \"vkcs_mlplatform_spark_k8s\" \"spark_k8s\" {\n  name              = \"tf-example\"\n  availability_zone = \"GZ1\"\n  network_id        = vkcs_networking_network.app.id\n  subnet_id         = vkcs_networking_subnet.app.id\n\n  node_groups = [\n    {\n      node_count          = 2\n      flavor_id           = data.vkcs_compute_flavor.basic.id\n      autoscaling_enabled = true\n      min_nodes           = 2\n      max_nodes           = 100\n    }\n  ]\n  cluster_mode = \"DEV\"\n  registry_id  = vkcs_mlplatform_k8s_registry.k8s_registry.id\n  ip_pool      = data.vkcs_networking_network.extnet.id\n\n  suspend_after_inactive_min = 120\n  delete_after_inactive_min  = 1440\n\n  spark_configuration   = yamlencode(local.spark_configuration)\n  environment_variables = yamlencode(local.spark_environment_variables)\n}\n\nHere:\n\nnetwork_id — an ID of the network where the cluster will be hosted. The cluster can be hosted on an existing network or a new one. You can specify the ID in the manifest or get it from the data source or resource.\n\n    Examples\n\n  network_id = vkcs_networking_network.default.id: the cluster will be hosted on a new network, which will be created by the vkcs_networking_network resource. The resource will be generated further.\n  network_id = data.vkcs_networking_network.default.id: the cluster will be hosted on an existing network. Its ID is taken from the vkcs_networking_network data source. The source will be generated further.\n  network_id = \"bb76507d-yyyy-yyyy-yyyy-2bca1a4c4cfc\": the cluster will be hosted on an existing network. Its ID is taken from the list of networks in your VK Cloud personal account or via the Openstack CLI.\n\n\nflavor_id — a VM type ID. You can specify the ID in the manifest or get it from the data source.\n\n    Examples\n\n  flavor_id = data.vkcs_compute_flavor.basic.id: the ID is taken from the vkcs_compute_flavor data source, which will be generated further.\n  flavor_id = \"aee06bce-xxxx-xxxx-xxxx-ec4210cc6bac\": the ID is taken using the OpenStack CLI.\n\n\nregistry_id — an ID of the Docker registry that provides images for running Spark jobs. You can create a new registry or use an existing one.\n\n    Examples\n\n  registry_id = vkcs_mlplatform_k8s_registry.k8s_registry.id: a new Docker registry will be created and hosted on a dedicated K8S Docker Registry virtual machine, which is not part of the cluster and is billed separately. The ID will be taken after the vkcs_mlplatform_k8s_registry resource is created. The resource will be generated further.\n  registry_id = \"a57e9e91-yyyy-yyyy-yyyy-fedc7ac78c33\": an ID of an existing K8S Docker Registry. To get the K8S Docker Registry ID:\n\n    Go to your VK Cloud personal account.\n    Go to ML Platform → Spark in k8s.\n    Go to the Instances tab.\n    Click the name of the Docker Registry instance you need.\n\n\nip_pool — an external network ID for the cluster IP address pool.\n\n    Examples\n\n  ip_pool = data.vkcs_networking_network.extnet.id: the ID is in the vkcs_networking_network data source, which is specified in the cluster network manifest.\n  ip_pool = \"bb76507d-aaaa-aaaa-aaaa-2bca1a4c4cfc\": the external network ID is taken from the network list in your VK Cloud personal account or via the Openstack CLI.\n\n\nspark_configuration — properties of the Spark configuration.\n\nenvironment_variables — a list of the Spark environment variables.\n\n2. Create a K8S Docker Registry instance manifest file\n\nCreate a Terraform configuration file named registry.tf with the content:\n\nresource \"vkcs_mlplatform_k8s_registry\" \"k8s_registry\" {\n  name              = \"tf-example\"\n  admin_name        = \"admin\"\n  admin_password    = \"Password12!Password\"\n  flavor_id         = data.vkcs_compute_flavor.basic.id\n  availability_zone = \"GZ1\"\n  boot_volume = {\n    volume_type = \"ceph-ssd\"\n  }\n  networks = [\n    {\n      network_id = vkcs_networking_network.app.id\n      ip_pool = data.vkcs_networking_network.extnet.id\n    },\n  ]\n}\n\nHere:\n\nflavor_id — a VM type ID. You can specify the ID in the manifest or get it from the data source.\n\n    Examples\n\n  flavor_id = data.vkcs_compute_flavor.basic.id: the ID is taken from the vkcs_compute_flavor data source, which will be generated further.\n  flavor_id = \"aee06bce-xxxx-xxxx-xxxx-ec4210cc6bac\": the ID is taken using the OpenStack CLI.\n\n\nnetwork_id — an ID of the network where the cluster will be hosted. The cluster can be hosted on an existing network or a new one. You can specify the ID in the manifest or get it from the data source or resource.\n\n    Examples\n\n  network_id = vkcs_networking_network.default.id: the cluster will be hosted on a new network, which will be created by the vkcs_networking_network resource. The resource will be generated further.\n  network_id = data.vkcs_networking_network.default.id: the cluster will be hosted on an existing network. Its ID is taken from the vkcs_networking_network data source. The source will be generated further.\n  network_id = \"bb76507d-yyyy-yyyy-yyyy-2bca1a4c4cfc\": the cluster will be hosted on an existing network. Its ID is taken from the list of networks in your VK Cloud personal account or via the Openstack CLI.\n\n\nip_pool — an external network ID for the cluster IP address pool.\n\n    Examples\n\n  ip_pool = data.vkcs_networking_network.extnet.id: the ID is in the vkcs_networking_network data source, which is specified in the cluster network manifest.\n  ip_pool = \"bb76507d-aaaa-aaaa-aaaa-2bca1a4c4cfc\": the external network ID taken from the network list in your VK Cloud personal account or via the Openstack CLI.\n\n\n3. (Optional) Create a file describing the data source for the VM type\n\nCreate a Terraform configuration file named flavor.tf to describe the VM type:\n\ndata \"vkcs_compute_flavor\" \"basic\" {\n  name = \"STD2-4-4\"\n}\n\nThe specified VM type will be used to create clusters in your Terraform project.\n\n4. (Optional) Create a file describing the network infrastructure for the cluster\n\nCreate a Terraform configuration file named network.tf with the network infrastructure description:\n\n\n\nExisting network\nNew network\n\n\n\nExample data source for a network existing in your project:\n\ndata \"vkcs_networking_network\" \"default\" {\n  name = \"default\"\n  sdn = \"neutron\"\n}\n\nCreate networks\nresource \"vkcs_networking_network\" \"app\" {\n  name        = \"app-tf-example\"\n  description = \"Application network\"\n  sdn = \"neutron\"\n}\n\nresource \"vkcs_networking_subnet\" \"app\" {\n  name       = \"app-tf-example\"\n  network_id = vkcs_networking_network.app.id\n  cidr       = \"192.168.199.0/24\"\n}\n\nGet external network with Inernet access\ndata \"vkcs_networking_network\" \"extnet\" {\n  name = \"ext-net\"\n}\n\nCreate a router to connect netwoks\nresource \"vkcs_networking_router\" \"router\" {\n  name = \"router-tf-example\"\nConnect router to Internet\n  external_network_id = data.vkcs_networking_network.extnet.id\n}\n\nConnect networks to the router\nresource \"vkcs_networking_router_interface\" \"app\" {\n  router_id = vkcs_networking_router.router.id\n  subnet_id = vkcs_networking_subnet.app.id\n}\n\n5. Create the necessary resources using Terraform\n\nPut the Terraform configuration files in one directory:\n\n   vkcs_provider.tf\n   main.tf\n   registry.tf\n   flavor.tf (if created)\n   network.tf(if created)\n\nOpen this directory.\nMake sure that the configuration files are correct and contain the required changes:\n\n      terraform validate && terraform plan\n\nApply the changes:\n\n      terraform apply\n\n   Enter yes to confirm.\n\nWait for the operation to complete.\n\n6. Check configuration application\n\nVerify that the Spark cluster was successfully created:\n\nGo to your VK Cloud personal account.\nGo to ML Platform → Instances. Make sure your Spark cluster is created and active.\n\nDelete unused resources\n\nIf you no longer need the Terraform resources, delete them:\n\nOpen the directory that contains the Terraform configuration files.\nRun the command:\n\n      terraform destroy\n\n   Enter yes to confirm.\n\nWait for the operation to complete.\n","url":"/en/tools-for-using-services/terraform/how-to-guides/spark/create","description":"How to create a Spark cluster using Terraform.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"17ed1e7e-b603-4017-a78b-5f71577f8dee","title":"Billing","data":"Terraform is free of charge. You pay for resources that are created with Terraform.\n","url":"/en/tools-for-using-services/terraform/tariffication","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"db00c548-d487-4e90-b438-a69ce9785116","title":"Basic Terraform commands","data":"Terraform command line is the terraform command that accepts a variety of subcommands.\n\nMain commands used:\n\nterraform init — initializes the Terraform working directory.\nterraform validate — validates the syntax of the Terraform file.\nterraform plan — generates the file of changes and shows what will change on startup. It is recommended that you run this command before running the apply command to ensure that the results are as intended.\nterraform apply — builds or changes the infrastructure. The command will show the execution plan and require a \"yes\" or \"no\" answer (unless the --auto-approve flag is used, which will cause the command to be executed automatically).\nterraform refresh — updates the local state file with respect to real resources. This ensures that Terraform has an accurate idea of what is in the current environment.\nterraform destroy — deletes infrastructure managed by Terraform. This will permanently remove everything created and stored in the state file from the cluster.\n\nBefore using the commands, switch to the working directory that contains the .tf configuration files.\n\nIn cases where commands are used in automation scripts, for convenience, the -chdir= option is used to change the working directory before applying the command:\n\nterraform -chdir=terraform/configuration apply\n","url":"/en/tools-for-using-services/terraform/reference/commands","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"de47c0fe-6e0d-4d2f-b932-d7f400bb881f","title":"Configuration files","data":"When creating resources, Terraform works with configuration files. They have the .tf extension.\n\nIt is recommended to create a separate working directory for each VK Cloud project. The following files are placed in the working directory:\n\nthe provider config file (project-specific);\nthe configuration files of the resources you are planning to create.\n\nFor correct Terraform operation, an additional file is required — the Terraform mirror config file.\n\nThe Terraform provider config file\n\nThe file contains the settings of Terraform providers and user authentication details for the current project.\n\nThe file can have any name. It is recommended to name it provider.tf to highlight that the file contains the key settings for connecting to VK Cloud.\n\nThe file must be present in all Terraform working directories.\n\nThe file is available for download in your personal account, on the Project settings page, on the Terraform tab. The downloaded file has the name vkcs_provider.tf and contains the settings of one provider (vkcs). After downloading the file, edit it — specify your account password in the password argument.\n\nAn example of the file contents:\n\nterraform {\n    required_providers {\n        vkcs = {\n            source = \"vk-cs/vkcs\"\n        }\n    }\n}\n\nprovider \"vkcs\" {\nYour user account.\n    username = \"\"\n\nThe password of the account\n    password = \"\"\n\nThe tenant token can be taken from the project Settings tab - > API keys.\nProject ID will be our token.\n    project_id = \"\"\n\nRegion name\n    region = \"\"\n\n    auth_url = \"\"\n}\n\nThe file consists of the following sections:\n\nterraform.required_providers {} — a list of Terraform providers required for operation. To work with VK Cloud, the vkcs provider must be specified, its source and versions. If you are going to use additional providers, add them to this section.\nprovider \"vkcs\" {} — settings for user authentication in the project:\n\n  password: specify your account password;\n  username, project_id, region: the values in the file that is downloaded from the personal account are valid for working with the current project. These values can also be obtained in your personal account, on the Project settings page, on the Terraform tab.\n\n  The values of some arguments depend on the region:\n\n  region:\n\n    for Moscow region: RegionOne;\n    for Kazakhstan region: kz;\n\n  auth_url:\n\n    for Moscow region: https://infra.mail.ru:35357/v3/;\n    for Kazakhstan region: https://kz.infra.mail.ru:35357/v3/.\n\nResources configuration files\n\nResource configuration files describe the resources created via Terraform, their settings and dependencies.\n\nResource configuration files can have any names. Examples in this documentation use the following naming standards:\n\nvariables.tf — variables that are used in the configuration. Setting frequently changed parameters as variables makes it easier to change the infrastructure configuration for a new project.\nmain.tf — contains a description of the main resource that is created in a scenario. For example, in the scenario for creating a virtual machine the main.tf file should typically contain the description of a virtual machine.\n\nSometimes files are named after the types of the resources they describe:\n\nnetwork.tf — the description of the cloud network.\nkubernetes.tf — the description of Kubernetes resources.\n\nA typical configuration contains:\n\nresource \"resource_type\" \"resource_name\" {} — describes the resource being created, such as a network, a subnet, a Kubernetes cluster, or a database cluster.\ndata \"data_type\" \"data_name\" {} — allows you to use data specified outside the Terraform configuration that exists in the cloud or locally. For example, VM configuration, cluster template/version, etc.\nvariable \"parameter\" {} — input variables. Used to declare variables that are used in the configuration (variables.tf).\noutput \"parameter\" {} — output variables. Used to output data to the command line.\n\nTo determine the sequence of creating resources and their dependencies, you can use the depends_on meta-argument. The meta-argument indicates the resource on which the resource being created depends:\n\ndepends_on = [\n    vkcs_kubernetes_cluster.k8s-cluster,\n]\n\nFirst, the resource set by the meta-argument is created, then the resource for which the dependency is specified is created.\n\nThe Terraform mirror config file\n\nThe file is required for Terraform startup, it contains the address of the VK Cloud Terraform mirror. The file must be named terraform.rc for Windows and .terraformrc for other OS. The file content is the same for all projects and regions.\n\nThe file is available for download on the Project settings page in your personal account, on the Terraform tab. After downloading, the file can be used as is.\n\nThe file must be located:\n\nFor Windows — in the Application Data directory of the user.\nFor other OS — in the home directory of the user.\n\nThe file content for VK Cloud projects:\n\nprovider_installation {\n    network_mirror {\n        url = \"https://terraform-mirror.mcs.mail.ru\"\n        include = [\"registry.terraform.io//\"]\n    }\n    direct {\n        exclude = [\"registry.terraform.io//\"]\n    }\n}\n","url":"/en/tools-for-using-services/terraform/reference/configuration","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"Tools for working with services","link":"/en/tools-for-using-services","isActive":false,"evenSimpler":true},{"title":"Terraform","link":"/en/tools-for-using-services/terraform","isActive":false,"evenSimpler":true}]},{"uuid":"3c72b783-4095-4de8-8d28-43eefcf1f0d6","title":"Installing WordPress on Linux","data":"WordPress is a content management system (CMS) written in PHP and using a MySQL database. Allows you to create blogs, websites or web applications.\n\nThis instruction will help you deploy the latest version of the WordPress CMS in the Almalinux 9 operating system in VK Cloud, as well as configure a DNS record for domain name access. MySQL 8.0 Single configuration is used as a DBMS.\n\nPreparatory steps\n\nRegister at VK Cloud.\nCreate network1 network with internet access and a subnet with the address 10.0.0.0/24.\nCreate VM:\n\n   name: Almalinux_9_WP;\n   operating system Almalinux 9;\n   network: network1 with subnet 10.0.0.0/24;\n   assign a public IP address. The example will use 212.233.95.135;\n   security groups (firewall settings): default, ssh+www.\n\nCreate DB instance:\n\n   name: MySQL-9341;\n   DBMS: MySQL 8.0;\n   configuration type: Single;\n   network: network1;\n   DB name: wordpress-db;\n   DB username: wordpress;\n   DB password: AN0r25e0ae4d626p;\n\n   In the example, the internal IP of the created instance: 10.0.0.7.\n\nCreate DNS zone.\n\n\n   Make sure that the DNS zone is delegated successfully and the NS records are configured correctly: the zone must have the status NS records are configured correctly.\n\n\nCreate an record in the selected zone:\n\n   record type: A;\n   name: for example, site-wp.example.vk.cloud;\n   IP address: external VM address 212.233.95.135.\n\n(Optional) Check the name resolution to the IP address using the command nslookup site-wp.example.vk.cloud. Output upon successful operation:\n\n      Non-authoritative answer:\n   Name:   site-wp.example.vk.cloud\n   Address: 212.233.95.135\n\n2. Install WordPress on VM\n\nConnect to the Almalinux_9_WP VM.\nUpdate the packages to the current version and reboot the VM using the commands:\n\n      sudo dnf update -y\n   sudo systemctl reboot\n\nDownload the necessary repositories by sequentially executing the commands:\n\n      sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm -y\n   sudo dnf install https://rpms.remirepo.net/enterprise/remi-release-9.rpm -y\n   sudo dnf module enable php:remi-8.2 -y\n   sudo dnf install wget httpd php php-mysqlnd php-gd php-intl php-bcmath php-pecl-zip -y\n\nLaunch the httpd daemon:\n\n      sudo systemctl enable httpd.service --now\n\nDownload the WordPress CMS repository and deploy it on a running web server:\n\n      sudo wget https://wordpress.org/latest.tar.gz\n   sudo tar xzf latest.tar.gz -C /var/www/html/\n   sudo chown -R apache:apache /var/www/html/wordpress/\n\nSet the SELinux parameters for the correct operation of the web server:\n\n      sudo setsebool -P httpd_can_network_connect on\n   sudo setsebool -P httpd_unified on\n   sudo setsebool -P httpd_enable_cgi on\n   sudo setsebool -P httpd_builtin_scripting on\n\nIn the browser, enter the VM's public IP address with /wordpress. In the current instruction it is site-wp.example.vk.cloud/wordpress.\nIn the installation wizard, specify the English language and read the information about installing the CMS.\nSpecify the parameters MySQL-9341:\n\n   Database name: wordpress-db.\n   Username: wordpress.\n   Password: AN0r25e0ae4d626p.\n   Database Host: 10.0.0.7.\n   Table Prefix: wp_\n\nSpecify the CMS credentials:\n\n   Site Title: site-wp.example.vk.cloud.\n   Username: admin.\n   Password: come up with an administrator password.\n   Your Email: specify the email address of the CMS administrator.\n\nClick the Install WordPress button.\n\n3. Check the functionality of WordPress\n\nIn the browser, go to http://site-wp.example.vk.cloud/wordpress/.\nIn the authorization window, enter the username and password of the CMS administrator.\n\nThe WordPress CMS admin panel opens.\n\nDelete unused resources\n\nDeployed virtual resources are charged. If you don't need them anymore:\n\nDelete Almalinux_9_WP VM.\nDelete MySQL-9341 instance.\nIf necessary, delete 212.233.95.135 floating IP address.\nDelete site-wp.example.vk.cloud DNS record.\n","url":"/en/cases/cases-cms/cases-wp","description":"Installing WordPress on the AlmaLinux 9 operating system in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"Installing CMS on Linux","link":"/en/cases/cases-cms","isActive":false,"evenSimpler":true}]},{"uuid":"fd069161-e077-45d4-8df1-c4b5472cf314","title":"Installing Joomla! on Linux","data":"Joomla! is a content management system (CMS) written in PHP and JavaScript languages and using MySQL DBMS as a database storage.\n\nThis instruction will help you deploy CMS Joomla version 4.3.4 in the Almalinux 9 operating system in VK Cloud, as well as configure a DNS record for domain name access. MySQL 8.0 Single configuration is used as a DBMS.\n\nPreparatory steps\n\nRegister at VK Cloud.\nCreate network1 network with internet access and a subnet with the address 10.0.0.0/24.\nCreate VM:\n\n   name: Almalinux_9_Joomla;\n   operating system: Almalinux 9;\n   network: network1 with subnet 10.0.0.0/24;\n   assign a public IP address. The example will use 87.239.105.44;\n   security groups (firewall settings): default, ssh+www.\n\nCreate DB instance:\n\n   name: MySQL-7313;\n   DBMS: MySQL 8.0;\n   configuration type: Single;\n   network: network1;\n   DB name: MySQL-7313;\n   DB username: user;\n   DB password: AN0r25e0ae4d626p;\n\n   In the example, the internal IP of the created instance: 10.0.0.7.\n\nCreate DNS zone.\n\n\n   Make sure that the DNS zone is delegated successfully and the NS records are configured correctly: the zone must have the status NS records are configured correctly.\n\n\nCreate an record in the selected zone:\n\n   record type: A;\n   name: for example, site-joomla.example.vk.cloud;\n   IP address: external VM address 87.239.105.44.\n\n(Optional) Check the name resolution to the IP address using the command nslookup site-joomla.example.vk.cloud. Output upon successful operation:\n\n      Non-authoritative answer:\n   Name:   site-joomla.example.vk.cloud\n   Address: 87.239.105.44\n\n2. Install Joomla! on VM\n\nConnect to the Almalinux_9_Joomla VM.\nUpdate the packages to the current version and reboot the VM using the commands:\n\n      sudo dnf update -y\n   sudo systemctl reboot\n\nDownload the necessary repositories by sequentially executing the commands:\n\n      sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm -y\n   sudo dnf install https://rpms.remirepo.net/enterprise/remi-release-9.rpm -y\n   sudo dnf module enable php:remi-8.2 -y\n   sudo dnf install wget httpd php php-mysqlnd php-gd php-xml php-mbstring php-intl php-pecl-zip -y\n\nLaunch the httpd daemon:\n\n      sudo systemctl enable httpd.service --now\n\nDownload the Joomla CMS repository! and deploy it in the joomla subdirectory on the running web server:\n\n      wget https://github.com/joomla/joomla-cms/releases/download/4.3.4/Joomla_4.3.4-Stable-Full_Package.tar.gz\n   sudo mkdir -p /var/www/html/joomla\n   sudo tar xzf Joomla_4.3.4-Stable-Full_Package.tar.gz -C /var/www/html/joomla/\n   sudo chown -R apache:apache /var/www/html/joomla\n\nSet the SELinux parameters for the correct operation of the web server:\n\n      sudo setsebool -P httpd_enable_cgi on\n   sudo setsebool -P httpd_unified on\n   sudo setsebool -P httpd_builtin_scripting on\n   sudo setsebool -P httpd_can_network_connect on\n\nIn the browser, enter the VM's public IP address with /joomla. In the current instruction it is 87.239.105.44/joomla.\nIn the installation wizard, specify the English language and the name of the site — site-joomla.example.vk.cloud.\nAt the “Account Settings” step, specify the CMS administrator credentials.\nAt the “Database Parameters” step of the database, specify the parameters MySQL-7313:\n\n   Database type: MySQL (PDO).\n   Hostname: 10.0.0.7.\n   Database username: user.\n   Password of the database user: AN0r25e0ae4d626p.\n   Database name: MySQL-7313.\n\n(Optional) Create or delete a file in the directory /var/www/html/joomla/installation: the file name and its location will be displayed in the pop-up window of the installation wizard.\n\n3. Check the functionality of Joomla!\n\nIn the browser, go to http://site-joomla.example.vk.cloud/joomla/. Upon successful installation, the CMS Joomla! home page will open.\n\nDelete unused resources\n\nDeployed virtual resources are charged. If you don't need them anymore:\n\nDelete Almalinux_9_Joomla VM.\nDelete MySQL-7313 instance.\nIf necessary, delete 87.239.105.44 floating IP address.\nDelete site-joomla.example.vk.cloud DNS record.\n","url":"/en/cases/cases-cms/cases-joomla","description":"Installing Joomla 4.3.4 on Almalinux 9 operating system","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"Installing CMS on Linux","link":"/en/cases/cases-cms","isActive":false,"evenSimpler":true}]},{"uuid":"4d1c1424-e0c8-43fa-85ef-791ff9b0f4b4","title":"Installing OpenCart on Linux","data":"OpenCart is a platform for creating an online store. OpenCart is built on the MVC principle and can be installed on any web server with PHP and MySQL support.\n\nThis instruction will help you deploy OpenCart version 4.0.2.3 in the Ubuntu 22.04 operating system in VK Cloud, as well as configure a DNS record for domain name access. MySQL 8.0 Single configuration is used as a DBMS.\n\nPreparatory steps\n\nRegister at VK Cloud.\nCreate network1 network with internet access and a subnet with the address 10.0.0.0/24.\nCreate VM:\n\n   name: Ubuntu_22_04_OpenCart;\n   operating system: Ubuntu 22.04;\n   network: network1 with subnet 10.0.0.0/24;\n   assign a public IP address. The example will use 87.239.106.48;\n   security groups (firewall settings): default, ssh+www.\n\nCreate DB instance:\n\n   name: MySQL-7313;\n   DBMS: MySQL 8.0;\n   configuration type: Single;\n   network: network1;\n   DB name: MySQL-9341;\n   DB username: user;\n   DB password: AN0r25e0ae4d626p;\n\n   In the example, the internal IP of the created instance: 10.0.0.7.\n\nCreate DNS zone.\n\n\n   Make sure that the DNS zone is delegated successfully and the NS records are configured correctly: the zone must have the status NS records are configured correctly.\n\n\nCreate an record in the selected zone:\n\n   record type: A;\n   name: for example, site-opencart.example.vk.cloud;\n   IP address: external VM address 87.239.106.48.\n\n(Optional) Check the name resolution to the IP address using the command nslookup site-opencart.example.vk.cloud. Output upon successful operation:\n\n      Non-authoritative answer:\n   Name:   site-opencart.example.vk.cloud\n   Address: 87.239.106.48\n\nInstall OpenCart on VM\n\nConnect to the Ubuntu_22_04_OpenCart VM.\nUpdate the packages to the current version and reboot the VM using the commands:\n\n      sudo dnf update -y\n   sudo systemctl reboot\n\nDownload the necessary repositories and start the web server:\n\n      sudo apt install apache2 apache2-utils libapache2-mod-php php8.1 php8.1-cli php8.1-curl php8.1-fpm php8.1-gd php8.1-intl php8.1-mbstring php8.1-mysql php8.1-opcache php8.1-readline php8.1-soap php8.1-xml php8.1-xmlrpc php8.1-zip php-gd -y\n   sudo systemctl enable apache2 --now\n\nDownload the OpenCart repository and deploy it in the opencart subdirectory on the running web server:\n\n      cd ~\n   wget https://github.com/opencart/opencart/archive/refs/tags/4.0.2.3.tar.gz\n   tar xzf 4.0.2.3.tar.gz\n   sudo cp -r opencart-4.0.2.3/upload /var/www/html/opencart\n   sudo chown -R www-data:www-data /var/www/html/opencart\n   sudo mv /var/www/html/opencart/config-dist.php /var/www/html/opencart/config.php\n   sudo mv /var/www/html/opencart/admin/config-dist.php /var/www/html/opencart/admin/config.php\n\nIn the browser, enter the VM's public IP address with /opencart. In the current instruction it is site-opencart.example.vk.cloud/opencart.\nIn the installation wizard, read and accept the terms of the OpenCart license agreement.\nAt the “Pre-Installation” step, check that the VM is ready to install OpenCart — all checks must be completed successfully.\nAt the “Configuration” step, specify the parameters MySQL-9341:\n\n   DB Driver: MySQLi.\n   Hostname: 10.0.0.7.\n   Username: user.\n   Password: AN0r25e0ae4d626p.\n   Database: MySQL-9341.\n   Port: 3306.\n\n    At the same step, specify the administrator credentials.\n\nWait for the installation to complete: the page Installation complete will appear.\n(Optional) Customize OpenCart according to the recommendations of the software developer:\n\n   Delete the install directory from the web server:\n\n            sudo rm -rf /var/www/html/opencart/install\n\n   Move the storage directory to /var/www:\n\n            sudo mv /var/www/html/opencart/system/storage/ /var/www\n\n   At configuration files /var/www/html/opencart/config.php and /var/www/html/opencart/admin/config.php replace:\n\n            // source string\n      define('DIR_STORAGE', DIR_SYSTEM . 'storage/');\n\n      // replaced string\n      define('DIR_STORAGE', '/var/www/storage/');\n\n3. Check the functionality of OpenCart\n\nIn the browser, go to http://site-opencart.example.vk.cloud/opencart/. Upon successful installation, a page with a demo store will open.\n\nDelete unused resources\n\nDeployed virtual resources are charged. If you don't need them anymore:\n\nDelete Ubuntu_22_04_OpenCart VM.\nDelete MySQL-9341 instance.\nIf necessary, delete 87.239.106.48 floating IP address.\nDelete site-opencart.example.vk.cloud DNS record.\n","url":"/en/cases/cases-cms/cases-opencart","description":"Installing OpenCart 4.0.2.3 on Ubuntu 22.04 operating system in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"Installing CMS on Linux","link":"/en/cases/cases-cms","isActive":false,"evenSimpler":true}]},{"uuid":"4d820acb-a00c-4bcc-a317-25561340f982","title":"Installing 1C-Bitrix","data":"1C-Bitrix: Site Management is a professional Internet resource management system with which you can create and maintain information portals, online stores and corporate websites.\n\nThis instruction will help you deploy 1C-Bitrix: Site management of the latest version in the Ubuntu 22.04 operating system in VK Cloud, as well as configure a DNS record for domain name access. MySQL 8.0 Single configuration is used as a DBMS.\n\nPreparatory steps\n\nRegister at VK Cloud.\nCreate network1 network with internet access and a subnet with the address 10.0.0.0/24.\nCreate VM:\n\n   name: Ubuntu_22_04_Bitrix;\n   operating system: Ubuntu 22.04;\n   network: network1 with subnet 10.0.0.0/24;\n   assign a public IP address. The example will use 87.239.105.191;\n   security groups (firewall settings): default, ssh+www.\n\nCreate DB instance:\n\n   name: MySQL-1111;\n   DBMS: MySQL 8.0;\n   configuration type: Single;\n   network: network1;\n   DB name: MySQL-1111;\n   DB username: user;\n   DB password: AN0r25e0ae4d626p;\n\n   In the example, the internal IP of the created instance: 10.0.0.7.\n\nCreate DNS zone.\n\n\n   Make sure that the DNS zone is delegated successfully and the NS records are configured correctly: the zone must have the status NS records are configured correctly.\n\n\nCreate an record in the selected zone:\n\n   record type: A;\n   name: for example, site-bitrix.example.vk.cloud;\n   IP address: external VM address 87.239.105.191.\n\n(Optional) Check the name resolution to the IP address using the command nslookup site-bitrix.example.vk.cloud. Output upon successful operation:\n\n      Non-authoritative answer:\n   Name:   site-bitrix.example.vk.cloud\n   Address: 87.239.105.191\n\n2. Install Bitrix on VM\n\nConnect to the Ubuntu_22_04_Bitrix VM.\nUpdate the packages to the current version and reboot the VM using the commands:\n\n      sudo dnf update -y\n   sudo apt upgrade -y\n   sudo systemctl reboot\n\nInstall the necessary packages for CMS Bitrix:\n\n      sudo apt install apache2 apache2-utils libapache2-mod-php php8.1 php8.1-cli php8.1-curl php8.1-fpm php8.1-gd php8.1-intl php8.1-mbstring php8.1-mysql php8.1-opcache php8.1-readline php8.1-soap php8.1-xml php8.1-xmlrpc php8.1-zip php-gd -y\n\nFind the file /etc/php/8.1/apache2/php.ini, uncomment and change the parameters in it:\n\n      short_open_tag = On\n\n   opcache.revalidate_freq = 0\n\n   date.timezone = Europe/Moscow\n\nFind the file /etc/apache2/sites-available/000-default.conf and add a fragment to it after DocumentRoot /var/www/html block:\n\n     AllowOverride All\n\nCheck the web server configuration for syntax errors:\n\n      apachectl configtest\n\n   If the check is successful, a message like Syntax OK will appear.\n\nRestart the web server with the command:\n\n      sudo systemctl restart apache2\n\nDownload the CMS Bitrix repository of the “Start” edition from the official website and unpack it to the web server:\n\n      cd ~\n   wget https://www.1c-bitrix.ru/download/start_encode.tar.gz\n   sudo rm -rf /var/www/html/*\n   sudo tar xzf start_encode.tar.gz -C /var/www/html/\n   sudo chown -R www-data:www-data /var/www/html/\n\nIn the browser, enter the VM's public address (in the current instructions, this is site-bitrix.example.vk.cloud).\nIn the Installation wizard, start the CMS installation process.\nAt the “License Agreement” step, accept the terms of the CMS Bitrix license agreement.\nAt the “Registration” step register the product.\nAt the “Preliminary check” step, make sure that all the parameters correspond to the required ones (highlighted in green).\nAt the “Creating a database” step, specify MySQL-1111  parameters:\n\n   Host: 10.0.0.7.\n   DB user: Existing.\n   DB username: user.\n   DB password: AN0r25e0ae4d626p.\n   Database: Existing.\n   Database name: MySQL-1111.\n\nAt the “Product Installation” step, wait for the product to be installed, it may take some time.\nIn the window that appears specify the administrator credentials.\n\n3. Check the functionality of Bitrix\n\nAfter specifying the administrator credentials click the Go to website. The CMS Bitrix home page opens.\n\nDelete unused resources\n\nDeployed virtual resources are charged. If you don't need them anymore:\n\nDelete Ubuntu_22_04_Bitrix VM.\nDelete MySQL-1111 instance.\nIf necessary, delete 87.239.105.191 floating IP address.\nDelete site-bitrix.example.vk.cloud DNS record.\n","url":"/en/cases/cases-cms/cases-bitrix","description":"Installing 1C-Bitrix: Site Management on Ubuntu 22.04 operating system.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"Installing CMS on Linux","link":"/en/cases/cases-cms","isActive":false,"evenSimpler":true}]},{"uuid":"f919d208-35dd-4d62-b32b-e0b76ec76dde","title":"Installing Docker CE on Ubuntu","data":"Docker CE (Community Edition) is a platform that allows working with containerized applications. Such applications are shipped in the containers, which contain all the necessary resources for the applications to operate. This allows to quickly build, deploy, and debug applications.\n\nContainers are similar to virtual machines, but more lightweight. To isolate containerized application's resources, containers rely on the operation system on which they are running.\n\nPreparatory steps\n\nMake sure that:\n\nYou have an access to the terminal of the Ubuntu host, on which you are planning to install Docker CE.\nYou can use sudo on that host to run the commands on behalf of superuser (root).\n\n1. Install Docker CE\n\nConnect to the terminal.\n\n   All further actions listed below are to be performed in this terminal.\n\nInstall the packages that are required for the further installation process:\n\n      sudo apt update && \\\n   sudo apt install -y \\\n     apt-transport-https \\\n     ca-certificates \\\n     curl \\\n     software-properties-common\n\nAdd the Docker repository's GPG key to the keyring:\n\n      curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n\nMake sure that the GPG key was successfully added:\n\n      apt-key list\n\n   The command's output should contain the information about Docker repository's GPG key.\n\n   Example output\n\n      /etc/apt/trusted.gpg\n   --------------------\n   pub   rsa4096 2017-02-22 [SCEA]\n         9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88\n   uid           [ unknown] Docker Release (CE deb)\n   sub   rsa4096 2017-02-22 [S]\n\n   ...\n\n\nAdd the Docker repository:\n\n      sudo add-apt-repository -u \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"\n\nMake sure that the repository was successfully added:\n\n      apt-cache policy docker-ce\n\n   The command's output should contain the information about the installation candidates for the docker-ce package.\n\n   Example output\n\n      docker-ce:\n     Installed: (none)\n     Candidate: 5:23.0.1-1ubuntu.18.04bionic\n     Version table:\n        5:23.0.1-1ubuntu.18.04bionic 500\n           500 https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages\n        5:23.0.0-1ubuntu.18.04bionic 500\n           500 https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages\n        5:20.10.233-0ubuntu-bionic 500\n           500 https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages\n        5:20.10.223-0ubuntu-bionic 500\n           500 https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages\n        5:20.10.213-0ubuntu-bionic 500\n           500 https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages\n        5:20.10.203-0ubuntu-bionic 500\n\n       ...\n\n\nInstall Docker CE:\n\n      sudo apt install docker-ce -y\n\n2. Check the correctness of the installation\n\nConnect to the terminal.\n\n   All further actions listed below are to be performed in this terminal.\n\nMake sure that Docker CE is installed and running:\n\n      sudo systemctl status docker --no-pager -l\n\n   The command's output should contain the information that the service is in active (running) state.\n\n   Example output\n\n      ● docker.service - Docker Application Container Engine\n      Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)\n      Active: active (running) since Thu 2023-03-23 10:32:23 UTC; 1min 59s ago\n        Docs: https://docs.docker.com\n    Main PID: 8981 (dockerd)\n       Tasks: 7\n      CGroup: /system.slice/docker.service\n              └─8981 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n\n   Mar 23 10:32:20 ubuntu18 dockerd[8981]: time=\"2023-03-23T10:32:20.707940356Z\" level=info msg=\"[core] [Channel #4] Channel Connectivity change to READY\" module=grpc\n   Mar 23 10:32:22 ubuntu18 dockerd[8981]: time=\"2023-03-23T10:32:22.407748502Z\" level=info msg=\"Loading containers: start.\"\n   Mar 23 10:32:22 ubuntu18 dockerd[8981]: time=\"2023-03-23T10:32:22.569711943Z\" level=info msg=\"Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address\"\n   Mar 23 10:32:22 ubuntu18 dockerd[8981]: time=\"2023-03-23T10:32:22.648337609Z\" level=info msg=\"Loading containers: done.\"\n   Mar 23 10:32:23 ubuntu18 dockerd[8981]: time=\"2023-03-23T10:32:23.214584382Z\" level=warning msg=\"WARNING: No swap limit support\"\n   Mar 23 10:32:23 ubuntu18 dockerd[8981]: time=\"2023-03-23T10:32:23.215205554Z\" level=info msg=\"Docker daemon\" commit=bc3805a graphdriver=overlay2 version=23.0.1\n   Mar 23 10:32:23 ubuntu18 dockerd[8981]: time=\"2023-03-23T10:32:23.215556611Z\" level=info msg=\"Daemon has completed initialization\"\n   Mar 23 10:32:23 ubuntu18 dockerd[8981]: time=\"2023-03-23T10:32:23.343812634Z\" level=info msg=\"[core] [Server #7] Server created\" module=grpc\n   Mar 23 10:32:23 ubuntu18 systemd[1]: Started Docker Application Container Engine.\n   Mar 23 10:32:23 ubuntu18 dockerd[8981]: time=\"2023-03-23T10:32:23.360190638Z\" level=info msg=\"API listen on /run/docker.sock\"\n\n\nGet general information about the installed Docker CE:\n\n      sudo docker info\n\n   Example output\n\n      Client:\n    Context:    default\n    Debug Mode: false\n\n   Server:\n    Containers: 0\n     Running: 0\n     Paused: 0\n     Stopped: 0\n    Images: 0\n    Server Version: 23.0.1\n    Storage Driver: overlay2\n     Backing Filesystem: extfs\n     Supports d_type: true\n     Using metacopy: false\n     Native Overlay Diff: true\n     userxattr: false\n    Logging Driver: json-file\n    Cgroup Driver: cgroupfs\n    Cgroup Version: 1\n    Plugins:\n     Volume: local\n     Network: bridge host ipvlan macvlan null overlay\n     Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n    Swarm: inactive\n    Runtimes: io.containerd.runc.v2 runc\n    Default Runtime: runc\n    Init Binary: docker-init\n    containerd version: 2456e983eb9e37e47538f59ea18f2043c9a73640\n    runc version: v1.1.4-0-g5fd4c4d\n    init version: de40ad0\n    Security Options:\n     apparmor\n     seccomp\n      Profile: builtin\n    Kernel Version: 4.15.0-88-generic\n    Operating System: Ubuntu 18.04.4 LTS\n    OSType: linux\n    Architecture: x86_64\n    CPUs: 1\n    Total Memory: 1.946GiB\n    Name: ubuntu18\n    ID: 999b21f9-7ff7-4fde-ac59-xxxxxxxxxxxx\n    Docker Root Dir: /var/lib/docker\n    Debug Mode: false\n    Registry: https://index.docker.io/v1/\n    Experimental: false\n    Insecure Registries:\n     127.0.0.0/8\n    Live Restore Enabled: false\n\n   WARNING: No swap limit support\n\n\nRun the hello-world test container:\n\n      sudo docker run --rm hello-world\n\n   The container will display a greeting message and terminate.\n\n   Example output\n\n      Hello from Docker!\n   This message shows that your installation appears to be working correctly.\n\n   To generate this message, Docker took the following steps:\n    The Docker client contacted the Docker daemon.\n    The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n       (amd64)\n    The Docker daemon created a new container from that image which runs the\n       executable that produces the output you are currently reading.\n    The Docker daemon streamed that output to the Docker client, which sent it\n       to your terminal.\n\n   To try something more ambitious, you can run an Ubuntu container with:\n    $ docker run -it ubuntu bash\n\n   Share images, automate workflows, and more with a free Docker ID:\n    https://hub.docker.com/\n\n   For more examples and ideas, visit:\n    https://docs.docker.com/get-started/\n\n","url":"/en/cases/cases-docker-ce/docker-ce-u18","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"Installing and using Docker CE on Linux","link":"/en/cases/cases-docker-ce","isActive":false,"evenSimpler":true}]},{"uuid":"aa6c17ed-fc38-4ef6-b16a-5fd9d99388c2","title":"Using Docker CE","data":"Docker CE allows working with Docker images and managing containers. To do this, the Docker CLI commands are used. The Docker command comprises options and subcommands:\n\ndocker [OPTIONS] SUBCOMMAND\n\nTo view the Docker CLI help, run the commands:\n\ndocker --help\n\ndocker SUBCOMMAND --help\n\nSee the official documentation for more details about working with Docker.\n\nBefore you begin\n\nMake sure that:\n\nYou have an access to the terminal of the host, on which Docker CE is installed.\nYou can use sudo on that host to run the commands on behalf of superuser (root).\n\nWorking with images\n\nContainers are created from the Docker images. By default, Docker pulls images from the Docker Hub (it is a Docker registry).\n\nThe Ubuntu OS image will be used as example to demonstrate how to work with images:\n\nView available Ubuntu images in Docker Hub:\n\n      sudo docker search ubuntu\n\n   The command's output will contain the list of images, which satisfy the search criteria.\n\n   Example output\n\n      NAME                             DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\n   ubuntu                           Ubuntu is a Debian-based Linux operating sys…   15748     [OK]\n   websphere-liberty                WebSphere Liberty multi-architecture images …   293       [OK]\n   open-liberty                     Open Liberty multi-architecture images based…   59        [OK]\n   neurodebian                      NeuroDebian provides neuroscience research s…   99        [OK]\n   ubuntu-debootstrap               DEPRECATED; use \"ubuntu\" instead                50        [OK]\n   ubuntu-upstart                   DEPRECATED, as is Upstart (find other proces…   112       [OK]\n   ubuntu/nginx                     Nginx, a high-performance reverse proxy & we…   83\n\n   ...\n\n\nPull the ubuntu image:\n\n      sudo docker pull ubuntu\n\n   The command's output will contain the information about the pulling progress.\n\n   Example output\n\n      Using default tag: latest\n   latest: Pulling from library/ubuntu\n   2ab09b027e7f: Pull complete\n   Digest: sha256:67211c14fa74f070d27cc59d69a7fa9aeff8e28ea118ef3babc295a0428a6d21\n   Status: Downloaded newer image for ubuntu:latest\n   docker.io/library/ubuntu:latest\n\n\nMake sure that the image was pulled by viewing the pulled images:\n\n      sudo docker images\n\n   The command's output will contain the list of pulled images.\n\n   Example output\n\n      REPOSITORY    TAG       IMAGE ID       CREATED         SIZE\n   ubuntu        latest    08d22c0ceb15   2 weeks ago     77.8MB\n   hello-world   latest    feb5d9fea6a5   18 months ago   13.3kB\n\n\nCreating and running container\n\nTo create and run a container, which uses the pulled image, employ the docker run command. This command is a combination of the docker create and the docker start commands.\n\nThe operation will be demonstrated for the previously pulled ubuntu image:\n\nCreate and run the container:\n\n      sudo docker run -it ubuntu\n\n   After running the container, you will get access to the container's terminal (TTY) with the bash session (this is achieved with -it options). As a result, you will get the bash command line prompt, which looks like:\n\n      root@8502eb90112b:/#\n\n   In this example output, 8502eb90112b is an identifier of the created container.\n\nTo check that the container is operational, install Node.js into it.\n\n   Run the command:\n\n            apt update && apt install nodejs -y\n\n   View the information about installed Node.js version:\n\n            node -v\n\n      Example output:\n\n            v12.22.9\n\nExit the container's bash command line:\n\n      exit\n\n   The container will be stopped.\n\nManaging containers\n\nTo manage containers it is necessary to obtain their identifiers. To do this, view the list of the containers:\n\n   The list of all containers\n   The list of the running containers\n\n\n      sudo docker ps -a\n\n\n      sudo docker ps\n\n   Example docker ps -a output\n\n      CONTAINER ID   IMAGE         COMMAND       CREATED          STATUS                       PORTS     NAMES\n   8502eb90112b   ubuntu        \"/bin/bash\"   11 minutes ago   Exited (127) 7 seconds ago             sharp_tharp\n   794ef863c235   hello-world   \"/hello\"      19 hours ago     Exited (0) 19 hours ago                dazzling_keldysh\n\n\n   A container identifier is listed in the CONTAINER ID column.\n\nManage a container, using its identifier:\n\n   Start container\n   Stop container\n   Remove container\n\n\n      sudo docker start\n\n   Example command to start the ubuntu container with the 8502eb90112b identifier:\n\n      sudo docker start 8502eb90112b\n\n\n      sudo docker stop\n\n   Example command to stop the ubuntu container with the 8502eb90112b identifier:\n\n      sudo docker stop 8502eb90112b\n\n\n      sudo docker rm\n\n   It is impossible to remove the running container. Either stop it first, or do removal with the --force option.\n\n   Example command to remove the ubuntu container with the 8502eb90112b identifier:\n\n      sudo docker rm 8502eb90112b\n\n","url":"/en/cases/cases-docker-ce/using-docker-ce","description":"How to use installed Docker CE.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"Installing and using Docker CE on Linux","link":"/en/cases/cases-docker-ce","isActive":false,"evenSimpler":true}]},{"uuid":"1b4993c3-fe71-48ed-be6e-6c226c9eef19","title":"Configuring Application Auto-Deployment to a Kubernetes Cluster","data":"In this article, we will look at how to set up auto-deployment of an application to a Kubernetes cluster.\n\nBefore that:\n\nInstall and configure Docker.\nInstall and configure GitLab.\nInstall and configure Harbor.\n\nSet up GitLab-runner\n\nGitLab-runner is an environment for autobuilding GitLab projects. To set up autobuild, install and register runner with GitLab. You can register a specific runner for each project (specific runner) or a common runner for multiple projects (shared runner). Let's set up a common runner.\n\nFor this:\n\nSign in to the GitLab web interface with administrator rights:\n\n\n\nCopy the registration token and run the following in the console on the server where GitLab-runner is installed:\n\nroot@ubuntu-std3-2-4-40gb:~# docker exec -it gitlab-runner gitlab-runner register -n --url https:/// --executor docker --registration-token ua2k238fbMtAxMBBRf_z -- description \"shared-runner\" --docker-image=\"docker:dind\" --tag-list \"shared_runner\" --docker-privileged --docker-volumes /var/run/docker.sock:/var/run/docker .sock\n\n\n\nAttention\n\n\\--tag-list - specifies the tags that runner will accept. Further in the project, this tag is specified so that the project is assembled by this runner.\n\n\\--docker-privileged and --docker-volumes are required for the running Docker container to have access to the parent Docker to build images (details read here).\n\n\n\nAs a result, runner will be displayed in the web interface:\n\n\n\nSet the runtime variables. To do this, select Settings / CI CD and next to Variables click Expand:\n\n\n\nSet a few variables that will be used later in the autobuild file .gitlab-ci.yml:\n\n\n\nVariables:\n\nDOCKER_USER - user to access the repository in Harbor. In our example, k8s.\nDOCKER_PASSWORD - The password for the k8s user that you entered when you created the user in Harbor.\n\nPlease note that Masked is enabled for the password - thanks to this, when you try to display text in a variable in the script, it is masked and the password is not visible.\n\nDOCKER_REGISTRY is the name of the host where the Harbor is located. In our example, .\n\nSet up the autobuild file\n\nGo to the directory with the downloaded repository and in a text editor create a .gitlab-ci.yml file with the following content:\n\nimage:docker:latest\n\nstages:\n  builds\n  test\n  release\n\nvariables:\n  REGISTRY_URL: https://$DOCKER_REGISTRY:8443\n  IMAGE: $DOCKER_REGISTRY:8443/$DOCKER_USER/$CI_PROJECT_NAME:$CI_COMMIT_REF_NAME\n  RELEASE: $DOCKER_REGISTRY:8443/$DOCKER_USER/$CI_PROJECT_NAME:latest\n\nbefore_script:\n   docker login $REGISTRY_URL -u $DOCKER_USER -p $DOCKER_PASSWORD\n\nbuild:\n  stage:builds\n  tags:\n    shared_runner\n  script:\n   cd app && docker build --pull -t $IMAGE .\n   docker push $IMAGE\n\nrelease:\n  stage: release\n  tags:\n    shared_runner\n  script:\n    docker pull $IMAGE\n    docker tag $IMAGE $RELEASE\n    docker push $RELEASE\n  only:\nmaster\n\nConsider this file.\n\nGeneral Purpose Variables:\n\nimage - specifies the docker image in which the build will run. Since we are building a Docker image, we need an image that contains the utilities we need to build. Usually image docker:latest is used.\nstages - describes the stages of image assembly. In our example, the test stage is skipped.\n\nVariables used for work:\n\nbefore_script - the stage that is executed first. We sign in to the register using the variables that are specified in the GitLab runner settings.\nbuild - image build. Standard build of a Docker image using a Dockerfile in the repository.\n\n\n\nImportant!\n\ntags: shared_runner is the tag that was specified when registering the runner. Specifying this tag in the .gitlab-ci.yml file allows GitLab-runner to execute this script. After assembly, the assembled image is entered into the registry with the CI_COMMIT_REF_NAME tag. For details on the variables that can be used during the build read the article). In our example, since we are committing to the master branch, the image name will be k8s/k8s-conf-demo:master.\n\n\n\nrelease - final image generation section. In our example, we simply take the image built in the previous stage, add the latest tag to it, and upload it to the repository.\n\nUpload the created file to the repository:\n\nash-work:k8s-conf-demo git add .\nash-work:k8s-conf-demo git commit -m \"create .gitlab-ci.yml\"\n[master 55dd5fa] create .gitlab-ci.yml\n1 file changed, 1 insertion(+), 1 deletion(-)\nash-work:k8s-conf-demo git push\nEnumeration of objects: 5, done.\nObject count: 100% (5/5), done.\nCompressing changes uses up to 4 streams\nObject compression: 100% (3/3), done.\nObject Writing: 100% (3/3), 299 bytes | 299.00 KiB/s, done.\n3 total (2 changes), 0 reused (0 changes)\nTo testrom.ddns.net:ash/k8s-conf-demo.git\n7c91eab..55dd5fa master -> master\n\nAs soon as the .gitlab-ci.yml file appears in the repository, GitLab will automatically start building it.\n\nYou can see how the build is going in the GitLab web interface in the project, CI / CD / Pipelines:\n\n\n\nBy clicking on running, you can see the current progress of the build:\n\n\n\nBy clicking on the build stage, you can see the build console and what is happening in it. Example for the build stage:\n\n\n\nExample for the release stage:\n\n\n\nThe console logs show that both build and release completed successfully. The assembled image was posted in the Harbor repository, which can be seen in the corresponding web interface:\n\nDeploying an application to a Kubernetes cluster\n\nAfter successfully building the project, we will set up auto-deployment of the application to the Kubernetes cluster. For example, we use Kubernetes cluster from VK Cloud.\n\nAfter the cluster is deployed in the cloud, a configuration file of the form kubernetes-cluster-5011_kubeconfig.yaml is loaded on the local computer, intended for authorization in the cluster for utilities like kubectl.\n\nConnect the configuration file:\n\nash-work:~ export KUBECONFIG=kubernetes-cluster-5011_kubeconfig.yaml\n\nMake sure that authorization is successful and the cluster is healthy:\n\nash-work:~ kubectl cluster-info\nKubernetes master is running at https://89.208.197.244:6443\nCoreDNS is running at https://89.208.197.244:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\nThe cluster responds.\n\n3 Grant the cluster access rights to the Harbor image repository. To do this, create the following secret:\n\nash-work:~ kubectl create secret docker-registry myprivateregistry --docker-server=https://:8443 --docker-username=k8s --docker-password=\nsecret/myprivateregistry created.\nwhere ` is the Harbor server name, ` is the Harbor user k8s password.\n\nVerify that the secret was successfully created:\n\nash-work:~ kubectl get secret myprivateregistry --output=\"jsonpath={.data.\\.dockerconfigjson}\" | base64 --decode\n{\"auths\":{\"https://:8443\":{\"username\":\"k8s\",\"password\":\"\",\"auth\":\"sdasdsdsdsdsdsdsdsdsdssd==\"}}}%\n\nCreate a deployment.yml file with the following content:\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: myapp\nspec:\nselector:\nmatch Labels:\nrun: myapp\ntemplate:\nmetadata:\nlabels:\nrun: myapp\nspec:\ncontainers:\n-name:myapp\nimage: :8443/k8s/k8s-conf-demo:latest\nimagePullPolicy: Always\nenv:\nname: HTTP_PORT\nvalue: \"8081\"\nports:\ncontainerPort: 8081\nimagePullSecrets:\n-name: myprivateregistry\n\nApply this file:\n\nash-work:~ kubectl create -f deployment.yaml\ndeployment.apps/myapp-deployment created\n\nAfter a while, make sure that the container has risen:\n\nash-work:~ kubectl get pods\nNAME READY STATUS RESTARTS AGE\nmyapp-deployment-66d55bcbd5-s86m6 1/1 Running 0 39s\n\nCreate a service.yml file:\n\napiVersion: v1\nkind: Service\nmetadata:\nname: myapp-svc\nlabels:\nrun: myapp\nspec:\nports:\n-protocol:TCP\nport: 8081\ntargetPort: 8081\nselector:\nrun: myapp\n\nCreate a service:\n\nash-work:~ kubectl create -f service.yaml\nservice/myapp-svc created\n\nTo provide access to the application from the external network, configure the ingress controller. To do this, create a ingress.yaml file:\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\nname: myapp-ingress\nspec:\nrules:\nhost: echo.com\nhttp:\npaths:\npath: /\nbackend:\nserviceName: myapp-svc\nservicePort: 8081\n\nIn this file, specify the domain, when accessed, the transition to the application will be performed. You can specify any domain, we will write it locally for tests.\n\nApply the ingress controller:\n\nash-work:~ kubectl create -f ingress.yaml\ningress.extensions/myapp-ingress created\n\nView the state of the ingress controller:\n\nash-work:~ kubectl describe ingress myapp-ingress\nName: myapp-ingress\nNamespace:default\naddress:\nDefault backend: default-http-backend:80 ()\nRules:\nHost Path Backends\n---- --------\necho.com\n/ myapp-svc:8081 (10.100.69.71:8081)\nAnnotations:\nevents:\nType Reason Age From Message\n------ ---- ---- -------\nNormal CREATE 45s nginx-ingress-controller Ingress default/myapp-ingress\nNormal UPDATE 5s nginx-ingress-controller Ingress default/myapp-ingress\n\nThe external IP address associated with the ingress controller can be viewed in the web interface of the Mail.ru cloud. It's called the load balancer IP address for the Ingress Controller. Let's designate it as .\n\nLet's test the application:\n\nash-work:~ curl --resolve echo.com:80: http://echo.com/handler\nOK%\n\nThe --resolve option is responsible for the local resolve when requesting curl, since we came up with the domain ourselves and there is no real resolve.\n\nThus, we have deployed the application to the Kubernetes cluster manually.\n\nDelete the created:\n\nash-work:~ kubectl delete -f ingress.yaml\ningress.extensions \"myapp-ingress\" deleted\nash-work:~ kubectl delete -f service.yaml\nservice \"myapp-svc\" deleted\nash-work:~ kubectl delete -f deployment.yaml\ndeployment.apps \"myapp\" deleted\n\nDeploying an application to a Kubernetes cluster using GitLab CI/CD\n\nGitLab supports Kubernetes cluster integration by default. To set up the integration, get a few cluster options.\n\nFor this:\n\nGet the API URL:\n\nash-work:~ kubectl cluster-info | grep 'Kubernetes master' | awk '/http/ {print $NF}'\nhttps://89.208.197.244:6443\n\nGet a list of cluster secrets:\n\nash-work:~ kubectl get secrets\nNAME TYPE DATA AGE\ndashboard-sa-token-xnvmp kubernetes.io/service-account-token 3 41h\ndefault-token-fhvxq kubernetes.io/service-account-token 3 41h\nmyprivateregistry kubernetes.io/dockerconfigjson 1 39h\nregcred kubernetes.io/dockerconfigjson 1 39h\n\nGet the PEM certificate of the default-token-\\* secret:\n\nash-work:~ kubectl get secret default-token-fhvxq -o jsonpath=\"{'data'}\" | base64 --decode\nBEGIN CERTIFICATE-----\nMIIC9DCCAdygAwIBAgIQQf4DP2XYQaew1MEtxJtVBzANBgkqhkiG9w0BAQsFADAi\nMSAwHgYDVQQDDBdrdWJlcm5ldGVzLWNsdXN0ZXItNTAxMTAeFw0xOTEyMDkxNTI2\nMDlaFw0yNDEyMDgxNTI2MDlaMCIxIDAeBgNVBAMMF2t1YmVybmV0ZXMtY2x1c3Rl\nci01MDExMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA47Nd4cEMEdtW\nyxo3VEm02wB+k7HytzchyYOlxJdYhQV4yjWR8MpAd9JKWgOdJ/qzitIjYdr0cKCI\ndLxRmKWGJJhTYZ4yBQS3XJ52n6bpV1Nzj0Xsq9Bxs7OgG1T4oZn7FXY4ZrJ10w0s\nwa0w5AbU2LbpprWsNki2uFkUusgtUSLSSwe90yVKT5ZnW3kUrmMZlY3ys4KLhDbA\nCS5xs03t10apRjfRq4WQ0ja+AYkzvwnpiX5nnJk2YCn31c4tVUSuoblzoWEokD2v\nDLzZaHAg53Payp2PUP7S5kMCjfrRIEBO7SULve/P/7GRJEHzzOREn/qMSOWK5u1O\nk1yk4ARP4wIDAQABoyYwJDASBgNVHRMBAf8ECDAGAQH/AgEAMA4GA1UdDwEB/wQE\nAwICBDANBgkqhkiG9w0BAQsFAAOCAQEAYxdbkMTOL4/pbQmJNIY54y+L8eGqEzTc\nis9KAZmoD4t4A88r1xZ/dp/3PVhFaOutQh6H7FzNDEiWbTFUa3edGXBmL4jB05Tm\nepj1iYEY6Nv5KGoSZutZ11Y8W+77pu9zKzzbtXMyCsYpPWrPyXiP1Z1zY6F4MtAQ\nGF9ONh9lDVttkFjaerKR4y4/E/X+e2Mi2dsyJmVHCrZTHozy8oZayC//JfzS+pK9\n2IvcwlBgp9q4VO+lmkozWzWcO5mjk/70t7w5UHNpJOxeMzbhx6JkWZ9bN+Ub7RHN\n1PUeNfZJKHEgSZw8M+poK3SqsyGMQ13geGXpM85VQvrqCW43YfgjiQ==\nEND CERTIFICATE-----\n\nNow create a gitlab-admin-service-account.yaml file that describes GitLab's access rights to the cluster. File contents:\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: gitlab-admin\nnamespace: kube-system\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\nname: gitlab-admin\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: ClusterRole\nname: cluster-admin\nsubjects: subjects\n-kind: ServiceAccount\nname: gitlab-admin\n  namespace: kube-system\n\nApply rights:\n\nash-work:~ kubectl apply -f gitlab-admin-service-account.yaml\nserviceaccount/gitlab-admin created\nclusterrolebinding.rbac.authorization.k8s.io/gitlab-admin created\n\nAnd get the cluster access token:\n\nash-work:~ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep gitlab-admin | awk '{print $1}')\nName: gitlab-admin-token-kcmd8\nNamespace: kube-system\nLabels:\nAnnotations: kubernetes.io/service-account.name: gitlab-admin\nkubernetes.io/service-account.uid: d9aa6095-6086-4430-b1ae-711df5765064\n\n\nType: kubernetes.io/service-account-token\n\n\nData\n\nca.crt: 1087 bytes\nnamespace: 11 bytes\n\n\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJnaXRsYWItYWRtaW4tdG9rZW4ta2NtZDgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZ2l0bGFiLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZDlhYTYwOTUtNjA4Ni00NDMwLWIxYWUtNzExZGY1NzY1MDY0Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmdpdGxhYi1hZG1pbiJ9.CaBJMUdWwTPGBla1OZZnsftdUue1-XSyF-SEaHhNdWaUkX_5aUi4uZrgx0UGLbSOFkTmij2_lv1lAkm9-W4VCi4z9cVjw41o6TA6279rx_HEammNzFV8v1HvpSkMXH8wVzaoLwtVQehM7fozykgv4y3wmHAe-T0vXNRN48FYmDXReRSdGuldV--OZLZeOVGrRIkttXoMoSVW_LbnOiBJU4NUQq4dNpvklQkLTSBowu-E0lDJJoMQjniSO1j8H8fmy7Micpgy20Hi1RIoJWfPj-EY3CyhjMht8iTIokQHgHgpCY_RQPexJqHiXTQgyZ93WNw8foIfISduNXyynfGzmQ\n\n\nGo to the GitLab admin interface and click Add Kubernetes Cluster:\n\n\n\nSelect the Add Existing cluster tab, enter the previously remembered parameters (API URL, PEM, Token) and click Add Kubernetes Cluster:\n\n\n\nCluster added:\n\n\n\nPlace the files deployment.yaml, service.yaml, ingress.yaml in the deployments directory of the project.\n\nAdd the deploy section to the .gitlab-ci.yml file:\n\nimage:docker:latest\n\nstages:\nbuild\ntest\nrelease\ndeploy\n\nvariables:\nREGISTRY_URL: https://$DOCKER_REGISTRY:8443\nIMAGE: $DOCKER_REGISTRY:8443/$DOCKER_USER/$CI_PROJECT_NAME:$CI_COMMIT_REF_NAME\nRELEASE: $DOCKER_REGISTRY:8443/$DOCKER_USER/$CI_PROJECT_NAME:latest\nbefore_script:\ndocker login $REGISTRY_URL -u $DOCKER_USER -p $DOCKER_PASSWORD\n\nbuild:\nstage: build\ntags:\nshared_runner\nscript:\ncd app && docker build --pull -t $IMAGE .\ndocker push $IMAGE\n\nrelease:\nstage: release\ntags:\nshared_runner\nscript:\ndocker pull $IMAGE\ndocker tag $IMAGE $RELEASE\ndocker push $RELEASE\nonly:\nmaster\n\ndeploy:\nstage: deploy\nbefore_script:\napk add --no-cache curl\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/ amd64/kubectl\nchmod +x ./kubectl\ntags:\nshared_runner\nenvironment:\nname: production\nscript:\necho $KUBECONFIG\nexport KUBECONFIG=$KUBECONFIG\n./kubectl create secret docker-registry myprivateregistry --docker-server=$REGISTRY_URL --docker-username=$DOCKER_USER --docker-password=$DOCKER_PASSWORD --dry-run -o yaml | ./kubectl apply -f -\n./kubectl apply -f manifests/deployment.yaml\n./kubectl apply -f manifests/service.yaml\n./kubectl apply -f manifests/ingress.yaml\n./kubectl rollout restart deployment\n\nConsider the deploy section.\n\nIn the before_script section, curl is installed into the system, with its help the latest stable version of kubectl is downloaded.\n\nScript section: Since the cluster is managed by GitLab, there are preset variables - KUBECONFIG stores the name of the cluster access configuration file.\n\nSince the namespace is set automatically, in this namespace you need to create a secret with a login and password to access our register, which stores the application image compiled at the release stage.\n\nNext, the deploy, service, and ingress controller manifests are applied.\n\nThe last command restarts the deployment to download the new version of the application.\n\nThe result of executing the deploy section:\n\n\n\nCheck what was created in the cluster. We look at the namespace:\n\nash-work:~ kubectl get namespaces\nNAME STATUS AGE\ndefault Active 45h\ngitlab-managed-apps Active 67m\ningress-nginx Active 45h\nk8s-conf-demo-1-production Active 57m\nkube-node-lease Active 45h\nkube-public Active 45h\nkube-system Active 45h\nmagnum-tiller Active 45h\n\n\nOur namespace is k8s-conf-demo-1-production. Let's look at pods, services and ingress:\n\nash-work:~ kubectl get pods -n k8s-conf-demo-1-production\nNAME READY STATUS RESTARTS AGE\nmyapp-65f4bf95b5-m9s8l 1/1 Running 0 39m\nash-work:~ kubectl get services -n k8s-conf-demo-1-production\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\nmyapp-svc ClusterIP 10.254.243.199  8081/TCP 32m\nash-work:~ kubectl get ingress -n k8s-conf-demo-1-production\nNAME HOSTS ADDRESS PORTS AGE\nmyapp-ingress echo.com 80 32m\nash-work:~\n\nCheck the health of the application:\n\nash-work:~ curl --resolve echo.com: http://echo.com/handler\nOK%\n\nTo test auto-deploy, modify the application code a little. In the app/app.py repository file, change the return 'OK' line to return 'HANDLER OK'.\n\nCommit the changes:\n\nash-work:k8s-conf-demo git add . && git commit -m \"update\" && git push\n[master b863fad] update\n1 file changed, 1 insertion(+), 1 deletion(-)\n\nEnumeration of objects: 7, done.\nObject count: 100% (7/7), done.\nCompressing changes uses up to 4 streams\nObject compression: 100% (4/4), done.\nWrite objects: 100% (4/4), 359 bytes | 359.00 KiB/s, done.\nTotal 4 (changes 3), reused 0 (changes 0)\n\nWait for the CI/CD execution to finish and check the application output again:\n\nash-work:~ curl --resolve echo.com: http://echo.com/handler\nHANDLER OK%\n\nThe auto-deployment of the new version was successful.\n\n\n\nNote\n\nThe configuration files given in this article are test files and are intended for mastering the mechanisms of GitLab, registering and deploying images to a cluster at an initial level.\n\n\n","url":"/en/cases/cases-gitlab/case-k8s-app","description":"How to set up GitLab, Harbor, and auto-deployment of an application to a Kubernetes cluster.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"CI/CD based on GitLab","link":"/en/cases/cases-gitlab","isActive":false,"evenSimpler":true}]},{"uuid":"107a346d-053e-48c3-b85b-9b35d1e3c7be","title":"Installing and configuring GitLab","data":"GitLab is a tool for collaborating on software development projects. It provides storage and management of Git repositories, as well as code version control. GitLab automates CI/CD processes: building, testing, and deploying software. To launch and automatically execute CI/CD tasks, GitLab uses the GitLab Runner application.\n\nBelow is an example of installing the free versions of GitLab and GitLab Runner on a virtual machine running Ubuntu 22.04. The Docker containerization platform and its Docker Compose plugin are used as the installation tool.\n\nPreparatory steps\n\nRegister at VK Cloud.\nCreate a network network1 with internet access and a subnet with the address 10.0.0.0/24.\nCreate a security group gitlab and add inbound permissions to it for the ports:\n\n   80 (HTTP),\n   443 (HTTPS),\n   22 (SSH),\n   35242 (SSH).\n\n   Instead of 35242, you can use any port that is not reserved by the operating system.\n\nCreate a VM running Ubuntu 22.04.\n\n   When selecting VM options, consider the hardware requirements for installing GitLab and GitLab Runner.\n\n   The following VM configuration is used as an example:\n\n     name: OA-Ubuntu-docker;\n     operating system: Ubuntu 22.04;\n     network: network1 with subnet 10.0.0.0/24;\n     flavor: STD2-4-12;\n     network HDD drive: 50 GB;\n     public IP address: assigned, further will be used 185.185.185.185;\n     security groups: default, gitlab.\n\n\n   When creating a VM running Ubuntu, a user is automatically created with the ubuntu name and full rights to use sudo.\n\n\n(Optional) Assign a domain name to the VM to use for accessing GitLab. Do it in one of the following ways:\n\n   If you have a domain, add your VM to it.\n   If you do not have a domain, use, for example, NoIP — one of the services that provides dynamic DNS. For this, install a dynamic update client on the VM. More details in the NoIP service documentation.\n\n        What is the purpose of assigning a domain name?\n        You can install GitLab on a VM that does not have a domain name. During installation, self-signed SSL certificates will be automatically issued for connecting to GitLab over HTTPS. However, you will not be able to issue a public SSL certificate for GitLab. As a result, when opening the authorization page of your GitLab server, users will see the warning: “The connection is not secure.”.\n\n\nConnect to the OA-Ubuntu-docker VM via SSH.\nCheck the status of your operating system firewall and disable it if it is active:\n\n        sudo ufw status\n    sudo ufw disable\n\n\n    There is no need for the operating system firewall, since the VK Cloud firewall controls the incoming and outbound traffic on the VM.\n\n\nInstall and configure Docker.\nInstall the Docker Compose plugin:\n\n   Update the list of available Ubuntu packages and their versions:\n\n                sudo apt-get update\n\n   Install the latest version of the Docker Compose plugin:\n\n                sudo apt-get install docker-compose-plugin\n\n   Verify that the plugin is installed correctly by requesting its version:\n\n                docker compose version\n\n        Expected result:\n\n                Docker Compose version vN.N.N\n\n        Here N.N.N is the plugin version number.\n\n1. Change the port for connecting to the VM via SSH\n\nFor access via SSH, GitLab by default uses port 22, which is reserved by the operating system for connecting to the VM via SSH. To avoid conflict, change the system SSH port to another one.\n\nOpen the /etc/ssh/sshd_config file for editing:\n\n        sudo nano /etc/ssh/sshd_config\n\nReplace the line #Port 22 with Port 35242.\n\n\n    The number 35242 is used as an example. If you are going to use a different number, open it for SSH connections in the VM firewall settings. More details in the Managing firewall rules section.\n\n\nSave the file and exit the editor by pressing CTRL+O and then CTRL+X.\n\nRestart the sshd service:\n\n        sudo systemctl restart sshd\n\nClose the current connection to the VM:\n\n        exit\n\nConnect to the OA-Ubuntu-docker VM via SSH using the new port:\n\n        ssh -i  ubuntu@185.185.185.185 -p 35242\n\n    Instead of 185.185.185.185 you can use the fully qualified domain name of the VM if it exists.\n\n2. Install GitLab and GitLab Runner using Docker Compose\n\nCreate directories for the GitLab persistent repository by sequentially running the commands:\n\n        sudo mkdir -p /opt/gitlab\n    sudo mkdir -p /opt/gitlab/config\n    sudo mkdir -p /opt/gitlab/logs\n    sudo mkdir -p /opt/gitlab/data\n    sudo mkdir -p /opt/gitlab-runner\n    sudo mkdir -p /opt/gitlab-runner/config\n    sudo mkdir -p /opt/gitlab-runner/data\n\nCreate and open for editing a Docker Compose configuration file:\n\n        sudo nano docker-compose.yml\n\nCopy the following content into the editor window, replacing 185.185.185.185 with the external IP address of the VM or its fully qualified domain name:\n\n      Content of the docker-compose.yml file\n\n            version: '3.7'\n      services:\n        gitlab:\n          container_name: gitlab\n          image: 'gitlab/gitlab-ce:latest'\n          restart: always\n          hostname: '185.185.185.185'\n          environment:\n            GITLAB_OMNIBUS_CONFIG: |\n              external_url 'https://185.185.185.185'\nAdd any other gitlab.rb configuration parameters here, each on its own line\n          ports:\n            '80:80'\n            '443:443'\n            '22:22'\n          volumes:\n            '/opt/gitlab/config:/etc/gitlab'\n            '/opt/gitlab/logs:/var/log/gitlab'\n            '/opt/gitlab/data:/var/opt/gitlab'\n\n        gitlab-runner:\n          container_name: gitlab-runner\n          image: gitlab/gitlab-runner:latest\n          restart: always\n          volumes:\n            '/opt/gitlab-runner/data:/home/gitlab_ci_multi_runner/data'\n            '/opt/gitlab-runner/config:/etc/gitlab-runner'\n            '/var/run/docker.sock:/var/run/docker.sock:rw'\n          environment:\n            CI_SERVER_URL=https://185.185.185.185/ci\n\n\n    If necessary, specify additional settings below the # Add… comment line. The configuration parameters for the Docker Compose plugin are described in the official Docker documentation.\n\n\nSave the file and exit the editor by pressing CTRL+O and then CTRL+X.\nLaunch the Docker Compose plugin:\n\n        sudo docker compose up -d\n\n      Output upon successful operation\n\n            [+] Running 13/13\n      ✔ gitlab-runner 3 layers [⣿⣿⣿]      0B/0B      Pulled                 19.4s\n      ✔ 527f5363b98e Pull complete                                            1.7s\n      ✔ 5aa2f01642ad Pull complete                                            5.8s\n      ✔ 112312283fb7 Pull complete                                            2.2s\n      ✔ gitlab 8 layers [⣿⣿⣿⣿⣿⣿⣿⣿]      0B/0B      Pulled                 83.0s\n      ✔ 3dd181f9be59 Pull complete                                            0.9s\n      ✔ 5222e10cb5b3 Pull complete                                            0.7s\n      ✔ b86fffbd1d96 Pull complete                                            0.6s\n      ✔ a8f85f865bd2 Pull complete                                            1.0s\n      ✔ fd086081fce9 Pull complete                                            1.2s\n      ✔ 9c3df03dc259 Pull complete                                            1.4s\n      ✔ 539bd3fbd6f5 Pull complete                                            1.5s\n      ✔ fceb275916b3 Pull complete                                           13.3s\n      [+] Running 3/3\n      ✔ Network ubuntu_default   Created                                      1.4s\n      ✔ Container gitlab         Started                                     49.4s\n      ✔ Container gitlab-runner  Started                                     49.4s\n\n\n3. Check the status of your GitLab container\n\nRun the command:\n\nsudo docker ps\n\nOutput upon successful operation:\n\nCONTAINER ID   IMAGE                         COMMAND                  CREATED         STATUS                            PORTS                                                                                                         NAMES\n1e6cee4fe37a   gitlab/gitlab-ce:latest       \"/assets/wrapper\"        4 minutes ago   Up 9 seconds (health: starting)   0.0.0.0:22->22/tcp, :::22->22/tcp, 0.0.0.0:80->80/tcp, :::80->80/tcp, 0.0.0.0:443->443/tcp, :::443->443/tcp   gitlab\n882fc3fb80f5   gitlab/gitlab-runner:latest   \"/usr/bin/dumb-init …\"   4 minutes ago   Up 4 minutes                                                                                                                                    gitlab-runner\n\n4. (Optional) Issue a public SSL certificate for the VM\n\nIf the VM on which GitLab is installed has a domain name, you can issue a public SSL certificate for your GitLab instance, for example from Let’s Encrypt. More details in the official GitLab documentation.\n\n5. Check the functionality of GitLab\n\nObtain and copy the automatically generated GitLab admin password:\n\n        sudo cat /opt/gitlab/config/initial_root_password\n\n    Output upon successful operation:\n\nWARNING: This value is valid only in the following conditions\n1. If provided manually (either via GITLAB_ROOT_PASSWORD environment variable or via gitlab_rails['initial_root_password'] setting in gitlab.rb, it was provided before database was seeded for the first time (usually, the first reconfigure run).\n2. Password hasn't been changed manually, either via UI or via command line.\n\nIf the password shown here doesn't work, you must reset the admin password following https://docs.gitlab.com/ee/security/reset_user_password.html#reset-your-root-password.\n\n    Password: /XR7tRH_ХХХХ=\n\nNOTE: This file will be automatically deleted in the first reconfigure run after 24 hours.\n\nIn the browser, go to https://185.185.185.185.\n\n    Instead of 185.185.185.185 you can use the fully qualified domain name of the VM if it exists.\n\n    The GitLab authorization page will open.\n\nTo sign in, use the administrator login (root) and the copied password.\n\n    The GitLab dashboard will open. The installation is complete, GitLab is ready for operation.\n\nDelete unused resources\n\nDeployed virtual resources are charged. If you do not need them anymore:\n\nDelete the OA-Ubuntu-docker VM.\nIf necessary, delete the floating IP address 185.185.185.185.\n","url":"/en/cases/cases-gitlab/case-gitlab","description":"How to install and configure GitLab on an Ubuntu virtual machine in VK Cloud in order to configure auto-deployment of applications","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"CI/CD based on GitLab","link":"/en/cases/cases-gitlab","isActive":false,"evenSimpler":true}]},{"uuid":"3ba1e554-7698-4df7-8d8b-1594826b3d14","title":"Installing and configuring Harbor","data":"In this article, we will look at how to install and configure Harbor repository repositories. After that, you can configure application auto-deployment to a Kubernetes cluster.\n\nInstalling the Harbor repository repository\n\nBefore installing Harbor:\n\nInstall and configure Docker.\nInstall and configure GitLab.\n\nHarbor is installed as a Docker image using the online installer.\n\nTo install the Harbor repositories:\n\nDownload the online installer script and unzip it:\n\nroot@ubuntu-std3-2-4-40gb:~# wget https://github.com/goharbor/harbor/releases/download/v1.9.3/harbor-online-installer-v1.9.3.tgz\nroot@ubuntu-std3-2-4-40gb:~# tar -zxvf harbor-online-installer-v1.9.3.tgz\n\nIn the resulting harbor directory, configure the harbor.yml file:\n\nhostname:\nhttp:\nport for http, default is 80. If https enabled, this port will redirect to https port\n    port: 8080\nhttps related config\n    https:\n# https port for harbor, default is 443\n    port: 8443\n# The path of cert and key files for nginx\n    certificate: /opt/gitlab/config/ssl/.crt\n    private_key: /opt/gitlab/config/ssl/.key\nThe default data volume\ndata_volume: /opt/harbor\n\nAnd:\n\nThe hostname is the same as the GitLab name because the deployment is done on the server that hosts GitLab.\nNon-standard HTTP and HTTPS ports are used, as the standard ones are used by the GitLab web interface.\nUses the LetsEncrypt certificate and key that was created when you set up GitLab.\n\nRun the install.sh script:\n\nroot@ubuntu-std3-2-4-40gb:~/harbor# ./install.sh\n\n[Step 0]: checking installation environment ...\nNote: docker version: 19.03.5\nNote: docker-compose version: 1.25.0\n\n[Step 1]: preparing environment ...\n[Step 2]: starting Harbor ...\nCreating harbor-log ... done\nCreating registryctl ... done\nCreating redis ... done\nCreating harbor-db ... done\nCreating harbor portal ... done\nCreating registry ... done\nCreating harbor-core ... done\nCreating nginx ... done\nCreating harbor-jobservice ... done\n\n✔ ----Harbor has been installed and started successfully.----\n\nHarbor launched.\n\nSetting up the repository repository Harbor\n\nSign in to Harbor.\n\nSince we are using non-standard ports, the URL will look like this:\n\nhttps://:8443\n\nThe default login is admin. The primary password is set in the harbor.yml file (default is Harbor12345).\n\n**\n\nChange the password for the admin user. To do this, click on admin in the upper right corner and select Change Password:\n\n**\n\nSpecify the current and new passwords:\n\n**\n\nCreate a user under which GitLab will work with Harbor. To do this, select Users on the left:\n\n\n\nSpecify the data of the new user:\n\n\n\n\n\nAttention\n\nRemember the password entered for the user, it will be required for integration with GitLab.\n\n\n\nCreate a new project that will contain the built images from GitLab. To do this, select Projects on the left:\n\n\n\nEnter the details of the new project:\n\n\n\nAdd a user to the project:\n\n**\n\nSpecify a name and role for the user:\n\n\n\nNow set up auto-deployment of the application to the Kubernetes cluster.\n","url":"/en/cases/cases-gitlab/case-harbor","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"CI/CD based on GitLab","link":"/en/cases/cases-gitlab","isActive":false,"evenSimpler":true}]},{"uuid":"13c45a64-6373-4170-b300-3c279414fd97","title":"Installing LAMP on Linux","data":"The LAMP stack includes a Linux operating system, an Apache web server, a MySQL database management system, and a server-side scripting language for processing dynamic PHP content. All this is used to support dynamic websites and web applications.\n\nThis instruction will help you deploy an Apache server in the Ubuntu 22.04 operating system in VK Cloud, install PHP, and configure a DNS record for domain name access. MySQL 8.0 Single configuration is used as a DBMS.\n\nPreparatory steps\n\nRegister at VK Cloud.\nCreate network1 network with internet access and a subnet with the address 10.0.0.0/24.\nCreate VM:\n\n   name: Ubuntu_22_04_LAMP;\n   flavor: STD3-2-6;\n   operating system: Ubuntu 22.04;\n   network: network1 with subnet 10.0.0.0/24;\n   assign a public IP address. The example will use 211.243.95.137;\n   security groups (firewall settings): default, ssh+www.\n\nCreate DB instance:\n\n   name: MySQL-5864;\n   DBMS: MySQL 8.0;\n   configuration type: Single;\n   network: network1.\n\n   Choose the other parameters at your discretion.\n\n\n   Internal IP of the created instance: 10.0.0.7 — use it for further work with the stack.\n\n\nCreate DNS zone.\n\n\n   Make sure that the DNS zone is delegated successfully and the NS records are configured correctly: the zone must have the status NS records are configured correctly.\n\n\nCreate an record in the selected zone:\n\n   record type: A;\n   name: for example, site-lamp.example.vk.cloud;\n   IP address: external VM address 211.243.95.137.\n\n(Optional) Check the name resolution to the IP address using the command nslookup site-lamp.example.vk.cloud. Output upon successful operation:\n\n      Non-authoritative answer:\n   Name:   site-lamp.example.vk.cloud\n   Address: 211.243.95.137\n\n2. Install Apache and PHP on the VM\n\nConnect to the Ubuntu_22_04_LAMP VM.\nUpdate the packages to the current version and reboot the VM using the commands:\n\n      sudo apt update && sudo apt upgrade -y\n   sudo reboot\n\nDownload the necessary repositories and start the web server:\n\n      sudo apt install apache2 apache2-utils libapache2-mod-php php8.1 php8.1-cli php8.1-curl php8.1-fpm php8.1-gd php8.1-intl php8.1-mbstring php8.1-mysql php8.1-opcache php8.1-readline php8.1-soap php8.1-xml php8.1-xmlrpc php8.1-zip php-gd -y\n   sudo systemctl enable apache2 --now\n\n3. Check the performance of the web server\n\nOn the Ubuntu_22_04_LAMP VM create a file /var/www/html/info.php with content:\n\n   In the browser, go to http://site-lamp.example.vk.cloud/info.php.\n\n   A page opens with information about the installed version of PHP.\n\n\n(Optional) Delete the file /var/www/html/info.php from a web server.\n\nDelete unused resources\n\nDeployed virtual resources are charged. If you don't need them anymore:\n\nDelete Ubuntu_22_04_LAMP VM.\nDelete MySQL-5864 instance.\nIf necessary, delete 211.243.95.137 floating IP address.\nDelete site-lamp.example.vk.cloud DNS record.\n","url":"/en/cases/cases-lamp","description":"Installing the LAMP stack on Ubuntu 22.04 and MySQL 8.0 in VK Cloud.","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true}]},{"uuid":"5fc4b032-84fe-484c-8ea7-6b3beb7092f7","title":"How to Set Up Centralized Logging for Docker Swarm with Fluentd","data":"Hardware configuration\n\nTo run this monitoring scenario, install and configure servers using the following hardware:\n\nDocker on Ubuntu 18.04 LTS x86_64.\nElasticsearch and Kibana on Ubuntu 18.04 LTS x86_64.\n\n\n\nIf you use other servers and hardware, some script steps may differ from those described below.\n\nScheme of work\n\nScheme of work{params[width=74%; height=74%]}\n\nDocker Swarm is Docker in cluster mode. A cluster can consist of one node or several nodes. For this scenario, one node is enough.\n\nFluentd is a software package responsible for collecting, transforming logs and transferring them for storage. Logstash, a standard component of the ELK stack, has similar functions. However, Fluentd has more options for transferring logs for storage (claimed [more than 40 data outputs] (https://www.fluentd.org/dataoutputs)), as well as higher speed and low resource requirements (when consuming RAM about 40 megabytes processed 13,000 lines per second). Fluentd is currently used and supported by major companies such as Atlassian, Microsoft, and Amazon. Part of the Fluentd project is Fluent-bit, a lightweight log collector/transformer (details read here). In addition, Fluentd, along with projects such as Kubernetes and Prometheus (for details [read here] (https://www.cncf.io/projects/))), is supported by CNCF (Cloud Native Computing Foundation).\n\nTo run a script:\n\nFrom the ELK stack, we use only Elasticsearch to store the logs that Fluentd will transmit, and Kibana to display them.\nIn the Docker Swarm cluster, we will deploy a simple application from several containers, set up the collection of logs from them, as well as the transfer and visualization of logs to ELK. Let's deploy a Wordpress blog as a test application. To directly pass logs to the Fluentd daemon, we use the Fluentd log driver. By default, logs are written to files that can be read by the Fluent-bit daemon, which reduces the chance of logs being lost, since a copy of them is stored in a file. However, using the log driver is more standard practice for Docker Swarm/k8s clusters.\n\nInstalling and configuring Docker Swarm\n\nLogin to the Docker node as root.\nInstall packages:\n\nroot@ubuntu-std1-1:~# apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common\n\nAdd the Docker repository key:\n\nroot@ubuntu-std1-1:~# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt key add-\nOK\n\nAdd the Docker repository:\n\nroot@ubuntu-std1-1:~# add-apt-repository \\\n\"deb [arch=amd64] https://download.docker.com/linux/ubuntu\\\n$(lsb_release -cs) \\\nstable\"\n\nInstall Docker:\n\nroot@ubuntu-std1-1:~# apt-get update && apt-get install -y docker-ce docker-ce-cli containerd.io\n\nInitialize the cluster:\n\nroot@ubuntu-std1-1:~# docker swarm init\n\nRun Wordpress in a container\n\nCreate a directory /root/wordpress and place the following docker-compose.yml file there:\n\nversion: '3'\n\nnetworks:\nfrontend:\nbackend:\n\nvolumes:\ndb_data: {}\nwordpress_data: {}\n\nservices:\ndb:\nimage:mysql:5.7\nvolumes:\n-db_data:/var/lib/mysql\nenvironment:\nMYSQL_RANDOM_ROOT_PASSWORD: '1'\nMYSQL_DATABASE: wordpress\nMYSQL_USER: wordpress\nMYSQL_PASSWORD: wordpressPASS\nnetworks:\nbackend\nlogging:\ndriver: \"fluentd\"\noptions:\nfluentd-async-connect: \"true\"\ntag: \"mysql\"\n\nwordpress:\ndepends_on:\n-db\nimage:wordpress:latest\nvolumes:\nwordpress_data:/var/www/html/wp-content\nenvironment:\nWORDPRESS_DB_HOST:db:3306\nWORDPRESS_DB_USER: wordpress\nWORDPRESS_DB_PASSWORD: wordpressPASS\nWORDPRESS_DB_NAME: wordpress\nnetworks:\nfront end\nbackend\nlogging:\ndriver: \"fluentd\"\noptions:\nfluentd-async-connect: \"true\"\ntag: \"wordpress\"\n\nnginx:\ndepends_on:\nwordpress\n-db\nimage:nginx:latest\nvolumes:\n./nginx.conf:/etc/nginx/nginx.conf\nports:\n80:80\nnetworks:\nfront end\nlogging:\ndriver: \"fluentd\"\noptions:\nfluentd-async-connect: \"true\"\ntag: \"nginx\"\n\n\n\n\nAttention\n\nchange wordpressPASS to a random password.\n\n\n\nFor each container, the Fluentd log driver is described, the background connection to the Fluentd collector is indicated, and additional tags are affixed for further processing (if necessary).\n\nPlace the nginx.conf configuration file in the /root/wordpress directory:\n\nevents {\n\n}\n\nhttp {\nclient_max_body_size 20m;\nproxy_cache_path /etc/nginx/cache keys_zone=one:32m max_size=64m;\nserver {\nserver_name_default;\nlisten 80;\nproxy_cache one;\nlocation / {\nproxy_pass http://wordpress:80;\nproxy_set_header Host $http_host;\nproxy_set_header X-Forwarded-Host $http_host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Proto $scheme;\n}\n}\n}\n\nRun containers:\n\nroot@ubuntu-std1-1:~# docker stack deploy -c /root/wordpress/docker-compose.yml blog\nCreating network blog_backend\nCreating network blog_frontend\nCreating service blog_wordpress\nCreating service blog_nginx\nCreating service blog_db\n\nMake sure everything started successfully:\n\nroot@ubuntu-std1-1:~# docker service ls\nID . NAME . . . . . MODE . . . REPLICAS .\n12jo1tmdr8ni 1/1 mysql:5.7\nrbdwd7oar6nv . . blog_nginx . . . replicated . . 1/1 . . nginx:latest . . \\*:80->80/tcp\noejvg6xgzcwj . . blog_wordpress . . replicated . . 1/1 . . . wordpress:latest\n\nIn the address bar of the browser, enter the IP address of the server and finish setting up Wordpress:\n\n\n\nThe result is a system consisting of three containers: a MySQL database, Nginx as a frontend proxy, and an Apache/Modphp container for running the Wordpress codebase. Each container will have its own logs, which we will add for collection and processing.\n\nInstall Fluentd\n\n\n\nNote\n\nThe Fluentd version being used is td-agent 3.5.1-0.\n\n\n\nInstall fluentd:\n\nroot@ubuntu-std1-1:~# curl -L https://toolbelt.treasuredata.com/sh/install-ubuntu-bionic-td-agent3.sh | sh\n\nAdd fluentd to startup:\n\nroot@ubuntu-std1-1:~# systemctl enable td-agent\nSynchronizing state of td-agent.service with SysV service script with /lib/systemd/systemd-sysv-install.\nExecuting: /lib/systemd/systemd-sysv-install enable td-agent\n\nFluentd configuration options\n\nThe fluentd configuration file is located in the /etc/td-agent/td-agent.conf directory. It consists of several sections, consider them.\n\nSource section \\- contains a description of the source of the logs. The Docker Fluentd log driver sends logs to tcp://localhost:24224 by default. Let's describe the source section for receiving logs:\n\n\n@type forward\nport 24224\n\n\n@type forward means the fluentd protocol that runs over a TCP connection and is used by Docker to send logs to the Fluentd daemon.\n\nData output section in elasticsearch:\n\n@type elasticsearch\nhost\nport 9200\nlogstash_format true\n\n\nIn , specify the DNS name or IP address of the Elasticsearch server.\n\nSuch a configuration file is the minimum for sending logs to Elasticsearh, but fluentd's capabilities are not limited to this. It has extensive filtering, parsing and data formatting capabilities.\n\nA typical example of filtering is setting up a selection by regexp:\n\n\n@type grep\n\nkey message\npattern /cool/\n\n\nkey hostname\npattern /^web\\d+\\.example\\.com$/\n\n\nkey message\npattern /uncool/\n\n\n\nThis example will select records from the stream that contain the word cool in the message field, the hostname field, for example, www123.example.com, and do not contain the word uncool in the tag field. The following data will be verified:\n\n{\"message\":\"It's cool outside today\", \"hostname\":\"web001.example.com\"}\n{\"message\":\"That's not cool\", \"hostname\":\"web1337.example.com\"}\n\nThe following are not:\n\n{\"message\":\"I am cool but you are uncool\", \"hostname\":\"db001.example.com\"}\n{\"hostname\":\"web001.example.com\"}\n{\"message\":\"It's cool outside today\"}\n\nThis example is taken from the fluentd manual. Another good example of using a filter is adding geodata.\n\nParsers are designed to parse logs of a standard structure (for example, Nginx logs). Parsers are specified in the source section:\n\n\n@typetail\npath /path/to/input/file\n\n@type nginx\nkeep_time_key true\n\n\n\nThis is a typical example of parsing Nginx logs. Data formatting is used to change the format or structure of output data and is described in the output section.\n\nThe possibilities of fluentd are very wide, their description is beyond the scope of this article. To learn more about fluentd, see the documentation.\n\nView logs\n\nWhen entering Elasticsearch, the logs will be added to the logstash-YYYY-MM-DD index. If you need more complex processing of logs, you can send the logs not directly to Elasticsearch, but to Logstash, and parse and arrange them there.\n\nTo view logs:\n\nIn a browser, go to the Kibana web console, then click on the Management / Index patterns link.\n\n\n\nIn the Index Pattern input window, enter logstash-\\* and click Next Step.\n\n\n\nIn the Time filter field name window, select @timestamp and click Create index pattern:\n\n\n\nIndex pattern created.\n\n\n\nGo to Discover, select an index. There will be container logs:\n\n\n\nNext, try creating a couple of test posts in Wordpress and look in Kibana for changes in the logs.\n","url":"/en/cases/cases-logs/case-swarm","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"Logging","link":"/en/cases/cases-logs","isActive":false,"evenSimpler":true}]},{"uuid":"d2e2c045-1b01-46a5-b488-399f3f58f0b4","title":"Installing the ELK Stack on Ubuntu 18.04","data":"This article describes the installation of the ELK stack on a Linux operating system — Ubuntu 18.04.\n\nThe ELK stack is a powerful set of tools for efficiently solving a wide range of data collection, storage and analysis tasks:\n\nElasticsearch is a full text search solution built on top of Apache Lucene with added convenience.\nLogstash is a utility for collecting, filtering and then redirecting to the final data storage. This mechanism provides a real-time pipeline. It can take data from multiple sources and convert it into JSON documents.\nKibana is an application that allows you to take and search data from Elasticsearch and build visual graphs.\n\nRequirements\n\nOperating system Ubuntu version 18.04.\nInstalled Nginx web server.\nInstalled Java virtual machine.\nUser with access to the sudo command.\n\nInstalling the Nginx web server\n\nCompared to the Apache web server, the Nginx web server uses fewer resources to host large, high-traffic sites. Thanks to the Nginx architecture, you can easily scale up to hundreds of thousands of concurrent connections.\n\nTo install and perform the initial configuration of the Nginx web server:\n\nOpen a terminal window.\nUpdate the package indexes by running the command:\n\n      sudo apt update\n\nInstall the Nginx web server by running the command:\n\n      sudo apt install nginx -y\n\nTo test the operation of the web server, launch a web browser and enter the IP address of the web server in the address bar.\n\n   If the installation is successful, the following web server page will open:\n\n   **\n\nInstalling the Java Virtual Machine\n\nThe ELK stack requires a Java virtual machine to run. To install JVM:\n\nOpen a terminal window.\nInstall the JVM software package by running the command:\n\n      sudo apt install default-jre -y\n\n   This will install the Java Runtime Environment (JRE) package.\n\nInstall the JDK software package, which includes the Java compiler, standard Java class libraries, examples, documentation, and various utilities. To do this, run the command:\n\n      sudo apt install default-jdk -y\n\nInstalling and configuring Elasticsearch\n\nTo install and perform the initial configuration of Elasticsearch:\n\nTo check the current version of Elasticsearch, go to: https://www.elastic.co/downloads/elasticsearch.\n\nOpen a terminal window.\n\nImport the GPG Elasticsearch public key, which is used to protect Elastic packages, by running the command:\n\n      sudo wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt key add\n\nAdd the Elastic packages to the sources.list.d system repositories directory by running the command:\n\n      sudo echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list\n\nUpdate the package indexes by running the command:\n\n      sudo apt update\n\nInstall Elasticsearch by running the command:\n\n      sudo apt install elasticsearch\n\nMake changes to the elasticsearch.yml configuration file. For this:\n\n   Open this file for editing by running the command:\n\n            sudo nano /etc/elasticsearch/elasticsearch.\n\n   Find the line:\n\n            #network.host: 192.168.0.1\n\n      Replace it with the line:\n\n                  network.host: localhost\n\n\n         Note\n\n         To search within a file, use the keyboard shortcut CTRL+W.\n\n\n         After editing the .yml config file, make sure it doesn't have extra spaces and/or indents!\n\n   Save your changes using the keyboard shortcut CTRL+O and finish editing using the keyboard shortcut CTRL+X.\n\nStart the Elasticsearch service by running the command:\n\n      sudo systemctl start elasticsearch\n\nCheck the startup status of the Elasticsearch service by running the command:\n\n      sudo systemctl status elasticsearch\n\nIf an error is displayed:\n\n   **\n\n   Do the following:\n\n      Open the file containing the Java virtual machine settings by running the command:\n\n                  sudo nano /etc/elasticsearch/jvm.options\n\n      Find the parameters that define the minimum and maximum amount of RAM for Java:\n\n\n         Note\n\n         Details about Xms and Xmx parameters read here. For machines with low RAM, we recommend limiting the amount of memory used by the JVM.\n\n\n      Specify the required values ​​in the -Xms1g and -Xmx1g parameters. For example, for an operating system with 1 GB of RAM, you can specify:\n\n                  -Xms128m\n         -Xmx128m\n\n      Save your changes using the CTRL+O key combination and finish editing using the CTRL+X key combination.\n      Start the Elasticsearch service and check the status. If there are no errors, the following will be displayed:\n\n         **\n\nTo start the Elasticsearch service automatically when the operating system is restarted, run the command:\n\n      sudo systemctl enable elasticsearch\n\nTo test access to the Elasticsearch service, send an HTTP request by running the command:\n\n      curl -X GET localhost:9200\n\n   If the installation of Elasticsearch was successful, the following information will be displayed:\n\n      **\n\nInstalling and configuring Kibana\n\nTo install and perform the initial configuration of Kibana, do the following:\n\nMake sure you have successfully installed Elasticsearch.\nOpen a terminal window.\nInstall Kibana by running the command:\n\n      sudo apt install kibana\n\nStart Kibana by running the command:\n\n      sudo systemctl start kibana\n\nTo start the Kibana service automatically when the operating system is restarted, run the command:\n\n      sudo systemctl enable kibana\n\nTo check the status of Kibana, run the command:\n\n      sudo systemctl status kibana\n\nMake changes to the kibana.yml configuration file. For this:\n\n   Open this file by running the command:\n\n            sudo nano /etc/kibana/kibana.yml\n\n   Find the line:\n\n            #server.port: 5601\n\n      And replace it with the line:\n\n                  server.port: 5601\n\n   Find the line\n\n            #server.host: \"localhost\"\n\n      And replace it with the line:\n\n                  server.host: \"localhost\"\n\n   Find the line:\n\n            #elasticsearch.hosts: [\"http://localhost:9200\"]\n\n      And replace it with the line:\n\n                  elasticsearch.hosts: [\"http://localhost:9200\"]\n\n   Save changes using CTRL+O and finish editing using CTRL+X\n\nCreate an administrator account to access the Kibana web interface. To do this, run the command:\n\n      echo \"mcskibadmin:\\openssl passwd -apr1\\\" | sudo tee -a /etc/nginx/htpasswd.users\n\n   where mcskibadmin is the login of the administrator account, htpasswd.users is the file where credentials are stored.\n\n   Then enter the password.\n\nCreate a file with a virtual site for the Nginx web server by running the command:\n\n      sudo nano /etc/nginx/sites-available/elk\n\nAdd the following information to this file:\n\n      server {\n   listen 80;\n\n   server_name ;\n\n   auth_basic \"Restricted Access\";\n   auth_basic_user_file /etc/nginx/htpasswd.users;\n\n   location / {\n   proxy_pass http://localhost:5601;\n   proxy_http_version 1.1;\n   proxy_set_header Upgrade $http_upgrade;\n   proxy_set_header Connection 'upgrade';\n   proxy_set_header Host $host;\n   proxy_cache_bypass $http_upgrade;\n   }\n   }\n\n   Save your changes using the keyboard shortcut CTRL+O and finish editing using the keyboard shortcut CTRL+X.\n\nActivate the new Nginx configuration by running the command:\n\n      sudo ln -s /etc/nginx/sites-available/elk /etc/nginx/sites-enabled/\n\nRestart Kibana by running the command:\n\n      sudo systemctl restart kibana\n\nRestart the Nginx web server by running the command:\n\n      sudo systemctl restart nginx\n\nMake sure that the syntax of the nginx configuration file does not contain errors by running the command:\n\n      sudo nginx -t\n\nInstalling and configuring Logstash\n\nTo install and perform the initial setup of Logstash:\n\nInstall Logstash by running the command:\n\n      sudo apt install logstash\n\nCreate and configure a configuration file containing rules for receiving information from beats agents. For this:\n\n\n   Note\n\n   The following is one of the possible settings. For more information read here.\n\n\n   Create the file 02-beats-input.conf by running the command:\n\n            sudo nano /etc/logstash/conf.d/02-beats-input.conf\n\n   Add the following lines to this file:\n\n            input {\n      beat {\n      port => 5044\n      }\n      }\n\n   Save your changes using the CTRL+O key combination and finish editing using the CTRL+X key combination.\n\nCreate and configure the 30-elasticsearch-output.conf configuration file containing the rules for storing beats in Elasticsearch information. For this:\n\n   Create the 30-elasticsearch-output.conf file by running the command:\n\n            sudo nano /etc/logstash/conf.d/30-elasticsearch-output.conf\n\n   Add the following lines to this file:\n\n            output {\n      elasticsearch {\n      hosts => [\"localhost:9200\"]\n      sniffing => true\n      manage_template => false\n      template_overwrite => true\n      index => \"%{@metadata}-%{+YYYY.MM.dd}\"\n      document_type => \"%{@metadata}\"\n      }\n      }\n\n   Save your changes using the CTRL+O key combination and finish editing using the CTRL+X key combination.\n\nCreate a file containing rules for filtering and structuring incoming data. For this:\n\n   Create the file 10-system-filter.conf by running the command:\n\n            sudo nano /etc/logstash/conf.d/10-logstash-filter.conf\n\n   In the file that opens, place the following lines:\n\n            input { stdin { } }\n      filter {\n      grok {\n         match => { \"message\" => \"%{COMBINEDAPACHELOG}\" }\n      }\n      date {\n         match => [ \"timestamp\" , \"dd/MMM/yyyy:HH:mm:ss Z\" ]\n      }\n      }\n      output {\n      elasticsearch { hosts => [\"localhost:9200\"] }\n      stdout { codec => rubydebug }\n      }\n\n   Save your changes using the CTRL+O key combination and finish editing using the CTRL+X key combination.\n\nCheck the Logstash configuration by running the command:\n            sudo -u logstash /usr/share/logstash/bin/logstash --path.settings /etc/logstash -t\n\nStart Logstash by running the command:\n\n      sudo systemctl start logstash\n\nTo start the Logstash service automatically when the operating system is rebooted, run the command:\n\n      sudo systemctl enable logstash\n\nInstalling and configuring Filebeat\n\nFilebeat allows you to collect data (beats) from various sources and transfer them to Logstash or Elasticsearch on Linux-like systems.\n\nTo install Filebeat:\n\nOpen a terminal.\n\nInstall Filebeat by running the command:\n\n      sudo apt install filebeat\n\nSet up the filebeat.yml configuration file. For this:\n\n   Open this file:\n\n            sudo nano /etc/filebeat/filebeat.yml\n\n   Prevent Filebeat from sending data directly to Elasticsearch. To do this, find the lines:\n\n            output.elasticsearch:\nArray of hosts to connect to.\n      hosts: [\"localhost:9200\"]\n\n      And replace them with the lines:\n\n                  #output.elasticsearch:\nArray of hosts to connect to.\n         #hosts: [\"localhost:9200\"]\n\n   Tell the Filebeat service to use Logstash as a log collector. To do this, find the lines:\n\n            #output.logstash:\nThe Logstash hosts\n      #hosts: [\"localhost:5044\"]\n\n      And replace them with the lines:\n\n                  output.logstash:\nThe Logstash hosts\n         hosts: [\"localhost:5044\"]\n\n         Save your changes using the CTRL+O key combination and finish editing using the CTRL+X key combination.\n\nEnable the Logstash module. To do this, run the command:\n\n      sudo sudo filebeat modules enable logstash\n\n\n   Note\n\n   Learn more about filebeat modules read here.\n\n\nTo view the included modules, run the command:\n\n      sudo filebeat modules list\n\nDownload the Elasticsearch index template by running the command:\n\n      sudo filebeat setup --template -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=[\"localhost:9200\"]'\n\n\n   Note\n\n   Elasticsearch indexes are a collection of documents that have similar characteristics. They are identified by names that are used to refer to indexes when performing various index operations. The index template is loaded automatically when new indexes are created.\n\n\nDashboards allow you to visualize the Filebeat data sent to Kibana. To enable the dashboard, run the command:\n\n      sudo filebeat setup -e -E output.logstash.enabled=false -E output.elasticsearch.hosts=['localhost:9200'] -E setup.kibana.host=localhost:5601\n\nStart Filebeat by running the command:\n\n      sudo systemctl start filebeat\n\nTo start the filebeat service automatically when the operating system is rebooted, run the command:\n\n      sudo systemctl enable filebeat\n\nTo verify that Elasticsearch is receiving data, query the Filebeat index with the command:\n\n      curl -XGET 'http://localhost:9200/filebeat-\\*/_search?pretty'\n\nThe installation of the ELK stack is complete.\n\nIn the address bar of your web browser, enter the IP address of your Elastic server. Use your administrator credentials to sign in. After successful authorization, you will be redirected to the Kibana main page.\n","url":"/en/cases/cases-logs/elk-u18","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"Logging","link":"/en/cases/cases-logs","isActive":false,"evenSimpler":true}]},{"uuid":"1b73b6f0-c36e-42a8-a2be-d5c0803402b2","title":"Installing and configuring ELK, collecting, analyzing and visualizing logs","data":"Hardware configuration\n\nThis scenario requires an installed and configured server for ELK on Ubuntu 18.04 LTS x86_64.\n\n\n\nAttention!\n\nIf you use other servers and hardware, some script steps may differ from those described below.\n\nScheme of work\n\nScheme of work{params[width=68%; height=68%]}\n\nThe ELK stack consists of three components:\n\nElasticsearch — an engine for storing, indexing and processing data in a shared storage, as well as for full-text data search.\nLogstash — a utility for collecting, filtering, aggregating, changing and then redirecting the source data to the final storage.\nKibana — a web interface for viewing and analyzing data from the repository.\n\nInstalling Elasticsearch, Logstash and Kibana\n\nLogin to the Ubuntu server as root.\nImport the Elasticsearch repository key:\n\nroot@ubuntu-std1-1:~# wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt key add -\nOK\n\nInstall apt-transport-https:\n\nroot@ubuntu-std1-1:~# apt-get install apt-transport-https\n\nAdd a repository:\n\nroot@ubuntu-std1-1:~# echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list\ndeb https://artifacts.elastic.co/packages/7.x/apt stable main\n\nInstall Elasticsearch:\n\nroot@ubuntu-std1-1:~# apt-get update && apt-get install elasticsearch\n\nInstall Kibana:\n\nroot@ubuntu-std1-1:~# apt-get install kibana\n\nInstall OpenJDK for Logstash to work:\n\nroot@ubuntu-std1-1:~# apt-get install openjdk-8-jre\n\nInstall Logstash:\n\nroot@ubuntu-std1-1:~# apt-get install logstash\n\nSetting up Elasticsearch\n\nElasticsearch is configured using three configuration files:\n\nelasticsearch.yml — main configuration file;**\njvm.options — file for configuring a Java machine to run Elasticsearch;**\nlog4j2.properties — file to configure Elasticsearch logging.**\n\njvm.options\n\nThe most important thing in this file is the setting of the memory allocated for the JVM (Heap Size). For Elasticsearch, this parameter directly affects how large data arrays it can process. Heap Size is determined by a couple of parameters:\n\nXms — initial value;\nXmx — maximum value.\n\nThe default Heap Size is 1 GB. If the amount of memory on the server allows, increase this value (more about Heap Size). To do this, find the lines:\n\nXms1g\nXmx1g\n\nand replace them, for example, with the lines:\n\nXms4g\nxmx4g\n\nlog4j2.properties\n\nFor convenience, you can change appender.rolling.policies.size.size, which specifies the size of the log at which rotation is performed (default is 128 MB). More about logging see here.\n\nelasticsearch.yml\n\nCustomize:\n\nnode.name: elasticsearch — specify the name of the node;\nnetwork.host: 127.0.0.1 — set to listen only to localhost.\n\nStart elasticsearch:\n\nroot@ubuntu-std1-1:~# systemctl start elasticsearch.service\n\nIf you specify too large a Heap Size value, the launch will fail. In this case, the following will be in the logs:\n\nroot@ubuntu-std1-1:~# systemctl start elasticsearch.service\nJob for elasticsearch.service failed because the control process exited with error code.\nSee \"systemctl status elasticsearch.service\" and \"journalctl -xe\" for details.\nroot@ubuntu-std1-1:~# journalctl -xe\n-- Unit elasticsearch.service has run starting up.\nNov 12 12:48:12 ubuntu-std1-1 elasticsearch[29841]: Exception in thread \"main\" java.lang.RuntimeException: starting java failed with [1]\nNov 12 12:48:12 ubuntu-std1-1 elasticsearch[29841]: output:\nNov 12 12:48:12 ubuntu-std1-1 elasticsearch[29841]:\nNov 12 12:48:12 ubuntu-std1-1 elasticsearch[29841]: # There is insufficient memory for the Java Runtime Environment to continue.\nNov 12 12:48:12 ubuntu-std1-1 elasticsearch[29841]: # Native memory allocation (mmap) failed to map 986513408 bytes for committing reserved memory.\nNov 12 12:48:12 ubuntu-std1-1 elasticsearch[29841]: # An error report file with more information is saved as:\nNov 12 12:48:12 ubuntu-std1-1 elasticsearch[29841]: # /var/log/elasticsearch/hs_err_pid29900.log\n\nIf successful, add Elasticsearch to the list of processes to start automatically:\nroot@ubuntu-std1-1:~# systemctl enable elasticsearch.service\nSynchronizing state of elasticsearch.service with SysV service script with /lib/systemd/systemd-sysv-install.\nExecuting: /lib/systemd/systemd-sysv-install enable elasticsearch\nCreated symlink /etc/systemd/system/multi-user.target.wants/elasticsearch.service → /usr/lib/systemd/system/elasticsearch.service.\n\nVerify that Elasticsearch is responding:\n\nroot@ubuntu-std1-1:~# curl http://localhost:9200\n{\n\"name\" : \"ubuntu-std1-1\",\n\"cluster_name\" : \"elasticsearch\",\n\"cluster_uuid\" : \"ZGDKK_5dQXaAOr75OQGw3g\",\n\"version\" : {\n\"number\" : \"7.4.2\",\n\"build_flavor\" : \"default\",\n\"build_type\" : \"deb\",\n\"build_hash\" : \"2f90bbf7b93631e52bafb59b3b049cb44ec25e96\",\n\"build_date\" : \"2019-10-28T20:40:44.881551Z\",\n\"build_snapshot\" : false,\n\"lucene_version\" : \"8.2.0\",\n\"minimum_wire_compatibility_version\" : \"6.8.0\",\n\"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n},\n\"tagline\" : \"You Know, for Search\"\n}\n\nKibana setup\n\nBy default, the Kibana configuration file /etc/kibana/kibana.yml contains all the necessary settings. The only setting you need to change is server.host: \"localhost\". By default, Kibana is only available locally. To access Kibana remotely, replace “localhost” with the external IP address of the server where Kibana is installed. Also, if Elasticsearch is not on the same host as Kibana, change the elasticsearch.hosts setting: \"[http://localhost:9200\"].\n\nStart Kibana:\n\nroot@ubuntu-std1-1:/etc/kibana# systemctl start kibana.service\n\nAdd Kibana to the list of applications that start automatically:\n\nroot@ubuntu-std1-1:/etc/kibana# systemctl enable kibana.service\nSynchronizing state of kibana.service with SysV service script with /lib/systemd/systemd-sysv-install.\nExecuting: /lib/systemd/systemd-sysv-install enable kibana\n\nIn a browser, navigate to http://:5601.\n\nIf Kibana is running, the following will be displayed:\n\nConfiguring Kibana and Elasticsearch Security\n\nBy default, Elasticsearch and Kibana are fully accessible to everyone. Access can be restricted in one of the following ways:\n\nUse Nginx as a reverse proxy with authorization and access control.\nUse the built-in elasticsearch mechanism xpack.security (for details see here or here).\n\nConsider the most popular first method.\n\nInstall Nginx:\n\nroot@ubuntu-std1-1:~# apt-get install nginx\n\nMake sure that in the configuration file /etc/elasticsearch/elasticsearch.yml the network.host parameter is set to 127.0.0.1 or localhost. If necessary, make this configuration and restart the elasticsearch daemon:\n\nroot@ubuntu-std1-1:~# cat /etc/elasticsearch/elasticsearch.yml | grep network.host\nnetwork.host: 127.0.0.1\nroot@ubuntu-std1-1:~# systemctl restart elasticsearch.service\n\nMake sure that the server.host parameter in the configuration file /etc/kibana/kibana.yml is set to 127.0.0.1 or localhost. If necessary, make this configuration and restart the kibana daemon:\n\nroot@ubuntu-std1-1:~# cat /etc/kibana/kibana.yml | grep server.host\nserver.host: \"127.0.0.1\"\nWhen this setting's value is true Kibana uses the hostname specified in the server.host\nroot@ubuntu-std1-1:~# systemctl restart kibana.service\n\nMake sure Elasticsearh and Kibana are using interface 127.0.0.1:\n\nroot@ubuntu-std1-1:~# netstat -tulpn | grep9200\ntcp6 0 0 127.0.0.1:9200 :::\\* LISTEN 10512/java\nroot@ubuntu-std1-1:~# netstat -tulpn | grep 5601\ntcp 0 0 127.0.0.1:5601 0.0.0.0:\\* LISTEN 11029/node\n\nIn /etc/nginx/sites-available create a kibana.conf file and add the following to it:\n\nserver {\nlisten :5601;\nserver_name kibana;\nerror_log /var/log/nginx/kibana.error.log;\naccess_log /var/log/nginx/kibana.access.log;\n\nlocation / {\nauth_basic \"Restricted Access\";\nauth_basic_user_file /etc/nginx/htpasswd;\nrewrite ^/(.\\*) /$1 break;\nproxy_ignore_client_abort on;\nproxy_pass http://localhost:5601;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header Host $http_host;\n}\n}\n\nSpecify the username (USER) and password (PASSWORD):\n\nroot@ubuntu-std1-1:/etc/nginx# printf \"USER:$(openssl passwd -crypt PASSWORD)\\n\" >> /etc/nginx/htpasswd\n\nTo enable the site, create a symlink to the /etc/nginx/sites-enabled directory:\n\nroot@ubuntu-std1-1:~# ln -s /etc/nginx/sites-available/kibana.conf /etc/nginx/sites-enabled/kibana.conf\n\nStart Nginx:\n\nroot@ubuntu-std1-1:~# systemctl start nginx\n\nIn a browser, navigate to http://:5601. In the window that opens, enter your login and password to access the Kibana web interface.\n\nSimilarly configure Nginx as a reverse proxy for Elasticsearh (port 9200) and Logstash (typically port 5044).\n\nTo get familiar with Kibana, you can use the test dataset:\n\nInstall Filebeat\n\nBeats is part of the Elasticsearch infrastructure, the so-called Data Shippers (data providers). These are lightweight agents that take data from various sources and transform it for transmission to Elasticsearch. The functionality of Beats partially duplicates Logstash, but Beats is lighter, easier to set up, works faster and does not require the Java stack to be installed. Typically, the nodes where the logs are generated have the appropriate Beats agents installed, which transfer the logs to Logstash. Logstash aggregates, transforms logs and passes them to Elasticsearch. There are many different Beats, the standard set includes the following agents:\n\nFilebeat — collection of logs from various log files.\nPacketbeat — collection of network statistics.\nWinlogbeat — collection of logs on the Windows platform.\nMetricbeat — collection of various metrics.\nHeartbeat — collection of data on infrastructure availability.\nAuditbeat — collection of system audit data.\nFunctionbeat — data collection from Serverless projects (AWS Lambda).\nJournalbeat — collection of Journald logs.\n\nThe most common agent is Filebeat, we use it to collect Nginx logs.\n\nInstall Filebeat:\n\nroot@ubuntu-std1-1:~# apt-get install filebeat\n\nAllow Nginx log processing:\n\nroot@ubuntu-std1-1:~# mv /etc/filebeat/modules.d/nginx.yml.disabled /etc/filebeat/modules.d/nginx.yml\n\nIf the logs are in a non-standard location, or only part of the logs needs to be processed, uncomment and fill in the var.paths variables in the /etc/filebeat/modules.d/nginx.yml file.\n\nIn the example below, we will collect and analyze logs for accessing the Kibana service. When configuring Nginx, we specified that access logs would be stored in the /var/log/nginx/kibana.access.log and /var/log/nginx/kibana.error.log files.\n\nMake the /etc/filebeat/modules.d/nginx.yml file look like this:\n\nModule: nginx\nDocs: https://www.elastic.co/guide/en/beats/filebeat/7.4/filebeat-module-nginx.html\n-module: nginx\nAccess logs\naccess:\nenabled: true\n\nSet custom paths for the log files. if left empty,\nFilebeat will choose the paths depending on your OS.\nvar paths:\n/var/log/nginx/kibana.access.log\n\nError logs\nerror:\nenabled: true\n\nSet custom paths for the log files. if left empty,\nFilebeat will choose the paths depending on your OS.\nvar paths:\n/var/log/nginx/kibana.error.log\n\nIn the /etc/filebeat/filebeat.yml file, edit the setup.kibana section:\n\nsetup.kibana:\n  host: \":5601\"\n  username: \"login\"\n  password: \"password\"\n\n\n\nNote\n\nLogin and password are required for Filebeat access to Kibana in order to load typical dashboards for known datasets.\n\n\n\nThe logs will be sent to Logstash, so comment out the output.elasticsearch section and specify the IP address of the server hosting Logstash in the output.logstash section:\n\n#-------------------------- Elasticsearch output --------------------- ---------\n#output.elasticsearch:\nArray of hosts to connect to.\nhosts: [\"localhost:9200\"]\n\nOptional protocol and basic auth credentials.\n#protocol: \"https\"\n#username: \"elastic\"\n#password: \"changeme\"\n\n#---------------------- Logstash output ------------------ -------------\noutput.logstash:\nThe Logstash hosts\nhosts: [\":5044\"]\n\nOptional SSL. By default is off.\nList of root certificates for HTTPS server verifications\n#ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n\nCertificate for SSL client authentication\n#ssl.certificate: \"/etc/pki/client/cert.pem\"\n\nClient Certificate Key\n  #ssl.key: \"/etc/pki/client/cert.key\"\n\nMake sure there are no errors in the configuration file:\n\nroot@ubuntu-std1-1:/etc/filebeat# filebeat test config -c /etc/filebeat/filebeat.yml\nconfig OK\n\nBefore starting Filebeat, configure Logstash to accept logs.\n\nSetting up Logstash\n\nThe Logstash configuration file generally consists of three sections:\n\ninput — description of the destination of the logs.\nfilter — transformation of logs.\noutput — description of the destination of the converted logs.\n\nCreate a file /etc/logstash/conf.d/input-beats.conf containing the port number on which Beats (in particular, Filebeat) sends its logs:\n\ninput {\nbeat {\nport => 5044\n}\n}\n\nCreate the /etc/logstash/conf.d/output-elasticsearch.conf file and specify that logs should be sent to Elasticsearch at localhost and indexes should be named in the nginx- format (that is, a new index will be created every day , which is convenient for analysis):\n\noutput {\nelasticsearch {\nhosts => [ \"localhost:9200\" ]\nmanage_template => false\nindex => \"nginx-%{+YYYY.MM.dd}\"\n}\n}\n\nCreate a file /etc/logstash/conf.d/filter-nginx.conf with the following content:\n\nfilter {\nif event == ​​\"nginx.access\" {\ngrok {\nmatch => [ \"message\" , \"%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \\\"(?:%{WORD:verb} %{ NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\\\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer } %{QS:user_agent}\"]\noverwrite => [ \"message\" ]\n}\nmutate {\nconvert => [\"response\", \"integer\"]\nconvert => [\"bytes\", \"integer\"]\nconvert => [\"responsetime\", \"float\"]\n}\ngeoip {\nsource => \"clientip\"\ntarget => \"geoip\"\nadd_tag ​​=> [\"nginx-geoip\"]\n}\ndate {\nmatch => [ \"timestamp\" , \"dd/MMM/YYYY:HH:mm:ss Z\" ]\nremove_field => [ \"timestamp\" ]\n}\n\nuseragent {\nsource => \"user_agent\"\n}\n}\n}\nFilebeat, which will send Nginx logs to Logstash, writes the entire Nginx log line in the message field. Therefore, this field needs to be parsed into variables that can be worked with in Elasticsearch. This parsing is done in the grok section of the NGINX ACCESS LOG format.\n\nIn the mutate section, you can change the data storage format (for example, so that the bytes field from the log is stored as a number, not as a string).\n\nIn the geoip section, geolocation fields are added to the log by the request IP address.\n\nThe date section is used to parse the query date field from the log and convert it to pass it to Elasticsearch.\n\nThe useragent section fills in the fields according to the field from the log. Note that the agent field is usually used in these tutorials. This field will not work with Filebeat + Logstash as it is intended to be used when writing directly from Filebeat to Elasticsearh. When used in Logstash, an error will be thrown:\n\n2019-11-19T09:55:46,254logstash.filters.useragent Uknown error while parsing user agent data {:exception=>#, :field=>\"agent\", :event=>#}\n\nFor the same reason, you don't need to use the %{COMBINEDAPACHELOG} macro in the grok match section.\n\nTo track errors in Logstash, enable debugging. To do this, add the following line to the output section:\n\nstdout { codec => rubydebug }\n\nAs a result, the output to the Elasticsearch database will be duplicated by the output to the console/syslog. It is also useful to use the Grok Debugger to test grok match expressions.\n\nLaunch Logstash and add it to the list of applications to start automatically:\n\nroot@ubuntu-std1-1:~# systemctl start logstash\nroot@ubuntu-std1-1:~# systemctl enable logstash\nCreated symlink /etc/systemd/system/multi-user.target.wants/logstash.service → /etc/systemd/system/logstash.service.\n\nMake sure the service has started:\n\nroot@ubuntu-std1-1:~# netstat -tulpn | grep5044\ntcp6 0 0 :::5044 :::\\* LISTEN 18857/java\n\nTest Filebeat:\n\nroot@ubuntu-std1-1:~# service filebeat start\n\nSet up Kibana Templates\n\nAfter starting Filebeat, Kibana access logs go to Logstash, then to Elasticsearch. To view these logs, Kibana needs to configure templates.\n\nGo to Kibana, click the gear in the left menu, select Kibana > Index Patterns and click Create Index Pattern.\n\n\n\nTo select all entries, enter nginx-\\* in the Index pattern field and click Next step.\n\n\n\nTo use timestamps from log files, select @timestamp in the Time Filter field name and click Create index pattern.\n\n\n\nAn index pattern will be created.\n\n\n\nTo see the logs that got into Elasticsearch, go to Discover.\n\nSet up Kibana Visualizations\n\nThe dashboard in Kibana is made up of visualizations. A visualization is a kind of graph built on specific queries from Elasticsearch.\n\nLet's build the first visualization - top 10 clients.\n\nSelect Visualizations from the left menu and click the Create new visualization button.\n\n\n\nSelect Vertical bar.\n\n\n\nSelect the nginx-\\* template.\n\n\n\nAdd an X axis.\n\n\n\nEnter data:\n\nAggregation: Terms — returns the specified number of top values.\nField: clientip.keyword — select a client by IP address.\nSize: 10 — 10 top values.\nCustom Label: Top 10 clients — the name of the visualization.\n\n\n\nRun the query and save the visualization.\n\n\n\nAs a result, the visualization shows the top 10 IP addresses from which requests were made.\n\nLet's build a second visualization - a pie chart showing the top 5 countries from which users contacted.\n\nSelect Visualizations from the left menu and click the Create new visualization button.\nSelect Pie.\nSelect the nginx-\\* template.\nAdd an X axis.\nTo display data in slices, select Add bucket / Split slices.\n\n\n\nEnter the following data:\n\nAggregation: Terms — select the top data value.\nField: geoip.country_code2.keyword — two-letter country code.\nSize:5 — select top 5.\nCustom label: Top 5 countries — chart title.\n\n\n\nRun the query and save the visualization.\n\n\n\nThe graph will display the top 5 countries from which there was access.\n\nLet's build the third visualization - a graph of the number of requests, divided by response codes.\n\nSelect Visualizations from the left menu and click the Create new visualization button.\nSelect TSVB.\nSelect the nginx-\\* template.\nTo get all server response codes (that is, all requests sent to the server), in the window that appears, enter Requests in the name, select group by filter, and specify response:\\* in the query string.\nTo add a second line to the chart, .\nTo get a sample of the number of server responses \"200 OK\" per unit of time on the second chart, click \"+\", select a different color, specify Responce:200 in the title, response:200 in the query string.\nClick \"+\" and add the 302 response code in the same way. Then save the visualization.\n\nKibana Dashboard setup\n\nKibana Dashboard is a set of visualizations.\n\nClick Dashboards, then Create New Dashboard.\n\n\n\nOn the top menu, click Add.\n\n\n\nIn the window that opens, select the visualizations you created.\n\n\n\nReorder and resize the visualizations as needed, then click Save.\n\n\n","url":"/en/cases/cases-logs/case-logging","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"Logging","link":"/en/cases/cases-logs","isActive":false,"evenSimpler":true}]},{"uuid":"4300e020-2d5f-47c2-838c-c49d1c863152","title":"Collecting logs from nodes with docker-runtime and crio-runtime using Fluent Bit","data":"To unify the collection of logs from nodes with docker-runtime and crio-runtime, you should use Fluent Bit (a lightweight and more performant alternative to fluentd).\n\nSettings file values.yaml\n\nAs values.yaml for the Helm, you should use the file from the link below - the necessary filters and parsers are already configured in it.\n\nFile values.yaml.\n\nIn the attached values.yaml, you need to correct the parameters for [OUTPUT] - in the example, PostgreSQL is used for storing logs with two tables in one database:\n\nTable fluentbit_21_host - for storing logs of host services (kublet.service, docker.service, crio.service).\nTable fluentbit_21_kube - stores logs directly from pod logs.\n\nIn a similar way, logs can be saved, for example, in Elasticsearch. For a complete list of supported storage types, see link.\n\nThe separation is intentional to show the possibility of using different tables in the database (indexes in Elasticsearch).\n\nInstall Fluent Bit with Helm 3\n\nYou can install Fluent Bit with the configuration file with the following commands:\n\nhelm repo add fluent https://fluent.github.io/helm-charts\nhelm install fluent-bit fluent/fluent-bit --values ​​values.yaml\n\nYou can find more information about installing Fluent Bit in the official Fluent Bit documentation: https://docs.fluentbit.io/manual/installation/kubernetes#installing-with-helm-chart\n\nSample logs\n\nDocker logs:\n\n{\n\t\"ts\": 1638261063.088225,\n\t\"msg\": \"ignoring event\\\" module=libcontainerd namespace=moby topic=/tasks/delete type=\\\"*events.TaskDelete\",\n\t\"time\": \"2021-11-30T08:31:03.088224558Z\",\n\t\"level\": \"info\"\n}\n\nCRI-O logs:\n\n{\n\t\"id\": \"415aaaf7-8105-4b95-a435-b6fa6994c68d\",\n\t\"ts\": 1638229757.742216,\n\t\"msg\": \"Checking image status: registry.infra.mail.ru:5010/pause:3.0\",\n\t\"name\": \"/runtime.v1alpha2.ImageService/ImageStatus\",\n\t\"time\": \"2021-11-29 23:49:17.742215883Z\",\n\t\"level\": \"info\"\n}\n\nPod log, regardless of runtime:\n\n{\n\t\"ts\": 1638282885.830729,\n\t\"log\": \"[2021/11/30 14:34:45] [ info] [engine] started (pid=1)\\n\",\n\t\"time\": \"2021-11-30T14:34:45.830729304Z\",\n\t\"stream\": \"stderr\",\n\t\"kubernetes\": {\n\t\t\"host\": \"runtime-613-v19-default-group-0\",\n\t\t\"labels\": {\n\t\t\t\"app.kubernetes.io/name\": \"fluent-bit\",\n\t\t\t\"pod-template-generation\": \"10\",\n\t\t\t\"controller-revision-hash\": \"686bff7448\",\n\t\t\t\"app.kubernetes.io/instance\": \"fluent-bit\"\n\t\t},\n\t\t\"pod_id\": \"516d9ae1-671c-4c2e-bfbd-f469de3eceb6\",\n\t\t\"pod_name\": \"fluent-bit-tbnfb\",\n\t\t\"docker_id\": \"e0961103bfd584c5a7ba960bbd60aae3544d1cf0d9c1658b90eaae2fb7a83c50\",\n\t\t\"annotations\": {\n\t\t\t\"checksum/config\": \"6de425db2bca3061e94e50f648dfa35e9c9f77788f1755df389c303a7124359d\",\n\t\t\t\"checksum/luascripts\": \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\"\n\t\t},\n\t\t\"container_name\": \"fluent-bit\",\n\t\t\"namespace_name\": \"default\",\n\t\t\"container_image\": \"fluent/fluent-bit:1.8.10\"\n\t}\n}\n\nVarious parsers available out of the box can be found in GitHub repo.\n","url":"/en/cases/cases-logs/case-fluent-bit","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"Logging","link":"/en/cases/cases-logs","isActive":false,"evenSimpler":true}]},{"uuid":"4d4a4497-9386-4f73-90d5-b395a29fab66","title":"How to set up centralized logging from Kubernetes using Fluentd","data":"Why Kubernetes needs a special logging solution\n\nIt is inconvenient to write logs to containers because:\n\ndata generated in containerized applications exist as long as the container exists. When the docker container is restarted, data, including application logs, is deleted;\nlogs cannot be rotated in a container since rotation is an additional process in addition to the logged one, and more than one process cannot be started in one container.\n\n\n\nLog rotation is the process of processing, cleaning, archiving, and sending logs using utilities.\n\n\n\nTo access the logs of containerized applications in Kubernetes, Docker containers must transfer their logs to standard output streams (stdout) and errors (stderr). By default, Docker logging driver writes logs to a JSON file on the node, from where they can be obtained using the command:\n\nkubectl logs pod_name\n\n\n\nDocker logging driver is a log collection mechanism built into the Docker engine and supports many log rotation tools.\n\n\n\nWhen the Kubernetes orchestrator manages the lifecycle of Docker containers, pods with containers are often and unpredictably created, reloaded and deleted. If the settings of the Docker logging driver allow this, you can access the last logs of the pod before the reboot using the --previous argument:\n\nkubectl logs pod_name --previous\n\nBut it is impossible to get logs for more than one reboot back in this way. Deleting a pod means deleting all information about it, including logs.\n\nTherefore, to work with application logs in Kubernetes, a system is needed to collect, aggregate, save and extract useful information from logs. A bundle of Elasticsearch search engine, Fluentd logging agent, and Kibana — EFK-stack dashboard is suitable for this task.\n\nThe scheme of the logging system in the Kubernetes cluster:\n\nThe scheme of the logging system in the Kubernetes cluster{params[width=70%; height=70%]}\n\nDepending on the amount of information required in Elasticsearch processing, you can choose different ways to install the EFK stack:\n\nWith expected loads of tens of thousands of logs per second, installing Elasticsearch in a Kubernetes cluster is not recommended. For a highly loaded solution in production, it is better to allocate separate virtual machines for the Elasticsearch cluster itself and send logs from Kubernetes using log aggregators.\nIf the system does not produce tens of thousands of logs, and you need to monitor the logs on dev, test environments, the EFK stack can be installed in the Kubernetes cluster, closer to the applications.\n\nAs an agent of the log collector, we use fluentd, an application for collecting, filtering, and aggregating logs, written in C and Ruby languages and with various plugins to expand the application's basic functionality.\n\nWe organize a centralized logging system for Kubernetes.\n\nInstalling Elasticsearch in Kubernetes using helm\n\nCreate namespace kube-logging:\n\nkubectl create ns kube-logging\n\n\n\nWhen installing from helm, you will need to set the storage-class for the application.\n\n\n\nFind out the storage class available in the Kubernetes cluster:\n\nadmin@k8s:~$ kubectl get sc \nNAME            PROVISIONER            AGE\nhdd (default)   kubernetes.io/cinder   103d\nhdd-retain      kubernetes.io/cinder   103d\nssd             kubernetes.io/cinder   103d\nssd-retain      kubernetes.io/cinder   103d\n\nInstall Elasticsearch in the Kubernetes cluster with the specified variables:\n\n      helm install stable/elasticsearch \\\n         --name elastic \\\n         --set client.replicas=1 \\\n         --set master.replicas=1 \\\n         --set data.replicas=1 \\\n         --set master.persistence.storageClass=hdd \\\n         --set data.persistence.storageClass=hdd \\\n         --set master.podDisruptionBudget.minAvailable=1 \\\n         --set resources.requests.memory=4Gi \\\n         --set cluster.env.MINIMUM_MASTER_NODES=1 \\\n         --set cluster.env.RECOVER_AFTER_MASTER_NODES=1 \\\n         --set cluster.env.EXPECTED_MASTER_NODES=1 \\\n         --namespace kube-logging\n\n   As a result, an Elasticsearch cluster will be installed, consisting of one master node, one data storage node, and one client node.\nMake sure that all the pods are ready to work:\n\n      kubectl get po -n kube-logging\n\n      NAME                                           READY   STATUS    RESTARTS   AGE\n   elastic-elasticsearch-client-c74598797-9m7pm   1/1     Running  \n   elastic-elasticsearch-data-0                   1/1     Running  \n   elastic-elasticsearch-master-0                 1/1     Running\nFind out the name of the services in kube-logging:\n\n      kubectl get svc -n kube-logging\n\n      NAME                              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\n   elastic-elasticsearch-client      ClusterIP   10.233.8.213           9200/TCP   11m\n   elastic-elasticsearch-discovery   ClusterIP   None                   9300/TCP   11m\n\nThe elastic-elasticsearch-client service will be used for linking with kibana and fluentd. The Kibana dashboard will also be installed using helm, but we will write the name of the elastic-elasticsearch-client service in the variables of its helm chart.\n\nDownload kibana helm chart variables for editing:\n\nhelm fetch --untar stable/kibana\n\nGo to the kibana directory and edit the values file.yaml:\n\ncd kibana/ && vim values.yaml\n\nEnter the name of the elastic-elasticsearch-client service in the elasticsearch hosts section:\n\nfiles:\n  kibana.yml:\nDefault Kibana configuration from kibana-docker.\n    server.name: kibana\n    server.host: \"0\"\nFor kibana ` elements. The configmap below states that fluentd will collect information from application container logs, from kubernetes container logs and kubernetes node system logs.\n\nCreate a configmap for fluentd with the following content:\n\n**kind: ConfigMap\napiVersion: v1\ndata:\n  containers.input.conf: |-\n\n      @type tail\n      path /var/log/containers/*.log\n      pos_file /var/log/es-containers.log.pos\n      time_format %Y-%m-%dT%H:%M:%S.%NZ\n      tag kubernetes.*\n      read_from_head true\n      format multi_format\n\n        format json\n        time_key time\n        time_format %Y-%m-%dT%H:%M:%S.%NZ\n\n        format /^(?.+) (?stdout|stderr) * (?.*)$/\n        time_format %Y-%m-%dT%H:%M:%S.%N%:z\n\n  system.input.conf: |-\n\n      @type tail\n      format /^time=\"(?)\" level=(?) msg=\"(?)\"( err=\"(?)\")?( statusCode=($\\d+))?/\n      path /var/log/docker.log\n      pos_file /var/log/es-docker.log.pos\n      tag docker\n\n      @type tail\n      format none\n      path /var/log/etcd.log\n      pos_file /var/log/es-etcd.log.pos\n      tag etcd\n\n      @type tail\n      format multiline\n      multiline_flush_interval 5s\n      format_firstline /^\\w\\d{4}/\n      format1 /^(?\\w)(?\\d{4} )\\s+(?\\d+)\\s+(?]+)\\] (?.)/\n      time_format %m%d %H:%M:%S.%N\n      path /var/log/kubelet.log\n      pos_file /var/log/es-kubelet.log.pos\n      tag kubelet\n\n      @type tail\n      format multiline\n      multiline_flush_interval 5s\n      format_firstline /^\\w\\d{4}/\n      format1 /^(?\\w)(?\\d{4} )\\s+(?\\d+)\\s+(?]+)\\] (?.)/\n      time_format %m%d %H:%M:%S.%N\n      path /var/log/kube-proxy.log\n      pos_file /var/log/es-kube-proxy.log.pos\n      tag kube-proxy\n\n      @type tail\n      format multiline\n      multiline_flush_interval 5s\n      format_firstline /^\\w\\d{4}/\n      format1 /^(?\\w)(?\\d{4} )\\s+(?\\d+)\\s+(?]+)\\] (?.)/\n      time_format %m%d %H:%M:%S.%N\n      path /var/log/kube-apiserver.log\n      pos_file /var/log/es-kube-apiserver.log.pos\n      tag kube-apiserver\n\n      @type tail\n      format multiline\n","url":"/en/cases/cases-logs/case-fluentd","description":"","tags":"","breadcrumbs":[{"title":"Documentation","link":"/en","isActive":false,"evenSimpler":true},{"title":"How-to guides","link":"/en/cases","isActive":false,"evenSimpler":true},{"title":"Logging","link":"/en/cases/cases-logs","isActive":false,"evenSimpler":true}]}]