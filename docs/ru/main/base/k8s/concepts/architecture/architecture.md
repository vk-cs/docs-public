В основе платформы VK Cloud лежит [OpenStack](https://www.openstack.org/). Архитектура сервиса Kubernetes, опираясь на OpenStack, дает пользователям широкие возможности для работы, обеспечивает отказоустойчивость, масштабируемость и интеграцию с другими сервисами платформы.

## Топологии кластера

Кластер Kubernetes состоит из двух типов узлов (nodes) — master-узлов и worker-узлов:

- Master-узлы хранят информацию о состоянии всего кластера и управляют распределением рабочей нагрузки по worker-узлам.

- Worker-узлы выполняют рабочую нагрузку ([workload](https://kubernetes.io/docs/concepts/workloads/)). Они могут быть организованы в группы worker-узлов. Для повышения отказоустойчивости разместите группы в разных [зонах доступности](/ru/additionals/start/architecture#zony_dostupnosti_d9f6db93).

Отказоустойчивость кластера зависит от количества master-узлов. Возможные конфигурации:

- Кластер с одним master-узлом.

  Такой кластер не отказоустойчив: даже если worker-узлов несколько и они организованы в группы, при потере единственного master-узла кластер станет неработоспособен.

- Кластер из нечетного числа master-узлов (не менее трех).

  Такой кластер отказоустойчив: при потере нескольких master-узлов он сохранит свою работоспособность, пока есть хотя бы один работающий master-узел. Дальнейший уровень отказоустойчивости зависит от числа и конфигурации групп worker-узлов.

Вне зависимости от выбранной топологии кластера master-узлы используют распределенное хранилище «ключ-значение» [etcd](https://etcd.io/) для хранения информации о состоянии кластера:

- Кластер с одним master-узлом имеет один экземпляр `etcd`.
- В кластерах с несколькими master-узлами есть несколько экземпляров `etcd`, работающих в кластерном режиме для отказоустойчивости.
- Под каждый экземпляр `etcd` выделен отдельный высокопроизводительный SSD-диск (High-IOPS). Это позволяет организовать максимально быстрое взаимодействие с API-эндпоинтом кластера при минимальных задержках.

Для отказоустойчивости на уровне worker-узлов рекомендуется создать несколько групп worker-узлов в разных зонах доступности и размещать реплики приложения на этих узлах так, чтобы реплики тоже были в разных зонах доступности.

## Окружение кластера

На master- и worker-узлах используются следующие операционные системы:

- CentOS (до версии Kubernetes 1.20).
- AlmaLinux (начиная с версии Kubernetes 1.21). AlmaLinux является форком (fork) CentOS.

Кластер запускает контейнеры через Kubernetes [Container Runtime Interface](https://kubernetes.io/docs/concepts/architecture/cri/) (CRI) с помощью CRI-O (начиная с версии Kubernetes 1.20).

<info>

До версии Kubernetes 1.19 для запуска контейнеров использовался Docker\\[Dockershim](https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/), который признан устаревшим (deprecated).

</info>

Подробнее в разделе [Доступные версии Kubernetes и политика поддержки версий](../versions/).

## Интеграция с Kubernetes API

Все взаимодействие с кластером происходит через [Kubernetes API](https://kubernetes.io/ru/docs/concepts/overview/kubernetes-api/).

API-эндпоинт кластеров Kubernetes VK Cloud размещен за [отдельным балансировщиком нагрузки](../network/), поэтому доступ к API кластера можно получить по одному и тому же IP-адресу вне зависимости от количества master-узлов.

## Интеграция с платформой VK Cloud

Интеграция с платформой VK Cloud осуществляется через стандартные интерфейсы Kubernetes:

- [Container Storage Interface](https://kubernetes-csi.github.io/docs/) (CSI) — интеграция с сервисами хранения данных.

  Позволяет использовать в кластерах хранилище VK Cloud в виде постоянных томов ([persistent volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)).
  Доступно использование Persistent Volume Claim (PVC).

  Интеграция достигается с помощью OpenStack Cinder API. Подробнее в разделе [Хранилище в кластере](../storage/).

- [Container Network Interface](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) (CNI) — интеграция с сетевыми сервисами.

  В каждом кластере Kubernetes VK Cloud присутствует плагин [Calico](https://projectcalico.docs.tigera.io/about/about-calico), поддерживающий этот интерфейс. Этот плагин обеспечивает:

  - сетевую связность между контейнерами, [подами](../../k8s-reference/pods) и узлами кластера;
  - применение и соблюдение [сетевых политик](https://kubernetes.io/docs/concepts/services-networking/network-policies/) (Network Policies) Kubernetes.

  Calico интегрируется с платформой VK Cloud с помощью OpenStack Neutron API. Подробнее в разделе [Сеть в кластере](../network/).

## Встроенная поддержка Open Policy Agent

[Поды](https://kubernetes.io/docs/concepts/workloads/pods/) могут иметь доступ к различной информации и компонентам кластера Kubernetes. При определенных настройках подов сами поды и кластер Kubernetes, в котором они выполняются, могут быть уязвимы к атакам.

Для повышения устойчивости кластера к атакам, начиная с версии Kubernetes 1.21, в кластер встроен [Open Policy Agent Gatekeeper](../../k8s-reference/gatekeeper/). Он позволяет применять ограничения (constraints), которые помогают повысить безопасность эксплуатации развернутой рабочей нагрузки.

Эти ограничения создаются на основе шаблонов ограничений (constraint templates). Кластеры Kubernetes VK Cloud уже содержат [преднастроенные шаблоны и ограничения](../addons-and-settings/settings#prednastroennye_shablony_i_ogranicheniya_gatekeeper). Можно создавать собственные шаблоны и ограничения.

Для кластеров версий ниже 1.21 рекомендуется [установить Gatekeeper](../../install-tools/gatekeeper) вручную или обновить кластер до актуальной версии.

## Возможности масштабирования кластера

Кластер Kubernetes VK Cloud имеет встроенные возможности масштабирования. Можно выполнить масштабирование любого типа до пределов [действующих квот](../../../../base/account/concepts/quotasandlimits/).

Доступны:

- Ручное масштабирование:

  - Вертикальное, при котором изменяется количество вычислительных мощностей master-узлов кластера.
  
    Количество master-узлов не изменяется.

  - Горизонтальное, при котором изменяется количество worker-узлов в группе узлов кластера.

    Вычислительные мощности worker-узлов не изменяются.

- Автоматическое масштабирование (autoscaling), при котором количество worker-узлов в группе узлов изменяется в зависимости от нагрузки.

  Благодаря этому приложения моментально получают дополнительные мощности в момент повышения нагрузки и снижают их потребление при падении нагрузки. Автоматическое масштабирование позволяет экономить до 60% на вычислительных мощностях.

  Автоматическое масштабирование можно [настроить](../../operations/scale) в пределах от 1 узла до 100 узлов.
