Для кластеров Kubernetes VK Cloud доступны различные аддоны (дополнительные сервисы). Их можно выбрать в любой комбинации и установить либо при [создании кластера с помощью Terraform](../../../operations/create-cluster/create-terraform), либо [позднее](../../../operations/addons/manage-addons#ustanovka_addona) в уже существующий кластер. Процесс установки автоматизирован и требует минимального вмешательства пользователя.

## Особенности установки аддонов

- Аддоны устанавливаются на worker-узлы кластера и потребляют их вычислительные ресурсы.

  Ниже приведены системные требования аддонов, исходящие из стандартных значений [requests и limits](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits) для ресурсов Kubernetes в коде настройки аддона. При использовании нестандартных значений системные требования аддонов изменятся.

- Аддоны могут быть установлены на выделенную группу worker-узлов или на выбранные планировщиком Kubernetes worker-узлы. Использование первого подхода позволяет исключить влияние аддонов на работу production-сервисов, развернутых в кластере.

  Вычислительных ресурсов выделенной группы worker-узлов должно быть достаточно для всех аддонов, даже если каждый аддон потребляет максимум ресурсов, указанных в системных требованиях. Рекомендуется настроить автоматическое масштабирование для такой группы узлов.

- Для аддонов возможно выполнить:

  - **Стандартную установку** на выбранные планировщиком Kubernetes worker-узлы с изменением кода настройки аддона.
  - **Установку на выделенные worker-узлы** с изменением кода настройки аддона.
  - **Быструю установку** на выбранные планировщиком Kubernetes worker-узлы без изменения кода настройки аддона (с настройками по умолчанию). Не все аддоны поддерживают такой вариант установки.

  Процесс установки описан в разделе [Настройка и установка аддонов](../../../operations/addons/advanced-installation/).

## Доступные аддоны

<info>

Доступность конкретных аддонов зависит от [региона](/ru/base/account/concepts/regions), в котором планируется разместить кластер.

</info>

### Kube Prometheus Stack

<tabs>
<tablist>
<tab>Описание</tab>
<tab>Системные требования</tab>
</tablist>
<tabpanel>

Система мониторинга состояния кластера и развернутых в нем сервисов реализована на базе [Prometheus](https://prometheus.io/) и инструмента визуализации [Grafana](https://grafana.com/).

Подробнее в разделе [Мониторинг кластера](../../../monitoring#ispolzovanie_grafana).

</tabpanel>
<tabpanel>

- **CPU**: 850m–2500m.
- **RAM**: 968Mi–3804Mi.
- **HDD**: 2GB.
- **SSD**: 10GB.

</tabpanel>
</tabs>

### Docker Registry

<warn>

При установке аддона для него будет создан [стандартный балансировщик нагрузки](/ru/networks/vnet/concepts/load-balancer#tipy_balansirovshchikov_nagruzki).

Использование балансировщика [тарифицируется](/ru/networks/vnet/tariffs).

</warn>

<tabs>
<tablist>
<tab>Описание</tab>
<tab>Системные требования</tab>
</tablist>
<tabpanel>

[Реестр Docker](https://docs.docker.com/registry/) предназначен для размещения и хранения образов Docker. Работает в отказоустойчивой конфигурации (high availability, HA). Образы из реестра можно использовать при развертывании сервисов в кластере.

Подробнее в разделе [Подключение к реестру Docker](../../../connect/docker-registry/).

</tabpanel>
<tabpanel>

- **CPU**: 100m.
- **RAM**: 128Mi–512 Mi.
- **Объем объектного хранилища S3**: зависит от размера и количества образов, которые планируется размещать в реестре.
- **Стандартный балансировщик нагрузки**: одна штука.
- **Плавающий IP**: одна штука.

</tabpanel>
</tabs>

### Ingress Controller (NGINX)

<warn>

При установке аддона для него будет создан [стандартный балансировщик нагрузки](/ru/networks/vnet/concepts/load-balancer#tipy_balansirovshchikov_nagruzki).

Использование балансировщика [тарифицируется](/ru/networks/vnet/tariffs).

</warn>

<tabs>
<tablist>
<tab>Описание</tab>
<tab>Системные требования</tab>
</tablist>
<tabpanel>

[Ingress-контроллер](https://kubernetes.io/docs/concepts/services-networking/ingress/) на базе [NGINX](https://docs.nginx.com/nginx-ingress-controller/intro/overview/) работает в качестве обратного прокси (reverse proxy) и позволяет организовать единую точку входа для сервисов в кластере, которые работают по HTTP или HTTPS.

При наличии контроллера достаточно создать [ресурс Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/), чтобы такие сервисы стали доступны извне кластера Kubernetes.

Ingress-контроллер тесно интегрируется с платформой VK Cloud. Подробнее в разделе [Сеть в кластере](../../network/).

</tabpanel>
<tabpanel>

- **CPU**: 210m–610m.
- **RAM**: 238Mi–660Mi.
- **Стандартный балансировщик нагрузки**: одна штука.
- **Плавающий IP**: одна штука (при установке с [настройками по умолчанию](../../../operations/addons/advanced-installation/install-advanced-ingress#ustanovka_addona)).

</tabpanel>
</tabs>

### Istio

<tabs>
<tablist>
<tab>Описание</tab>
<tab>Системные требования</tab>
</tablist>
<tabpanel>

[Istio](https://istio.io/latest/) — это фреймворк, реализующий концепцию сервисной сетки ([service mesh](https://istio.io/latest/about/service-mesh/#what-is-a-service-mesh)), при которой для взаимодействия между сервисами приложения выделяется отдельный слой. Использование Istio обеспечивает для сервисов управление трафиком без изменения кода самих сервисов (используются sidecar-контейнеры). Преимущества Istio:

- Расширяются возможности для безопасной передачи трафика:

  - можно настраивать политики для трафика;
  - можно использовать TLS для коммуникации между сервисами;

- Расширяются возможности мониторинга трафика.
- Возможна сложная маршрутизация и балансировка трафика между сервисами.

</tabpanel>
<tabpanel>

- **CPU**: 500m.
- **RAM**: 2Gi.

</tabpanel>
</tabs>

### Kiali

<tabs>
<tablist>
<tab>Описание</tab>
<tab>Системные требования</tab>
</tablist>
<tabpanel>

[Kiali](https://kiali.io/) — веб-интерфейс для работы с [Istio](#istio_92986643). Он позволяет управлять сервисной сеткой (service mesh), отслеживать ее состояние и визуализировать ее.

</tabpanel>
<tabpanel>

- **CPU**: 10m—500m.
- **RAM**: 64Mi—1Gi.

</tabpanel>
</tabs>

### Jaeger

<tabs>
<tablist>
<tab>Описание</tab>
<tab>Системные требования</tab>
</tablist>
<tabpanel>

В распределенных системах, основанных на микросервисах, постоянно идет обмен запросами (requests). Платформа [Jaeger](https://www.jaegertracing.io) создана для распределенной трассировки запросов. Jaeger отслеживает путь потока запросов через микросервисы и позволяет:

- собирать информацию о взаимосвязях компонентов системы с точки зрения потока запросов;
- обнаруживать проблемы с запросами или узкие места в архитектуре системы, связанные с обработкой потока запросов.

Такой инструмент необходим, потому что факторы, связанные с запросами, могут значительно влиять на поведение и производительность этих систем в целом. Недостаточно обеспечить наблюдение только за отдельными микросервисами.

Jaeger выполняет трассировку запросов на основе данных, которые получает от микросервисов. Поэтому в микросервисы [необходимо интегрировать](https://www.jaegertracing.io/docs/latest/architecture/#tracing-sdks) инструментальный стек [OpenTelemetry](https://opentelemetry.io) для отправки данных о запросах. Познакомиться с интеграцией OpenTelemetry в микросервисное приложение можно на примере [Hot R.O.D](https://github.com/jaegertracing/jaeger/tree/main/examples/hotrod).

</tabpanel>
<tabpanel>

Требования аддона:

- Количество worker-узлов должно быть не менее выбранного количества реплик Elasticsearch.

  Elasticsearch используется в качестве бэкенда для хранилища. Каждая реплика Elasticsearch будет размещена на отдельном worker-узле для обеспечения отказоустойчивости.
  
  Подробнее про выбор количества реплик читайте в [разделе про установку Jaeger](../../../operations/addons/advanced-installation/install-advanced-jaeger).

- Worker-узлы должны использовать конфигурацию вычислительных ресурсов:

  - `Standard-4-4` или лучше (для тестового окружения);
  - `Standard-6-6` или лучше (для production-окружения).

Требования отдельных компонентов аддона:

- [Elasticsearch](https://www.jaegertracing.io/docs/latest/deployment/#elasticsearch):
  - **CPU**: 100m–1000m.
  - **RAM**: 512M.

- [Agent](https://www.jaegertracing.io/docs/latest/architecture/#agent):
  - **CPU**: 250m–500m.
  - **RAM**: 128M–512M.

- [Collector](https://www.jaegertracing.io/docs/latest/architecture/#collector):
  - **CPU**: 500m–1000m.
  - **RAM**: 512M–1024M.

- [Query](https://www.jaegertracing.io/docs/latest/architecture/#query):
  - **CPU**: 250m–500m.
  - **RAM**: 128M–512M.

<info>

Для обеспечения стабильной работы Jaeger рекомендуется установить его на выделенную группу worker-узлов, которая удовлетворяет перечисленным выше требованиям.

</info>

</tabpanel>
</tabs>
