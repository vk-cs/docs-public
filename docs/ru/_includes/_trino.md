{includetag(open)}

1. [Перейдите](https://msk.cloud.vk.com/app/) в личный кабинет VK Cloud.
1. Перейдите в раздел **Data Platform → Экземпляры сервисов**.
1. Нажмите на название нужного экземпляра.

{/includetag}
{includetag(connect)}

1. Выберите источник:

   - `PostgreSQL` — подключение к базе данных PostgreSQL.
   - `Greenplum` — подключение к базе данных Greenplum.
   - `Clickhouse` — подключение к базе данных ClickHouse.
   - `Apache Iceberg с S3 VK Cloud` — подключение к VK Object Storage в том же проекте.
   - `Apache Iceberg с внешним S3` — подключение к внешнему объектному хранилищу S3.
   - `Hive Metastore c хранением данных на HDFS` — подключение к серверу Hive Metastore с хранилищем HDFS.
   - `Hive Metastore c хранением данных на внешнем S3` — подключение к серверу Hive Metastore с хранилищем S3.
   - `Hive Metastore c хранением данных на VK Cloud S3` — подключение к серверу Hive Metastore с хранилищем [VK Object Storage](/ru/storage/s3/concepts/about).
1. Настройте параметры подключения к источнику:

   {tabs}
   
   {tab(PostgreSQL, Greenplum, Clickhouse)}

   - **Название**: имя подключения.
   - **db_name**: имя базы данных, к которой Trino будет подключаться.
   - **hostname**: адрес сервера для подключения. Пример: `postgresql-primary.postgresqlcluster-gttxnz96.svc:5432`.
   - **username**: имя учетной записи пользователя базы данных.
   - **password**: пароль учетной записи пользователя базы данных.
   - **tls_crt**: путь к файлу TLS-сертификата. Использование TLS-сертификата помогает защитить данные при передаче и повысить безопасность подключения.

   {/tab}

   {tab(S3 VK Cloud)}

   - **Название**: имя подключения.
   - **s3_bucket**: имя бакета VK Object Storage, к которому Trino будет подключаться.
   - **s3_folder**: имя директории в бакете VK Object Storage, к которой Trino будет подключаться.
   - **parquet.small-file-threshold**: пороговый размер файла. Файл меньшего размера может быть обработан особым образом (например, объединен с другими файлами) для улучшения производительности и уменьшения количества файлов в хранилище.

   {/tab}

   {tab(Внешний S3)}

   - **Название**: имя подключения.
   - **s3_access_key**: публичный ключ для доступа к хранилищу.
   - **s3_secret_key**: приватный ключ для доступа к хранилищу.
   - **s3_endpoint**: URL-адрес вашего хранилища S3.
   - **s3_bucket**: имя бакета S3, к которому Trino будет подключаться.
   - **s3_folder**: имя директории в бакете S3, к которой Trino будет подключаться.
   - **s3_region**: регион, в котором расположено ваше хранилище S3.
   - **parquet.small-file-threshold**: пороговый размер файла. Файл меньшего размера может быть обработан особым образом (например, объединен с другими файлами) для улучшения производительности и уменьшения количества файлов в хранилище.

   {/tab}

   {tab(Hive Metastore (HDFS))}

   - **Название**: имя подключения.
   - **hostname:port Thrift сервиса Hive Metastore**: пара ключ/значение в формате `<FQDN>:<ПОРТ>` или `<IP-АДРЕС>:<ПОРТ>`. Данные указываются для Thrift-сервиса Hive Metastore.
   - **Keytab файл в формате base64 принципала Cloud Trino сервиса**: содержание Keytab-файла, перекодированное в формат base64. 
   
      Для подключения к кластеру Hive Metastore с HDFS хранилищем используются учетные данные Kerberos. Обратитесь к администратору Hive Metastore, чтобы создать нового принципала (principal) клиента и получить для него Keytab-файл. Этот файл нужно перекодировать в формат base64.

      {cut(Как получить значение в формате base64)}

      {tabs}
      
      {tab(macOS)}

      Выполните команду:

      ```bash
      base64 -i <ПУТЬ_ДО_KEYTAB-ФАЙЛА>
      ```

      {/tab}

      {tab(Linux)}

      Выполните команду:

      ```bash
      base64 -w 0 <ПУТЬ_ДО_KEYTAB-ФАЙЛА>
      ```

      {/tab}

      {tab(Windows)}

      В PowerShell выполните команду:

      ```ps
      [Convert]::ToBase64String([IO.File]::ReadAllBytes("<ПУТЬ_ДО_KEYTAB-ФАЙЛА>"))
      ```

      {/tab}

      {/tabs}

      {/cut}

   - **Имя каталога в Hive Metastore**: запросите у администратора Hive Metastore. Для получения списка каталогов в Hive Metastore используется запрос:

      ```sql
      SHOW CATALOGS;
      ```

      Такое же имя каталога используется в Cloud Trino для запросов к подключению Hive Metastore.

   - **hostname:port KDC сервиса**: пара ключ/значение в формате `<FQDN>:<ПОРТ>` или `<IP-АДРЕС>:<ПОРТ>`. Данные указываются для сервиса KDC (Key Distribution Center).
   - **Имя principal сервиса Hive Metastore**: имя принципала, под которым работает Hive Metastore.
   - **Имя principal клиента**: имя принципала, которое использовалось для созданного Keytab-файла.
   - **Включить User Impersonation**: включите опцию, чтобы выполнять запросы к Hive Metastore от имени пользователя, инициировавшего запрос в Cloud Trino.
   - **Запись во внешние Hive таблицы**: включите опцию, чтобы Cloud Trino мог вести запись во внешние (external) таблицы, подключенные к Hive Metastore. При выключенной опции будет доступна только запись в управляемые (managed) таблицы.

   {/tab}

   {tab(Hive Metastore (S3))}

   - **Название**: имя подключения.
   - **Регион**: регион, в котором расположено хранилище S3.
   - **Access Key**: публичный ключ для доступа к хранилищу S3.
   - **Secret Key**: приватный ключ для доступа к хранилищу S3.
   - **S3 URL**: URL-адрес хранилища S3.
   - **hostname:port Thrift сервиса Hive Metastore**: пара ключ/значение в формате `<FQDN>:<ПОРТ>` или `<IP-АДРЕС>:<ПОРТ>`. Данные указываются для Thrift-сервиса Hive Metastore.
   - **Keytab файл в формате base64 принципала Cloud Trino сервиса**: содержание Keytab-файла, перекодированное в формат base64.
   
      Для подключения к кластеру Hive Metastore с S3 хранилищем используются не только учетные данные S3, но и данные Kerberos. Обратитесь к администратору Hive Metastore, чтобы создать нового принципала (principal) клиента и получить для него Keytab-файл. Этот файл нужно перекодировать в формат base64.

      {cut(Как получить значение в формате base64)}

      {tabs}
      
      {tab(macOS)}

      Выполните команду:

      ```bash
      base64 -i <ПУТЬ_ДО_KEYTAB-ФАЙЛА>
      ```

      {/tab}

      {tab(Linux)}

      Выполните команду:

      ```bash
      base64 -w 0 <ПУТЬ_ДО_KEYTAB-ФАЙЛА>
      ```

      {/tab}

      {tab(Windows)}

      В PowerShell выполните команду:

      ```ps
      [Convert]::ToBase64String([IO.File]::ReadAllBytes("<ПУТЬ_ДО_KEYTAB-ФАЙЛА>"))
      ```

      {/tab}

      {/tabs}

      {/cut}

   - **Bucket**: имя бакета в хранилище S3.
   - **Название каталога**: запросите у администратора Hive Metastore. Для получения списка каталогов в Hive Metastore используется запрос:

      ```sql
      SHOW CATALOGS;
      ```

      Такое же имя каталога используется в Cloud Trino для запросов к подключению Hive Metastore.

   - **hostname:port KDC сервиса**: пара ключ/значение в формате `<FQDN>:<ПОРТ>` или `<IP-АДРЕС>:<ПОРТ>`. Данные указываются для сервиса KDC (Key Distribution Center).
   - **Имя principal сервиса Hive Metastore**: имя принципала, под которым работает Hive Metastore.
   - **Имя principal клиента**: имя принципала, которое использовалось для созданного Keytab-файла.
   - **Использовать блум фильтр**: включите опцию, чтобы использовать фильтр Блума при чтении файлов Parquet.
   - **Игнорировать статистику**: включите опцию, чтобы игнорировать статистику в файлах Parquet при чтении данных.
   - **Процент валидируемых паркет файлов**: процент файлов Parquet, которые будут проверяться после записи.
   - **Размер записываемых страниц**: максимальный размер одной страницы Parquet.
   - **Количество записываемых страниц**: максимальное количество значений на одной странице Parquet.
   - **Размер записываемых групп строк**: максимальный размер блока при записи файла Parquet.
   - **Количество строк, обрабатываемых во время записи**: количество записей, которые будут собраны в пакет (batch) перед тем, как записать их в файл Parquet за одну операцию.
   - **Количество строк, обрабатываемых за один раз во время чтения**: максимальное количество строк, которые читаются за один раз из одного блока.
   - **Максимальный размер маленьких файлов**: значение, которое определяет, какой размер файла считается маленьким.
   - **Включить Java Vector API (SIMD)**: включите опцию, чтобы использовать векторизованное декодирование при чтении файлов Parquet.
   - **Разрешить выполнять процедуру register_table**: включите опцию, чтобы разрешить регистрировать существующие таблицы формата Apache Iceberg в Hive Metastore. При вызове функции в SQL-запросе требуется указать полный путь до файла метаданных.
   - **Разрешить выполнять процедуру add_files**: включите опцию, чтобы разрешить добавлять новые файлы данных (например, файлы Parquet) в уже существующую таблицу формата Apache Iceberg.
   - **Включить User Impersonation**: включите опцию, чтобы выполнять запросы к Hive Metastore от имени пользователя, инициировавшего запрос в Cloud Trino.

   {/tab}

   {tab(Hive Metastore (VK Object Storage))}

      - **Название**: имя подключения.
   - **hostname:port Thrift сервиса Hive Metastore**: пара ключ/значение в формате `<FQDN>:<ПОРТ>` или `<IP-АДРЕС>:<ПОРТ>`. Данные указываются для Thrift-сервиса Hive Metastore.
   - **Keytab файл в формате base64 принципала Cloud Trino сервиса**: содержание Keytab-файла, перекодированное в формат base64.
   
      Для подключения к кластеру Hive Metastore с VK Object Storage используются учетные данные Kerberos. Обратитесь к администратору Hive Metastore, чтобы создать нового принципала (principal) клиента и получить для него Keytab-файл. Этот файл нужно перекодировать в формат base64.

      {cut(Как получить значение в формате base64)}

      {tabs}
      
      {tab(macOS)}

      Выполните команду:

      ```bash
      base64 -i <ПУТЬ_ДО_KEYTAB-ФАЙЛА>
      ```

      {/tab}

      {tab(Linux)}

      Выполните команду:

      ```bash
      base64 -w 0 <ПУТЬ_ДО_KEYTAB-ФАЙЛА>
      ```

      {/tab}

      {tab(Windows)}

      В PowerShell выполните команду:

      ```ps
      [Convert]::ToBase64String([IO.File]::ReadAllBytes("<ПУТЬ_ДО_KEYTAB-ФАЙЛА>"))
      ```

      {/tab}

      {/tabs}

      {/cut}

   - **Название каталога**: запросите у администратора Hive Metastore. Для получения списка каталогов в Hive Metastore используется запрос:

      ```sql
      SHOW CATALOGS;
      ```

      Такое же имя каталога используется в Cloud Trino для запросов к подключению Hive Metastore.

   - **hostname:port KDC сервиса**: пара ключ/значение в формате `<FQDN>:<ПОРТ>` или `<IP-АДРЕС>:<ПОРТ>`. Данные указываются для сервиса KDC (Key Distribution Center).
   - **Имя principal сервиса Hive Metastore**: имя принципала, под которым работает Hive Metastore.
   - **Имя principal клиента**: имя принципала, которое использовалось для созданного Keytab-файла.
   - **Использовать блум фильтр**: включите опцию, чтобы использовать фильтр Блума при чтении файлов Parquet.
   - **Игнорировать статистику**: включите опцию, чтобы игнорировать статистику в файлах Parquet при чтении данных.
   - **Процент валидируемых паркет файлов**: процент файлов Parquet, которые будут проверяться после записи.
   - **Размер записываемых страниц**: максимальный размер одной страницы Parquet.
   - **Количество записываемых страниц**: максимальное количество значений на одной странице.
   - **Размер записываемых групп строк**: максимальный размер блока при записи файла Parquet.
   - **Количество строк, обрабатываемых во время записи**: количество записей, которые будут собраны в пакет (batch) перед тем, как записать их в файл Parquet за одну операцию.
   - **Количество строк, обрабатываемых за один раз во время чтения**: максимальное количество строк, которые читаются за один раз из одного блока.
   - **Максимальный размер маленьких файлов**: значение, которое определяет, какой размер файла считается маленьким.
   - **Включить Java Vector API (SIMD)**: включите опцию, чтобы использовать векторизованное декодирование при чтении файлов Parquet.
   - **Bucket**: имя бакета VK Object Storage.
   - **Путь к файлам в bucket**: директория в бакете VK Object Storage.
   - **Разрешить выполнять процедуру register_table**: включите опцию, чтобы разрешить регистрировать существующие таблицы формата Apache Iceberg в Hive Metastore. При вызове функции в SQL-запросе требуется указать полный путь до файла метаданных.
   - **Разрешить выполнять процедуру add_files**: включите опцию, чтобы разрешить добавлять новые файлы данных (например, файлы Parquet) в уже существующую таблицу формата Apache Iceberg.
   - **Включить User Impersonation**: включите опцию, чтобы выполнять запросы к Hive Metastore от имени пользователя, инициировавшего запрос в Cloud Trino.

   {/tab}

   {/tabs}

{/includetag}
{includetag(password)}

Требования к паролю:

- может содержать только заглавные и строчные латинские буквы, цифры и символы `!`, `?`, `%`, `#`, `/`, `(`, `)`, `+`, `-`,`*`, `$`;
- должен состоять как минимум из 16 символов;
- должен содержать хотя бы одну заглавную и одну строчную букву латинского алфавита, хотя бы одну цифру и один символ.

{/includetag}