{includetag(intro)}

Apache Spark — сервис для распределенной обработки больших данных. Он состоит из API-интерфейсов на Java, Scala, Python и R, а также инструментов обработки [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) для SQL, [Pandas API](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html), [MLlib](https://spark.apache.org/docs/latest/ml-guide.html) для машинного обучения, [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html) для обработки графиков и [Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) для потоковой обработки. Чаще всего Spark используется в составе Hadoop-кластера.

Cloud Spark — решение на базе [Apache Spark Operator](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator) и [PaaS Kubernetes](/ru/kubernetes/k8s) от VK Cloud. Оно позволяет развернуть Spark внутри Kubernetes с помощью образа из Docker Registry, без использования Hadoop-кластера.

## Для каких задач подходит сервис

- Распределенная обработка больших данных.
- Чтение данных из объектного хранилища с их дальнейшим экспортом в БД для обработки (ClickHouse, Greenplum, PostgreSQL). Также возможна передача данных из БД в объектное хранилище.
- Распределенное обучение ML-моделей с использованием больших данных.
- Графовые вычисления с применением компонента [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html).

{/includetag}

{includetag(open)}

1. [Перейдите](https://msk.cloud.vk.com/app/) в личный кабинет VK Cloud.
1. Перейдите в раздел **Data Platform** → **Экземпляры сервисов**.
1. Нажмите на имя нужного экземпляра.

{/includetag}

{includetag(login_password)}

1. Укажите логин администратора для доступа к Spark. Требования к логину:

    - допустимы только цифры, латинские буквы и символ `_`;
    - первый символ — латинская буква любого регистра или `_`;
    - недопустимые имена: `os_admin`, `root`, `dataplatform_moth`.

1. Укажите пароль администратора для доступа к Spark. Чтобы задать пароль, нажмите кнопку **Сгенерировать** или укажите собственный.

    Требования к паролю:

    - не менее 16 символов;
    - минимум одна заглавная и одна строчная буквы латинского алфавита;
    - минимум одна цифра;
    - минимум один из символов: `!`, `?`, `%`, `#`, `/`, `(`, `)`, `-`, `+`, `*`.

    {note:warn}

    Сохраните пароль. Возможность восстановления пароля не предусмотрена.

    {/note}

{/includetag}

{includetag(connection)}

1. Нажмите кнопку **Добавить подключение**.
1. Выберите источник данных:

    - `S3 внешний` — подключение к внешнему объектному хранилищу S3.
    - `S3 VK Cloud` — подключение к хранилищу Cloud Storage в том же проекте.
    - `PostgreSQL` — подключение к базе данных PostgreSQL.

1. Настройте параметры подключения к источнику:

    <tabs>
    <tablist>
    <tab>S3 внешний</tab>
    <tab>S3 VK Cloud</tab>
    <tab>PostgreSQL</tab>
    </tablist>
    <tabpanel>

    - **Имя внешнего подключения S3**: задайте имя подключения. Имя должно начинаться со строчной латинской буквы и содержать только заглавные и строчные латинские буквы, цифры и знаки `_`.
    - **Регион**: регион, в котором расположено ваше хранилище S3.
    - **Access Key**: уникальный идентификатор, используемый для аутентификации при доступе к хранилищу.
    - **Secret Key**: приватный ключ для доступа к хранилищу.
    - **S3 URL**: URL-адрес вашего хранилища S3.
    - **Бакет**: имя бакета в хранилище S3, к которому будет подключаться экземпляр Spark.
    - **Путь в бакете S3**: путь к ранее созданной папке в бакете, которая будет доступна экземпляру Spark.

    </tabpanel>
    <tabpanel>

    - **Имя внутреннего подключения S3**: задайте имя подключения. Имя должно начинаться со строчной латинской буквы и содержать только заглавные и строчные латинские буквы, цифры и знаки `_`.
    - **Бакет**: имя бакета Cloud Storage, к которому будет подключаться экземпляр Spark.
    - **Путь в бакете S3**: путь к ранее созданной папке в бакете, которая будет доступна экземпляру Spark.

    </tabpanel>
    <tabpanel>

    - **PostgreSQL**: задайте имя подключения. Имя должно начинаться со строчной латинской буквы и содержать только заглавные и строчные латинские буквы, цифры и знаки `_`.
    - **Имя базы данных**: имя базы данных, к которой будет подключаться экземпляр Spark.
    - **Имя хоста БД**: адрес сервера для подключения.
    - **Имя пользователя**: имя учетной записи пользователя базы данных.
    - **Пароль пользователя**: пароль учетной записи пользователя базы данных.

    </tabpanel>
    </tabs>

1. Нажмите кнопку **Сохранить изменения**.

{/includetag}

{includetag(maintenance)}

1. Выберите дни недели и время начала технических работ с учетом указанной в блоке временной зоны.

   Длительность проведения технических работ, в том числе резервного копирования — 4 часа. В это время сервис может быть недоступен.

{/includetag}
