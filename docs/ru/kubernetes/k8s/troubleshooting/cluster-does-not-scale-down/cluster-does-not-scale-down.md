В кластере есть недозагруженные узлы, но они не удаляются автоматически, несмотря на работу [агента автоматического масштабирования (Cluster Autoscaler)](/ru/kubernetes/k8s/concepts/cluster-autoscaler). По умолчанию агент автоматического масштабирования должен удалять узлы, на которых нет запущенных подов или все поды которых можно безопасно вытеснить. 

У такого поведения может быть несколько причин. Последовательно выполните шаги, переходя от одного решения к другому, пока проблема не будет устранена. 

{cut(На узле запущен под, который не может быть вытеснен из-за настроек объекта PodDisruptionBudget)}

1. [Подключитесь к кластеру](../../connect/kubectl) с помощью `kubectl`.
1. Проверьте, есть ли в [логах агента автоматического масштабирования](/ru/kubernetes/k8s/how-to-guides/autoscaler-logs) предупреждения, содержащие `pdb blocking`. Если есть, то сейчас выполняется операция или набор операций, которые требуют прерывания (disruption) подов, и эти прерывания превышают допустимое количество, указанное в Pod Disruption Budget (PDB). 

   {cut(Как работает PDB)}

   PDB устанавливается на уровне кластера и обеспечивает контроль над количеством одновременно разрешенных прерываний в наборе подов. Это позволяет гарантировать, что в кластере всегда будет поддерживаться минимальное количество работающих подов, необходимое для непрерывной работы сервиса. При создании PDB указываются параметры:

   - `minAvailable` — минимальное количество подов, которые должны оставаться доступными и готовыми (находиться в статусе `ready`) в любой момент времени.
   - `maxUnavailable` — максимальное количество подов, которые могут быть прерваны одновременно.

   Когда выполняется операция, которая может привести к прерыванию пода, включая автоматическое масштабирование в сторону увеличения или уменьшения ресурсов, Kubernetes проверяет состояние PDB. Если количество допустимых прерываний, указанное в параметре `maxUnavailable`, достигнуто или превышено, Kubernetes блокирует операцию до тех пор, пока не освободится достаточно ресурсов для новых прерываний.
   
   {/cut}

1. Разрешите прерывание большего количества подов, увеличив значение параметра `maxUnavailable`:
   
   1. Отобразите список всех развернутых объектов с типом `PodDisruptionBudget` в пространстве имен с помощью команды:

      ```console
      kubectl get pdb <ИМЯ_ПРОСТРАНСТВА_ИМЕН>
      ```
   1. Откройте файл манифеста PDB. 
   1. Увеличьте значение параметра `maxUnavailable` в блоке `spec`.
   1. Примените изменения в манифесте PDB:

      ```console
      kubectl apply -f <ИМЯ_МАНИФЕСТА_PDB>
      ```

Подробнее о прерываниях и PDB в [официальной документации Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/).

{/cut}

{cut(На узле работают системные поды без PDB)}

1. [Подключитесь к кластеру](../../connect/kubectl) с помощью `kubectl`.
1. Отобразите информацию о системных подах кластера в пространстве имен `kube-system` с помощью команды:

   ```console
   kubectl get pods -n kube-system -o wide
   ```
1. Отобразите список всех развернутых объектов с типом `PodDisruptionBudget` в пространстве имен `kube-system` с помощью команды:

   ```console
   kubectl get pdb kube-system
   ```
   Если PDB для этих подов не создан, отобразится сообщение: `No resources found in kube-system namespace`.

1. Обратитесь в [техническую поддержку](/ru/contacts) с указанием обнаруженной ошибки.   

{/cut}

{cut(На узле запущен под без контроллера рабочей нагрузки)}

1. [Подключитесь к кластеру](../../connect/kubectl) с помощью `kubectl`.
1. Выведите манифест пода, в котором наблюдается проблема, с помощью команды:

   ```console
   kubectl describe pod <ИМЯ_ПОДА> -n kube-system
   ```
1. В параметре `Owner` или `OwnerReferences` проверьте, указан ли у пода его [контроллер рабочей нагрузки](https://kubernetes.io/docs/concepts/workloads/controllers/) (workload controller). Если этих параметров в манифесте нет или в них ничего не указано, у пода нет контроллера рабочей нагрузки. 
   
   Такое возможно, например, если под создан вручную, а не как часть [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/), [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) и т.д. В таком случае под управляется вручную, из-за чего могут быть проблемы с доступностью, обновлением и автоматическим масштабированием приложений, а также может быть невозможно автоматическое восстановление приложения после сбоя, поскольку за это отвечают контроллеры рабочей нагрузки.

1. Пересоздайте под с указанием нужного типа контроллера рабочей нагрузки в параметре `kind` в манифесте пода.

{/cut}

{cut(Узел явно помечен аннотацией, которая запрещает его удаление при автоматическом масштабировании)}

1. [Подключитесь к кластеру](../../connect/kubectl) с помощью `kubectl`.
1. Проверьте, указана ли для узла, в котором наблюдается проблема, аннотация, запрещающая его удаление при автоматическом масштабировании. Для этого выполните команду:

   ```console
   kubectl describe node <ИМЯ_УЗЛА> | grep scale-down-disabled
   ```
   Команда отфильтрует информацию об узле по строчке `scale-down-disabled` и найдет аннотацию (если она есть):

   ```console
   cluster-autoscaler.kubernetes.io/scale-down-disabled: true
   ```

   Значение аннотации может быть `true`, если этот узел нельзя удалять при автоматическом масштабировании, даже если он пустой. Такие аннотации могут стоять у узлов, на которых есть поды, которые нельзя вытеснять: системные компоненты, критически важные сервисы, поды, которые должны работать только на определенном узле и т.д. В таком случае делать ничего не нужно.

   Если вы все же хотите разрешить удаление этого узла:

   1. Убедитесь, что его удаление не приведет к проблемам в работе кластера.
   2. Удалите аннотацию.  

{/cut}

{cut(У пустого узла еще не истекло установленное время простоя для удаления)}

1. [Подключитесь к кластеру](../../connect/kubectl) с помощью `kubectl`.
1. Проверьте, есть ли в [логах агента автоматического масштабирования](/ru/kubernetes/k8s/how-to-guides/autoscaler-logs) сообщения, содержащие строчку:

   ```console
   scale-down: node <ИМЯ_УЗЛА> is not marked as unneeded for deletion
   ``` 
   
   Если есть, то у этого узла еще не истекло установленное время простоя, после которого он будет удален. Если время простоя еще не истекло, делать ничего не нужно.

   В Cloud Containers время простоя составляет 5 минут. Если узел пуст в течение 5 минут (все поды с него вытеснены), и за это время ни один под не появляется, агент автоматического масштабирования инициирует удаление этого узла.

{/cut}

Если проблема сохраняется, обратитесь в [техническую поддержку](/ru/contacts).