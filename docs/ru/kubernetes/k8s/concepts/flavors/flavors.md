## Шаблоны конфигурации

При создании кластеров Kubernetes можно выбрать различные [шаблоны конфигураций](/ru/computing/iaas/concepts/about#flavors) виртуальных машин, которые определяют вычислительные ресурсы, доступные узлам кластера. Часть шаблонов доступна сразу, а часть — по запросу в [службу поддержки](/ru/contacts/).

Рекомендации по выбору CPU и RAM для кластеров:

- Для небольших тестовых кластеров: 1 vCPU, 2 или 4 ГБ RAM.

- Для большинства кластеров: от 2 до 6 vCPU, от 2 до 24 ГБ RAM.

- Для высокопроизводительных кластеров: от 8 до 16 vCPU, от 8 до 64 ГБ RAM.

По запросу в [службу технической поддержки](/ru/contacts) становятся доступны:

- Шаблоны с высокопроизводительными CPU: от 16 vCPU, от 64 ГБ RAM. Эти процессоры более производительные.

- Индивидуальные шаблоны конфигурации под особые потребности.

<info>

Стоимость пользовательских конфигураций и конфигураций с высокопроизводительными CPU рассчитывается индивидуально.

</info>

## Расчет подходящей конфигурации кластера

Чтобы кластер Kubernetes работал стабильно:

1. Оцените объем ресурсов, требуемый для планируемой рабочей нагрузки.
1. Создайте кластер со следующими параметрами:

   - **Количество vCPU:** на 5-10% больше, чем расчетное количество для планируемой нагрузки.
   - **Количество RAM:** на 20% больше, чем расчетный объем для планируемой нагрузки.
   - **Объём хранилища:** на 10% больше, чем планируемый объем данных.
   - **Тип хранилища:** `Ceph SSD` или `High-IOPS SSD`.
   - **Количество master-узлов:** нечетное число для отказоустойчивой конфигурации (не менее трех).
   - **Количество worker-узлов:** на 10% больше, чем расчетное количество для планируемой нагрузки.

## Расчет доступных вычислительных ресурсов CPU и RAM

На каждом узле в кластере Kubernetes работает агент kubelet. Kubelet, системные процессы и сама операционная система требуют определенного количества ресурсов, то есть не все ресурсы узла доступны для запуска контейнеров.

В большинстве случаев ресурсы CPU резервируются следующим образом:

- 6% от первого ядра.
- 1% от второго ядра (до 2 ядер).
- 0.5% от третьего и четвертого ядра (до 4 ядер).
- 0.25% от всех ядер выше четырех.

Резервирование памяти обычно устроено так:

- 25% из первых 4 ГБ памяти.
- 20% из следующих 4 ГБ (до 8 ГБ).
- 10% из следующих 8 ГБ (до 16 ГБ).
- 6% из следующих 112 ГБ (до 128 ГБ).
- 2% от любой памяти выше 128 ГБ.

В дополнение к зарезервированным ресурсам, на узлах создаются системные поды, которые работают постоянно и также потребляют память:

| Название пода | CPU Requests | CPU Limits | RAM Requests | RAM Limits |
| --- | --- | --- | --- | --- |
| calico-node | 100 m | 0 | 512 Mi | 0 |
| csi-cinder-nodeplugin | 75 m | 300 m | 96 Mi | 384 Mi |
| haproxy | 20 m | 800 m | 80 Mi | 500 Mi |

Порог вытеснения (eviction threshold) обычно составляет 100 МБ. Это количество свободной памяти узла, ниже которого kubelet начинает вытеснять поды из-за недостатка памяти.

Более крупные узлы могут использовать ресурсы более эффективно благодаря меньшему процентному резервированию в сравнении с узлами меньшего размера. Однако масштабирование кластера потребует времени на запуск дополнительных узлов. При добавлении новых узлов возрастет время, необходимое для развертывания и кеширования образов контейнеров, что критично для поддержания отказоустойчивости.

Количество запускаемых подов на узле влияет на производительность кластера. Максимально можно запустить 110 подов, но для лучшей производительности узла планируйте 30–40 подов на один узел.

При выборе размера узлов и их количества в кластере отдавайте предпочтение тому распределению, которое максимизирует использование ресурсов и сохраняет нужный уровень отказоустойчивости и масштабируемости.

<info>

Если изменить тип ВМ для группы узлов, резервируемые ресурсы не изменяются, так как они задаются при создании узла.

</info>

## Доступные вычислительные ресурсы на узле

Доступные ресурсы CPU на узле:

|Node CPU |	CPU Available |
| --- | --- |
| 1 | 0,94 |
| 2 | 1,93 |
| 4 |	3,92 |
| 6 | 5,915 |
| 8 |	7,91 |
| 12 | 11,9 |
| 16 | 15,89 |

Доступные ресурсы RAM на узле:

| Node RAM (ГБ) | Node RAM Capacity (Ki) |	RAM Allocatable (Ki)	| RAM Allocatable (Mi) |
| --- | --- | --- | --- |
| 2 | 2 005 132 | 1 641 612 | 1 603 |
| 4 | 4 000 132 | 2 849 156 | 2 782 |
| 6 | 6 056 116 | 4 905 140 | 4 790 |
| 8 |	8 122 536 | 6 132 699 | 5 989 |
| 12 |	12 238 576 |	10 248 739 | 10 009 |
| 16 | 16 365 320 | 13 536 622 |	13 219 |
| 18 | 18 406 708 |	15 452 181 | 15 090 |
| 24 | 24 599 856 | 21 267 842 | 20 769 |
| 32 | 32 856 944 | 29 021 613 | 28 341 |
| 36 | 36 935 672 | 32 848 683 | 32 079 |
| 48 | 49 321 092 | 44 479 128 | 43 437 |
| 64 | 65 836 152 |	59 987 556 | 58 582 |

## {heading(Шаблоны конфигурации с GPU)[id=gpu]}

[GPU-ускоренные серверы](/ru/computing/gpu/about) позволяют проводить сложные и ресурсозатратные вычисления в области машинного обучения и обработки больших данных. 

Чтобы использовать шаблоны конфигурации с GPU на узлах кластера, [оставьте заявку](https://cloud.vk.com/cloud-gpu/) на подключение сервиса Cloud GPU.

### {heading(Особенности создания кластера с GPU)[id=gpu-features]}

Вы можете использовать шаблон конфигурации с GPU для master- и worker-узлов кластера. Шаблон конфигурации с GPU должен находиться в той же [зоне доступности](/ru/intro/start/concepts/architecture#az), что и узел, который его использует.

Если вы создаете или редактируете кластер через Terraform или CLI, укажите для узла с GPU ту зону доступности, в которой расположен предоставленный вам GPU. Иначе кластер не будет создан или не будет работать правильно.

При работе через личный кабинет VK Cloud неподходящие зоны доступности будут автоматически скрыты:

- Для master-узла стандартного кластера в поле **Тип виртуальной машины Master** будут доступны только шаблоны с GPU, которые находятся в одной зоне доступности с этим узлом.
- Для master-узлов регионального кластера в поле **Тип виртуальной машины Master** будут доступны только шаблоны с GPU, которые находятся во всех зонах доступности.
- Для worker-узла в поле **Тип Node-узлов** будут доступны только шаблоны с GPU, которые находятся в одной зоне доступности с этим узлом.

### {heading(Технологии совместного использования GPU)[id=gpu-sharing]}

На практике процессы машинного обучения и обработки больших данных не используют всего ресурса GPU. Поэтому процессоры GPU либо работают только частично, либо выполняют обычные задачи в дополнение к сложным. Это делает затратным использование GPU.

*Технологии совместного использования GPU* позволяют использовать один графический процессор несколькими процессами или инстансами одновременно: разные приложения или разные data-специалисты могут совместно использовать один GPU в своей работе.

На платформе VK Cloud в кластерах Kubernetes доступны две технологии такого рода — MIG и MPS.

*MIG (Multi-Instance Graphics)* — это технология виртуализации GPU, которая позволяет разделить один графический процессор на несколько независимых GPU.  Каждая часть имеет собственные ресурсы (память, вычислительные блоки) и может работать изолированно от других.

Технология предназначена для повышения эффективности использования графических процессоров в виртуализированных средах (например, на серверах), где важно минимизировать затраты, обеспечивая изолированный доступ каждому пользователю или приложению.

Технология MIG доступна не для всех моделей GPU. В официальной документации NVIDIA подробнее описана [технология](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html) и [модели GPU](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus), которым она доступна.

*MPS (CUDA Multi-Process Service)* — это механизм, который позволяет разделить вычислительную мощность GPU между несколькими процессами. Каждый процесс получает выделенный набор ресурсов GPU, и они взаимодействуют друг с другом через межпроцессное взаимодействие (IPC).

Эта технология подходит для задач, где важна высокая параллельность и вычислительная мощность GPU, но приложениям не требуется непосредственного доступа к памяти или ресурсам друг друга.

Подробнее в [официальной документации NVIDIA](https://docs.nvidia.com/deploy/mps/index.html).