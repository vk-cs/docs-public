Масштабирование позволяет адаптировать кластер к меняющимся потребностям рабочих нагрузок ([workloads](https://kubernetes.io/docs/concepts/workloads/)).

## Типы масштабирования

Поддерживается два типа масштабирования:

- _Вертикальное масштабирование_: изменяются [шаблоны виртуальных машин](../flavors), которые используются master- и worker-узлами кластера. Таким образом можно управлять вычислительными ресурсами кластера, не влияя на количество узлов.

  Масштабирование worker-узлов выполняется в рамках [группы worker-узлов](../architecture#topologii_klastera).

- _Горизонтальное масштабирование_: изменяется количество worker-узлов в рамках отдельной группы узлов. Таким образом можно управлять вычислительными ресурсами кластера, не влияя на шаблоны виртуальных машин, которые используются узлами.

Масштабирование обоих типов можно выполнить вручную, также действует [автоматическое масштабирование](#autoscaling).

## {heading(Автоматическое масштабирование)[id=autoscaling]}

_Вертикальное автоматическое масштабирование_ master-узлов действует для всех кластеров, его нельзя отключить.

Агент автоматического масштабирования оценивает загрузку master-узла по CPU и RAM, отслеживая следующие пороговые значения:

- загрузка CPU превышает 80% на протяжении 60 секунд;
- загрузка CPU превышает 60% на протяжении 5 минут;
- загрузка RAM превышает 90% на протяжении 60 секунд.

Если хотя бы один из порогов превышен, то в сервис [Cloud Containers](/ru/kubernetes/k8s) будет отправлен запрос на изменение шаблона ВМ master-узла. При этом значения CPU и RAM будут увеличены вдвое. Например, шаблон `STD2-2-6` будет изменен на `STD2-4-12`.

Изменить шаблон ВМ master-узла на шаблон с меньшими CPU и RAM можно только [в ручном режиме](../../service-management/scale#scale_master_nodes).

_Горизонтальное автоматическое масштабирование_ для группы узлов (autoscaling) [настраивается](../../service-management/scale#autoscale_worker_nodes) пользователем. Количество worker-узлов в группе будет автоматически регулироваться в зависимости от потребностей рабочей нагрузки. Этот механизм позволяет экономить до 60% на вычислительных мощностях.

## Действующие ограничения

- Возможности вертикального масштабирования ограничены [действующими квотами](/ru/tools-for-using-services/account/concepts/quotasandlimits#kvoty) и доступными [шаблонами виртуальных машин](../flavors#shablony_konfiguracii).
- Возможности горизонтального масштабирования ограничены действующими квотами и пределом на количество worker-узлов в отдельной группе узлов: от 1 до 100 узлов.
- Невозможно выполнить горизонтальное масштабирование вручную, если настроено автоматическое масштабирование. Чтобы выполнить масштабирование вручную, [выключите автоматическое масштабирование](../../service-management/scale#masshtabirovanie_grupp_worker_uzlov_3e7a5fdf).

## Обеспечение доступности при вертикальном масштабировании

При вертикальном масштабировании любых узлов кластера происходит последовательный перезапуск виртуальных машин, которые используются узлами. Это нужно для применения нового шаблона виртуальной машины. Поэтому процесс масштабирования влияет на доступность как API кластера, так и размещенных в кластере рабочих нагрузок:

- Если кластер не является [отказоустойчивым](../architecture#topologii_klastera) и содержит один master-узел, то Kubernetes API будет недоступен до завершения масштабирования.
- Если в группе узлов содержится только один worker-узел, то размещенная на нем рабочая нагрузка будет недоступна до завершения масштабирования.
- Если в группе узлов содержится несколько worker-узлов, то размещенная на них рабочая нагрузка будет недоступна до завершения масштабирования, если репликация для нагрузки не настроена или настроена некорректно.

  Например, если разместить все реплики на одном worker-узле, то при его перезапуске рабочая нагрузка станет недоступна, даже если есть другие worker-узлы в группе узлов.

  Настройте репликацию так, чтобы часть реплик рабочей нагрузки была доступна при перезапуске worker-узлов.

- Если после масштабирования группы узлов будет недостаточно вычислительных ресурсов для рабочей нагрузки, то эта нагрузка может работать некорректно или стать недоступной.

  Следите, чтобы при масштабировании в сторону уменьшения вычислительных ресурсов хватало итогового объема ресурсов в группе узлов.
