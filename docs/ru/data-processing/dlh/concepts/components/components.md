## S3 VK Cloud Storage

*S3 VK Cloud Storage* — экономичное объектное хранилище неструктурированных данных. Является ключевым компонентом DLH, так как обеспечивает гибкость, масштабируемость и экономическую эффективность хранения данных.

Компонент выполняет следующие функции:

- Распределенное хранение объектов различных форматов.
- Управление метаданными хранилища на уровне сущностей:

    - `Projects` — проекты, к которым относятся файлы S3.
    - `Buckets` — подключенные к S3 локальные диски внутри проектов.
    - `Objects` — ключевые параметры обработки объектов (файлов) S3.

- Определение правил загрузки, хранения, обработки и скачивания файлов.
- Управление ролевой моделью доступа к данным с поддержкой Access Control List (ACL) – кто может получать доступ к файлу и какие именно операции разрешено выполнять.
- Поддержка стандартного S3 API Amazon для доступа к данным.
- Организация входящих и исходящих потоков передачи контента в хранилище.
- Multipart – организация отказоустойчивой загрузки больших файлов в несколько потоков, в том числе:

    - политика повторных (retry) попыток;
    - логика разделения файлов.

- Контроль успешности загрузки и отсутствия битых файлов в S3.
- Быстрый поиск объектов в S3.
- Предоставление данных S3 по запросам внешних сервисов.

## Cloud Iceberg Metastore

*Cloud Iceberg Metastore* — каталог данных DLH, определяющий представление структуры объектов и правила/интерфейсы доступа к данным S3-хранилища, для организации взаимодействия хранилища с внешними сервисами. Является предпочтительным вариантом для использования в DLH, так как за счет открытого формата таблиц обеспечивает высокопроизводительное чтение огромных объемов данных из хранилища через сложные аналитические SQL-запросы.

Компонент выполняет следующие функции:

- Каталогизирует объекты S3-хранилища.
- Содержит схему, структуру таблиц и расположение наборов данных в хранилище DLH.
- Формирует представление объектов DLH в виде наборов данных, включая информацию об их местонахождении и структуре по столбцам, именам, типам данных и пр.
- Определяет структуру, аналогичную реляционным базам данных, поверх файлового хранилища, для возможности совместного унифицированного использования объектов S3-хранилища несколькими приложениями.
- Обеспечивает согласованные результаты обработки данных между различными приложениями и упрощает управление данными.
- Содержит полную историю операций с таблицами, включая изменения схемы данных и самих объектов, а также функции сериализуемой изоляции, перемещения во времени на основе моментальных снимков и предсказуемой эволюции схемы.
- Позволяет внешним приложениям напрямую работать с таблицами в хранилище DLH, что снижает затраты на обработку за счет использования преимуществ архитектуры DLH и значительно повышает скорость вычислений, поскольку все приложения могут работать с наборами данных сразу на месте без их переноса между несколькими отдельными и закрытыми системами.
- Обеспечивает возможность изменения существующих данных, включая операции обновления и удаления 
- Гарантирует безопасность параллельной записи в один и тот же набор данных без риска потери данных
- Позволяет избежать деградации при выполнении SQL-запросов в условиях роста объема данных в S3-хранилище

## Cloud Trino

*Cloud Trino* – это массивно-параллельный аналитический SQL-движок обработки больших объемов данных из разных источников с поддержкой сценариев:

- Параллельного исполнения SQL-запросов на чтение данных из нескольких источников и хранилищ.
- Организации ETL/ELT-пайплайнов без использования специализированных сервисов в архитектуре DLH.

*Cloud Trino* позволяет переносить аналитическую нагрузку из дорогих в обслуживании и трудно масштабируемых корпоративных хранилищ (например, на базе Greenplum) в более дешевые озера данных и DLH.

Компонент выполняет следующие функции:

- Запрос данных из источников по PULL-модели (для простых сценариев).
- Выполнение через API Apache Iceberg операций чтения/записи данных в объектном S3-хранилище.
- Балансировка входящего потока SQL-запросов от источников/потребителей в DLH.
- Публикация информации об источниках и структуре данных DLH во внешнем каталоге метаданных для управления данными и отслеживания их происхождения.
- Предоставление данных по запросам внешних сервисов напрямую из S3-хранилища для формирования отчетов, BI-аналитики, машинного обучения.
- Взаимодействие со специализированными аналитическими СУБД (например, Clickhouse, PostgreSQL) для выполнения аналитики реального времени.
- Многофазная оптимизация плана запроса — формирование фиксированной последовательности действий при выполнении SQL-запроса:

    - применение фильтров;
    - выбор оптимального порядка JOIN;
    - замена в запросе таблиц на VIEW;
    - выполнение операций трансформации данных;

## Cloud Spark

*Cloud Spark* — framework для обработки данных в режиме реального времени с возможностью распределения задач между несколькими вычислительными узлами, объединенными в кластеры. Опционально включается в архитектуру DLH при необходимости реализации сложных сквозных многоэтапных ETL-пайплайнов обработки данных от источников через S3-хранилище до конечных потребителей.

*Cloud Spark* ускоряет обработку больших данных благодаря реализованным технологическим решениям:

- In-memory processing: хранение и обработка данных в оперативной памяти.
- Lazy Evaluation: стратегия обработки данных, при которой выполнение вычислений откладывается до момента, пока не понадобится их результат.
- Resilient Distributed Datasets (RDD): стратегия хранения данных как распределенного набора объектов, при которой исходные датасеты и результаты вычислений реплицируются на нескольких узлах кластера, что позволяет ускорить обработку запросов и повысить отказоустойчивость системы.
- Параллельная обработка и комбинирование операций: балансировка нагрузки по узлам кластера для возможности параллельного выполнения операций в режиме реального времени.

Компонент выполняет следующие функции:

- Предоставляет API для возможности выгрузки данных из источников – интеграционные сервисы могут быть написаны на языках Python, Java и R, что сильно снижает порог входа в DLH.
- Трансформация данных, полученных из источников, до их сохранения в основное S3-хранилище DLH.
- Предоставляет библиотеки для двустороннего обмена данными с основным S3-хранилищем DLH с поддержкой различных типов данных:

    - Spark SQL – выполнение типичных для реляционных СУБД операций со структурированными данными.
    - Spark Streaming – обработка входящего потока данных в режиме реального времени.
    - GraphX – выполнение вычислений над огромными массивами связанных объектов «Вершина - Ребро» для решения типовых задач графовой аналитики (например, subgraph, joinVerticles и aggregateMessages).
    - MLlib – выполнение классификации, регрессии, кластеризации данных для подготовки датасетов и алгоритмов обучения ML-моделей
    - Преобразует данные DLH к формату, требуемому для отчетов, аналитики BI, машинного обучения.
    - Взаимодействует с потребителями информации DLH напрямую или через специализированные СУБД для аналитики, например, Clickhouse или PostgreSQL.

## ClickHouse

*ClickHouse* – это колоночная реляционная СУБД с открытым исходным кодом для быстрой обработки аналитических SQL-запросов на структурированных больших данных в режиме реального времени. Опционально включается в архитектуру DLH при необходимости выполнения оперативной аналитики реального времени на входящем потоке событий без необходимости сохранения данных в S3-хранилище.

Компонент выполняет следующие функции:

- Долговременное хранение данных в блочно-колоночном формате для выполнения BI-аналитики без запросов в основное хранилище DLH.
- Обработка сложных SQL-запросов на чтение для BI-аналитики с ускоренным полным сканированием. 
- Поддержка OLTP и OLAP сценариев в рамках единой кластерной СУБД.
- Поддержка шардирования данных и горизонтального масштабирования до 100 серверов.
- Возможность прямого импорта данных и организации ETL/CDC из реляционных СУБД источников данных.
- Аналитика по горячим данным для выполнения оперативной аналитики на потоке входящих событий из источников.
- Наличие Clickhouse в архитектуре удорожает стоимость DLH, но в то же время позволяет запустить ряд бизнес-сценариев в реальном времени, без развертывания полного набора компонентов DLH, что сильно сокращает ресурсоемкость и T2M решения для крупных Enterprise инсталляций.

## Cloud Airflow

*Cloud Airflow* – это оркестратор потоков данных и инструмент управления запуском процессов ETL/ELT для высокопроизводительных, масштабируемых приложений реального времени. Опционально включается в архитектуру DLH при необходимости интеграции хранилища с источниками данных по PULL-модели.

Компонент выполняет следующие функции:

- Управление запуском ETL-процессов в DLH.
- Интеграция источников неструктурированных данных напрямую с S3-хранилищем в режиме файлового обмена.
- Создание графика выполнения периодических IN/OUT операций.
- Мониторинг выполнения ETL-операций и рассылка диагностических сообщений.
- Чтение данных из S3-хранилища через API компонента Apache Iceberg.
