Быстрый старт включает базовый сценарий развертывания сервиса Data LakeHouse (DLH) на основе трех компонентов:

* [Cloud Storage](/ru/storage/s3) — S3-хранилище, которое используется в качестве недорогого бесконечно расширяемого хранилища неструктурированных данных, получаемых из различных БД, например PostgreSQL.
* [Iceberg Metastore]() — используется для организации каталога метаданных на базе Apache Iceberg. Обеспечивает согласованность операций между различными компонентами сервиса DLH.
* [Cloud Trino](/ru/data-processing/trino) — в связке с Iceberg Metastore обеспечивает клиентам привычный SQL-интерфейс для выполнения следующих операций:

   * перенос данных (ETL);
   * манипуляции с захваченными данными (ELT);
   * комбинирование данных из разных источников (Data Mesh);
   * прочие операции с каталогом и данными.

Пройдя все шаги быстрого старта, вы:

1. Создадите экземпляры сервисов Iceberg Metastore и Cloud Trino.
1. Создадите инстанс БД PostgreSQL и настроите подключение к нему в Cloud Trino.

Бакет S3-объектного хранилища будет создан автоматически при настройке подключений для экземпляра сервиса Cloud Trino.

## {heading(1. Создайте экземпляр сервиса Iceberg Metastore)[id=create_iceberg_instance]}

1. Перейдите в раздел **Data Platform → Экземпляры сервисов**.
1. Нажмите кнопку **Создать экземпляр**.
1. На шаге **Конфигурация**:

   1. Выберите тип сервиса `Iceberg Metastore`.
   1. Выберите версию `16.2.0`.
   1. Выберите конфигурацию `Кластер`.

1. Нажмите кнопку **Следующий шаг**.
1. На шаге **Параметры**:

   1. Запишите имя базы данных, сгенерированное автоматически. Оно понадобится для подключения к Cloud Trino.
   1. Выберите пункт **Создать новый кластер**.
   1. Убедитесь, что опция **Назначить внешний IP** включена.
   1. Выберите одну из доступных сетей в списке или создайте новую, нажав **Создать новую сеть**. При создании новой подсети укажите [SDN](/ru/networks/vnet/concepts/sdn) и адрес подсети.
   1. Выберите [зону доступности](/ru/intro/start/concepts/architecture#az), например: `Москва (ME1)`.
   1. Выберите вариант настройки `Базовая`, шаблон кластера `Малый`.

1. Нажмите кнопку **Следующий шаг**.
1. На шаге **Обслуживание**:

   1. Выберите дни недели и время начала проведения технических работ. В это время на сервере могут выполняться технические работы, в том числе создание резервных копий — сервис может быть временно недоступен.

   <info>

   Длительность технических работ — 4 часа. Временная зона определяется автоматически по вашему текущему местоположению.

   </info>

   1. Для остальных полей оставьте значения по умолчанию.

1. Нажмите кнопку **Следующий шаг**.
1. На шаге **Учетные данные**:

   1. В поле **Логин** оставьте значение по умолчанию. Запишите логин администратора для доступа к Iceberg Metastore, он понадобится вам позже.
   1. В поле **Пароль** нажмите кнопку **Сгенерировать**, чтобы сгенерировать пароль администратора. Сохраните пароль, он понадобится вам позже.

      <info>

      Утерянный пароль невозможно восстановить, но можно [поменять]() на новый.

      </info>

1. Нажмите кнопку **Создать**.

Дождитесь окончания процесса создания экземпляра сервиса Iceberg Metastore.

## {heading(2. Создайте инстанс БД PostgreSQL)[id=create_postgresql_instance]}

БД PostgreSQL в базовой конфигурации сервиса DLH выступает в качестве внешнего источника данных. Чтобы развернуть инстанс PostgreSQL:

{include(/ru/_includes/_postgresql_create.md)}

## {heading(3. Создайте экземпляр сервиса Cloud Trino)[id=create_trino_instance]}

Cloud Trino используется в качестве горизонтально масштабируемого движка SQL-запросов к внешнему источнику (PostgreSQL).

Чтобы развернуть экземпляр сервиса Cloud Trino и настроить для него подключения к PostgreSQL:

1. Перейдите в раздел **Data Platform → Экземпляры сервисов**.
1. Нажмите кнопку **Создать экземпляр**.
1. На шаге **Конфигурация**:

   1. Выберите тип сервиса `Cloud Trino`.
   1. Выберите версию `0.468.0`.
   1. Выберите конфигурацию `Single`.

1. Нажмите кнопку **Следующий шаг**.
1. На шаге **Параметры**:

   1. В поле **Название экземпляра** задайте имя экземпляра или используйте значение по умолчанию.
   2. Убедитесь, что опция **Назначить внешний IP** включена.
   3. Выберите сеть, которую указывали при создании экземпляра сервиса Iceberg Metastore.
   4. Выберите вариант настройки `Базовая`, шаблон кластера `Малый`.
   5. Нажмите кнопку **Следующий шаг**.

1. На шаге **Обслуживание**:

   1. **Техническое окно**: оставьте значение по умолчанию. В это время на сервере могут выполняться технические работы, в том числе создание резервных копий — сервис может быть временно недоступен.

      <info>
      
      Длительность технических работ — 4 часа. Временная зона определяется автоматически по вашему текущему местоположению.
      
      </info>
   
   1. Для остальных полей оставьте значения по умолчанию.

1. Нажмите кнопку **Следующий шаг**.
1. На шаге **Подключения**:

   1. Добавьте к экземпляру сервиса Cloud Trino источник данных PostgreSQL:
  
      1. Нажмите кнопку **Добавить подключение**.
      1. Выберите тип подключение `PostgreSQL` и укажите:
         
         * **Название**: придумайте название подключения, например: `postgres_trino`.
         * **Название базы данных**: имя базы данных внутри инстанса, например: `ad_db`. Доступен на вкладке **Список баз данных** после создания инстанса PostgreSQL.
         * **hostname:port**: укажите адрес подключения к созданному инстанса PostgreSQL в формате: `<IP-АДРЕС>:<ПОРТ>`. Например `10.0.1.2:5432`. Адрес подключения к БД можно [посмотреть](/ru/dbs/dbaas/service-management/manage-instance/postgresql#poluchenie_informacii_ob_instanse_bd) в карточке инстанса.
      
      1. **Логин**: значение, указанное при создании инстанса БД PostgreSQL. В этом примере: `tsuser1`.
      1. **Пароль**: пароль учетной записи пользователя базы данных, указанный при создании инстанса БД PostgreSQL.

   1. Добавьте к экземпляру сервиса Cloud Trino источник данных Apache Iceberg с S3 VK Cloud:

      1. Нажмите кнопку **Добавить подключение**.
      1. Выберите подключение `Apache Iceberg с S3 VK Cloud`.
      1. Укажите параметры подключения:
         
         * **Название**: придумайте название подключения, например: `iceberg_s3`.
         * **hostname:port сервиса Iceberg Metastore**: значение в формате `<IP-АДРЕС>:<ПОРТ>`. Адрес сервиса Iceberg Metastore нужно скопировать на вкладке **Общая информация** в поле **Postgres primary connection string** в виде: `postgres://<user>:<password>@<IP-АДРЕС>:<ПОРТ>/<database_name>`.
         * **Логин сервиса Iceberg Metastore**: логин администратора, указанный при создании экземпляра сервиса Iceberg Metastore. В этом примере: `vkdata`.
         * **Пароль сервиса Iceberg Metastore**: пароль учетной записи пользователя базы данных, указанный при создании инстанса Iceberg Metastore.
         * **Bucket**: задайте имя бакета для создания S3-объектного хранилища, следуя [правилам](/ru/storage/s3/service-management/buckets/create-bucket#bucket_naming) именования бакетов.

1. Нажмите кнопку **Следующий шаг**.
1. На шаге **Учетные данные**:

   1. В поле **Логин** оставьте значение по умолчанию. Запишите логин администратора для доступа к Cloud Trino, он понадобится вам позже.
   1. В поле **Пароль** нажмите кнопку **Сгенерировать**, чтобы сгенерировать пароль администратора. Сохраните пароль, он понадобится вам позже.

      <info>

      Утерянный пароль невозможно восстановить, но можно [поменять](/ru/data-processing/trino/service-management/management#change_password) на новый.

      </info>

1. Нажмите кнопку **Создать**.

Дождитесь окончания процесса создания экземпляра сервиса Cloud Trino.

После развертывания сервиса Trino в конфигурацию будут автоматически добавлены два подключения к виртуальным БД для генерации наборов синтетических данных большого объема и тестирования: `otpcds` и `otpch`.

## Что дальше?

1. [Подключите](/ru/data-processing/dlh/how-to-guides/connect-adb) аналитическую БД на базе ClickHouse, чтобы выполнять быструю обработку части данных до их попадания в S3-хранилище.
1. [Подключите](/ru/data-processing/dlh/how-to-guides/connect-spark) сервис Spark для работы со сложными ETL и ELT задачами.
