Включение в [базовую структуру](../../quick-start) Data LakeHouse (DLH) сервиса [Cloud Spark]() позволяет выполнять сложную трансформацию данных перед попаданием их в S3-объектное хранилище, в отличии от [Cloud Trino](), который может реализовывать относительно простые сценарии.

Для хранения оперативной информации может дополнительно использоваться аналитическая БД, например [Clickhouse](), хотя такая архитектура усложняет и удорожает решение.

## Подготовительные шаги

[Разверните](../../quick-start) базовую архитектуру сервиса Data LakeHouse.

## 1. Разверните экземпляр сервиса Cloud Spark

1. [Перейдите](https://msk.cloud.vk.com/app/) в личный кабинет VK Cloud.
1. Выберите проект, где нужно создать экземпляр сервиса.
1. Перейдите в раздел **Data Platform → Экземпляры сервисов**.
1. Нажмите кнопку **Создать экземпляр**.
1. На шаге **Конфигурация**:

   1. Выберите тип сервиса `Spark`.
   1. Выберите версию `3.5.1`.

1. Нажмите кнопку **Следующий шаг**.
1. На шаге **Параметры**:

   1. Задайте параметры экземпляра:

      - **Название**: укажите имя для экземпляра сервиса.
      - **Описание**: при необходимости добавьте описание.
      - **Назначить внешний IP**: включите опцию, чтобы экземпляр ClickHouse был доступен через интернет.
      - **Кластер Kubernetes**: выберите из выпадающего списка нужный кластер или создайте новый, выбрав в списке `Создать новый кластер`. При создании нового кластера задайте параметры:

         - **Сеть**: выберите из списка подсеть, в которой будет доступен кластер, или создайте новую, выбрав в списке `Создать новую сеть`. При создании новой подсети укажите [SDN](/ru/networks/vnet/concepts/sdn) и адрес подсети.
         - **Зона доступности**: выберите из списка [зону доступности](/ru/intro/start/concepts/architecture#az) кластера.
   
   1. В поле **Spark Connect** задайте количество ядер процессора и объем оперативной памяти, которые необходимо выделить для обработки данных в режиме реального времени.
   1. В поле **Spark History Server** задайте количество ядер процессора и объем оперативной памяти, которые необходимо выделить для логирования запуска задач (jobs) сервиса Cloud Spark.

1. Нажмите кнопку **Следующий шаг**.
1. На шаге **Обслуживание**:

   1. Выберите дни недели и время начала проведения технических работ. В это время на сервере могут выполняться технические работы, в том числе создание резервных копий — сервис будет недоступен.

    <info>

    Длительность технических работ — 4 часа. Временная зона определяется автоматически по вашему текущему местоположению.

    </info>

1. Нажмите кнопку **Следующий шаг**.
1. На шаге **Настройки** нажмите кнопку **Редактировать** и выберите версию Cloud Spark из списка поддерживаемых — она будет использоваться для запуска задач (jobs).
1. Нажмите кнопку **Следующий шаг**.
1. На шаге **Подключения**:

    1. Добавьте подключние к PostgreSQL, созданной при [развертывании]() базовой конфигурации Data LakeHouse:
  
        1. Нажмите кнопку **Добавить подключение**.
        1. Выберите подключение **PostgreSQL** и укажите:
        
            * **PostgreSQL**: придумайте название подключения, например: `postgres_spark`.
            * **Имя базы данных**: имя базы данных источника PostgreSQL, к которой Cloud Spark будет подключаться для выполнения запросов и интеграции данных, например: `ad_db`. Доступен на вкладке **Список баз данных** после создания инстанса PostgreSQL.
            * **Имя хоста БД**: значение в формате `<IP-АДРЕС>:<ПОРТ>`. Данные для подключения нужно скопировать на странице экземпляра PostgreSQL во вкладке **Общая информация**, из значения поля **Postgres primary connection string**: `postgres://<user>:<password>@<IP-АДРЕС>:<ПОРТ>/<ИМЯ_БД>`. Например: `10.0.1.2:5432`.
            * **Имя пользователя**: логин для PostgreSQL, созданной при развертывании базовой конфигурации Data LakeHouse.
            * **Пароль пользователя**: пароль учетной записи пользователя базы данных, указанный при создании инстанса БД PostgreSQL.

   1. Добавьте подключение к S3-объектному хранилищу, созданный при [развертывании]() базовой конфигурации Data LakeHouse:

        1. Нажмите кнопку **Добавить подключение**.
        1. Выберите подключение `S3 VK Cloud`.
        1. Укажите параметры подключения:
            
            * **Имя внутреннего подключения S3**: придумайте название подключения, например: `iceberg_s3`.
            * **Бакет**: имя бакета, заданное при создании S3 VK Cloud.
            * **Путь в бакете S3**: адрес размещения файлов в S3-хранилище. Параметр доступен на странице экземпляра S3 VK Cloud.

1. Нажмите кнопку **Следующий шаг**.
1. На шаге **Учетные данные**:
   
    1. В поле **Логин** оставьте значение по умолчанию. Запишите логин администратора для доступа к ClickHouse, он понадобится вам позже.
    1. В поле **Пароль** нажмите кнопку **Сгенерировать**, чтобы сгенерировать пароль администратора. Сохраните пароль, он понадобится вам позже.

        <warn>

        Возможность восстановления пароля не предусмотрена.

        </warn>
   
1. Нажмите кнопку **Создать**.

Дождитесь окончания процесса создания экземпляра сервиса Cloud Spark.
